'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/db-learn/docs/mysql/access-method/','title':"Access Method",'content':""});index.add({'id':1,'href':'/db-learn/docs/mysql/data-directory/','title':"Data Directory",'content':""});index.add({'id':2,'href':'/db-learn/docs/mysql/12_b-tree/','title':"B+ 树索引",'content':"B+ 树索引\r#\r\r没有索引的查找\r#\r\r没有索引的时候是怎么查找记录的？比如：\nSELECT [列名列表] FROM 表名 WHERE 列名 = xxx; 在一个页中的查找\r#\r\r如果表中的记录比较少，所有的记录都可以被存放到一个页中，在查找记录的时候可以根据搜索条件的不同分为两种情况：\n  以主键为搜索条件 在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。\n  以其他列作为搜索条件 对非主键列，数据页中并没有对非主键列建立所谓的页目录，所以无法通过二分法快速定位相应的槽。这种情况下只能从最小记录开始 依次遍历单链表中的每条记录，然后对比每条记录是不是符合搜索条件。很显然，这种查找的效率是非常低的。\n  在很多页中查找\r#\r\r大部分情况下表中存放的记录都是非常多的，需要好多的数据页来存储这些记录。在很多页中查找记录的话可以分为两个步骤：\n 定位到记录所在的页。 从所在的页内中查找相应的记录。  由于并不能快速的定位到记录所在的页，所以只能从第一个页沿着双向链表一直往下找，每一个页中再使用上面一个页中的查找方法。非常 低效。\n索引\r#\r\r先建一个表：\nmysql\u0026gt; CREATE TABLE index_demo( -\u0026gt; c1 INT, -\u0026gt; c2 INT, -\u0026gt; c3 CHAR(1), -\u0026gt; PRIMARY KEY(c1) -\u0026gt; ) ROW_FORMAT = Compact; Query OK, 0 rows affected (0.03 sec) 简单的索引方案\r#\r\r根据某个搜索条件查找一些记录时为什么要遍历所有的数据页？\n因为各个页中的记录并没有规律，我们并不知道搜索条件匹配哪些页中的记录，所以不得不依次遍历所有的数据页。\n那么，如何快速定位记录所在的数据页？\n也可以建一个目录：\nB+ 树\r#\r\r聚簇索引\r#\r\r 使用记录主键值的大小进行记录和页的排序 B+ 树的叶子节点存储完整的用户记录（记录中存储了所有列的值，包括隐藏列）。  具有这两种特性的 B+ 树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这就是所谓的索引即数据，数 据即索引。\n二级索引\r#\r\r联合索引\r#\r\rB+ 树索引的注意事项\r#\r\r索引的使用\r#\r\r"});index.add({'id':3,'href':'/db-learn/docs/','title':"Docs",'content':""});index.add({'id':4,'href':'/db-learn/docs/redis/18_geohash/','title':"GeoHash",'content':"GeoHash\r#\r\rRedis 在 3.2 版本以后增加了地理位置 GEO 模块，可以实现类似外卖 APP 附近的餐馆的功能。\n地图元素的位置数据使用二维的经纬度表示，经度范围 (-180, 180]，纬度范围 (-90, 90]，纬度正负以赤道为界，北正南负，经度正负以 本初子午线 (英国格林尼治天文台) 为界，东正西负。\n当两个元素的距离不是很远时，可以直接使用勾股定理就能算得元素之间的距离。平时使用的附近的人的功能，元素距离都不是很大，勾股 定理算距离足矣。不过需要注意的是，经纬度坐标的密度不一样 (地球是一个椭圆)，勾股定律计算平方差时之后再求和时，需要按一定的系数比加权 求和。\n给定一个元素的坐标，然后计算这个坐标附近的其它元素，按照距离进行排序，如何做？\n\r如果现在元素的经纬度坐标使用关系数据库 (元素 id, 经度 x, 纬度 y) 存储，如何计算？\n不可能通过遍历来计算所有的元素和目标元素的距离然后再进行排序，这个计算量太大了。一般的方法都是通过矩形区域来限定元素的数量，然后对 区域内的元素进行全量距离计算再排序。这样可以明显减少计算量。如何划分矩形区域呢？可以指定一个半径 r，使用一条 SQL 就可以圈出来。当 用户对筛出来的结果不满意，那就扩大半径继续筛选。\nselect id from positions where x0-r \u0026lt; x \u0026lt; x0+r and y0-r \u0026lt; y \u0026lt; y0+r 可以在经纬度坐标加上双向复合索引 (x, y)，这样可以最大优化查询性能。\n但是数据库查询性能毕竟有限，如果查询请求非常多，在高并发场合，这并不是一个好的方案。\nGeoHash 算法\r#\r\r业界比较通用的地理位置距离排序算法是 GeoHash 算法。GeoHash 算法将二维的经纬度数据映射到一维的整数，这样所有的元素都将在挂载到一 条线上，距离靠近的二维坐标映射到一维后的点之间距离也会很接近。当我们想要计算附近的人时，首先将目标位置映射到这条线上，然后在这 个一维的线上获取附近的点就行了。\n映射算法\r#\r\r将整个地球看成一个二维平面，然后划分成了一系列正方形的方格，就好比围棋棋盘。所有的地图元素坐标都将放置于唯一的方格中。方格越小，坐标 越精确。然后对这些方格进行整数编码，越是靠近的方格编码越是接近。如何编码？一个最简单的方案就是切蛋糕法。设想一个正方形的蛋糕摆在 你面前，二刀下去均分分成四块小正方形，这四个小正方形可以分别标记为 00,01,10,11 四个二进制整数。然后对每一个小正方形继续用二刀法切割 一下，这时每个小小正方形就可以使用 4bit 的二进制整数予以表示。然后继续切下去，正方形就会越来越小，二进制整数也会越来越长，精确度就会 越来越高。\n真实算法中还会有很多其它刀法，最终编码出来的整数数字也都不一样。\n编码之后，每个地图元素的坐标都将变成一个整数，通过这个整数可以还原出元素的坐标，整数越长，还原出来的坐标值的损失程度就越小。 对于附近的人这个功能而言，损失的一点精确度可以忽略不计。\nGeoHash 算法会继续对这个整数做一次 base32 编码 (0-9,a-z 去掉 a,i,l,o 四个字母) 变成一个字符串。在 Redis 里面，经纬度使用 52 位 的整数进行编码，放进了 zset 里面，zset 的 value 是元素的 key，score 是 GeoHash 的 52 位整数值。zset 的 score 虽然是浮点数， 但是对于 52 位的整数值，它可以无损存储。\n在使用 Redis 进行 Geo 查询时，通过 zset 的 score 排序就可以得到坐标附近的其它元素 (实际情况要复杂一些，不过这样理解足够了)，通过 将 score 还原成坐标值就可以得到元素的原始坐标。\nGeo 指令\r#\r\rgeoadd 指令携带集合名称以及多个经纬度名称三元组，注意这里可以加入多个三元组\n127.0.0.1:6379\u0026gt; geoadd company 116.48105 39.996794 juejin (integer) 1 127.0.0.1:6379\u0026gt; geoadd company 116.514203 39.905409 ireader (integer) 1 127.0.0.1:6379\u0026gt; geoadd company 116.489033 40.007669 meituan (integer) 1 127.0.0.1:6379\u0026gt; geoadd company 116.562108 39.787602 jd 116.334255 40.027400 xiaomi (integer) 2 geo 存储结构上使用的是 zset，意味着可以使用 zset 相关的指令来操作 geo 数据，所以删除指令可以直接使用 zrem 指令。\ngeodist 指令可以用来计算两个元素之间的距离，携带集合名称、2 个名称和距离单位。\n127.0.0.1:6379\u0026gt; geodist company juejin ireader km \u0026#34;10.5501\u0026#34; 127.0.0.1:6379\u0026gt; geodist company juejin meituan km \u0026#34;1.3878\u0026#34; 127.0.0.1:6379\u0026gt; geodist company juejin jd km \u0026#34;24.2739\u0026#34; 127.0.0.1:6379\u0026gt; geodist company juejin xiaomi km \u0026#34;12.9606\u0026#34; 127.0.0.1:6379\u0026gt; geodist company juejin juejin km \u0026#34;0.0000\u0026#34; 掘金离美团最近，因为它们都在望京。距离单位可以是 m、km、ml、ft，分别代表米、千米、英里和尺。\ngeopos 指令可以获取集合中任意元素的经纬度坐标，可以一次获取多个。\n127.0.0.1:6379\u0026gt; geopos company juejin 1) 1) \u0026#34;116.48104995489120483\u0026#34; 2) \u0026#34;39.99679348858259686\u0026#34; 127.0.0.1:6379\u0026gt; geopos company ireader 1) 1) \u0026#34;116.5142020583152771\u0026#34; 2) \u0026#34;39.90540918662494363\u0026#34; 127.0.0.1:6379\u0026gt; geopos company juejin ireader 1) 1) \u0026#34;116.48104995489120483\u0026#34; 2) \u0026#34;39.99679348858259686\u0026#34; 2) 1) \u0026#34;116.5142020583152771\u0026#34; 2) \u0026#34;39.90540918662494363\u0026#34; 获取的经纬度坐标和 geoadd 进去的坐标有轻微的误差，原因是 geohash 对二维坐标进行的一维映射是有损的，通过映射再还原回来的值会出现较 小的差别。\ngeohash 可以获取元素的经纬度编码字符串，上面已经提到，它是 base32 编码。\n127.0.0.1:6379\u0026gt; geohash company ireader 1) \u0026#34;wx4g52e1ce0\u0026#34; 127.0.0.1:6379\u0026gt; geohash company juejin 1) \u0026#34;wx4gd94yjn0\u0026#34; 可以使用 geohash 来解编码。格式 http://geohash.org/wx4g52e1ce0。\ngeoradiusbymember 指令是最为关键的指令，它可以用来查询指定元素附近的其它元素。\n# 范围 20 公里以内最多 3 个元素按距离正排，它不会排除自身 127.0.0.1:6379\u0026gt; georadiusbymember company ireader 20 km count 3 asc 1) \u0026#34;ireader\u0026#34; 2) \u0026#34;juejin\u0026#34; 3) \u0026#34;meituan\u0026#34; # 范围 20 公里以内最多 3 个元素按距离倒排 127.0.0.1:6379\u0026gt; georadiusbymember company ireader 20 km count 3 desc 1) \u0026#34;jd\u0026#34; 2) \u0026#34;meituan\u0026#34; 3) \u0026#34;juejin\u0026#34; # 三个可选参数 withcoord withdist withhash 用来携带附加参数 # withdist 很有用，它可以用来显示距离 127.0.0.1:6379\u0026gt; georadiusbymember company ireader 20 km withcoord withdist withhash count 3 asc 1) 1) \u0026#34;ireader\u0026#34; 2) \u0026#34;0.0000\u0026#34; 3) (integer) 4069886008361398 4) 1) \u0026#34;116.5142020583152771\u0026#34; 2) \u0026#34;39.90540918662494363\u0026#34; 2) 1) \u0026#34;juejin\u0026#34; 2) \u0026#34;10.5501\u0026#34; 3) (integer) 4069887154388167 4) 1) \u0026#34;116.48104995489120483\u0026#34; 2) \u0026#34;39.99679348858259686\u0026#34; 3) 1) \u0026#34;meituan\u0026#34; 2) \u0026#34;11.5748\u0026#34; 3) (integer) 4069887179083478 4) 1) \u0026#34;116.48903220891952515\u0026#34; 2) \u0026#34;40.00766997707732031\u0026#34; georadius 可以用来查询指定坐标附近的其它元素。参数和 georadiusbymember 基本一致。\n127.0.0.1:6379\u0026gt; georadius company 116.514202 39.905409 20 km withdist count 3 asc 1) 1) \u0026#34;ireader\u0026#34; 2) \u0026#34;0.0000\u0026#34; 2) 1) \u0026#34;juejin\u0026#34; 2) \u0026#34;10.5501\u0026#34; 3) 1) \u0026#34;meituan\u0026#34; 2) \u0026#34;11.5748\u0026#34; 注意\r#\r\r地图应用中，车的数据、餐馆的数据可能会有百万千万条，如果使用 Redis 的 Geo 数据结构，它们将全部放在一个 zset 集合中。在 Redis 的集群 环境中，集合可能会从一个节点迁移到另一个节点，如果单个 key 的数据过大，会对集群的迁移工作造成较大的影响，在集群环境中单个 key 对应的 数据量不宜超过 1M，否则会导致集群迁移出现卡顿现象，影响线上服务的正常运行。\n建议 Geo 的数据使用单独的 Redis 实例部署，不使用集群环境。\n如果数据量过亿甚至更大，就需要对 Geo 数据进行拆分，按国家拆分、按省拆分，按市拆分，在人口特大城市甚至可以按区拆分。这样就可以显著降低 单个 zset 集合的大小。\n"});index.add({'id':5,'href':'/db-learn/docs/redis/15_hyperloglog/','title':"HyperLogLog",'content':"HyperLogLog\r#\r\r如果你负责开发维护一个大型的网站，有一天老板找产品经理要网站每个网页每天的 UV 数据，然后让你来开发这个统计模块，如何实现？\n如果统计 PV 那非常好办，给每个网页一个独立的 Redis 计数器就可以了，这个计数器的 key 后缀加上当天的日期。这样来一个请求，incrby 一 次，最终就可以统计出所有的 PV 数据。\n但是 UV 不一样，它要去重，同一个用户一天之内的多次访问请求只能计数一次。这就要求每一个网页请求都需要带上用户的 ID，无论是登陆用户还是 未登陆用户都需要一个唯一 ID 来标识。\n你也许已经想到了一个简单的方案，那就是为每一个页面一个独立的 set 集合来存储所有当天访问过此页面的用户 ID。当一个请求过来时，我们使 用 sadd 将用户 ID 塞进去就可以了。通过 scard 可以取出这个集合的大小，这个数字就是这个页面的 UV 数据。没错，这是一个非常简单的方案。\n但是，如果你的页面访问量非常大，比如一个爆款页面几千万的 UV，你需要一个很大的 set 集合来统计，这就非常浪费空间。如果这样的页面 很多，那所需要的存储空间是惊人的。\nRedis 提供了 HyperLogLog 数据结构就是用来解决这种统计问题的。HyperLogLog 提供不精确的去重计数方案，虽然不精确但是也不是非 常不精确，标准误差是 0.81%，这样的精确度已经可以满足上面的 UV 统计需求了。\nHyperLogLog 这个数据结构需要占据 12k 的存储空间，Redis 对 HyperLogLog 的存储进行了优化，在计数比较小时，它的存储空间采用稀 疏矩阵存储，空间占用很小，仅仅在计数慢慢变大，稀疏矩阵占用空间渐渐超过了阈值时才会一次性转变成稠密矩阵，才会占用 12k 的空间。\n使用\r#\r\rHyperLogLog 提供了两个指令 pfadd 和 pfcount，根据字面意义很好理解，一个是增加计数，一个是获取计数。\n127.0.0.1:6379\u0026gt; pfadd codehole user1 (integer) 1 127.0.0.1:6379\u0026gt; pfcount codehole (integer) 1 127.0.0.1:6379\u0026gt; pfadd codehole user2 (integer) 1 127.0.0.1:6379\u0026gt; pfcount codehole (integer) 2 127.0.0.1:6379\u0026gt; pfadd codehole user3 (integer) 1 127.0.0.1:6379\u0026gt; pfcount codehole (integer) 3 127.0.0.1:6379\u0026gt; pfadd codehole user4 (integer) 1 127.0.0.1:6379\u0026gt; pfcount codehole (integer) 4 127.0.0.1:6379\u0026gt; pfadd codehole user5 (integer) 1 127.0.0.1:6379\u0026gt; pfcount codehole (integer) 5 127.0.0.1:6379\u0026gt; pfadd codehole user6 (integer) 1 127.0.0.1:6379\u0026gt; pfcount codehole (integer) 6 127.0.0.1:6379\u0026gt; pfadd codehole user7 user8 user9 user10 (integer) 1 127.0.0.1:6379\u0026gt; pfcount codehole (integer) 10 但是在数据量比较大的时候，pfcount 的结果就会出现误差。\npfmerge\r#\r\rpfmerge 用于将多个 pf 计数值累加在一起形成一个新的 pf 值。\n比如在网站中我们有两个内容差不多的页面，运营说需要这两个页面的数据进行合并。其中页面的 UV 访问量也需要合并，那这个时候 pfmerge 就可 以派上用场了。\nHyperLogLog 原理\r#\r\rHyperLogLog 算法是一种非常巧妙的近似统计海量去重元素数量的算法。它内部维护了 16384 个桶（bucket）来记录各自桶的元素数量。当一 个元素到来时，它会散列到其中一个桶，以一定的概率影响这个桶的计数值。因为是概率算法，所以单个桶的计数值并不准确，但是将所有的桶计 数值进行调合均值累加起来，结果就会非常接近真实的计数值。\n"});index.add({'id':6,'href':'/db-learn/docs/redis/24_info/','title':"INFO 指令",'content':"INFO 指令\r#\r\rInfo 指令，可以清晰地知道 Redis 内部一系列运行参数。\nInfo 指令显示的信息非常繁多，分为 9 大块，每个块都有非常多的参数，这 9 个块分别是:\n Server 服务器运行的环境参数 Clients 客户端相关信息 Memory 服务器运行内存统计数据 Persistence 持久化信息 Stats 通用统计数据 Replication 主从复制相关信息 CPU CPU 使用情况 Cluster 集群信息 KeySpace 键值对统计数量信息  Info 可以一次性获取所有的信息，也可以按块取信息。\n# 获取所有信息 \u0026gt; info # 获取内存相关信息 \u0026gt; info memory # 获取复制相关信息 \u0026gt; info replication "});index.add({'id':7,'href':'/db-learn/docs/mysql/11_innodb-page-structure/','title':"InnoDB 数据页结构",'content':"InnoDB 数据页结构\r#\r\rInnoDB 管理存储空间的基本单位是页，一个页的大小一般是 16KB。InnoDB 设计了多种不同类型的页，比如存放表空间头部信息的页， 存放 Insert Buffer 信息的页等等。我们聚焦的是那些存放我们表中记录的那种类型的页，官方称这种存放记录的页为索引（INDEX）页。我们 暂叫做数据页吧。\n数据页结构\r#\r\r一个 InnoDB 数据页的存储空间大致被划分成了 7 个部分：\n   名称 中文名 占用空间 简单描述     File Header 文件头部 38 字节 页的一些通用信息   Page Header 页面头部 56 字节 数据页专有的一些信息   Infimum + Supremum 最小记录和最大记录 26 字节 两个虚拟的行记录   User Records 用户记录 不确定 实际存储的行记录内容   Free Space 空闲空间 不确定 页中尚未使用的空间   Page Directory 页面目录 不确定 页中的某些记录的相对位置   File Trailer 文件尾部 8 字节 校验页是否完整    记录在页中的存储\r#\r\r我们自己存储的记录会按照我们指定的行格式存储到 User Records 部分。但是在一开始生成页的时候，其实并没有 User Records 这个部分，每当插入 一条记录，都会从 Free Space 部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到 User Records 部分，当 Free Space 部分的 空间全部被 User Records 部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了。\n\r记录头信息\r#\r\r以 compact 格式为例： 插入四条记录：\nmysql\u0026gt; INSERT INTO page_demo VALUES(1, 100, \u0026#39;aaaa\u0026#39;), (2, 200, \u0026#39;bbbb\u0026#39;), (3, 300, \u0026#39;cccc\u0026#39;), (4, 400, \u0026#39;dddd\u0026#39;); Query OK, 4 rows affected (0.00 sec) Records: 4 Duplicates: 0 Warnings: 0 \rdelete_mask\r#\r\r这个属性标记着当前记录是否被删除，占用 1 个二进制位，为 1 的时候代表记录被删除掉了。\n被删除的记录还在页中么？\n是的，你以为它删除了，可它还在真实的磁盘上。这些被删除的记录之所以不立即从磁盘上移除，是因为移除它们之后把其他的记录在磁盘上重新排列需要性能消耗， 所以只是打一个删除标记而已，所有被删除掉的记录都会组成一个所谓的垃圾链表，在这个链表中的记录占用的空间称之为所谓的可重用空间，之后如果有 新记录插入到表中的话，可以把这些被删除的记录占用的存储空间覆盖掉。\nheap_no\r#\r\r表示当前记录在本页中的位置。注意上面插入 4 条记录的示意图，4 条记录的位置分别是 2、3、4、5。\n那么 0 和 1 去哪了？\n因为 InnoDB 给每个页都自动添加了两个记录。这两个记录称为伪记录或者虚拟记录。这两个分别是最小记录，最大记录。\n两条记录的构造十分简单，都是由 5 字节大小的记录头信息和 8 字节大小的一个固定的部分组成：\n\r它们并不存放在页的 User Records 部分，被单独放在一个称为 Infimum + Supremum 的部分。\nnext_record\r#\r\r表示从当前记录的\u0026quot;真实数据\u0026quot;到下一条记录的\u0026quot;真实数据\u0026quot;的地址偏移量。如，第一条记录的 next_record 值为 32，意味着从第一条记录的真实数据的 地址处向后找 32 个字节便是下一条记录的真实数据。这其实是个链表。\u0026ldquo;下一条记录\u0026rdquo; 指得并不是按照我们插入顺序的下一条记录，而是按照主键值由小到 大的顺序的下一条记录。下图箭头来替代一下 next_record 中的地址偏移量：\n\r记录按照主键从小到大的顺序形成了一个单链表。最大记录的 next_record 的值为 0。\n如果删掉第 2 条记录：\n\r 第 2 条记录的 delete_mask 值设置为 1。 第 2 条记录的 next_record 值变为了 0，意味着该记录没有下一条记录了。 第 1 条记录的 next_record 指向了第 3 条记录。 最大记录的 n_owned 值从 5 变成了 4。  对页中的记录做任何的增删改操作，InnoDB 始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。\nnext_record 指针为什么要指向记录头信息和真实数据之间的位置？\r#\r\r因为这个位置，向左读取就是记录头信息，向右读取就是真实数据。\n并且变长字段长度列表、NULL 值列表中的信息都是逆序存放，这样可以使记录中位置靠前的字段和它们对应的字段长度信息在内存中的距离更近，可能会提高 高速缓存的命中率。\nPage Directory（页目录）\r#\r\r如何根据主键值查询页中的记录？\nSELECT * FROM page_demo WHERE c1 = 3; 因为记录在页中按照主键值由小到大顺序串联成了一个单链表，那么就可以从 Infimum 记录（最小记录）开始遍历链表，当找到主键值大于你想要查找的主键值时， 就可以停止了。但是这种方法，如果记录多了，效率及会很差。\nInnoDB 的 Page Directory，是一个类似目录的设计：\n 将所有正常的记录（包括最大和最小记录，不包括标记为已删除的记录）划分为几个组。 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的 n_owned 属性表示该记录拥有多少条记录，也就是该组内共有几条记录。 将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页的尾部的地方，这个地方就是所谓的 Page Directory，也就是页目录。页面目 录中的这些地址偏移量被称为槽（Slot），所以这个页面目录就是由槽组成的。  比如 page_demo 表中共有 6 条记录，InnoDB 会把它们分成两组，第一组中只有一个最小记录，第二组中是剩余的 5 条记录：\n\r注意几点：\n 最小记录的 n_owned 值为 1，这个分组中只有 1 条记录，也就是最小记录本身。 最大记录的 n_owned 值为 5，这个分组中有 5 条记录，包括最大记录本身还有我们自己插入的 4 条记录。  最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。 所以分组是按照下边的步骤进行的：\n 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的 n_owned 值加 1，表示本组内又添加 了一条记录，直到该组中的记录数等于 8 个。 在一个组中的记录数等于 8 个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中 4 条记录，另一个 5 条记录。这个过程会在页目录中新增一个 槽来记录这个新增分组中最大的那条记录的偏移量。  如果现在再往 page_demo 表中添加 12 条记录：\n\r各个槽代表的记录的主键值都是从小到大排序的，所以我们可以使用所谓的二分法来进行快速查找。5 个槽的编号分别是：0、1、2、3、4，所以初始情况 下最低的槽就是 low=0，最高的槽就是 high=4。比如找到主键值为 6 的记录，过程是这样的：\n 计算中间槽的位置：(0+4)/2=2，所以查看槽 2 对应记录的主键值为 8，又因为 8 \u0026gt; 6，所以设置 high=2，low 保持不变。 重新计算中间槽的位置：(0+2)/2=1，所以查看槽 1 对应的主键值为 4，又因为 4 \u0026lt; 6，所以设置 low=1，high 保持不变。 high - low 的值为 1，所以确定主键值为 6 的记录在槽 2 对应的组中。槽 2 对应的记录是主键值为 8 的记录（改组的最大记录），但是槽 1 对 应的记录（主键值为 4），该条记录的下一条记录就是槽 2 中主键值最小的记录，该记录的主键值为 5。所以可以从这条主键值为 5 的记录出发，遍历槽 2 中的各 条记录。  Page Header（页面头部）\r#\r\rPage Header 占用固定的 56 个字节，专门存储各种状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等。\nFile Header（文件头部）\r#\r\r不同类型的页都会以 File Header 作为第一个组成部分，描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁等等。 File Header 占用固定的 38 个字节。\n   名称 占用空间大小 描述     FIL_PAGE_SPACE_OR_CHKSUM 4字节 页的校验和（checksum值）   FIL_PAGE_OFFSET 4字节 页号   FIL_PAGE_PREV 4字节 上一个页的页号   FIL_PAGE_NEXT 4字节 下一个页的页号   FIL_PAGE_LSN 8字节 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number）   FIL_PAGE_TYPE 2字节 该页的类型   FIL_PAGE_FILE_FLUSH_LSN 8字节 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值   FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4字节 页属于哪个表空间    几个重要的部分：\n FIL_PAGE_SPACE_OR_CHKSUM，当前页面的校验和。在比较两个很长的字节串之前先比较这两个长字节串的校验和，如果校验和都不一样两个长字节串肯 定是不同的，所以省去了直接比较两个比较长的字节串的时间损耗。 FIL_PAGE_OFFSET，每一个页都有一个唯一的页号。 FIL_PAGE_TYPE，页的类型。 FIL_PAGE_PREV 和 FIL_PAGE_NEXT，代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了，而无需这 些页在物理上真正连着。并不是所有类型的页都有上一个和下一个页的属性，但是**数据页*（也就是类型为 FIL_PAGE_INDEX 的页）是有这两个属性的，所 以所有的数据页其实是一个双向链表。  \rFile Trailer\r#\r\r用于检验页是否完整的部分，占用固定的 8 个字节。File Trailer 也是所有类型的页通用的。\n 前 4 个字节代表页的校验和，和 File Header 中的校验和相对应的。每当一个页面在内存中修改了，在同步之前就要把它的校验和算出来，因 为 File Header 在页面的前边，所以校验和会被首先同步到磁盘，当完全写完时，校验和也会被写到页的尾部，如果完全同步成功，则页的首部和尾部的校验和应 该是一致的。如果写了一半儿断电了，那么在 File Header 中的校验和就代表着已经修改过的页，而在 File Trailer 中的校验和代表着原先的页，二者不同 则意味着同步中间出了错。 后 4 个字节代表页面被最后修改时对应的日志序列位置，也是为了校验页的完整性的。  "});index.add({'id':8,'href':'/db-learn/docs/mysql/10_innodb-record-store-structure/','title':"InnoDB 记录存储结构",'content':"InnoDB 记录存储结构\r#\r\rMySQL 服务端负责对表中数据的读取和写入工作的部分是存储引擎，MySQL 服务端支持不同类型的存储引擎。真实数据在不同存储引擎中存放的格式一般是 不同的，甚至有 Memory 存储引擎都不用磁盘来存储数据。\nInnoDB 页简介\r#\r\rInnoDB 是一个将表中的数据存储到磁盘上的存储引擎。而读写磁盘的速度非常慢，所以 InnoDB 采取的方式是：**将数据划分为若干个页，以页作为磁盘和内存 之间交互的基本单位，\nInnoDB 中页的大小一般为 16 KB。也就是在一般情况下，一次最少从磁盘中读取 16KB 的内容到内存中，一次最少把内存中的 16KB 内容刷新到磁盘中。\nInnoDB 行格式\r#\r\r我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式被称为 行格式 或者 记录格式。InnoDB 存储引擎目前有 4 种不同类型的行格式， 分别是 Compact、Redundant、Dynamic 和 Compressed 行格式。\n指定行格式的语法\r#\r\r创建或修改表的语句中指定行格式：\nCREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 比如在 test 数据库里创建一个演示用的表 record_format_demo，可以这样指定它的行格式：\nmysql\u0026gt; USE test; Database changed mysql\u0026gt; CREATE TABLE record_format_demo ( -\u0026gt; c1 VARCHAR(10), -\u0026gt; c2 VARCHAR(10) NOT NULL, -\u0026gt; c3 CHAR(10), -\u0026gt; c4 VARCHAR(10) -\u0026gt; ) CHARSET=ascii ROW_FORMAT=COMPACT; Query OK, 0 rows affected (0.03 sec) 刚刚创建的这个表的行格式就是 Compact，另外显式指定了这个表的字符集为 ascii，因为 ascii 字符集只包括空格、标点符号、数字、大小写字 母和一些不可见字符，所以汉字是不能存到这个表里的。现在向这个表中插入两条记录：\nmysql\u0026gt; INSERT INTO record_format_demo(c1, c2, c3, c4) VALUES(\u0026#39;aaaa\u0026#39;, \u0026#39;bbb\u0026#39;, \u0026#39;cc\u0026#39;, \u0026#39;d\u0026#39;), (\u0026#39;eeee\u0026#39;, \u0026#39;fff\u0026#39;, NULL, NULL); Query OK, 2 rows affected (0.02 sec) Records: 2 Duplicates: 0 Warnings: 0 COMPACT行格式\r#\r\r\r一条完整的记录其实可以被分为记录的额外信息和记录的真实数据两大部分。\n记录的额外信息\r#\r\r为了描述这条记录而不得不额外添加的一些信息，分为 3 类，分别是变长字段长度列表，NULL值列表和记录头信息。\n变长字段长度列表\r#\r\rMySQL 支持一些变长的数据类型，比如 VARCHAR(M)、VARBINARY(M)、各种 TEXT 类型，各种 BLOB 类型，这些数据类型的列被称为变长字段。 变长字段中存储多少字节的数据是不固定的，所以我们在存储真实数据的时候需要顺便把这些数据占用的字节数也存起来，这样才不至于把 MySQL 服务器搞懵，所以这 些变长字段占用的存储空间分为两部分：\n 真正的数据内容 占用的字节数  在 Compact 行格式中，把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长字段长度列表，各变长字段数据占用的字 节数按照列的顺序逆序存放，注意是逆序存放。\n拿 record_format_demo 表中的第一条记录来举个例子。因为 record_format_demo 表的 c1、c2、c4 列都是 VARCHAR(10) 类型的，也 就是变长的数据类型，所以这三个列的值的长度都需要保存在记录开头处，因为 record_format_demo 表中的各个列都使用的是 ascii 字符集，所以每个 字符只需要1个字节来进行编码，来看一下第一条记录各变长字段内容的长度：\n   列名 存储内容 内容长度（十进制表示） 内容长度（十六进制表示）     c1 \u0026lsquo;aaaa\u0026rsquo; 4 0x04   c2 \u0026lsquo;bbb\u0026rsquo; 3 0x03   c4 \u0026rsquo;d\u0026rsquo; 1 0x01    这些长度值需要按照列的逆序存放，所以最后 变长字段长度列表 的字节串用十六进制表示的效果就是（各个字节之间实际上没有空格，用空格隔开只是方便理解）：\n01 03 04\r把这个字节串组成的变长字段长度列表填入上边的示意图中的效果就是： 由于第一行记录中 c1、c2、c4 列中的字符串都比较短，也就是说内容占用的字节数比较小，用 1 个字节就可以表示，但是如果变长 列的内容占用的字节数比较多，可能就需要用 2 个字节来表示。InnoDB 有它的一套规则，我们首先声明一下 W、M 和 L 的意思：\n 假设某个字符集中表示一个字符最多需要使用的字节数为 W，也就是使用 SHOW CHARSET 语句的结果中的 Maxlen 列，比如 utf8 字符集 中的 W 就是 3，gbk 字符集中的 W 就是 2，ascii 字符集中的 W 就是 1。 对于变长类型 VARCHAR(M) 来说，这种类型表示能存储最多 M 个字符（注意是字符不是字节），所以这个类型能表示的字符串最多 占用的字节数就是 M×W。 假设它实际存储的字符串占用的字节数是 L。  所以确定使用 1 个字节还是 2 个字节表示真正字符串占用的字节数的规则就是这样：\n 如果 M×W \u0026lt;= 255，那么使用 1 个字节来表示真正字符串占用的字节数。 如果 M×W \u0026gt; 255，则分为两种情况：  如果 L \u0026lt;= 127，则用 1 个字节来表示真正字符串占用的字节数。 如果 L \u0026gt; 127，则用 2 个字节来表示真正字符串占用的字节数。    总结一下就是说：如果该可变字段允许存储的最大字节数（M×W）超过 255 字节并且真实存储的字节数（L）超过 127 字节，则使用 2 个字节，否则 使用 1 个字节。\n注意，变长字段长度列表中只存储值为 非 NULL 的列内容占用的长度，值为 NULL 的列的长度是不储存的。也就是说对于第二条记录来说，因为 c4 列的值为 NULL，所以第二条记录的变长字段长度列表只需要存储 c1 和 c2 列的长度即可。并不是所有记录都有这个 变长字段长度列表 部分，比方 说表中所有的列都不是变长的数据类型的话，这一部分就不需要有。\nNULL 值列表\r#\r\r表中的某些列可能存储 NULL 值，如果把这些 NULL 值都放到记录的真实数据中存储会很占地方，所以 Compact 行格式把这些值为 NULL 的列统一 管理起来，存储到 NULL 值列表中：\n  首先统计表中允许存储 NULL 的列有哪些。 主键列、被 NOT NULL 修饰的列都是不可以存储 NULL 值的，所以在统计的时候不会把这些列算进去。比方说表 record_format_demo 的 3 个列 c1、c3、c4 都是允许存储 NULL 值的，而 c2 列是被 NOT NULL 修饰，不允许存储 NULL 值。\n  如果表中没有允许存储 NULL 的列，则 NULL 值列表也不存在了，否则将每个允许存储 NULL 的列对应一个二进制位，二进制位按照列的顺 序逆序排列，二进制位表示的意义如下：\n   二进制位的值为 1 时，代表该列的值为 NULL。 二进制位的值为 0 时，代表该列的值不为 NULL。  MySQL 规定 NULL 值列表必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补 0。 表 record_format_demo 只有 3 个值允许为 NULL 的列，对应 3 个二进制位，不足一个字节，所以在字节的高位补 0，效果就是这样：   这两条记录在填充了 NULL 值列表后的示意图就是这样： 记录头信息\r#\r\r记录头信息，它是由固定的 5 个字节组成。5 个字节也就是 40 个二进制位，不同的位代表不同的意思： 这些二进制位代表的详细信息如下表：\n   名称 大小（单位：bit） 描述     预留位1 1 没有使用   预留位2 1 没有使用   delete_mask 1 标记该记录是否被删除   min_rec_mask 1 B+ 树的每层非叶子节点中的最小记录都会添加该标记   n_owned 4 表示当前记录拥有的记录数   heap_no 13 表示当前记录在记录堆的位置信息   record_type 3 表示当前记录的类型，0 表示普通记录，1 表示 B+ 树非叶子节点记录，2 表示最小记录，3 表示最大记录   next_record 16 表示下一条记录的相对位置    我们现在直接看一下 record_format_demo 中的两条记录的头信息分别是什么： 记录的真实数据\r#\r\r对于 record_format_demo 表来说，记录的真实数据除了 c1、c2、c3、c4 这几个自己定义的列的数据以外，MySQL 会为每个记录默认的添 加一些列（也称为隐藏列），具体的列如下：\n   列名 是否必须 占用空间 描述     row_id 否 6 字节 行 ID，唯一标识一条记录   transaction_id 是 6 字节 事务 ID   roll_pointer 是 7 字节 回滚指针     实际上这几个列的真正名称其实是：DB_ROW_ID、DB_TRX_ID、DB_ROLL_PTR。\n InnoDB 表对主键的生成策略：优先使用用户自定义主键作为主键，如果用户没有定义主键，则选取一个Unique 键作为主键，如果表中连 Unique 键都没 有定义的话，则 InnoDB 会为表默认添加一个名为 row_id 的隐藏列作为主键。\nInnoDB 存储引擎会为每条记录都添加 transaction_id 和 roll_pointer 这两个列。\n看一下 record_format_demo 加上记录的真实数据的两个记录长什么样： 注意：\n 表 record_format_demo 使用的是 ascii 字符集，所以 0x61616161 就表示字符串 'aaaa'，0x626262 就表示字符串 'bbb'，以 此类推。 注意第 1 条记录中 c3 列的值，它是 CHAR(10) 类型的，它实际存储的字符串是：'cc'，而 ascii 字符集中的字节表示是 '0x6363'，虽然 表示这个字符串只占用了 2 个字节，但整个 c3 列仍然占用了 10 个字节的空间，除真实数据以外的 8 个字节的统统都用空格字符填充，空格字符 在 ascii 字符集的表示就是 0x20。  CHAR(M) 列的存储格式\r#\r\rrecord_format_demo 表的 c3 列的类型是 CHAR(10)，我们说在 Compact 行格式下只会把变长类型的列的长度逆序存到变长字段长度列表中， 但是这只是因为 record_format_demo 表采用的是 ascii 字符集，这个字符集是一个定长字符集，也就是说表示一个字符采用固定的一个字节，如果 采用变长的字符集（也就是表示一个字符需要的字节数不确定，比如 gbk 表示一个字符要 1~2 个字节、utf8 表示一个字符要 1~3 个字节等）的话，c3列的长度也会被存储到变长字段长 度列表中，比如修改一下 record_format_demo 表的字符集：\nmysql\u0026gt; ALTER TABLE record_format_demo MODIFY COLUMN c3 CHAR(10) CHARACTER SET utf8; Query OK, 2 rows affected (0.02 sec) Records: 2 Duplicates: 0 Warnings: 0 修改该列字符集后记录的变长字段长度列表也发生了变化，如图： 对于 CHAR(M) 类型的列来说，当列采用的是定长字符集时，该列占用的字节数不会被加到变长字段长度列表，而如果采用变长字符集时，该列占用的字节 数也会被加到变长字段长度列表。\n 变长字符集的 CHAR(M) 类型的列要求至少占用 M 个字节，而 VARCHAR(M) 却没有这个要求。比方说对于使用 utf8 字符集的 CHAR(10) 的列来说，该列存储的数据字节长度的范围是 10～30 个字节。即使我们向该列中存储一个空字符串也会占用 10 个字节，这是怕将来更新该列的值的字节长 度大于原有值的字节长度而小于 10 个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有的记录空间成为所谓的碎片。\n Redundant 行格式\r#\r\r\r把表 record_format_demo 的行格式修改为 Redundant：\nmysql\u0026gt; ALTER TABLE record_format_demo ROW_FORMAT=Redundant; Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 \rRedundant 行格式有什么不同的地方：\n字段长度偏移列表\r#\r\rRedundant 行格式的开头是字段长度偏移列表，与变长字段长度列表有两处不同：\n 没有了 变长 两个字，意味着 Redundant 行格式会把该条记录中所有列（包括隐藏列）的长度信息都按照逆序存储 到 字段长度偏移列表。 多了个 偏移 两个字，这意味着计算列值长度的方式不像 Compact 行格式那么直观，它是采用两个相邻数值的差值来计 算各个列值的长度。 比如第一条记录的 字段长度偏移列表 是 25 24 1A 17 13 0C 06 按照列的顺序排列就是 06 0C 13 17 1A 24 25。 按照两个相邻数值的差值来计算各个列值的长度的意思就是：  第一列(`row_id`)的长度是 `0x06` 个字节，6 个字节。\r第二列(`transaction_id`)的长度是 (`0x0C - 0x06`)个字节，6 个字节。\r第三列(`roll_pointer`)的长度是 (`0x13 - 0x0C`)个字节，7 个字节。\r第四列(`c1`)的长度是 (`0x17 - 0x13`)个字节，4 个字节。\r第五列(`c2`)的长度是 (`0x1A - 0x17`)个字节，3 个字节。\r第六列(`c3`)的长度是 (`0x24 - 0x1A`)个字节，10 个字节。\r第七列(`c4`)的长度是 (`0x25 - 0x24`)个字节，1 个字节。\r记录头信息\r#\r\rRedundant 行格式的记录头信息占用 6 字节，48 个二进制位：\n   名称 大小（单位：bit） 描述     预留位1 1 没有使用   预留位2 1 没有使用   delete_mask 1 标记该记录是否被删除   min_rec_mask 1 B+ 树的每层非叶子节点中的最小记录都会添加该标记   n_owned 4 表示当前记录拥有的记录数   heap_no 13 表示当前记录在记录堆的位置信息   n_field 10 表示记录中列的数量   1byte_offs_flag 1 标记字段长度偏移列表中每个列对应的偏移量是使用1字节还是2字节表示的   next_record 16 表示下一条记录的相对位置    第一条记录中的头信息是：00 00 10 0F 00 BC，对应的值如下：\n预留位1：0x00\r预留位2：0x00\rdelete_mask: 0x00\rmin_rec_mask: 0x00\rn_owned: 0x00\rheap_no: 0x02\rn_field: 0x07\r1byte_offs_flag: 0x01\rnext_record:0xBC\r与 Compact 行格式的记录头信息对比，多了 n_field 和 1byte_offs_flag 这两个属性，少了 record_type 这个属性。\n1byte_offs_flag 值是怎么选择的\r#\r\r字段长度偏移列表实质上是存储每个列中的值占用的空间在记录的真实数据处结束的位置，以 record_format_demo 第一条记录为例，0x06 代表第一 个列在记录的真实数据第 6 个字节处结束，0x13 代表第三个列在记录的真实数据第 19 个字节处结束，最后一个列的偏移量值为 0x25，就意味着最后一个 列在记录的真实数据第 37 个字节处结束，也就是说整条记录的真实数据实际上占用 37 个字节。\n每个列对应的偏移量可以占用 1 个字节或者 2 个字节来存储，那到底什么时候用 1 个字节，还是 2 个字节？\n根据该条 Redundant 行格式记录的真实数据占用的总大小来判断：\n 当记录的真实数据占用的字节数不大于 127（十六进制 0x7F，二进制 01111111）时，每个列对应的偏移量占用 1 个字节。 当记录的真实数据占用的字节数大于 127，但不大于 32767（十六进制 0x7FFF，二进制 0111111111111111）时，每个列对应的偏移量占用 2 个字节。 当记录的真实数据占用的字节数大于 32767 时，记录已经存放到了溢出页中，在本页中只保留前 768 个字节和 20 个字节的溢出页面地址（这 20 个字节中还 记录了一些别的信息）。因为字段长度偏移列表处只需要记录每个列在本页面中的偏移就好了，所以每个列使用 2 个字节来存储偏移量就够了。  1byte_offs_flag 的属性，为 1 时，表明使用 1 个字节存储。为 0 时，表明使用 2 个字节存储。\nRedundant 行格式中 NULL 值的处理\r#\r\rRedundant 行格式并没有 NULL 值列表，所以设计 Redundant 行格式在字段长度偏移列表中的各个列对应的偏移量处做了一些特殊处理 —— 将列对 应的偏移量值的第一个 bit 作为是否为 NULL 的依据，该 bit 也可以被称之为 NULL 比特位。NULL 比特位 如果为 1，那么该列的值 就是 NULL，否则不是 NULL。\n 如果存储 NULL 值的字段是定长类型的，比方说 CHAR(M) 数据类型的，则 NULL 值也将占用记录的真实数据部分，并把该字段对应的数据使用 0x00 字 节填充。 如果存储 NULL 值的字段是变长数据类型的，则不在记录的真实数据处占用任何存储空间。  一个字节能表示的范围是 0～255，为什么在记录的真实数据占用的存储空间大于 127 时就采用 2 个字节表示各个列的偏移量？\r#\r\r正是因为第一个 bit 作为是否为 NULL 的依据被占用了。\nCHAR(M) 列的存储格式\r#\r\rRedundant 行格式中十分干脆，不管该列使用的字符集是啥，只要是使用 CHAR(M) 类型，占用的真实数据空间就是该字符集表示一个字符最多需要的字节数 和 M 的乘积。所以 Redundant 行格式的 CHAR(M) 类型的列是不会产生碎片的。\n行溢出数据\r#\r\rVARCHAR(M) 最多能存储的数据\r#\r\rVARCHAR(M)类型的列最多可以占用 65535 个字节。但是当你创建表时使用 VARCHAR(65535)：\nmysql\u0026gt; CREATE TABLE varchar_size_demo( -\u0026gt; c VARCHAR(65535) -\u0026gt; ) CHARSET=ascii ROW_FORMAT=Compact; ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs mysql\u0026gt; 报错了。ySQL对一条记录占用的最大存储空间是有限制的，除了 BLOB 或者 TEXT 类型的列之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加 起来不能超过 65535 个字节。\n记录中的数据太多产生的溢出\r#\r\rMySQL 中磁盘和内存交互的基本单位是页，记录都会被分配到某个页中存储。而一个页的大小一般是 16KB，也就是 16384 字节，而一个 VARCHAR(M) 类型的列就最多可以存储 65532 个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。\n在 Compact 和 Reduntant 行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其 他的页中，然后记录的真实数据处用 20 个字节存储指向这些页的地址（这 20 个字节中还包括这些分散在其他页面中的数据的占用的字节数），从而可以找到剩 余数据所在的页。这个过程也叫做行溢出。存储超出字节的那些页面也被称为溢出页。\n\r行溢出的临界点\r#\r\r在列存储多少字节的数据时就会发生行溢出？\nMySQL 中规定一个页中至少存放两行记录。\n假设一个只有一个列的表，乱七八糟的额外信息加起来需要 136 个字节的空间，每个记录需要的额外信息是 27 字节。那么发生行溢出现象时需要满足这个式子：\n136 + 2×(27 + n) \u0026gt; 16384\r得出的解是：n \u0026gt; 8098。也就是说如果一个列中存储的数据大于 8098 个字节，就会发生行溢出。这个列就叫溢出列。 这个公式只是针对只有一个列的表。\n我们只要知道如果在一个行中存储了很大的数据时，可能发生行溢出的现象。\nDynamic 和 Compressed 行格式\r#\r\rMySQL 5.7，它的默认行格式就是 Dynamic。\n这两个格式和 Compact 行格式挺像，只不过在处理行溢出数据时不同，它们不会在记录的真实数据处存储字段真实数据的前 768 个字节，而是 把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址\nCompressed 行格式和 Dynamic 不同的一点是，Compressed 行格式会采用压缩算法对页面进行压缩，以节省空间。\n"});index.add({'id':9,'href':'/db-learn/docs/mysql/join/','title':"join 原理",'content':"join 原理\r#\r\r"});index.add({'id':10,'href':'/db-learn/docs/mongo/01_overview/','title':"MongoDB 介绍",'content':"MongoDB 有各种语言的 官方驱动。\nMongoDB 相比 RDBMS 的优势\r#\r\r 模式较少：MongoDB 是一种文档数据库，一个集合可以包含各种不同的文档。每个文档的字段数、内容以及文档大小都可以各不相同。 采用单个对象的模式，清晰简洁。 没有复杂的连接功能。 深度查询功能。MongoDB 支持对文档执行动态查询，使用的是一种不逊色于 SQL 语言的基于文档的查询语言。 具有调优功能。 易于扩展。MongoDB 非常易于扩展。 不需要从应用对象到数据库对象的转换/映射。 使用内部存储存储（窗口化）工作集，能够更快地访问数据。  为何选择使用 MongoDB\r#\r\r 面向文档的存储：以 JSON 格式的文档保存数据。 任何属性都可以建立索引。 复制以及高可扩展性。 自动分片。 丰富的查询功能。 快速的即时更新。  适用场景\r#\r\r无模式 (Flexible Schema)\r#\r\r面向文档数据库经常吹嘘的一个好处就是，它不需要一个固定的模式。这使得他们比传统的数据库表要灵活得多。无模式是酷，可是大多数情况下你的数据结构还 是应当好好设计的。\n写操作\r#\r\rMongoDB 可以胜任的一个特殊角色是在日志领域。有两点使得 MongoDB 的写操作非常快。\n 发送了写操作命令之后立刻返回，而无须等到操作完成。 可以控制数据持久性的写行为。  受限集合\r#\r\rMongoDB 还提供了 受限集合(capped collection)。可以通 过 db.createCollection 命令来创建一个受限集合并标记它的限制:\n//limit our capped collection to 1 megabyte db.createCollection(\u0026#39;logs\u0026#39;, {capped: true, size: 1048576}) 另外一种限制可以基于文档个数，而不是大小，用 max 标记。\nTTL索引\r#\r\r如果想让你的数据 \u0026ldquo;过期\u0026rdquo; ，基于时间而不是整个集合的大小，可以用 TTL 索引 ， 所谓 TTL 是 \u0026ldquo;time-to-live\u0026rdquo; 的缩写。\n持久性 (Durability)\r#\r\r从 2.0 版的 MongoDB 开始，日志是默认启动的，该功能允许快速恢复服务器，比如遭遇到了服务器崩溃或者停电的情况。\n全文检索 (Full Text Search)\r#\r\r全文检索是在最近加入到 MongoDB 中的。它支持十五国语言，支持词形变化(stemming)和干扰字(stop words)。除了原生的 MongoDB 的全文检索支持，如果你 需要一个更强大更全面的全文检索引擎的话，你需要另找方案。\n事务 (Transactions)\r#\r\rMongoDB 不支持事务。这有两个代替案，一个很好用但有限制，另外一个比较麻烦但灵活。\n原子更新操作\r#\r\rMongoDB 提供了针对单个文档的原子操作。比如 $inc 和 $set。还有像 findAndModify 命令，可以更新或删除文档之后，自动返回修改过的文档。\n维持原子性的建议方法是利用内嵌文档（embedded document）将所有经常更新的相关信息都保存在一个文档中。这能确保所有针对单一文档的更新具有 原子性。\n两段提交\r#\r\r两段提交实际上在关系型数据库世界中非常常用，用来实现多数据库之间的事务。 MongoDB 网站 有个例子 演示了最典型的场合 (资金转账)。通常的想法是，把事务的状态保存到实际的原子更新的文档中，然后手工的进行 init-pending-commit/rollback 处理。\nMongoDB 支持内嵌文档以及它灵活的 schema 设计，让两步提交没那么痛苦，但是它仍然不是一个好处理。\n数据处理(Data Processing)\r#\r\rMongoDB 依赖 MapReduce 来解决大部分数据处理工作。\n在 2.2 版本，它追加了一个强力的功能，叫做 aggregation framework or pipeline， 因此你只要对那些尚未支持管道的，需要使用复杂方法的，不常见的聚合使用 MapReduce。\n地理空间查询(Geospatial)\r#\r\rMongoDB 支持 geospatial 索引。这允许你保存 geoJSON 或者 x 和 y 坐 标到文档，并查询文档，用如 $near 来获取坐标集，或者 $within 来获取一个矩形或圆中的点。\n"});index.add({'id':11,'href':'/db-learn/docs/mysql/07_architecture/','title':"MySQL 基础架构",'content':"MySQL 基础架构\r#\r\r一条 SQL 查询语句在 MySQL 内的执行过程，是怎样的？\n上图是 MySQL 的基本架构示意图。可以看出，MySQL 大致可以分为两部分：Server 层和存储引擎层。\nServer 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所 有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎。不同的存储引擎共用一个 Server 层。存储引擎向 Server 层提供统一 的调用接口（存储引擎 API），包含了几十个底层函数，像\u0026quot;读取索引第一条内容\u0026rdquo;、\u0026ldquo;读取索引下一条内容\u0026rdquo;、\u0026ldquo;插入记录\u0026quot;等等。\n从 MySQL 5.5.5 版本开始， InnoDB 成为了默认存储引擎。\n接下来看一条 SQL 查询语句的执行过程，如 select * from T where ID=10;。\n连接管理\r#\r\r第一步就是与服务端建立连接。连接器负责跟客户端建立连接、获取权限、维持和管理连接。\n客户端可以采用 TCP/IP、命名管道或共享内存、Unix 域套接字这几种方式之一来与服务端建立连接。\n连接命令：\nmysql -h$ip -P$port -u$user -p  注意，使用 -p 后面尽量不要跟着密码。-p 和密码值之间不能有空白字符（其他参数名之间可以有空白字符）。 如果服务端和客户端安装在同一台机器上，-h 参数可以省略。\n 建立 TCP 连接后，连接器会进行身份认证，并获取权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。意味着，一个用户成功建立 连接后，即使用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。只有再新建的连接才会使用新的权限设置。\n每当有一个客户端进程连接到服务器进程时，服务器进程都会创建一个线程来专门处理与这个客户端的交互，当该客户端退出时会与服务器断开连接，服务器并不会 立即把与该客户端交互的线程销毁掉，而是把它缓存起来，在另一个新的客户端再进行连接时，把这个缓存的线程分配给该新客户端。这样就起到了不频繁创建和销 毁线程的效果。\nMySQL 服务器会为每一个连接进来的客户端分配一个线程，但是线程分配的太多了会严重影响系统性能，所以也需要限制一下可以同时连接到服务器的客户端数量。\n连接完成后，如果没有后续的动作，这个连接就处于空闲状态，可以使用 show processlist 命令查看。Command 列显示为 \u0026ldquo;Sleep\u0026rdquo; 的，就表示是有一 个空闲连接。\n客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。\n如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：\u0026ldquo;Lost connection to MySQL server during query\u0026rdquo;。这时候如果要继续，就 需要重连，然后再执行请求了。\n数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。\n建立连接的过程通常是比较复杂的，所以建议在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。\n但是全部使用长连接后，可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源 会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。\n怎么解决这个问题？可以考虑以下两种方案。\n 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不 需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。  查询缓存\r#\r\rMySQL 处理查询请求时，会把刚刚处理过的查询请求和结果以 key-value 的形式，缓存在内存中。如果下一次有一模一样的请求过来，优先从缓存中查找结果。 如果命中缓存，就不需要再执行后面的复杂操作，直接返回结果。\n查询缓存可以在不同客户端之间共享。\n查询缓存的弊端\r#\r\r如果两个查询请求在任何字符上的不同（例如：空格、注释、大小写），都会导致缓存不会命中。\n如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema、 performance_schema 数据库中的表，那这 个请求就不会被缓存。\n查询缓存的失效非常频繁。MySQL 的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了 INSERT、 UPDATE、DELETE、 TRUNCATE TABLE、ALTER TABLE、DROP TABLE 或 DROP DATABASE 语句，那这个表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查 询缓存的命中率会非常低。除非你的业务表，很长时间才会更新一次。\n按需使用查询缓存\r#\r\rMySQL 提供了 query_cache_type 参数，当设置成 DEMAND 时，默认 SQL 语句不会使用查询缓存。而对于确定要使用查询缓存的语句，可以 用 SQL_CACHE 显式指定：\nselect SQL_CACHE * from T where ID=10; MySQL 在 8.0 中删除了查询缓存的功能。\n语法解析\r#\r\r如果查询缓存没有命中，接下来就需要真正执行查询语句了。因为客户端程序发送过来的请求只是一段文本而已，MySQL 首先要对这段文本做分析。\n分析器先会做词法分析。MySQL 需要识别出文本里面的字符串分别是什么，代表什么。比如从 \u0026ldquo;select\u0026rdquo; 这个关键字，可以判断出是一个查询语句。 把字符串 \u0026ldquo;T\u0026rdquo; 识别成 \u0026ldquo;表名 T\u0026rdquo;，把字符串 \u0026ldquo;ID\u0026rdquo; 识别成 \u0026ldquo;列 ID\u0026rdquo;。\n接下来要做语法分析，根据词法分析的结果，分析器根据语法规则，判断这个 SQL 语句是否满足 MySQL 语法。如果语句不对，就会收 到 “You have an error in your SQL syntax” 的错误提醒。\n查询优化\r#\r\r语法解析之后，MySQL 知道了要查询的列是哪些，表是哪个，搜索条件是什么等等。在开始执行之前，还要先经过优化器的处理。因为我们写的 SQL 语句执行起来 效率可能并不是很高，优化器会对语句做一些优化，如外连接转换为内连接、表达式简化、子查询转为连接、选择索引等等。\n优化器会生成一个执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的。可以使用 EXPLAIN 语句来查看某个语句的执行计划。\n执行器\r#\r\rMySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，接下来就进入了执行器阶段，开始执行语句。\n开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误。\n打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n比如例子中的表 T，ID 字段没有索引，执行器的执行流程：\n 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将结果集返回给客户端。  至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。\n数据库的慢查询日志中有一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。\n在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。\n存储引擎\r#\r\r数据库表是由一行一行的记录组成的，但这只是一个逻辑上的概念，物理上如何表示记录，怎么从表中读取数据，怎么把数据写入具体的物理存储器上，这都是存 储引擎负责的事情。为了实现不同的功能，MySQL 提供了各式各样的存储引擎，不同存储引擎管理的表具体的存储结构可能不同，采用的存取算法也可能不同。\n常用存储引擎\r#\r\r   存储引擎 描述     InnoDB 具备外键支持功能的事务存储引擎   MEMORY 置于内存的表   MyISAM 主要的非事务处理存储引擎    最常用的就是 InnoDB。\n查看当前服务器程序支持的存储引擎：\nSHOW ENGINES; mysql\u0026gt; SHOW ENGINES; +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ 9 rows in set (0.00 sec) mysql\u0026gt;  Support 表示该存储引擎是否可用，如果值为 DEFAULT 则表示是默认的存储引擎。 Transactions 表示该存储引擎是否支持事务处理。 XA 表示着该存储引擎是否支持分布式事务。 Savepoints 表示着该列是否支持部分事务回滚。  "});index.add({'id':12,'href':'/db-learn/docs/mysql/04_advanced_query/','title':"MySQL 复杂查询语句",'content':"子查询\r#\r\rSQL 还允许创建子查询（subquery），即嵌套在其他查询中的查询。\n利用子查询过滤\r#\r\r订单存储在两个表中。对于包含订单号、客户 ID、订单日期的每个订单，orders 表存储一行。各订单的物品存储在相关的 orderitems 表中。 orders 表不存储客户信息。它只存储客户的 ID。实际的客户信息存储在 customers 表中。\n假如需要列出订购物品 TNT2 的所有客户，需要下面几步：\n 检索包含物品 TNT2 的所有订单的编号。 检索具有前一步骤列出的订单编号的所有客户的 ID。 检索前一步骤返回的所有客户ID的客户信息。  # 检索包含物品 TNT2 的所有订单的编号 mysql\u0026gt; select order_num from orderitems where pod_id= \u0026#39;TNT2\u0026#39;; +-------------+ | order_num | +-------------+ | 2005 | | 2007 | +-------------+ # 查询具有订单 20005 和 20007 的客户 ID mysql\u0026gt; select cust_id from orders where order_num in (2005,2007); +-------------+ | cust_id | +-------------+ | 1001 | | 1004 | +-------------+ 把第一个查询（返回订单号的那一个）变为子查询组合两个查询:\nmysql\u0026gt; select cust_id from orders where order_num in (select order_num from orderitems where pod_id= \u0026#39;TNT2\u0026#39;); +-------------+ | cust_id | +-------------+ | 1001 | | 1004 | +-------------+ 子查询总是从内向外处理。在处理上面的 SELECT 语句时，MySQL 实际上执行了两个操作。\n进一步根据客户 id 查出客户信息：\nselect cust_name from customers where cust_id in ( select cust_id from orders where order_num in (select order_num from orderitems where pod_id= \u0026#39;TNT2\u0026#39;));  对于能嵌套的子查询的数目没有限制，不过在实际使用时由于性能的限制，不能嵌套太多的子查询。 在 WHERE 子句中使用子查询，应该保证 SELECT 语句具有与 WHERE 子句中相同数目的列。通常，子查询将返回单个列并且与 单个列匹配，但如果需要也可以使用多个列。\n 作为计算字段使用子查询\r#\r\r子查询的另一方法是创建计算字段。\n假如需要显示 customers 表中每个客户的订单总数。订单与相应的客户ID存储在 orders 表中。\nselect cust_name, (select COUNT(*) from orders where orders.cust_id = customers.cust_id) as orders from customers order by cust_name; orders 是一个计算字段，它是由圆括号中的子查询建立的。该子查询对检索出的每个客户执行一次。子查询中的 WHERE 子句使 用了完全限定列名。\n联结表\r#\r\rSQL 最强大的功能之一就是能在数据检索查询的执行中联结（join）表。\n关系表\r#\r\r假如有一个包含产品目录的数据库表，其中每种类别的物品占一行。对于每种物品要存储的信息包括产品描述和价格，以及生产该产品的供应商信息。\n现在，假如有由同一供应商生产的多种物品，那么在何处存储供应商信息（如，供应商名、地址、联系方法等）呢？将这些数据与产品信息分开存储的理 由如下。\n 因为同一供应商生产的每个产品的供应商信息都是相同的，对每个产品重复此信息既浪费时间又浪费存储空间。 如果供应商信息改变（例如，供应商搬家或电话号码变动），只需改动一次即可。 如果有重复数据（即每种产品都存储供应商信息），很难保证每次输入该数据的方式都相同。不一致的数据在报表中很难利用。 数据无重复，显然数据是一致的，这使得处理数据更简单。  相同数据出现多次决不是一件好事，此因素是关系数据库设计的基础。关系表的设计就是要保证把信息分解成多个表，一类数据一个表。各表通过某 些常用的值（即关系设计中的关系（relational））互相关联。\n为什么使用联结表\r#\r\r分解数据为多个表能更有效地存储，更方便地处理，并且具有更大的可伸缩性。但这些好处是有代价的。如果数据存储在多个表中，怎样用单条 SELECT 语句检索出数据？\n使用联结。\n创建联结\r#\r\rselect vend_name, prod_name, prod_price from vendors, products where vendors.vend_id = products.vend_id order by vend_name, prod_name; 列 prod_name 和 prod_price 在一个表中，而列 vend_name 在另一个表中。FROM 子句列出了两个表，分别是 vendors 和 products。 这两个表用 WHERE 子句联结。\n 笛卡儿积（cartesian product）由没有联结条件的表关系返回的结果为笛卡儿积。检索出的行的数目将是第一个表中的行数乘以第二个表中 的行数。\n 内部联结\r#\r\r目前为止所用的联结称为等值联结（equijoin），它基于两个表之间的相等测试。这种联结也称为内部联结。其实，对于这种联结可以使用稍 微不同的语法来明确指定联结的类型。\nselect vend_name, prod_name, prod_price from vendors inner join products on vendors.vend_id = products.vend_id; 此语句中的 FROM 子句与上面的不同。两个表之间的关系是 FROM 子句的组成部分，以 INNER JOIN 指定。在使用这种语法时，联 结条件用特定的 ON子句 而不是 WHERE 子句给出。传递给 ON 的实际条件与传递给 WHERE 的相同。\n SQL 规范首选 INNER JOIN 语法。此外，尽管使用 WHERE 子句定义联结的确比较简单，但是使用明确的联结语法能够确保不会忘记联结条件， 有时候这样做也能影响性能。不要联结不必要的表。联结的表越多，性能下降越厉害。\n 当两张表的数据量比较大，又需要连接查询时，应该使用 FROM table1 JOIN table2 ON xxx 的语法，避免使 用 FROM table1,table2 WHERE xxx 的语法，因为后者会在内存中先生成一张数据量比较大的笛卡尔积表，增加了内存的开销。\n联结多个表\r#\r\rSQL 对一条 SELECT 语句中可以联结的表的数目没有限制。\n使用前面子查询的例子：\nselect cust_name from customers where cust_id in ( select cust_id from orders where order_num in (select order_num from orderitems where pod_id = \u0026#39;TNT2\u0026#39;)); 可以改造为：\nselect cust_name from customers, orders, orderitems where customers.cust_id = orders.cust_id and orderitems.order_num = orders.order_num and pod_id = \u0026#39;TNT2\u0026#39;; 高级联结\r#\r\r表别名\r#\r\r使用表别名：\n 缩短 SQL 语句 允许在单条 SELECT 语句中多次使用相同的表  使用表别名的主要原因之一是能在单条 SELECT 语句中不止一次引用相同的表。\nselect cust_name from customers as c, orders as o, orderitems as oi where c.cust_id = o.cust_id and oi.order_num = o.order_num and pod_id = \u0026#39;TNT2\u0026#39;; 与上面的例子功能一样，却更简短。\n不同类型的联结\r#\r\r内部联结或等值联结（equijoin）是简单联结。其他联结类型有自联结、自然联结和外部联结。\n自联结\r#\r\r假如你发现某物品（其 ID 为 DTNTR）存在问题，因此想知道生产该物品的供应商生产的其他物品是否也存在这些问题。 此查询要求首先找到生产 ID 为 DTNTR 的物品的供应商，然后找出这个供应商生产的其他物品：\nselect prod_id, prod_name from products where vend_id = (select vend_id from products where prod_id = \u0026#39;DTNTR\u0026#39;); select p1.prod_id, p1.prod_name from products as p1, products as p2 where p1.vend_id = p2.vend_id an dp2.prod_id = \u0026#39;DTNTR\u0026#39;; 上面第一条使用了子查询，第二条使用了联结，并且一个表使用了两次。\n 自联结通常作为外部语句用来替代从相同表中检索数据时使用的子查询语句。虽然最终的结果是相同的，但有时候处理联结远比处理子查询快得多。应 该试一下两种方法，以确定哪一种的性能更好。\n 自然联结\r#\r\r标准的联结返回所有数据，甚至相同的列多次出现。自然联结排除多次出现，使每个列只返回一次。\n事实上，很可能永远都不会用到不是自然联结的内部联结。\n外部联结\r#\r\r许多联结将一个表中的行与另一个表中的行相关联。但有时候会需要包含没有关联行的那些行。例如，可能需要使用联结来完成以下工作：\n 对每个客户下了多少订单进行计数，包括那些至今尚未下订单的客户； 列出所有产品以及订购数量，包括没有人订购的产品； 计算平均销售规模，包括那些至今尚未下订单的客户。  在上述例子中，联结包含了那些在相关表中没有关联行的行。这种类型的联结称为外部联结。\nselect customers.cust_id, orders.order_num from customers inner join orders on customers.cust_id = orders.cust_id; 上面的语句很简单，就是检索所有客户及其订单。那么如果想要检索所有客户，包括没有订单的客户，如下：\nselect customers.cust_id, orders.order_num from customers left outer join orders on customers.cust_id = orders.cust_id; 这条语句使用了关键字 OUTER JOIN 来指定联结的类型。外部联结还包括没有关联行的行。在使用 OUTER JOIN 语法时，必须使用 RIGHT 或 LEFT 关键字指定包括其所有行的表（RIGHT 指的是 OUTER JOIN 右边的表，而 LEFT 指的是 OUTER JOIN 左边的表）。\n上面的例子使用 LEFT OUTER JOIN 从 FROM 子句的左边表（customers 表）中选择所有行。\n 可通过颠倒 FROM 或 WHERE 子句中表的顺序，来转换外部联结形式。两种类型的外部联结可互换使用，而究竟使用哪一种纯粹是根据方便而定。\n 带聚集函数的联结\r#\r\r检索所有客户及每个客户所下的订单数：\nselect customers.cust_id, COUNT(orders.order_num) as num_ord from customers left outer join orders on customers.cust_id = orders.cust_id group by customers.cust_id; 组合查询\r#\r\rMySQL 也允许执行多个查询（多条 SELECT 语句），并将结果作为单个查询结果集返回。这些组合查询通常称为并（union）或 复合查询（compound query）。\n需要使用组合查询的情况：\n 在单个查询中从不同的表返回类似结构的数据； 对单个表执行多个查询，按单个查询返回数据。  select vend_id, prod_id, prod_price from products where prod_price \u0026gt;= 5 union select vend_id, prod_id, prod_price from products where vend_id in (1001,1002); 转成多条 where 子句的写法：\nselect vend_id, prod_id, prod_price from products where prod_price \u0026gt;= 5 or vend_id in (1001,1002);  任何具有多个 WHERE 子句的 SELECT 语句都可以作为一个组合查询给出。这两种技术在不同的查询中性能也不同。因此，应该试一下这 两种技术，以确定对特定的查询哪一种性能更好。\n UNION 规则\r#\r\r UNION 必须由两条或两条以上的 SELECT 语句组成，语句之间用关键字 UNION 分隔。 UNION 中的每个查询必须包含相同的列、表达式或聚集函数（不过各个列不需要以相同的次序列出）。 列数据类型必须兼容：类型不必完全相同，但必须是 DBMS 可以隐含地转换的类型（例如，不同的数值类型或不同的日期类型）。  包含或取消重复的行\r#\r\rUNION 从查询结果集中自动去除了重复的行。如果想返回所有匹配行，可使用 UNION ALL 而不是 UNION。\n组合查询结果排序\r#\r\r用 UNION 组合查询时，只能使用一条 ORDER BY 子句，它必须出现在最后一条 SELECT 语句之后。对于结果集，不存在用一种方式排序一 部分，而又用另一种方式排序另一部分的情况，因此不允许使用多条 ORDER BY 子句。\n全文本搜索\r#\r\r两个最常使用的引擎为 MyISAM 和 InnoDB，前者支持全文本搜索，而后者不支持。\n使用 MySQL 的 Match() 和 Against() 函数进行全文本搜索。\n"});index.add({'id':13,'href':'/db-learn/docs/mysql/02_install/','title':"MySQL 安装",'content':"MySQL 的大部分安装包都包含了服务器程序和客户端程序\n 注意，在 Linux 下使用 RPM 安装，有时需要分别安装服务器 RPM 包和客户端 RPM 包。\n MySQL 的安装目录\r#\r\rLinux 下的安装目录一般为 /usr/local/mysql/。Windows 一般为 C:\\Program Files\\MySQL\\MySQL Server x.x （记住你自己的安装目录）。\nbin目录\r#\r\r打开 /usr/local/mysql/bin，执行 tree：\n.\r├── mysql\r├── mysql.server -\u0026gt; ../support-files/mysql.server\r├── mysqladmin\r├── mysqlbinlog\r├── mysqlcheck\r├── mysqld # mysql 的服务端程序\r├── mysqld_multi # 运行多个 MySQL 服务器进程\r├── mysqld_safe\r├── mysqldump\r├── mysqlimport\r├── mysqlpump\r... (省略其他文件)\r0 directories, 40 files\r这个目录一般需要配置到环境变量的 PATH 中，Linux 中各个路径以 : 分隔。也可以选择不配：\n/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/mysql/bin mysqld\r#\r\rmysqld 是 MySQL 服务端程序。不常用。\nmysqld_safe\r#\r\rmysqld_safe 是一个启动脚本，最终也是调用 mysqld，但是还会另外一个监控进程，在服务器进程挂了的时候，可以帮助重启服务器进程。 mysqld_safe 启动服务端程序时，会将服务端程序的出错信息和其他诊断信息重定向到某个文件中，产生出错日志，这样可以方便找出发生错误的原因。\nmysql.server\r#\r\rmysql.server 也是一个启动脚本，会间接的调用 mysqld_safe，使用 mysql.server 启动服务端程序：\nmysql.server start mysql.server 文件其实是一个链接文件，它的实际文件是 ../support-files/mysql.server。\nclient\r#\r\rbin 目录下的 mysql、mysqladmin、mysqldump、mysqlcheck 都是客户端程序，启动客户端程序连接服务器进程：\nmysql --host=\u0026lt;主机名\u0026gt; --user=\u0026lt;用户名\u0026gt; --password=\u0026lt;密码\u0026gt; 注意：\n 最好不要在一行命令中输入密码，可能会导致密码泄露。 如果使用的是类 UNIX 系统，并且省略 -u 参数后，会把你登陆操作系统的用户名当作 MySQL 的用户名去处理。  "});index.add({'id':14,'href':'/db-learn/docs/mysql/03_query/','title':"MySQL 简单查询",'content':"注意每条语句后面都要以;结尾。SQL语句是不区分大小写的。\nUSE 选择数据库\r#\r\rSHOW\r#\r\r SHOW DATABASES;，查看数据库列表。 SHOW TABLES;，查看数据库中的表。 SHOW COLUMNS，显示某个表中的列，比如 SHOW COLUMNS FROM users。也可以使用 DESCRIBE users，效果 和 SHOW COLUMNS FROM users 是一样的。 SHOW STATUS，用于显示广泛的服务器状态信息。 SHOW CREATE DATABASE 和 SHOW CREATE TABLE，分别用来显示创建特定数据库或表的语句。 SHOW GRANTS，用来显示授予用户（所有用户或特定用户）的安全权限。 SHOW ERRORS 和 SHOW WARNINGS，用来显示服务器错误或警告消息。 HELP SHOW; 查看 SHOW 的用法  SELECT\r#\r\r为了使用 SELECT 所搜表数据，必须至少给出两条信息——想选择什么，以及从什么地方选择。比如select name from users;，会找 出 users 表中的所有 name 列。\n检索多个列：\nselect name, age, phone from users; 检索所有列，使用星号 * 通配符：\nselect * from users; DISTINCT\r#\r\rDISTINCT 关键字用来去重。比如下面的语句，只会返回 name 不同的用户：\nselect distinct name from users;  DISTINCT 关键字应用于所有列而不仅是前置它的列。如果给出 SELECT DISTINCT vend_id, prod_price，会分别作用于了 vend_id 和 prod_price 列。\n LIMIT\r#\r\rSELECT 语句返回所有匹配的行，为了返回第一行或前几行，可使用 LIMIT 子句。\nselect name from users limit 5; 上面的语句最多返回5行。\nselect name from users limit 5,5; 上面的语句的意思是从第五行开始，最多返回 5 行。\n MySQL 5 支持 LIMIT 的另一种替代语法。LIMIT 4 OFFSET 3 意为从行 3 开始取 4 行，就像 LIMIT 3, 4 一样。\n 完全限定的表名和列名\r#\r\rselect users.name from demo.users; 上面的语句和 select name from users 没什么区别。但是有一些情形需要完全限定名。比如在涉及外部子查询的语句中，会使用完全限定列名， 避免列名可能存在多义性。\n 完全限定列名在引用的列可能出现二义性时，必须使用完全限定列名（用一个点分隔的表名和列名）。\n ORDER BY\r#\r\r使用 ORDER BY 子句对输出进行排序。比如 select name from users order by age，按照 age 排序。\n按多个列排序\r#\r\rselect name from users order by age, weight; 会先按照 age 排序，如果有多行有相同的 age，再按照 weight 排序。\n排序方向\r#\r\r升序排序（从 A 到 Z）是默认的排序顺序，如果要进行降序排序，需要指定 DESC 关键字。\nselect name, age from users order by age desc; 结果会是\n+--------+------+\r| name | age |\r+--------+------+\r| ming | 18 |\r| qiang | 17 |\r| liang | 16 |\r| long | 16 |\r+--------+------+\r多个列降序排序：\nselect name from users order by age desc, weight; 上面的示例，对 age 列降序排序，weight 还是默认的升序排序。可以看出 DESC 关键字只会作用到其前面的列。\n 如果想在多个列上进行降序排序，必须对每个列指定 DESC 关键字。 ASC 是升序排列的关键字，没什么用，因为默认就是升序。\n 过滤数据\r#\r\r通常数据库检索数据都会指定过滤条件（filter condition）。\n在 SELECT 语句中，数据根据 WHERE 子句中指定的搜索条件进行过滤。\nselect name from users where age = 18; select name from users where name = \u0026#39;ming\u0026#39;; 会找到 age 等于 18 的行。\n 如果同时使用 ORDER BY 和 WHERE 子句时，ORDER BY 必须位于 WHERE 之后，否则将会产生错误。\n WHERE条件操作符\r#\r\r   操作符 描述     = 等于   \u0026lt;\u0026gt; 不等于   != 不等于   \u0026lt; 小于   \u0026lt;= 小于等于   \u0026gt; 大于   \u0026gt;= 大于等于   BETWEEN 在指定的两个值之间    BETWEEN 操作符使用：\nselect name from users where age between 15 and 18; 检索年龄在 15 和 18 之间的用户。\nAND 关键字前后分别是开始值和结束值。查询到的数据包括指定的开始值和结束值。\n空值检查\r#\r\r创建表时，列的值可以为空值 NULL。\nIS NULL 子句可用来检查具有 NULL 值的列。\nselect name from users where phone IS NULL; AND\r#\r\r可使用 AND 操作符给 WHERE 子句附加条件。\nselect name from users where age = 18 and weight = 60; OR\r#\r\r和 AND 差不多，只不过是匹配任一满足的条件。\n操作符优先级\r#\r\rselect name from users where age = 18 or agr = 19 and weight \u0026gt;= 60; 上面的语句是什么结果？ 是找出年龄是 18 或者 19，体重在 60 以上的行？并不是。\nSQL在处理 OR 操作符前，优先处理 AND 操作符。当 SQL 看到上述 WHERE 子句时，它理解为由年龄为 19，并且体重在 60 以上的用户，或者 年龄为 18，不管体重多少的用户。\n正确的语法：\nselect name from users where (age = 18 or agr = 19) and weight \u0026gt;= 60; SQL 会首先过滤圆括号内的条件。\nIN\r#\r\rIN 操作符用来指定条件范围，匹配圆括号中的值。\nselect name from users where age in (18, 19); IN 操作符与 OR 有相同的功能。\nNOT\r#\r\rNOT 操作否定条件。\nselect name from users where age not in (18, 19); LIKE\r#\r\r如果要使用通配符，需要使用 LIKE 操作符。\n通配符可在搜索模式中任意位置使用，并且可以使用多个通配符。\n%\r#\r\r% 表示任何字符出现任意次数。例如找出所有以词 m 开头的用户：\nselect name from users where name like \u0026#39;m%\u0026#39;; 使用两个通配符，表示匹配任何位置包含文本 in 的值：\nselect name from users where name like \u0026#39;%in%\u0026#39;; _\r#\r\r下划线 _ 的用途与 % 一样，但下划线只匹配单个字符而不是多个字符。\n使用正则表达式\r#\r\rselect name from users where name REGEX \u0026#39;ing\u0026#39;; REGEXP 后跟的是正则表达式。\n正则表达式并没有什么优势，但是有些场景下可以考虑使用:\nselect name from users where weight REGEX \u0026#39;.6\u0026#39;; . 是正则表达式语言中一个特殊的字符。它表示匹配任意一个字符，因此，56 和 66 都匹配且返回。\nOR 匹配\r#\r\rselect name from users where weight REGEX \u0026#39;46|56\u0026#39;; | 为正则表达式的 OR 操作符。\n匹配几个字符之一\r#\r\r想匹配特定的字符可以指定一组用 [ 和 ] 括起来的字符来完成。\nselect name from users where weight REGEX \u0026#39;[456]6\u0026#39;; [456] 表示匹配 4，5，6。[] 是另一种形式的 OR 语句。可以使用一些正则的语法例如 [^123]，匹配除了 1，2，3 之外的字符， [1-9] 匹配 1 到 9 范围的字符。匹配特殊字符钱加 \\\\ 比如 . 要用 \\\\. 来查找。\n计算字段\r#\r\r存储在数据库表中的数据一般不是应用程序所需要的格式。\n拼接字段\r#\r\rConcat() 拼接串，即把多个串连接起来形成一个较长的串。Concat() 需要一个或多个指定的串，各个串之间用逗号分隔。\nmysql\u0026gt; select Concat(c1, \u0026#39;(\u0026#39;, c2, \u0026#39;)\u0026#39;) from record_format_demo; +--------------------------+ | Concat(c1, \u0026#39;(\u0026#39;, c2, \u0026#39;)\u0026#39;) | +--------------------------+ | aaaa(bbb) | | eeee(fff) | +--------------------------+ 2 rows in set (0.01 sec) RTrim() 函数可以删除数据右侧多余的空格，还有 LTrim() 和 Trim()，分别是删除左边空格和删除左右空格。\n别名\r#\r\r别名（alias）是一个字段或值的替换名。用 AS 关键字：\nmysql\u0026gt; select Concat(c1, \u0026#39;(\u0026#39;, c2, \u0026#39;)\u0026#39;) as c5 from record_format_demo; +-----------+ | c5 | +-----------+ | aaaa(bbb) | | eeee(fff) | +-----------+ 2 rows in set (0.01 sec) 算术计算\r#\r\rselect price, name, quantity from orders where order_num = 2005; price 是物品的价格，quantity 是数量，如果想汇总物品总价;\nselect price, name, quantity, quantity*price as total_price from orders where order_num = 2005; total_price 就是总价。\nMySQL 支持基本算术操作符 +，-，*，/。此外，圆括号可用来区分优先顺序。\n数据处理函数\r#\r\rTrim() 就是一个数据处理函数。\n文本函数\r#\r\r Upper() 函数将文本转换为大写，Upper(name) as newName Left() 返回串左边的字符 Length() 返回串的长度 Locate() 找出串的一个子串 Lower() 将串转换为小写 LTrim() 去掉串左边的空格 Right() 返回串右边的字符 RTrim() 去掉串右边的空格 Soundex() 返回串的 SOUNDEX 值 SubString() 返回子串的字符  SOUNDEX 是一个将任何文本串转换为描述其语音表示的字母数字模式的算法。\n日期和时间处理函数\r#\r\r AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分  数值函数\r#\r\r Abs() 返回一个数的绝对值 Cos() 返回一个角度的余弦 Exp() 返回一个数的指数值 Mod() 返回除操作的余数 Pi() 返回圆周率 Rand() 返回一个随机数 Sin() 返回一个角度的正弦 Sqrt() 返回一个数的平方根 Tan() 返回一个角度的正切  聚合函数\r#\r\rAVG\r#\r\rAVG 函数可用来返回所有列的平均值，也可以用来返回特定列或行的平均值。\nselect AVG(price) as avg_price from orders; 返回订单的平均价格。\nAVG() 函数忽略列值为NULL 的行。\nCOUNT\r#\r\rCOUNT(*) 对表中行的数目进行计数。COUNT(column) 对特定列中具有值的行进行计数，忽略 NULL 值。\nMAX\r#\r\r返回指定列中的最大值。忽略列值为 NULL 的行。例如 select max(price) as max_price from products; 返回 products 表中 最贵的物品的价格。\nMIN\r#\r\r与 MAX() 功能相反。\nSUM\r#\r\rSUM() 函数返回指定列值的和。忽略列值为 NULL 的行。例如 select SUM(item_price*quantity) as total_price from products;\n聚合不同值\r#\r\r上面的几个函数都可以使用 DISTINCT，比如 AVG(DISTINCT price) as avg_total_price\nDISTINCT 只能用于 COUNT()。DISTINCT 不能用于 COUNT(*)。\n分组\r#\r\rGROUP BY 子句用来创建分组。\nselect vend_id, COUNT(*) as prod_num from products group by vend_id; 上面的语句按 vend_id 排序并分组数据。\n注意：\n GROUP BY 句必须出现在 WHERE 子句之后，ORDER BY 子句之前。 如果分组列中具有 NULL 值，则 NULL 将作为一个分组返回。如果列中有多行 NULL 值，它们将分为一组。 GROUP BY 子句中列出的每个列都必须是检索列或有效的表达式（但不能是聚集函数）。如果在 SELECT 中使用表达式，则必须在 GROUP BY 子句中指定相同的表达式。不能使用别名。 除了聚集计算语句，SELECT 语句中的每个列都必须在 GROUP BY 子句中给出。  过滤分组\r#\r\rHAVING 子句过滤分组。HAVING 非常类似于 WHERE（WHERE 过滤的是行）。它们的句法是相同的，只是关键字有差别。\n也可以这么理解：WHERE 在数据分组前进行过滤，HAVING 在数据分组后进行过滤。\nselect vend_id, COUNT(*) as prod_num from products group by vend_id having COUNT(*) \u0026gt;= 2; 它过滤 COUNT(*) \u0026gt;=2 的那些分组。\n分组和排序\r#\r\rGROUP BY 和 ORDER BY 的差别：\n   order by group by     排序产生的输出 分组行。但输出可能不是分组的顺序   任意列都可以使用（甚至非选择的列也可以使用） 只可能使用选择列或表达式列，而且必须使用每个选择列表达式   不一定需要 如果与聚集函数一起使用列（或表达式），则必须使用     一般在使用 GROUP BY 子句时，应该也给出 ORDER BY 子句。这是保证数据正确排序的唯一方法。千万不要仅依赖 GROUP BY 排序数据。\n 检索总计订单价格大于等于 50 的订单的订单号和总计订单价格：\nselect order_num, SUM(quantity*price) as order_total from orders group by order_num having SUM(quantity*price) \u0026gt;= 50; 按总计订单价格排序输出：\nselect order_num, SUM(quantity*price) as order_total from orders group by order_num having SUM(quantity*price) \u0026gt;= 50 order by order_total; SELECT 子句顺序\r#\r\r   子句 是否必须使用     SELECT 是   FROM 仅在从表选择数据时使用   WHERE 否   GROUP BY 仅在按组计算聚集时使用   HAVING 否   ORDER BY 否   LIMIT 否    "});index.add({'id':15,'href':'/db-learn/docs/redis/08_redis-key/','title':"Redis Key 操作",'content':"Redis Key 操作\r#\r\r在日常开发中，查找某个，或某些铁定前缀的 key，修改他们的值，删除 key，都是很常用的操作。 Redis 如何从海量的 key 中找出满足特 定前缀的key列表？\nRedis 的 keys 指令。\nRedis 允许的最大 Key 长度是 512MB（对 Value 的长度限制也是 512MB），但是尽量不要使用过长的 key，不仅会消耗更多的 内存，还会导致查找的效率降低。key 也不应该过于短，开发中应该使用统一的规范来设计 key，可读性好，也易于维护。比 如 user:\u0026lt;user id\u0026gt;:followers。\n查找删除\r#\r\rKEYS\r#\r\r按指定的正则匹配模式 pattern 查找 key。\nKEYS pattern  KEYS * 匹配数据库中所有的 key KEYS h?llo 匹配 hello、hallo、hxllo 等 KEYS h*llo 匹配 hllo、heeeeello等 KEYS h[ae]llo 匹配 hello、hallo，但不匹配 hillo  KEYS 指令非常简单，但是有两个缺点：\n 没有 offset、limit 参数，会返回所有匹配到的 key。 执行 KEYS 会遍历所有的 key，如果 Redis 存储了海量的 key，由于 Redis 是单线程，KEYS 指令就会阻塞其他指令，直 到 KEYS 执行完毕。  所以在数据量很大的情况下，不建议使用 KEYS，会造成 Redis 服务卡顿，导致其他的指令延时甚至超时报错。 Redis 提供了 SCAN 指令来解决这个问题。\n# 4 个测试数据 redis\u0026gt; MSET one 1 two 2 three 3 four 4 OK redis\u0026gt; KEYS *o* 1) \u0026#34;four\u0026#34; 2) \u0026#34;two\u0026#34; 3) \u0026#34;one\u0026#34; redis\u0026gt; KEYS t?? 1) \u0026#34;two\u0026#34; redis\u0026gt; KEYS t[w]* 1) \u0026#34;two\u0026#34; # 匹配数据库内所有 key redis\u0026gt; KEYS * 1) \u0026#34;four\u0026#34; 2) \u0026#34;three\u0026#34; 3) \u0026#34;two\u0026#34; 4) \u0026#34;one\u0026#34; EXISTS\r#\r\r判断 key 是否存在。\nEXISTS key 存在返回 1，不存在返回 0。\nredis\u0026gt; SET db \u0026#34;redis\u0026#34; OK redis\u0026gt; EXISTS db (integer) 1 redis\u0026gt; DEL db (integer) 1 redis\u0026gt; EXISTS db (integer) 0 RANDOMKEY\r#\r\r随机返回一个 key\n# 设置多个 key redis\u0026gt; MSET fruit \u0026#34;apple\u0026#34; drink \u0026#34;beer\u0026#34; food \u0026#34;cookies\u0026#34; OK redis\u0026gt; RANDOMKEY \u0026#34;fruit\u0026#34; redis\u0026gt; RANDOMKEY \u0026#34;food\u0026#34; # 返回 key 但不删除 redis\u0026gt; KEYS * 1) \u0026#34;food\u0026#34; 2) \u0026#34;drink\u0026#34; 3) \u0026#34;fruit\u0026#34; # 删除当前数据库所有 key，数据库为空 redis\u0026gt; FLUSHDB OK redis\u0026gt; RANDOMKEY (nil) TYPE\r#\r\r返回 key 的值的类型。key 不存在返回 none，否则返回值得类型 string，list，set，zset，hash。\n# 字符串 redis\u0026gt; SET weather \u0026#34;sunny\u0026#34; OK redis\u0026gt; TYPE weather string # 列表 redis\u0026gt; LPUSH book_list \u0026#34;programming in scala\u0026#34; (integer) 1 redis\u0026gt; TYPE book_list list # 集合 redis\u0026gt; SADD pat \u0026#34;dog\u0026#34; (integer) 1 redis\u0026gt; TYPE pat set SORT\r#\r\r返回指定 key 中元素,并对元素进行排序，key 的类型是列表、集合、有序集合。排序默认以数字作为对象，值被解释为双精度浮点数，然后进 行比较。\nSORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern ...]] [ASC | DESC] [ALPHA] [STORE destination] 简单用法\r#\r\r SORT key，按从大到小顺序排序 SORT key DESC，按从小到大的顺序排序  # 开销金额列表 redis\u0026gt; LPUSH today_cost 30 1.5 10 8 (integer) 4 # 排序 redis\u0026gt; SORT today_cost 1) \u0026#34;1.5\u0026#34; 2) \u0026#34;8\u0026#34; 3) \u0026#34;10\u0026#34; 4) \u0026#34;30\u0026#34; # 倒序 redis\u0026gt; SORT today_cost DESC 1) \u0026#34;30\u0026#34; 2) \u0026#34;10\u0026#34; 3) \u0026#34;8\u0026#34; 4) \u0026#34;1.5\u0026#34; ALPHA 排序\r#\r\rSORT 默认以数字作为对象排序，如果需要对字符串进行排序，使用 ALPHA 参数：\n# 网址 redis\u0026gt; LPUSH website \u0026#34;www.reddit.com\u0026#34; (integer) 1 redis\u0026gt; LPUSH website \u0026#34;www.slashdot.com\u0026#34; (integer) 2 redis\u0026gt; LPUSH website \u0026#34;www.infoq.com\u0026#34; (integer) 3 # 默认（按数字）排序 redis\u0026gt; SORT website 1) \u0026#34;www.infoq.com\u0026#34; 2) \u0026#34;www.slashdot.com\u0026#34; 3) \u0026#34;www.reddit.com\u0026#34; # 按字符排序 redis\u0026gt; SORT website ALPHA 1) \u0026#34;www.infoq.com\u0026#34; 2) \u0026#34;www.reddit.com\u0026#34; 3) \u0026#34;www.slashdot.com\u0026#34; 使用 LIMIT\r#\r\r类似 SQL 的分页查询，两个参数：\n offset，指定偏移量 count，指定返回数量  # 添加测试数据，列表值为 1 指 10 redis\u0026gt; RPUSH rank 1 3 5 7 9 (integer) 5 redis\u0026gt; RPUSH rank 2 4 6 8 10 (integer) 10 # 返回列表中最小的 5 个值 redis\u0026gt; SORT rank LIMIT 0 5 1) \u0026#34;1\u0026#34; 2) \u0026#34;2\u0026#34; 3) \u0026#34;3\u0026#34; 4) \u0026#34;4\u0026#34; 5) \u0026#34;5\u0026#34; # 使用排序 redis\u0026gt; SORT rank LIMIT 0 5 DESC 1) \u0026#34;10\u0026#34; 2) \u0026#34;9\u0026#34; 3) \u0026#34;8\u0026#34; 4) \u0026#34;7\u0026#34; 5) \u0026#34;6\u0026#34; 使用外部 key 进行排序\r#\r\r可以使用外部 key 的数据作为权重，代替默认的直接对比键值的方式来进行排序。\n假设现在有用户数据如下：\n   uid user_name_{uid} user_level_{uid}     1 admin 9999   2 jack 10   3 peter 25   4 mary 70    以下代码将数据输入到 Redis 中：\n# admin redis 127.0.0.1:6379\u0026gt; LPUSH uid 1 (integer) 1 redis 127.0.0.1:6379\u0026gt; SET user_name_1 admin OK redis 127.0.0.1:6379\u0026gt; SET user_level_1 9999 OK # jack redis 127.0.0.1:6379\u0026gt; LPUSH uid 2 (integer) 2 redis 127.0.0.1:6379\u0026gt; SET user_name_2 jack OK redis 127.0.0.1:6379\u0026gt; SET user_level_2 10 OK # peter redis 127.0.0.1:6379\u0026gt; LPUSH uid 3 (integer) 3 redis 127.0.0.1:6379\u0026gt; SET user_name_3 peter OK redis 127.0.0.1:6379\u0026gt; SET user_level_3 25 OK # mary redis 127.0.0.1:6379\u0026gt; LPUSH uid 4 (integer) 4 redis 127.0.0.1:6379\u0026gt; SET user_name_4 mary OK redis 127.0.0.1:6379\u0026gt; SET user_level_4 70 OK BY 选项\r#\r\r默认情况下，SORT uid 直接按 uid 中的值排序：\nredis 127.0.0.1:6379\u0026gt; SORT uid 1) \u0026#34;1\u0026#34; # admin 2) \u0026#34;2\u0026#34; # jack 3) \u0026#34;3\u0026#34; # peter 4) \u0026#34;4\u0026#34; # mary 通过使用 BY 选项，可以让 uid 按其他键的元素来排序。\n比如说， 以下代码让 uid 键按照 user_level_{uid} 的大小来排序：\nredis 127.0.0.1:6379\u0026gt; SORT uid BY user_level_* 1) \u0026#34;2\u0026#34; # jack , level = 10 2) \u0026#34;3\u0026#34; # peter, level = 25 3) \u0026#34;4\u0026#34; # mary, level = 70 4) \u0026#34;1\u0026#34; # admin, level = 9999 user_level_* 是一个占位符， 它先取出 uid 中的值， 然后再用这个值来查找相应的键。 比如在对 uid 列表进行排序时， 程序就会先取出 uid 的值 1 、 2 、 3 、 4 ， 然后使用 user_level_1、user_level_2 、 user_level_3 和 user_level_4 的值作为排序 uid 的权重。\nGET 选项\r#\r\r使用 GET 选项， 可以根据排序的结果来取出相应的键值。\n比如说， 以下代码先排序 uid， 再取出键 user_name_{uid}的值：\nredis 127.0.0.1:6379\u0026gt; SORT uid GET user_name_* 1) \u0026#34;admin\u0026#34; 2) \u0026#34;jack\u0026#34; 3) \u0026#34;peter\u0026#34; 4) \u0026#34;mary\u0026#34; 组合使用 BY 和 GET\r#\r\r通过组合使用 BY 和 GET， 可以让排序结果以更直观的方式显示出来。\n比如说， 以下代码先按 user_level_{uid} 来排序 uid 列表， 再取出相应的 user_name_{uid} 的值：\nredis 127.0.0.1:6379\u0026gt; SORT uid BY user_level_* GET user_name_* 1) \u0026#34;jack\u0026#34; # level = 10 2) \u0026#34;peter\u0026#34; # level = 25 3) \u0026#34;mary\u0026#34; # level = 70 4) \u0026#34;admin\u0026#34; # level = 9999 现在的排序结果要比只使用 SORT uid BY user_level_*  要直观得多。\n获取多个外部键\r#\r\r可以同时使用多个 GET 选项， 获取多个外部键的值。\n以下代码就按 uid 分别获取 user_level_{uid} 和 user_name_{uid}：\nredis 127.0.0.1:6379\u0026gt; SORT uid GET user_level_* GET user_name_* 1) \u0026#34;9999\u0026#34; # level 2) \u0026#34;admin\u0026#34; # name 3) \u0026#34;10\u0026#34; 4) \u0026#34;jack\u0026#34; 5) \u0026#34;25\u0026#34; 6) \u0026#34;peter\u0026#34; 7) \u0026#34;70\u0026#34; 8) \u0026#34;mary\u0026#34; GET 有一个额外的参数规则，那就是 —— 可以用 # 获取被排序键的值。\n以下代码就将 uid 的值、及其相应的 user_level_* 和 user_name_* 都返回为结果：\nredis 127.0.0.1:6379\u0026gt; SORT uid GET # GET user_level_* GET user_name_* 1) \u0026#34;1\u0026#34; # uid 2) \u0026#34;9999\u0026#34; # level 3) \u0026#34;admin\u0026#34; # name 4) \u0026#34;2\u0026#34; 5) \u0026#34;10\u0026#34; 6) \u0026#34;jack\u0026#34; 7) \u0026#34;3\u0026#34; 8) \u0026#34;25\u0026#34; 9) \u0026#34;peter\u0026#34; 10) \u0026#34;4\u0026#34; 11) \u0026#34;70\u0026#34; 12) \u0026#34;mary\u0026#34; 获取外部键，但不进行排序\r#\r\r通过将一个不存在的键作为参数传给 BY 选项， 可以让 SORT 跳过排序操作， 直接返回结果：\nredis 127.0.0.1:6379\u0026gt; SORT uid BY not-exists-key 1) \u0026#34;4\u0026#34; 2) \u0026#34;3\u0026#34; 3) \u0026#34;2\u0026#34; 4) \u0026#34;1\u0026#34; 这种用法在单独使用时，没什么实际用处。\n不过，通过将这种用法和 GET 选项配合， 就可以在不排序的情况下， 获取多个外部键， 相当于执行一个整合的获取操作（类似于 SQL 数 据库的 join 关键字）。\n以下代码演示了，如何在不引起排序的情况下，使用 SORT 、 BY 和 GET 获取多个外部键：\nredis 127.0.0.1:6379\u0026gt; SORT uid BY not-exists-key GET # GET user_level_* GET user_name_* 1) \u0026#34;4\u0026#34; # id 2) \u0026#34;70\u0026#34; # level 3) \u0026#34;mary\u0026#34; # name 4) \u0026#34;3\u0026#34; 5) \u0026#34;25\u0026#34; 6) \u0026#34;peter\u0026#34; 7) \u0026#34;2\u0026#34; 8) \u0026#34;10\u0026#34; 9) \u0026#34;jack\u0026#34; 10) \u0026#34;1\u0026#34; 11) \u0026#34;9999\u0026#34; 12) \u0026#34;admin\u0026#34; 将哈希表作为 GET 或 BY 的参数\r#\r\r除了可以将字符串键之外， 哈希表也可以作为 GET 或 BY 选项的参数来使用。\n比如说，对于前面给出的用户信息表：\n   uid user_name_{uid} user_level_{uid}     1 admin 9999   2 jack 10   3 peter 25   4 mary 70    我们可以不将用户的名字和级别保存在 user_name_{uid} 和 user_level_{uid} 两个字符串键中， 而是用一个带有 name 域和 level 域 的哈希表 user_info_{uid} 来保存用户的名字和级别信息：\nredis 127.0.0.1:6379\u0026gt; HMSET user_info_1 name admin level 9999 OK redis 127.0.0.1:6379\u0026gt; HMSET user_info_2 name jack level 10 OK redis 127.0.0.1:6379\u0026gt; HMSET user_info_3 name peter level 25 OK redis 127.0.0.1:6379\u0026gt; HMSET user_info_4 name mary level 70 OK 之后， BY 和 GET 选项都可以用 key-\u0026gt;field 的格式来获取哈希表中的域的值， 其中 key 表示哈希表键， 而 field 则表示哈希表的域：\nredis 127.0.0.1:6379\u0026gt; SORT uid BY user_info_*-\u0026gt;level 1) \u0026#34;2\u0026#34; 2) \u0026#34;3\u0026#34; 3) \u0026#34;4\u0026#34; 4) \u0026#34;1\u0026#34; redis 127.0.0.1:6379\u0026gt; SORT uid BY user_info_*-\u0026gt;level GET user_info_*-\u0026gt;name 1) \u0026#34;jack\u0026#34; 2) \u0026#34;peter\u0026#34; 3) \u0026#34;mary\u0026#34; 4) \u0026#34;admin\u0026#34; 保存排序结果\r#\r\r我们可以把 SORT 命令返回的排序结果，保存到指定 key 上。如果 key 已存在，会覆盖。 没有使用 STORE，SORT命令返回列表形式的排序结果；使用 STORE 参数，SORT 命令返回排序结果的元素数量。\nredis\u0026gt; RPUSH numbers 1 3 5 7 9 (integer) 5 redis\u0026gt; RPUSH numbers 2 4 6 8 10 (integer) 10 redis\u0026gt; LRANGE numbers 0 -1 1) \u0026#34;1\u0026#34; 2) \u0026#34;3\u0026#34; 3) \u0026#34;5\u0026#34; 4) \u0026#34;7\u0026#34; 5) \u0026#34;9\u0026#34; 6) \u0026#34;2\u0026#34; 7) \u0026#34;4\u0026#34; 8) \u0026#34;6\u0026#34; 9) \u0026#34;8\u0026#34; 10) \u0026#34;10\u0026#34; redis\u0026gt; SORT numbers STORE sorted-numbers (integer) 10 # 排序后的结果 redis\u0026gt; LRANGE sorted-numbers 0 -1 1) \u0026#34;1\u0026#34; 2) \u0026#34;2\u0026#34; 3) \u0026#34;3\u0026#34; 4) \u0026#34;4\u0026#34; 5) \u0026#34;5\u0026#34; 6) \u0026#34;6\u0026#34; 7) \u0026#34;7\u0026#34; 8) \u0026#34;8\u0026#34; 9) \u0026#34;9\u0026#34; 10) \u0026#34;10\u0026#34; DEL\r#\r\r删除一个或多个 key。\nDEL key [key ...] 返回被删除的 key 的数量。\n# 删除单个 key redis\u0026gt; SET name huangz OK redis\u0026gt; DEL name (integer) 1 # 删除一个不存在的 key redis\u0026gt; EXISTS phone (integer) 0 redis\u0026gt; DEL phone # 失败，没有 key 被删除 (integer) 0 # 同时删除多个 key redis\u0026gt; SET name \u0026#34;redis\u0026#34; OK redis\u0026gt; SET type \u0026#34;key-value store\u0026#34; OK redis\u0026gt; SET website \u0026#34;redis.com\u0026#34; OK redis\u0026gt; DEL name type website (integer) 3 重命名\r#\r\rRENAME\r#\r\r将 key 重命名为 newkey。\nRENAME key newkey 如果 key 和 newkey 相同，或 key 不存在，返回一个错误。当 newkey 已存在，覆盖 newkey。\n# key 存在且 newkey 不存在 redis\u0026gt; SET message \u0026#34;hello world\u0026#34; OK redis\u0026gt; RENAME message greeting OK # message 不复存在 redis\u0026gt; EXISTS message (integer) 0 # 已被重命名为 greeting redis\u0026gt; EXISTS greeting (integer) 1 # 当 key 不存在时，返回错误 redis\u0026gt; RENAME fake_key never_exists (error) ERR no such key # newkey 已存在时， RENAME 会覆盖旧 newkey redis\u0026gt; SET pc \u0026#34;lenovo\u0026#34; OK redis\u0026gt; SET personal_computer \u0026#34;dell\u0026#34; OK redis\u0026gt; RENAME pc personal_computer OK redis\u0026gt; GET pc (nil) # 原来的值 dell 被覆盖了 redis:1\u0026gt; GET personal_computer \u0026#34;lenovo\u0026#34; RENAMENX\r#\r\r与 RENAME 类似，不同的是 RENAMENX 只有在 newkey 不存在的时候，才会重命名。\nRENAMENX key newkey 如果 newkey 已经存在返回 0。\n# newkey 不存在时，重命名成功 redis\u0026gt; SET player \u0026#34;MPlyaer\u0026#34; OK redis\u0026gt; EXISTS best_player (integer) 0 redis\u0026gt; RENAMENX player best_player (integer) 1 # newkey存在时，失败 redis\u0026gt; SET animal \u0026#34;bear\u0026#34; OK redis\u0026gt; SET favorite_animal \u0026#34;butterfly\u0026#34; OK redis\u0026gt; RENAMENX animal favorite_animal (integer) 0 redis\u0026gt; get animal \u0026#34;bear\u0026#34; redis\u0026gt; get favorite_animal \u0026#34;butterfly\u0026#34; 序列化和反序列化\r#\r\rDUMP\r#\r\r序列化指定的 key 的值，并返回被序列化的值。\nredis\u0026gt; SET greeting \u0026#34;hello, dumping world!\u0026#34; OK redis\u0026gt; DUMP greeting \u0026#34;\\x00\\x15hello, dumping world!\\x06\\x00E\\xa0Z\\x82\\xd8r\\xc1\\xde\u0026#34; redis\u0026gt; DUMP not-exists-key (nil) RESTORE\r#\r\r将序列化的值反序列化，并将反序列化的值存储到指定的 key。\nRESTORE key ttl serialized-value ttl 表示以毫秒为单位设置 key 的生存时间；如果 ttl 值为 0，表示不设置生存时间。 Redis 在进行反序化前，首先会对序列化值进行 RDB 较验，如果版本不符或数据不完整，会拒绝反序列化并返回一个错误\nredis\u0026gt; SET greeting \u0026#34;hello, dumping world!\u0026#34; OK redis\u0026gt; DUMP greeting \u0026#34;\\x00\\x15hello, dumping world!\\x06\\x00E\\xa0Z\\x82\\xd8r\\xc1\\xde\u0026#34; redis\u0026gt; RESTORE greeting-again 0 \u0026#34;\\x00\\x15hello, dumping world!\\x06\\x00E\\xa0Z\\x82\\xd8r\\xc1\\xde\u0026#34; OK redis\u0026gt; GET greeting-again \u0026#34;hello, dumping world!\u0026#34; # 使用错误的值进行反序列化 redis\u0026gt; RESTORE fake-message 0 \u0026#34;hello moto moto blah blah\u0026#34; ; (error) ERR DUMP payload version or checksum are wrong 生存时间\r#\r\rEXPIRE\r#\r\r为指定的 key 设置生存时间。当生存时间为 0 时，key 会自动删除。 key 设置生存时间后，可以再次执行 EXPIRE 命令更新生存时间。 注意对 key 的值进行修改甚至使用 RENAME 对 key 进行重命名时，都不会修改 key 的生存时间\nEXPIRE key seconds 如果 key 不存在或者不能设置生存时间时，返回 0。\nredis\u0026gt; SET cache_page \u0026#34;www.google.com\u0026#34; OK # 设置过期时间为 30 秒 redis\u0026gt; EXPIRE cache_page 30 (integer) 1 # 查看剩余生存时间 redis\u0026gt; TTL cache_page (integer) 23 # 更新过期时间 redis\u0026gt; EXPIRE cache_page 30000 (integer) 1 redis\u0026gt; TTL cache_page (integer) 29996 EXPIREAT\r#\r\r与 EXPIRE 命令类似，不同的是 EXPIREAT 设置的生存时间是 UNIX 时间戳，以秒为单位。\nEXPIREAT key timestamp 如果 key 不存在返回 0。\nredis\u0026gt; SET mykey \u0026#34;Hello\u0026#34; OK redis\u0026gt; EXISTS mykey (integer) 1 redis\u0026gt; EXPIREAT mykey 1293840000 (integer) 1 redis\u0026gt; EXISTS mykey (integer) 0 PERSIST\r#\r\r移除 key 的生存时间，将 key 持久化(永不过期的 key)。\n# 设置一个 key redis\u0026gt; SET mykey \u0026#34;Hello\u0026#34; OK # 为 key 设置生存时间 redis\u0026gt; EXPIRE mykey 10 (integer) 1 redis\u0026gt; TTL mykey (integer) 10 # 移除 key 的生存时间 redis\u0026gt; PERSIST mykey (integer) 1 redis\u0026gt; TTL mykey (integer) -1 PERSISTAT\r#\r\r与 PERSIST 命令类似，不同的是它以毫秒为单位设置 key 的过期 UNIX 时间戳。\nPEXPIREAT key milliseconds-timestamp 如果 key 不存在返回 0。\nredis\u0026gt; SET mykey \u0026#34;Hello\u0026#34; OK redis\u0026gt; PEXPIREAT mykey 1555555555005 (integer) 1 redis\u0026gt; TTL mykey (integer) 192569170 redis\u0026gt; PTTL mykey (integer) 192569169649 TTL\r#\r\r获取指定 key 的剩余生存时间。\nTTL key 如果 key 不存在时返回 -2，如果 key 但没有生存时间时，返回 -1。\n# 不存在的 key redis\u0026gt; FLUSHDB OK redis\u0026gt; TTL key (integer) -2 # key 存在，但没有设置剩余生存时间 redis\u0026gt; SET key value OK redis\u0026gt; TTL key (integer) -1 # 有剩余生存时间的 key redis\u0026gt; EXPIRE key 10086 (integer) 1 redis\u0026gt; TTL key (integer) 10084 PTTL\r#\r\r与 TTL 命令类似，不同的是剩余生存时间以毫秒为单位。\nPTTL key 如果 key 不存在返回 0。\n# 不存在的 key redis\u0026gt; FLUSHDB OK redis\u0026gt; PTTL key (integer) -2 # key 存在，但没有设置剩余生存时间 redis\u0026gt; SET key value OK redis\u0026gt; PTTL key (integer) -1 # 有剩余生存时间的 key redis\u0026gt; PEXPIRE key 10086 (integer) 1 redis\u0026gt; PTTL key (integer) 6179 迁移\r#\r\rMIGRATE\r#\r\r将指定 key 从当前实例迁移到到目标实例，并从当前实例删除。原子操作，由于 Redis 是单线程，所以该指令会造成阻塞，直到迁移完成， 失败或者超时。\nMIGRATE host port key destination-db timeout [COPY] [REPLACE]  timeout，超时时间，以毫秒为单位。 Redis 会在指定时间内完成 IO 操作，如果传送时间内发送 IO 错误或达到了超时时间，命令就会停止，并 返回一个 IOERR 错误。 可选参数： COPY：不移除源实例上的 key。 REPLACE：替换目标实例上已存在的 key。 迁移流程：   源实例执行 DUMP 命令进行序列化，并将序列化数据传送到目标实例。 目标实例使用 RESTORE 命令进行反序列化，并存储数据。 当前实例和目标实例一样，收到 RESTORE 命令返回的 ok，当前实例就执行 DEL 命令删除 key。  #启动实例，使用默认的 6379 端口 $ ./redis-server \u0026amp; [1] 3557 #启动实例，使用 7777 端口 $ ./redis-server --port 7777 \u0026amp; [2] 3560 #连接 6379 端口的实例 $ ./redis-cli redis\u0026gt; flushdb OK redis\u0026gt; SET greeting \u0026#34;Hello from 6379 instance\u0026#34; OK redis\u0026gt; MIGRATE 127.0.0.1 7777 greeting 0 1000 OK # 迁移成功后 key 会被删除 redis\u0026gt; EXISTS greeting (integer) 0 #查看 7777 端口的实例 $ ./redis-cli -p 7777 redis 127.0.0.1:7777\u0026gt; GET greeting \u0026#34;Hello from 6379 instance\u0026#34; MOVE\r#\r\r移动当前数据库中指定的 key 到指定数据库 db 中，MOVE 指令是在同一个实例中的迁移。\nMOVE key db 如果源数据库中 key 不存在，或者目标数据库中存在相同的 key，MOVE 命令无效。\n# key 存在于当前数据库 redis\u0026gt; SELECT 0 # redis默认使用数据库 0，为了清晰起见，这里再显式指定一次。 OK redis\u0026gt; SET song \u0026#34;secret base - Zone\u0026#34; OK redis\u0026gt; MOVE song 1 # 将 song 移动到数据库 1 (integer) 1 redis\u0026gt; EXISTS song # song 已经被移走 (integer) 0 redis\u0026gt; SELECT 1 # 使用数据库 1 OK redis:1\u0026gt; EXISTS song # 证实 song 被移到了数据库 1 (注意命令提示符变成了\u0026#34;redis:1\u0026#34;，表明正在使用数据库 1) (integer) 1 # 当 key 不存在的时候 redis:1\u0026gt; EXISTS fake_key (integer) 0 redis:1\u0026gt; MOVE fake_key 0 # 试图从数据库 1 移动一个不存在的 key 到数据库 0，失败 (integer) 0 redis:1\u0026gt; select 0 # 使用数据库0 OK redis\u0026gt; EXISTS fake_key # 证实 fake_key 不存在 (integer) 0 # 当源数据库和目标数据库有相同的 key 时 redis\u0026gt; SELECT 0 # 使用数据库0 OK redis\u0026gt; SET favorite_fruit \u0026#34;banana\u0026#34; OK redis\u0026gt; SELECT 1 # 使用数据库1 OK redis:1\u0026gt; SET favorite_fruit \u0026#34;apple\u0026#34; OK redis:1\u0026gt; SELECT 0 # 使用数据库0，并试图将 favorite_fruit 移动到数据库 1 OK redis\u0026gt; MOVE favorite_fruit 1 # 因为两个数据库有相同的 key，MOVE 失败 (integer) 0 redis\u0026gt; GET favorite_fruit # 数据库 0 的 favorite_fruit 没变 \u0026#34;banana\u0026#34; redis\u0026gt; SELECT 1 OK redis:1\u0026gt; GET favorite_fruit # 数据库 1 的 favorite_fruit 也是 \u0026#34;apple\u0026#34; SCAN\r#\r\r迭代当前数据库中的数据库键。\nSCAN cursor [MATCH pattern] [COUNT count] 选项:\n cursor，整数值，游标参数。 MATCH，指定正则匹配模式，对元素的模式匹配工作是在命令从数据集中取出元素之后， 向客户端返回元素之前的这段时间内进行的， 所以如果被迭代的数据集中只有少量元素和模式相匹配， 那么迭代命令或许会在多次执行中都不返回任何元素。 COUNT，指定每次迭代中从数据集里返回的元素数量，默认值为 10。COUNT 只是一个 hint，返回的结果可多可少。  相关命令：\n HSCAN，迭代哈希类型中的键值对。 SSCAN，迭代集合中的元素。 ZSCAN，迭代有序集合中的元素（包括元素成员和元素分值）。 SSCAN，HSCAN和ZSCAN与SCAN都返回一个包含两个元素的 multi-bulk 回复，第一个元素是游标，第二个元素也是一个 multi-bulk 回复，包含了本次被迭代的元素。 SSCAN，HSCAN 和 ZSCAN 与 SCAN 类似，不同的是这三个命令的的第一个参数是一个数据库键。 SCAN 它迭代的是当前数据库中的所有数据库键，所以不需要提供数据库键。 SSCAN，HSCAN 和 ZSCAN 与 SCAN 的返回值也不相同：  SCAN 返回的每个元素都是一个数据库键。 SSCAN 返回的每个元素都是一个集合成员。 HSCAN 返回的每个元素都是一个键值对，一个键值对由一个键和一个值组成。 ZSCAN 返回的每个元素都是一个有序集合元素，一个有序集合元素由一个成员（member）和一个分值（score）组成。    SCAN 是一个基于游标的迭代器：SCAN 每次被调用之后，都会向用户返回一个新的游标，用户在下次迭代时需要使用这个新游标作为 SCAN 的游 标参数，以此来延续之前的迭代过程。\n当 SCAN 命令的游标参数被设置为 0 时，服务器将开始一次新的迭代，而当服务器向用户返回值为 0 的游标时，表示迭代已结束。 当一个数据集不断地变大时，想要访问这个数据集中的所有元素就需要做越来越多的工作，能否结束一个迭代取决于用户执行迭代的速度是否比数据集 增长的速度更快。\n对于 SCAN 这类增量式迭代命令来说，因为在对键进行增量式迭代的过程中，键可能会被修改，所以增量式迭代命令只能对被返回的元素提供有限的 保证（offer limited guarantees about the returned elements）。\n注意返回的结果可能会有重复，需要客户端去重复。\nSCAN 对比 KEYS\r#\r\r 复杂度虽然也是 O(n)，但是它是通过游标分步进行的，不会阻塞线程; 提供 limit 参数，可以控制每次返回结果的最大条数，limit 只是一个 hint，返回的结果可多可少; 同 keys 一样，它也提供模式匹配功能; 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数; 返回的结果可能会有重复，需要客户端去重复; 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的; 单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零;  redis 127.0.0.1:6379\u0026gt; scan 0 1) \u0026#34;17\u0026#34; 2) 1) \u0026#34;key:12\u0026#34; 2) \u0026#34;key:8\u0026#34; 3) \u0026#34;key:4\u0026#34; 4) \u0026#34;key:14\u0026#34; 5) \u0026#34;key:16\u0026#34; 6) \u0026#34;key:17\u0026#34; 7) \u0026#34;key:15\u0026#34; 8) \u0026#34;key:10\u0026#34; 9) \u0026#34;key:3\u0026#34; 10) \u0026#34;key:7\u0026#34; 11) \u0026#34;key:1\u0026#34; redis 127.0.0.1:6379\u0026gt; scan 17 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;key:5\u0026#34; 2) \u0026#34;key:18\u0026#34; 3) \u0026#34;key:0\u0026#34; 4) \u0026#34;key:2\u0026#34; 5) \u0026#34;key:19\u0026#34; 6) \u0026#34;key:13\u0026#34; 7) \u0026#34;key:6\u0026#34; 8) \u0026#34;key:9\u0026#34; 9) \u0026#34;key:11\u0026#34; # 使用 MATCH redis 127.0.0.1:6379\u0026gt; sadd myset 1 2 3 foo foobar feelsgood (integer) 6 redis 127.0.0.1:6379\u0026gt; sscan myset 0 match f* 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;foo\u0026#34; 2) \u0026#34;feelsgood\u0026#34; 3) \u0026#34;foobar\u0026#34; # 匹配不到元素 redis 127.0.0.1:6379\u0026gt; scan 0 MATCH *11* 1) \u0026#34;288\u0026#34; 2) 1) \u0026#34;key:911\u0026#34; redis 127.0.0.1:6379\u0026gt; scan 288 MATCH *11* 1) \u0026#34;224\u0026#34; 2) (empty list or set) redis 127.0.0.1:6379\u0026gt; scan 224 MATCH *11* 1) \u0026#34;80\u0026#34; 2) (empty list or set) redis 127.0.0.1:6379\u0026gt; scan 80 MATCH *11* 1) \u0026#34;176\u0026#34; 2) (empty list or set) # cursor 值为 0 遍历结束 redis 127.0.0.1:6379\u0026gt; scan 176 MATCH *11* COUNT 1000 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;key:611\u0026#34; 2) \u0026#34;key:711\u0026#34; 3) \u0026#34;key:118\u0026#34; 4) \u0026#34;key:117\u0026#34; 5) \u0026#34;key:311\u0026#34; 6) \u0026#34;key:112\u0026#34; 7) \u0026#34;key:111\u0026#34; 8) \u0026#34;key:110\u0026#34; 9) \u0026#34;key:113\u0026#34; 10) \u0026#34;key:211\u0026#34; 11) \u0026#34;key:411\u0026#34; 12) \u0026#34;key:115\u0026#34; 13) \u0026#34;key:116\u0026#34; 14) \u0026#34;key:114\u0026#34; 15) \u0026#34;key:119\u0026#34; 16) \u0026#34;key:811\u0026#34; 17) \u0026#34;key:511\u0026#34; 18) \u0026#34;key:11\u0026#34; 上面的示例中提供的 limit 是 1000，但是返回的结果只有 18 个。因为这个 limit 不是限定返回结果的数量，而是限定服务器单次遍历的字典 槽位数量(约等于)。如果将 limit 设置为 10，发现返回结果是空的，但是游标值不为零，意味着遍历还没结束。\n字典结构\r#\r\rRedis 中所有的 key 都存储在一个很大的字典中，这个字典的结构是一维数组 + 二维链表结构，第一维数组的大小总是 2^n(n\u0026gt;=0)，扩容一次数组 大小空间加倍，也就是 n++。\nscan 指令返回的游标就是一维数组的位置索引，将这个位置索引称为槽 (slot)。如果不考虑字典的扩容缩容，直接按数组下标挨个遍历就 行了。limit 参数就表示需要遍历的槽位数，之所以返回的结果可能多可能少，是因为不是所有的槽位上都会挂接链表，有些槽位可能是空的，还 有些槽位上挂接的链表上的元素可能会有多个。每一次遍历都会将 limit 数量的槽位上挂接的所有链表元素进行模式匹配过滤后，一次性返回给客户端。\nscan 遍历顺序\r#\r\rscan 的遍历顺序不是从第一维数组的第 0 位一直遍历到末尾，而是采用了高位进位加法来遍历。之所以使用这样特殊的方式进行遍历，是考虑 到字典的扩容和缩容时避免槽位的遍历重复和遗漏。\n大 key 扫描\r#\r\r如果在 Redis 实例中形成很大的对象，比如一个很大的 hash，一个很大的 zset。这样的对象对 Redis 的集群数据迁移带来了很大的问题，因为在 集群环境下，如果某个 key 太大，会数据导致迁移卡顿。另外在内存分配上，如果一个 key 太大，那么当它需要扩容时，会一次性申请更大的一 块内存，这也会导致卡顿。如果这个大 key 被删除，内存会一次性回收，卡顿现象会再一次产生。\n尽量避免大 key 的产生。\n如果观察到 Redis 的内存大起大落，这极有可能是因为大 key 导致的。\n那如何定位大 key ？\r#\r\r用 scan 指令，对于扫描出来的每一个 key，使用 type 指令获得 key 的类型，然后使用相应数据结构的 size 或者 len 方法来得到它的大小，对 于每一种类型，保留大小的前 N 名作为扫描结果展示出来。\n上面这样的过程需要编写脚本，比较繁琐，不过 Redis 官方已经在 redis-cli 指令中提供了这样的扫描功能。\nredis-cli -h 127.0.0.1 -p 7001 –-bigkeys 如果你担心这个指令会大幅抬升 Redis 的 ops 导致线上报警，还可以增加一个休眠参数。\nredis-cli -h 127.0.0.1 -p 7001 –-bigkeys -i 0.1 上面这个指令每隔 100 条 scan 指令就会休眠 0.1s，ops 就不会剧烈抬升，但是扫描的时间会变长。\n"});index.add({'id':16,'href':'/db-learn/docs/redis/01_getting-started/','title':"Redis 入门",'content':"Redis 入门\r#\r\r\rRedis 中文官网的介绍：\nRedis（Remote Dictionary Service）是目前互联网技术领域使用最为广泛的存储中间件，它是一个开源（BSD 许可）的，内存中的数据结构存储系 统，它可以用作数据库、缓存和消息中间件。它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询，bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA 脚本（Lua scripting），LRU 驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁 盘持久化（persistence），并通过 Redis 哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。\n数据类型\r#\r\rRedis 一共支持 5 种数据类型：\n \r字符串(Strings) \r哈希(Hashs) \r列表(Lists) \r集合(Sets) \r有序集合(SortedSets)  String（字符串）\r#\r\rString 类型是最常用，也是最简单的的一种类型，string 类型是二进制安全的。也就是说 string 可以包含任何数据。比如 jpg 图片 或者 序列化的对象 。一个键最大能存储 512MB。\nredis\u0026gt; set testkey hello OK redis\u0026gt; get testkey \u0026#34;hello\u0026#34; Hash（哈希）\r#\r\rRedis 对 JSON 数据的支持不是很友好。通常把 JSON 转成 String 存储到 Redis 中，但现在的 JSON 数据都是连环嵌套的，每次更新 时都要先获取整个 JSON，然后更改其中一个字段再放上去。这种使用方式，如果在海量的请求下，JSON 字符串比较复杂，会导致在频繁更新数 据使网络 I/O 跑满，甚至导致系统超时、崩溃。所以 Redis 官方推荐采用哈希保存对象。\n HSET 设置哈希类型的值 HGET 获取单个哈希属性值 HGETALL 获取所有哈希属性和值  redis\u0026gt; HSET xiaoming age 18 (integer) 1 redis\u0026gt; HSET xiaoming phone 15676666666 (integer) 1 redis\u0026gt; HMSET xiaoqiang age 18 phone 13816666666 OK redis\u0026gt; HGET xiaoming age \u0026#34;18\u0026#34; redis\u0026gt; HGETALL xiaoming 1)\u0026#34;age\u0026#34; 2)\u0026#34;18\u0026#34; 3)\u0026#34;phone\u0026#34; 4)\u0026#34;15676666666\u0026#34; redis\u0026gt; HGETALL xiaoqiang 1)\u0026#34;age\u0026#34; 2)\u0026#34;18\u0026#34; 3)\u0026#34;phone\u0026#34; 4)\u0026#34;13816666666\u0026#34; List（列表）\r#\r\rRedis 列表是简单的字符串列表，并根据插入顺序进行排序。一个 Redis 列表中最多可存储 232-1 (40 亿)个元素。\n LPUSH 向列表的开头插入新元素， RPUSH 向列表的结尾插入新元素。 LPOP 返回并移除列表头部的元素 RPOP 返回并移除列表尾部的元素。 LSET 对列表中指定索引位的元素进行操作。LSET 不允许对不存在的列表进行操作。 LINDEX 获取列表指定索引位的元素  redis\u0026gt; LPUSH testlist one (integer) 1 redis\u0026gt; RPUSH testlist two (integer) 2 redis\u0026gt; LSET testlist 0 1 OK redis\u0026gt; LINDEX testlist 0 \u0026#34;1\u0026#34; redis\u0026gt; LPOP testlist \u0026#34;1\u0026#34; redis\u0026gt; RPOP testlist \u0026#34;two\u0026#34; redis\u0026gt; lindex testlist 0 (nil) Set（集合）\r#\r\rRedis 的 Set 是 string 类型的无序集合。集合中不允许重复成员的存在。一个 Redis 集合中最多可包含 232-1(40 亿)个元素。\n SADD 向集合中插入值 SMEMBERS 获取集合中的元素 SPOP 随机获取并删除一个值  redis\u0026gt; sadd class xiaoming xiaoqiang xiaogang (integer) 3 redis\u0026gt; smembers class 1) \u0026#34;xiaoming\u0026#34; 2) \u0026#34;xiaoqiang\u0026#34; 3) \u0026#34;xiaogang\u0026#34; redis\u0026gt; spop class \u0026#34;xiaoqiang\u0026#34; redis\u0026gt; smembers class 1) \u0026#34;xiaoming\u0026#34; 2) \u0026#34;xiaogang\u0026#34; 集合间的操作:\n SINTER 查询集合间的交集 SUNION获取集间的并集 SDIFF 获取集合间的差集 SMOVE 把集合中的元素从一个集合移到另一个集中  redis\u0026gt; sadd class1 xiaoming xiaoqiang xiaogang (integer) 1 redis\u0026gt; sadd class2 xiaoming xiaoli (integer) 1 redis\u0026gt; sinter class1 class2 1) \u0026#34;xiaoming\u0026#34; redis\u0026gt; sunion class1 class2 1) \u0026#34;xiaoming\u0026#34; 2) \u0026#34;xiaoqiang\u0026#34; 3) \u0026#34;xiaogang\u0026#34; 4) \u0026#34;xiaoming\u0026#34; 5) \u0026#34;xiaoli\u0026#34; redis\u0026gt; sdiff class1 class2 1) \u0026#34;xiaoqiang\u0026#34; 2) \u0026#34;xiaogang\u0026#34; redis\u0026gt; smove class1 class2 xiaoqiang (integer) 0 redis\u0026gt; smembers class2 1) \u0026#34;xiaoming\u0026#34; 2) \u0026#34;xiaoqiang\u0026#34; 3) \u0026#34;xiaoli\u0026#34; zset (sorted set, 有序集合)\r#\r\rRedis 的 zset 和 set 一样也是 string 类型元素的集合，且不允许重复的成员。不同的是每个元素都会关联一个 double 类型的 分数。Redis 正是通过分数来为集合中的成员进行从小到大的排序。zset 的成员是唯一的,但分数(score)却可以重复。\n ZADD 添加元素到集合，元素在集合中存在则更新对应 score ZRANGE 集合指定范围内的元素 ZRANK 获取指定成员的排名 ZSCORE 返回元素的权重(score)值  ZADD key score member redis\u0026gt; ZADD class 1 xiaoming (integer) 1 redis\u0026gt; ZADD class 2 xiaoqiang 3 xiaogang (integer) 2 redis\u0026gt; ZRANGE class 1 2 1) \u0026#34;xiaoming\u0026#34; 2) \u0026#34;xiaoqiang\u0026#34; redis\u0026gt; ZRANK class xiaogang (integer) 2 redis\u0026gt; ZSCORE class xiaogang \u0026#34;3\u0026#34; 容器型数据结构\r#\r\rlist/set/hash/zset 这四种都属于容器型数据结构，他们有两条通用规则：\n 如果容器不存在，那就创建一个，再进行操作。比如 RPUSH，如果列表不存在，Redis 就会自动创建一个，然后再执行 RPUSH。 如果容器里元素没有了，那么立即删除 key，释放内存。比如 LPOP 操作到最后一个元素，列表 key 就会自动删除。  "});index.add({'id':17,'href':'/db-learn/docs/redis/02_redis-config/','title':"Redis 安装配置",'content':"Redis 安装配置\r#\r\rRedis 安装，配置认证密码，配置 service 服务。\n安装\r#\r\r#下载 wget http://download.redis.io/releases/redis-x.x.x.tar.gz #解压 tar -xzvf redis-x.x.x.tar.gz # 编译安装 cd redis-x.x.x make make install make install 会在 /usr/local/bin 目录下生成以下文件：\n redis-server：Redis 服务器端启动程序 redis-cli：Redis 客户端操作工具。也可以用 telnet 根据其纯文本协议来操作 redis-benchmark：Redis 性能测试工具 redis-check-aof：数据修复工具 redis-check-dump：检查导出工具  如果出现以下错误：\nmake[1]: Entering directory `/root/redis/src\u0026#39; You need tcl 8.5 or newer in order to run the Redis test …… 这是因为没有安装 tcl 导致，yum 安装即可：\nyum install tcl 配置 Redis 复制配置文件到 /etc/ 目录：\ncp redis.conf /etc/ vim /etc/redis.conf #找到下面的内容 ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use \u0026#39;yes\u0026#39; if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. # NOT SUPPORTED ON WINDOWS daemonize no #修改daemonize配置项为yes，使Redis进程在后台运行 daemonize yes 启动 Redis\ncd /usr/local/bin ./redis-server /etc/redis.conf #检查是否启动成功： ps -ef | grep redis 一些常用的 Redis 启动参数：\n daemonize：是否以后台 daemon 方式运行 pidfile：pid 文件位置 port：监听的端口号 timeout：请求超时时间 loglevel：log 信息级别 logfile：log 文件位置 databases：开启数据库的数量，默认 16 个。 save * *：保存快照的频率，第一个 * 表示多长时间，第三个 * 表示执行多少次写操作。在一定时间内执行一定数量的写操作时，自动保存 快照。可设置多个条件。 rdbcompression：是否使用压缩 dbfilename：数据快照文件名（只是文件名） dir：数据快照的保存目录（仅目录） appendonly：是否开启 appendonlylog，开启的话每次写操作会记一条 log，这会提高数据抗风险能力，但影响效率。 appendfsync：appendonlylog 如何同步到磁盘。三个选项，分别是每次写都强制调用 fsync、每秒启用一次 fsync、不调用 fsync 等 待系统自己同步  配置密码认证登录\r#\r\rRedis 默认配置是不需要密码认证，需要手动配置启用 Redis 的认证密码，增加 Redis 的安全性。\n修改配置\r#\r\rlinux 下 Redis 的默认配置文件默认在 /etc/redis.conf。windows 下则是安装目录下的 redis.windows.conf 文件。 打开配置文件，找到下面的内容：\n################################## SECURITY ###################################\r# Require clients to issue AUTH \u0026lt;PASSWORD\u0026gt; before processing any other\r# commands. This might be useful in environments in which you do not trust\r# others with access to the host running redis-server.\r#\r# This should stay commented out for backward compatibility and because most\r# people do not need auth (e.g. they run their own servers).\r#\r# Warning: since Redis is pretty fast an outside user can try up to\r# 150k passwords per second against a good box. This means that you should\r# use a very strong password otherwise it will be very easy to break.\r#\r#requirepass foobared\r去掉前面的注释，并修改为你的认证密码：\nrequirepass {your password}\r修改后重启 Redis：\n# 如果已经配置为 service 服务 systemctl restart redis # 或者 /usr/local/bin/redis-cli shutdown /usr/local/bin/redis-server /etc/redis.conf 登录验证\r#\r\r重启后登录时需要使用 -a 参数输入密码，否则登录后没有任何操作权限。如下：\n./redis-cli -h 127.0.0.1 -p 6379 127.0.0.1:6379\u0026gt; set testkey (error) NOAUTH Authentication required. 使用密码认证登录：\n./redis-cli -h 127.0.0.1 -p 6379 -a myPassword 127.0.0.1:6379\u0026gt; set testkey hello OK 或者在连接后进行验证：\n./redis-cli -h 127.0.0.1 -p 6379 127.0.0.1:6379\u0026gt; auth yourpassword OK 127.0.0.1:6379\u0026gt; set testkey hello OK 客户端配置密码\r#\r\r127.0.0.1:6379\u0026gt; config set requirepass yourpassword OK 127.0.0.1:6379\u0026gt; config get requirepass 1) \u0026#34;requirepass\u0026#34; 2) \u0026#34;yourpassword\u0026#34;  注意：使用客户端配置密码，重启 Redis 后仍然会使用 redis.conf 配置文件中的密码。\n 在集群中配置认证密码\r#\r\r如果 Redis 使用了集群。除了在 master 中配置密码外，slave 中也需要配置。在 slave 的配置文件中找到如下行，去掉注释并修改 为与 master 相同的密码：\n# masterauth your-master-password\rRedis 配置到系统服务(systemd)\r#\r\r创建 redis.service 文件\r#\r\r进入 /usr/lib/systemd/system 目录，创建 redis.service 文件：\n[Unit]\r# 描述\rDescription=Redis\r# 启动时机,开机启动最好在网络服务启动后即启动\rAfter=syslog.target network.target remote-fs.target nss-lookup.target\r# 表示服务信息\r[Service]\rType=forking\r# 注意：需要和 redis.conf 配置文件中的信息一致\rPIDFile=/var/run/redis.pid\r# 启动服务的命令\r# redis-server 安装的路径和 redis.conf 配置文件的路径\rExecStart=/usr/local/bin/redis-server /etc/redis.conf\r# 停止服务的命令\rExecStop=/usr/local/bin/redis-cli shutdown\rRestart=always\r# 安装相关信息\r[Install]\r# 启动方式\r# multi-user.target 表明当系统以多用户方式（默认的运行级别）启动时，这个服务需要被自动运行。\rWantedBy=multi-user.target\rType=forking，forking 表示服务管理器是系统 init 的子进程，用于管理需要后台运行的服务。\n修改 /etc/redis.conf：\ndaemonize yes\rsupervised systemd\r# 使配置生效 systemctl daemon-reload # 设置开机启动 systemctl enable redis.service # 启动 systemctl start redis.service # 查看状态 systemctl status redis.service # 重启 systemctl restart redis.service # 停止 systemctl stop redis.service # 关闭开机启动 systemctl disable redis.service "});index.add({'id':18,'href':'/db-learn/docs/redis/04_redis-hash/','title':"Redis 数据类型 Hash",'content':"Redis 对 JSON 数据的支持不是很友好。通常把 JSON 转成 String 存储到 Redis 中，但现在的 JSON 数据都是连环嵌套的，每次更新 时都要先获取整个 JSON，然后更改其中一个字段再放上去。这种使用方式，如果在海量的请求下，JSON 字符串比较复杂，会导致在频繁更新数 据使网络I/O跑满，甚至导致系统超时、崩溃。所以 Redis 官方推荐采用 Hash(字典)保存对象。但是 Hash 结构的存储消耗要高于单个字符串。\nRedis 的字典和 Java的 HashMap 类似，它是无序字典。包括内部结构的实现也和 HashMap 也是一致的，同样的数组 + 链表二维结构。\n存取\r#\r\rHSET\r#\r\r设置字典 key 值的 field 字段值为 value。\nHSET key field value 如果 key 不存在，创建 key 并进行 HSET 操作。 如果 field 不存在，则增加新字段，设置成功，返回 1。如果 field 已存在，覆盖旧值，返回 0。\nredis\u0026gt; hset student name xiaoming (integer) 1 redis\u0026gt; hget student name \u0026#34;xiaoming\u0026#34; redis\u0026gt; hset student age 18 (integer) 0 redis\u0026gt; hget student age \u0026#34;18\u0026#34; HSETNX\r#\r\r和 HSET 一样，但是只在字段 field 不存时才会设置。设置成功，返回 1。失败，返回 0。\nHSETNX key field value 如果 field 字段已经存在，该操作无效，返回 0。\nredis\u0026gt; HSETNX student phone 16790624749 (integer) 1 redis\u0026gt; HSETNX student phone 16790624778 (integer) 0 HGET\r#\r\r获取指定字段 field 的值。\nHGET key field value key 存在且 field 存在则返回其值，否则返回 nil。\nredis\u0026gt; HGET student address (nil) redis\u0026gt; HGET student phone \u0026#34;16790624749\u0026#34; HGETALL\r#\r\r获取 key 所有字段的值。\nHGETALL key 以列表形式返回哈希表中的字段和值。若哈希表不存在，否则返回一个空列表。\nredis\u0026gt; HSET people jack \u0026#34;Jack Sparrow\u0026#34; (integer) 1 redis\u0026gt; HSET people gump \u0026#34;Forrest Gump\u0026#34; (integer) 1 redis\u0026gt; HGETALL people 1) \u0026#34;jack\u0026#34; # 域 2) \u0026#34;Jack Sparrow\u0026#34; # 值 3) \u0026#34;gump\u0026#34; 4) \u0026#34;Forrest Gump\u0026#34; 批量操作\r#\r\rHash 和 String 一样，支持操作多个字段。\nHMSET\r#\r\r将一个或多个 field/value 对设置到哈希表 key。\nHMSET key field value [field value ...] 如果 key 不存在，创建 key 并进行 HMSET 操作。 如果 field 不存在，则增加新字段，设置成功，返回 1。如果 field 已存在，覆盖旧值，返回 0。\nredis\u0026gt; HMSET website google www.google.com yahoo www.yahoo.com OK redis\u0026gt; HGET website google \u0026#34;www.google.com\u0026#34; redis\u0026gt; HGET website yahoo \u0026#34;www.yahoo.com\u0026#34; HMGET\r#\r\r返回 key 中一个或多个指定字段的值。\nHMGET key field [field ...] 返回一个列表，如果指定的字段在不存在，则返回一个 nil。如果 key 不存在将返回一个只带有 nil 值的表。如果 key 为非 Hash结构， 则返回一个错误。\nredis\u0026gt; HMSET pet dog \u0026#34;doudou\u0026#34; cat \u0026#34;nounou\u0026#34; OK redis\u0026gt; HMGET pet dog cat fake_pet 1) \u0026#34;doudou\u0026#34; 2) \u0026#34;nounou\u0026#34; 3) (nil) # 不存在,返回nil值 # 非Hash结构 redis\u0026gt; set istring \u0026#34;Hello\u0026#34; OK redis\u0026gt; hmget istring notexsitfield (error) WRONGTYPE Operation against a key holding the wrong kind of value 自增\r#\r\rHINCRBY\r#\r\r为 key 中的指定字段 field 增加一个增量 increment。\nHINCRBY key field increment increment 可以为负数，相当于对进行减法操作。 如果 key 不存在，创建 key，并执行 HINCRBY 操作。如果指定 field 不存在，先初始化为 0，再执行 HINCRBY。如果为非数字 值执行 HINCRBY 操作，则返回一个错误。 本操作的值被限制在 64 位(bit)有符号数字表示之内。\n# increment 为正数 redis\u0026gt; HEXISTS counter page_view # 对空域进行设置 (integer) 0 redis\u0026gt; HINCRBY counter page_view 200 (integer) 200 redis\u0026gt; HGET counter page_view \u0026#34;200\u0026#34; # increment 为负数 redis\u0026gt; HGET counter page_view \u0026#34;200\u0026#34; redis\u0026gt; HINCRBY counter page_view -50 (integer) 150 redis\u0026gt; HGET counter page_view \u0026#34;150\u0026#34; # 尝试对字符串值的域执行HINCRBY命令 redis\u0026gt; HSET myhash string hello,world # 设定一个字符串值 (integer) 1 redis\u0026gt; HGET myhash string \u0026#34;hello,world\u0026#34; redis\u0026gt; HINCRBY myhash string 1 # 命令执行失败，错误。 (error) ERR hash value is not an integer redis\u0026gt; HGET myhash string # 原值不变 \u0026#34;hello,world\u0026#34; HINCRBYFLOAT\r#\r\r与 HINCRBY 一样，不同的是 HINCRBYFLOAT 是为指定字段 field 增加一个浮点数增量 increment。\n# 值和增量都是普通小数 redis\u0026gt; HSET mykey field 10.50 (integer) 1 redis\u0026gt; HINCRBYFLOAT mykey field 0.1 \u0026#34;10.6\u0026#34; # 值和增量都是指数符号 redis\u0026gt; HSET mykey field 5.0e3 (integer) 0 redis\u0026gt; HINCRBYFLOAT mykey field 2.0e2 \u0026#34;5200\u0026#34; # 对不存在的键执行 HINCRBYFLOAT redis\u0026gt; EXISTS price (integer) 0 redis\u0026gt; HINCRBYFLOAT price milk 3.5 \u0026#34;3.5\u0026#34; redis\u0026gt; HGETALL price 1) \u0026#34;milk\u0026#34; 2) \u0026#34;3.5\u0026#34; # 对不存在的字段进行 HINCRBYFLOAT redis\u0026gt; HGETALL price 1) \u0026#34;milk\u0026#34; 2) \u0026#34;3.5\u0026#34; redis\u0026gt; HINCRBYFLOAT price coffee 4.5 \u0026#34;4.5\u0026#34; redis\u0026gt; HGETALL price 1) \u0026#34;milk\u0026#34; 2) \u0026#34;3.5\u0026#34; 3) \u0026#34;coffee\u0026#34; 4) \u0026#34;4.5\u0026#34; 其他\r#\r\rHDEL\r#\r\r删除 key 中的一个或多个字段。\nHDEL key field [field ...] 不存在的字段将被忽略。返回被成功删除的字段数量。\nredis\u0026gt; HGETALL abbr 1) \u0026#34;a\u0026#34; 2) \u0026#34;apple\u0026#34; 3) \u0026#34;b\u0026#34; 4) \u0026#34;banana\u0026#34; 5) \u0026#34;c\u0026#34; 6) \u0026#34;cat\u0026#34; 7) \u0026#34;d\u0026#34; 8) \u0026#34;dog\u0026#34; # 删除单个域 redis\u0026gt; HDEL abbr a (integer) 1 # 删除不存在的域 redis\u0026gt; HDEL abbr not-exists-field (integer) 0 # 删除多个域 redis\u0026gt; HDEL abbr b c (integer) 2 redis\u0026gt; HGETALL abbr 1) \u0026#34;d\u0026#34; 2) \u0026#34;dog\u0026#34; HEXISTS\r#\r\r判断 key 中指定的 field 是否存在。\nHEXISTS key field 指定字段存在，返回 1。不存在，返回 0。\nredis\u0026gt; HEXISTS phone myphone (integer) 0 redis\u0026gt; HSET phone myphone nokia-1110 (integer) 1 redis\u0026gt; HEXISTS phone myphone (integer) 1 HLEN\r#\r\r返回 key 哈希表的长度，也就是所有字段的数量。\nHLEN key 哈希表存在，返回字段数。哈希表不存在，返回 0。\nredis\u0026gt; HSET db redis redis.com (integer) 1 redis\u0026gt; HSET db mysql mysql.com (integer) 1 redis\u0026gt; HLEN db (integer) 2 redis\u0026gt; HSET db mongodb mongodb.org (integer) 1 redis\u0026gt; HLEN db (integer) 3 HKEYS\r#\r\r返回 key 中的所有字段。\nHKEYS key 哈希表存在，返回字段列表。哈希表不存在，返回空列表。\n# 哈希表非空 redis\u0026gt; HMSET website google www.google.com yahoo www.yahoo.com OK redis\u0026gt; HKEYS website 1) \u0026#34;google\u0026#34; 2) \u0026#34;yahoo\u0026#34; # 空哈希表不存在 redis\u0026gt; EXISTS fake_key (integer) 0 redis\u0026gt; HKEYS fake_key (empty list or set) HVALS\r#\r\r与 HKEYS 对应，HVALS 返回 key 中的所有字段的值。\nHVALS key 哈希表存在，返回字段数。哈希表不存在，返回 0。\n# 非空哈希表 redis\u0026gt; HMSET website google www.google.com yahoo www.yahoo.com OK redis\u0026gt; HVALS website 1) \u0026#34;www.google.com\u0026#34; 2) \u0026#34;www.yahoo.com\u0026#34; # 空哈希表/不存在的key redis\u0026gt; EXISTS not_exists (integer) 0 redis\u0026gt; HVALS not_exists (empty list or set) HSTLEN\r#\r\r返回 key 中指定 field 的 value 的字符串长度。\nHSTLEN key field 如果key或者field不存在，返回0。\nredis\u0026gt; HMSET myhash f1 HelloWorld f2 99 f3 -256 OK redis\u0026gt; HSTRLEN myhash f1 (integer) 10 redis\u0026gt; HSTRLEN myhash f2 (integer) 2 redis\u0026gt; HSTRLEN myhash f3 (integer) 4 HSCAN\r#\r\r参考 SCAN 命令。\nHSCAN key cursor [MATCH pattern] [COUNT count] "});index.add({'id':19,'href':'/db-learn/docs/redis/07_redis-list/','title':"Redis 数据类型 List",'content':"Redis 数据类型 List\r#\r\rRedis 列表(Lists)是简单的字符串列表，并根据插入顺序进行排序。一个 Redis 列表中最多可存储 232-1(40亿)个元素。\nRedis 的列表和 Java 的 LinkedList 类似，注意它是链表而不是数组。这意味着 List 的插入和删除操作非常快，但是索引定位很慢。 当列表移除了最后一个元素之后，该 key 会被自动被删除，内存被回收。\nRedis 的列表结构常用来做异步队列使用。将需要延后处理的任务结构体序列化成字符串塞进列表，另一个线程从这个列表中读取数据进行处理。\n存取\r#\r\rLPUSH\r#\r\r将一个或多个值 value 插入到列表 key 的头部。\nLPUSH key value [value ...] 如果有多个 value，那么从左到右依次插入列表。如果 key 不存在，首先会创建一个空列表再执行 LPUSH 操作。 命令执行成功，返回列表的长度。如果 key 存在，但不是 List 类型，会返回一个错误。\n# 加入单个元素 redis\u0026gt; LPUSH languages python (integer) 1 # 加入重复元素 redis\u0026gt; LPUSH languages python (integer) 2 redis\u0026gt; LRANGE languages 0 -1 # 列表允许重复元素 1) \u0026#34;python\u0026#34; 2) \u0026#34;python\u0026#34; # 加入多个元素 redis\u0026gt; LPUSH mylist a b c (integer) 3 redis\u0026gt; LRANGE mylist 0 -1 1) \u0026#34;c\u0026#34; 2) \u0026#34;b\u0026#34; 3) \u0026#34;a\u0026#34; LPUSHX\r#\r\rLPUSHX 和 LPUSH 相同，不同的是，LPUSHX 一次只能插入一个 value，而且只有当 key 存在且是 List 类型时，才会将值 value 插入到列表 key 的头部。如果 key 不存在，则不执行操作。\nLPUSHX key value 命令执行成功，返回列表的长度。如果 key 存在，但不是 List 类型，会返回一个错误。\n# 对空列表执行 LPUSHX redis\u0026gt; LLEN greet (integer) 0 redis\u0026gt; LPUSHX greet \u0026#34;hello\u0026#34; # LPUSHX 失败，因为列表为空 (integer) 0 # 对非空列表执行 LPUSHX redis\u0026gt; LPUSH greet \u0026#34;hello\u0026#34; # 这次 LPUSHX 执行成功 (integer) 1 redis\u0026gt; LPUSHX greet \u0026#34;good morning\u0026#34; (integer) 2 redis\u0026gt; LRANGE greet 0 -1 1) \u0026#34;good morning\u0026#34; 2) \u0026#34;hello\u0026#34; # 非列表类型，返回错误 redis\u0026gt; set key value OK redis\u0026gt; lpush key xxx (error) WRONGTYPE Operation against a key holding the wrong kind of value RPUSH\r#\r\rRPUSH 是将一个或多个值 value 插入到列表 key 的尾部。\nRPUSH key value [value ...] 如果 key 不存在，会首先创建一个空列表，再执行 RPUSH。返回执行 RPUSH 后列表的长度。\n# 添加单个元素 redis\u0026gt; RPUSH languages c (integer) 1 # 添加重复元素 redis\u0026gt; RPUSH languages c (integer) 2 redis\u0026gt; LRANGE languages 0 -1 # 列表允许重复元素 1) \u0026#34;c\u0026#34; 2) \u0026#34;c\u0026#34; # 添加多个元素 redis\u0026gt; RPUSH mylist a b c (integer) 3 redis\u0026gt; LRANGE mylist 0 -1 1) \u0026#34;a\u0026#34; 2) \u0026#34;b\u0026#34; 3) \u0026#34;c\u0026#34; RPUSHX\r#\r\rRPUSHX 和 RPUSH 相同，不同的是，RPUSHX 一次只能插入一个 value，而且只有当 key 存在且是 List 类型时，才会将值 value 插入到列表 key 的尾部。如果 key 不存在，则不执行操作。\nRPUSHX key value 返回执行 RPUSH 后列表的长度。\n# key不存在 redis\u0026gt; LLEN greet (integer) 0 # 对不存在的 key 进行 RPUSHX，PUSH 失败。 redis\u0026gt; RPUSHX greet \u0026#34;hello\u0026#34; (integer) 0 # key 存在且是一个非空列表 # 先用 RPUSH 插入一个元素 redis\u0026gt; RPUSH greet \u0026#34;hi\u0026#34; (integer) 1 # greet 是一个列表类型，RPUSHX 操作成功 redis\u0026gt; RPUSHX greet \u0026#34;hello\u0026#34; (integer) 2 redis\u0026gt; LRANGE greet 0 -1 1) \u0026#34;hi\u0026#34; 2) \u0026#34;hello\u0026#34; LINSERT\r#\r\r将 value 插入 key 中指定 pivot 元素的前面或后面。如果 pivot 或 key 不存在则不执行任何操作。\nLINSERT key BEFORE|AFTER pivot value 操作成功，返回插入之后，列表的长度。pivot 不存在，返回 -1。如果 key 不存在或为空，则返回 0，如果 key 不是一个列表类型， 则返回一个错误。\nredis\u0026gt; RPUSH mylist \u0026#34;Hello\u0026#34; (integer) 1 redis\u0026gt; RPUSH mylist \u0026#34;World\u0026#34; (integer) 2 redis\u0026gt; LINSERT mylist BEFORE \u0026#34;World\u0026#34; \u0026#34;There\u0026#34; (integer) 3 redis\u0026gt; LRANGE mylist 0 -1 1) \u0026#34;Hello\u0026#34; 2) \u0026#34;There\u0026#34; 3) \u0026#34;World\u0026#34; # 对一个非空列表插入，查找一个不存在的 pivot redis\u0026gt; LINSERT mylist BEFORE \u0026#34;go\u0026#34; \u0026#34;let\u0026#39;s\u0026#34; (integer) -1 # 失败 # 对一个空列表执行 LINSERT 命令 redis\u0026gt; EXISTS fake_list (integer) 0 redis\u0026gt; LINSERT fake_list BEFORE \u0026#34;nono\u0026#34; \u0026#34;gogogog\u0026#34; (integer) 0 # 失败 LPOP\r#\r\r返回 key 列表中的头元素。\nLINSERT key BEFORE|AFTER pivot value 如果 key 不存在，则返回 nil。\nredis\u0026gt; LLEN course (integer) 0 redis\u0026gt; RPUSH course algorithm001 (integer) 1 redis\u0026gt; RPUSH course c++101 (integer) 2 redis\u0026gt; LPOP course # 移除头元素 \u0026#34;algorithm001\u0026#34; BLPOP\r#\r\rBLPOP 是 LPOP 类似，但是 BLPOP 在列表为空时，当前连接会被 BLPOP 阻塞，直到超时或有另一个客户端 PUSH 了可弹出的元素为止。 当指定多个 key 参数时，会按 key 的先后顺序依次检查各个列表，并弹出第一个非空列表的头元素。timeout 参数表示阻塞的时长，单位 为秒，注意如果 timeout 为 0，表示可以无限期延长阻塞。\nBLPOP key [key ...] timeout 如果列表为空，返回一个 nil。 否则，返回一个含有两个元素的列表，第一个元素是被弹出元素所属的 key，第二个元素是被弹出元素的值。\nBLPOP 命令的非阻塞行为\r#\r\r例如，现在有 job、command 和 request 三个列表，job 不存在，而 command 和 request 都是非空列表：\n# 确保key都被删除 redis\u0026gt; DEL job command request (integer) 0 # 为command列表增加一个值 redis\u0026gt; LPUSH command \u0026#34;update system...\u0026#34; (integer) 1 # 为request列表增加一个值 redis\u0026gt; LPUSH request \u0026#34;visit page\u0026#34; (integer) 1 # job 列表为空，被跳过，紧接着 command 列表的第一个元素被弹出。 redis\u0026gt; BLPOP job command request 0 1) \u0026#34;command\u0026#34; # 弹出元素所属列表的key 2) \u0026#34;update system...\u0026#34; # 弹出元素的值 BLPOP 命令的阻塞行为\r#\r\r当指定的所有 key 都不存在或包含空列表，BLPOP 命令将阻塞连接，直到等待超时，或有可弹出元素为止。\nRPOP\r#\r\r移除并返回 key 列表的尾元素。\nRPOP key 当 key 不存在时，返回 nil。\nedis\u0026gt; RPUSH mylist \u0026#34;one\u0026#34; (integer) 1 redis\u0026gt; RPUSH mylist \u0026#34;two\u0026#34; (integer) 2 redis\u0026gt; RPUSH mylist \u0026#34;three\u0026#34; (integer) 3 redis\u0026gt; RPOP mylist # 返回被弹出的元素 \u0026#34;three\u0026#34; redis\u0026gt; LRANGE mylist 0 -1 # 列表剩下的元素 1) \u0026#34;one\u0026#34; 2) \u0026#34;two\u0026#34; BRPOP\r#\r\rBRPOP 和 BLPOP 基本相同，不同点在于一个弹出头部元素，一个是尾部元素，而且会阻塞操作。\nBRPOP key [key ...] timeout 注意 BRPOP 弹出的元素，一样会被移除。\nredis\u0026gt; LLEN course (integer) 0 redis\u0026gt; RPUSH course algorithm001 (integer) 1 redis\u0026gt; RPUSH course c++101 (integer) 2 redis\u0026gt; BRPOP course 30 1) \u0026#34;course\u0026#34; # 弹出元素的 key 2) \u0026#34;c++101\u0026#34; # 弹出元素的值 LINDEX\r#\r\r返回列表 key 中下标为 index 的元素。\nLINDEX key index index 可以是负数，比如：-1 表时倒数第一个元素，-2 表时倒数第二个元素，以次类推。 如果 index 不在列表有效范围内，返回一个 nil。如果 key 不是列表类型，返回一个错误。\nredis\u0026gt; LPUSH mylist \u0026#34;World\u0026#34; (integer) 1 redis\u0026gt; LPUSH mylist \u0026#34;Hello\u0026#34; (integer) 2 redis\u0026gt; LINDEX mylist 0 \u0026#34;Hello\u0026#34; redis\u0026gt; LINDEX mylist -1 \u0026#34;World\u0026#34; # index不在 mylist 的区间范围内 redis\u0026gt; LINDEX mylist 3 (nil) LRANGE\r#\r\r返回列表 key 中指定区间内的元素。以偏移量 start 和 stop 指定的区间内的元素。\nLRANGE key start stop start 和 stop 索引位的元素都包含在取值范围内，比如执行 LRANGE list 0 10，结果是一个包含 11 个元素的列表。 start 和 stop 超出范围的下标值不会引起错误。 如果 start 大于最大下标值 end 则会返回一个空列表。如果 stop 大于最大下标值 end，会自动设置 stop 的值设置为 end。\nredis\u0026gt; RPUSH fp-language lisp (integer) 1 redis\u0026gt; LRANGE fp-language 0 0 1) \u0026#34;lisp\u0026#34; redis\u0026gt; RPUSH fp-language scheme (integer) 2 redis\u0026gt; LRANGE fp-language 0 1 1) \u0026#34;lisp\u0026#34; 2) \u0026#34;scheme\u0026#34; 修改列表元素\r#\r\rLSET\r#\r\r设置列表 key 中下标为 index 的元素值为 value。\nLSET key index value 如果 index 超出范围，或对一个空列表进行设置时，会返回错误。\n# 对空列表进行 LSET redis\u0026gt; EXISTS list (integer) 0 redis\u0026gt; LSET list 0 item (error) ERR no such key # 对非空列表进行 LSET redis\u0026gt; LPUSH job \u0026#34;cook food\u0026#34; (integer) 1 redis\u0026gt; LRANGE job 0 0 1) \u0026#34;cook food\u0026#34; redis\u0026gt; LSET job 0 \u0026#34;play game\u0026#34; OK redis\u0026gt; LRANGE job 0 0 1) \u0026#34;play game\u0026#34; # index 超出范围 redis\u0026gt; LLEN list (integer) 1 redis\u0026gt; LSET list 3 \u0026#39;out of range\u0026#39; (error) ERR index out of range RPOPLPUSH\r#\r\rRPOPLPUSH 是 RPOP 和 LPUSH 两个操作的合并，会执行两个原子操作：\n 将列表 source 的尾元素弹出，并返回给客户端。 将 source 弹出的元素，作为 destination 列表的头元素插入。  RPOPLPUSH source destination 如果 source 不存在，返回 nil。如果 source 和 destination 是同一个列表，就会把尾元素移动至开头，这叫做列表 的旋转(rotation)操作。\n# source 和 destination 不同 redis\u0026gt; LRANGE alpha 0 -1 1) \u0026#34;a\u0026#34; 2) \u0026#34;b\u0026#34; 3) \u0026#34;c\u0026#34; 4) \u0026#34;d\u0026#34; redis\u0026gt; RPOPLPUSH alpha reciver \u0026#34;d\u0026#34; redis\u0026gt; LRANGE alpha 0 -1 1) \u0026#34;a\u0026#34; 2) \u0026#34;b\u0026#34; 3) \u0026#34;c\u0026#34; redis\u0026gt; LRANGE reciver 0 -1 1) \u0026#34;d\u0026#34; # 再执行一次，表明 RPOP 和 LPUSH 的位置正确 redis\u0026gt; RPOPLPUSH alpha reciver \u0026#34;c\u0026#34; redis\u0026gt; LRANGE alpha 0 -1 1) \u0026#34;a\u0026#34; 2) \u0026#34;b\u0026#34; redis\u0026gt; LRANGE reciver 0 -1 1) \u0026#34;c\u0026#34; 2) \u0026#34;d\u0026#34; # source 和 destination 相同 redis\u0026gt; LRANGE number 0 -1 1) \u0026#34;1\u0026#34; 2) \u0026#34;2\u0026#34; 3) \u0026#34;3\u0026#34; 4) \u0026#34;4\u0026#34; redis\u0026gt; RPOPLPUSH number number \u0026#34;4\u0026#34; # 4 被旋转到了表头 redis\u0026gt; LRANGE number 0 -1 1) \u0026#34;4\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;2\u0026#34; 4) \u0026#34;3\u0026#34; redis\u0026gt; RPOPLPUSH number number \u0026#34;3\u0026#34; redis\u0026gt; LRANGE number 0 -1 1) \u0026#34;3\u0026#34; 2) \u0026#34;4\u0026#34; 3) \u0026#34;1\u0026#34; 4) \u0026#34;2\u0026#34; BRPOPLPUSH\r#\r\rBRPOPLPUSH 和 RPOPLPUSH 基本相同，BRPOPLPUSH 是阻塞版本，当指定的源列表 source 不为空时，其表现和 RPOPLPUSH 一样。 当 source 为空时，连接将被 BRPOP 命令阻塞，直到等待超时或有可弹出元素为止。\nBRPOPLPUSH source destination timeout 如果指定时间内没有任何元素弹出，返回一个 nil。 否则，返回一个含有两个元素的列表，其中：第一个元素是被弹出元素所属的 key，第二 个元素是被弹出元素的值。\n# 非空列表 redis\u0026gt; BRPOPLPUSH msg reciver 500 \u0026#34;hello moto\u0026#34; # 弹出元素的值 (4.31s) # 等待时长 redis\u0026gt; LLEN reciver (integer) 1 redis\u0026gt; LRANGE reciver 0 0 1) \u0026#34;hello moto\u0026#34; # 空列表 redis\u0026gt; BRPOPLPUSH msg reciver 1 (nil) (2.24s) 其他\r#\r\rLLEN\r#\r\r返回列表 key 的长度。\nLLEN key 如果 key 不存在，返回 0。如果 key 不是列表类型，返回一个错误。\n# 空列表 redis\u0026gt; LLEN job (integer) 0 # 非空列表 redis\u0026gt; LPUSH job \u0026#34;cook food\u0026#34; (integer) 1 redis\u0026gt; LPUSH job \u0026#34;have lunch\u0026#34; (integer) 2 redis\u0026gt; LLEN job (integer) 2 LREM\r#\r\r移除元素，指定移除数量 count，移除列表 key 中与 value 相等的元素。\nLREM key count value count的值可以有下面三种情况：\n count \u0026gt; 0，从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count。 count \u0026lt; 0，从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count的 绝对值。 count = 0，移除表中所有与 value 相等的值。  如果 key 不存在，返回 0。\n# 先创建一个表，内容排列是 # morning hello morning helllo morning redis\u0026gt; LPUSH greet \u0026#34;morning\u0026#34; (integer) 1 redis\u0026gt; LPUSH greet \u0026#34;hello\u0026#34; (integer) 2 redis\u0026gt; LPUSH greet \u0026#34;morning\u0026#34; (integer) 3 redis\u0026gt; LPUSH greet \u0026#34;hello\u0026#34; (integer) 4 redis\u0026gt; LPUSH greet \u0026#34;morning\u0026#34; (integer) 5 redis\u0026gt; LRANGE greet 0 4 # 查看所有元素 1) \u0026#34;morning\u0026#34; 2) \u0026#34;hello\u0026#34; 3) \u0026#34;morning\u0026#34; 4) \u0026#34;hello\u0026#34; 5) \u0026#34;morning\u0026#34; redis\u0026gt; LREM greet 2 morning # 移除从表头到表尾，最先发现的两个 morning (integer) 2 # 两个元素被移除 redis\u0026gt; LLEN greet # 还剩 3 个元素 (integer) 3 redis\u0026gt; LRANGE greet 0 2 1) \u0026#34;hello\u0026#34; 2) \u0026#34;hello\u0026#34; 3) \u0026#34;morning\u0026#34; redis\u0026gt; LREM greet -1 morning # 移除从表尾到表头，第一个 morning (integer) 1 redis\u0026gt; LLEN greet # 剩下两个元素 (integer) 2 redis\u0026gt; LRANGE greet 0 1 1) \u0026#34;hello\u0026#34; 2) \u0026#34;hello\u0026#34; redis\u0026gt; LREM greet 0 hello # 移除表中所有 hello (integer) 2 # 两个 hello 被移除 redis\u0026gt; LLEN greet (integer) 0 LTRIM\r#\r\r对列表 key 进行修剪，通过 start 和 stop 指定区间，保留指定区间内的元素，其余的元素删除。 比如，执行 LTRIM list 0 10，表示保留列表 list 的前 11 个元素，其余元素删除。 start 和 stop 可以是负数，如，-1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。\nLTRIM key start stop 操作成功返回 OK，失败会返回错误信息。\n# 1. start 和 stop 都在列表的索引范围之内 # alpha 是一个包含 5 个字符串的列表 redis\u0026gt; LRANGE alpha 0 -1 1) \u0026#34;h\u0026#34; 2) \u0026#34;e\u0026#34; 3) \u0026#34;l\u0026#34; 4) \u0026#34;l\u0026#34; 5) \u0026#34;o\u0026#34; # 删除 alpha 列表索引为 0 的元素 redis\u0026gt; LTRIM alpha 1 -1 OK # \u0026#34;h\u0026#34; 已被删除 redis\u0026gt; LRANGE alpha 0 -1 1) \u0026#34;e\u0026#34; 2) \u0026#34;l\u0026#34; 3) \u0026#34;l\u0026#34; 4) \u0026#34;o\u0026#34; # 2. stop 大于最大下标值 # 保留 alpha 列表索引 1 至索引 10086 上的元素 redis\u0026gt; LTRIM alpha 1 10086 OK # 只有索引 0 上的元素 \u0026#34;e\u0026#34; 被删除了，其他元素还在 redis\u0026gt; LRANGE alpha 0 -1 1) \u0026#34;l\u0026#34; 2) \u0026#34;l\u0026#34; 3) \u0026#34;o\u0026#34; # 3. start 和 stop 都大于列表的最大下标，并且 start \u0026lt; stop redis\u0026gt; LTRIM alpha 10086 123321 OK redis\u0026gt; LRANGE alpha 0 -1 # 列表被清空 (empty list or set) # 4. start 和 stop 都大于列表的最大下标，并且 start \u0026gt; stop # 重新建立一个新列表 redis\u0026gt; RPUSH new-alpha \u0026#34;h\u0026#34; \u0026#34;e\u0026#34; \u0026#34;l\u0026#34; \u0026#34;l\u0026#34; \u0026#34;o\u0026#34; (integer) 5 redis\u0026gt; LRANGE new-alpha 0 -1 1) \u0026#34;h\u0026#34; 2) \u0026#34;e\u0026#34; 3) \u0026#34;l\u0026#34; 4) \u0026#34;l\u0026#34; 5) \u0026#34;o\u0026#34; # 执行 LTRIM redis\u0026gt; LTRIM new-alpha 123321 10086 OK # 同样被清空 redis\u0026gt; LRANGE new-alpha 0 -1 (empty list or set) "});index.add({'id':20,'href':'/db-learn/docs/redis/05_redis-set/','title':"Redis 数据类型 Set",'content':"Redis 的 Set 是 string 类型的无序集合，类似于 List 类型。但是集合中不允许重复成员的存在。一个 Redis 集合中最多可 包含 232-1(40亿) 个元素。Set 类型有一个非常重要的特性，就是支持集合之间的聚合计算操作，这些操作均在服务端完成，效率极高，而且也 节省了的网络 I/O 开销。\nRedis 的集合和 Java 的 HashSet 类似，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都 是一个值 NULL。当集合移除了最后一个元素之后，该 key 会被自动被删除，内存被回收。\n存取\r#\r\rSADD\r#\r\r添加一个或多个 member 元素到集合 key 中。\nSADD key member [member ...] 返回被添加到集合中的新元素的数量。如果集合 key 不存在，创建集合 key，并执行 SADD。如果 key 不是集合类型，将返回一个错误。\n# 添加单个元素 redis\u0026gt; SADD blog \u0026#34;segmentfault.com\u0026#34; (integer) 1 # 添加重复元素 redis\u0026gt; SADD blog \u0026#34;segmentfault.com\u0026#34; (integer) 0 # 添加多个元素 redis\u0026gt; SADD blog \u0026#34;csdn.net\u0026#34; \u0026#34;itbilu.com\u0026#34; (integer) 2 redis\u0026gt; SMEMBERS blog 1) \u0026#34;segmentfault.com\u0026#34; 2) \u0026#34;csdn.net\u0026#34; 3) \u0026#34;itbilu.com\u0026#34; SCARD\r#\r\r返回集合 key 中元素的数量。\nSCARD key 如果 key 不存在，返回 0。\nredis\u0026gt; SADD tool pc printer phone (integer) 3 redis\u0026gt; SCARD tool # 非空集合 (integer) 3 redis\u0026gt; DEL tool (integer) 1 redis\u0026gt; SCARD tool # 空集合 (integer) 0 SMEMBERS\r#\r\r返回集合 key 的所有成员。 注意当 SMEMBERS 处理一个很大的集合键时，由于 Redis 是单线程，它可能会阻塞服务器。\nSMEMBERS key 如果 key 不存在，返回空集合。\n# key 不存在或集合为空 redis\u0026gt; EXISTS not_exists_key (integer) 0 redis\u0026gt; SMEMBERS not_exists_key (empty list or set) # 非空集合 redis\u0026gt; SADD language Ruby Python Clojure (integer) 3 redis\u0026gt; SMEMBERS language 1) \u0026#34;Python\u0026#34; 2) \u0026#34;Ruby\u0026#34; 3) \u0026#34;Clojure\u0026#34; SISMEMBER\r#\r\r判断集合 key 中是否包含 member 元素。\nSISMEMBER key member 如果包含，返回 1。如果不包含或 key 不存在，返回 0。\nredis\u0026gt; SMEMBERS joe\u0026#39;s_movies 1) \u0026#34;hi, lady\u0026#34; 2) \u0026#34;Fast Five\u0026#34; 3) \u0026#34;2012\u0026#34; redis\u0026gt; SISMEMBER joe\u0026#39;s_movies \u0026#34;bet man\u0026#34; (integer) 0 redis\u0026gt; SISMEMBER joe\u0026#39;s_movies \u0026#34;Fast Five\u0026#34; (integer) 1 SRANDMEMBER\r#\r\r返回集合 key 中的一个或指定数量 count 的随机元素。与 SPOP 类似，但是 SPOP 会删除返回的随机元素，SRANDMEMBER 不会删除 元素。\nSRANDMEMBER key [count] count 的值可以有下面两种：\n count 为正数，且小于集合基数，返回一个包含 count 个元素的数组，数组中的元素各不相同。如果 count 大于等于集合基数，那么返 回整个集合。 count 为负数，返回一个数组，数组中的元素可能会重复出现多次，而数组的长度为 count 的绝对值。  # 添加元素 redis\u0026gt; SADD fruit apple banana cherry (integer) 3 # 只给定 key 参数，返回一个随机元素 redis\u0026gt; SRANDMEMBER fruit \u0026#34;cherry\u0026#34; redis\u0026gt; SRANDMEMBER fruit \u0026#34;apple\u0026#34; # 给定 3 为 count 参数，返回 3 个随机元素 # 每个随机元素都不相同 redis\u0026gt; SRANDMEMBER fruit 3 1) \u0026#34;apple\u0026#34; 2) \u0026#34;banana\u0026#34; 3) \u0026#34;cherry\u0026#34; # 给定 -3 为 count 参数，返回 3 个随机元素 # 元素可能会重复出现多次 redis\u0026gt; SRANDMEMBER fruit -3 1) \u0026#34;banana\u0026#34; 2) \u0026#34;cherry\u0026#34; 3) \u0026#34;apple\u0026#34; redis\u0026gt; SRANDMEMBER fruit -3 1) \u0026#34;apple\u0026#34; 2) \u0026#34;apple\u0026#34; 3) \u0026#34;cherry\u0026#34; # 如果 count 是整数，且大于等于集合基数，那么返回整个集合 redis\u0026gt; SRANDMEMBER fruit 10 1) \u0026#34;apple\u0026#34; 2) \u0026#34;banana\u0026#34; 3) \u0026#34;cherry\u0026#34; # 如果 count 是负数，且 count 的绝对值大于集合的基数 # 那么返回的数组的长度为 count 的绝对值 redis\u0026gt; SRANDMEMBER fruit -10 1) \u0026#34;banana\u0026#34; 2) \u0026#34;apple\u0026#34; 3) \u0026#34;banana\u0026#34; 4) \u0026#34;cherry\u0026#34; 5) \u0026#34;apple\u0026#34; 6) \u0026#34;apple\u0026#34; 7) \u0026#34;cherry\u0026#34; 8) \u0026#34;apple\u0026#34; 9) \u0026#34;apple\u0026#34; 10) \u0026#34;banana\u0026#34; # SRANDMEMBER 并不会修改集合内容 redis\u0026gt; SMEMBERS fruit 1) \u0026#34;apple\u0026#34; 2) \u0026#34;cherry\u0026#34; 3) \u0026#34;banana\u0026#34; # 集合为空时返回 nil 或者空数组 redis\u0026gt; SRANDMEMBER not-exists (nil) redis\u0026gt; SRANDMEMBER not-eixsts 10 (empty list or set) 移除\r#\r\rSPOP\r#\r\r删除并返回集合 key 中的一个随机元素。注意返回的是随机元素，不是头部也不是尾部元素。\nSPOP key 如果key不存在或key是空集，返回nil。\nredis\u0026gt; SMEMBERS db 1) \u0026#34;MySQL\u0026#34; 2) \u0026#34;MongoDB\u0026#34; 3) \u0026#34;Redis\u0026#34; redis\u0026gt; SPOP db \u0026#34;Redis\u0026#34; redis\u0026gt; SMEMBERS db 1) \u0026#34;MySQL\u0026#34; 2) \u0026#34;MongoDB\u0026#34; redis\u0026gt; SPOP db \u0026#34;MySQL\u0026#34; redis\u0026gt; SMEMBERS db 1) \u0026#34;MongoDB\u0026#34; SREM\r#\r\r删除集合 key 中的一个或多个元素。\nSREM key member [member ...] 如果 member 不存在，会被忽略。\n# 测试数据 redis\u0026gt; SMEMBERS languages 1) \u0026#34;c\u0026#34; 2) \u0026#34;lisp\u0026#34; 3) \u0026#34;python\u0026#34; 4) \u0026#34;ruby\u0026#34; # 移除单个元素 redis\u0026gt; SREM languages ruby (integer) 1 # 移除不存在元素 redis\u0026gt; SREM languages non-exists-language (integer) 0 # 移除多个元素 redis\u0026gt; SREM languages lisp python c (integer) 3 redis\u0026gt; SMEMBERS languages (empty list or set) 合并\r#\r\rSDIFF\r#\r\r返回指定的一个或多个集合的差集。\nSDIFF key [key ...] redis\u0026gt; SMEMBERS peter\u0026#39;s_movies 1) \u0026#34;bet man\u0026#34; 2) \u0026#34;start war\u0026#34; 3) \u0026#34;2012\u0026#34; redis\u0026gt; SMEMBERS joe\u0026#39;s_movies 1) \u0026#34;hi, lady\u0026#34; 2) \u0026#34;Fast Five\u0026#34; 3) \u0026#34;2012\u0026#34; redis\u0026gt; SDIFF peter\u0026#39;s_movies joe\u0026#39;s_movies 1) \u0026#34;bet man\u0026#34; 2) \u0026#34;start war\u0026#34; SDIFFSTORE\r#\r\r和 SDIFF 类似，但是 SDIFFSTORE 是将指定的一个或多个集合的差集存储到集合 destination 中。\nSDIFFSTORE destination key [key ...] 如果 destination 已存在，则覆盖。返回交集（注意这里不是差集）成员数量。\nredis\u0026gt; SMEMBERS joe\u0026#39;s_movies 1) \u0026#34;hi, lady\u0026#34; 2) \u0026#34;Fast Five\u0026#34; 3) \u0026#34;2012\u0026#34; redis\u0026gt; SMEMBERS peter\u0026#39;s_movies 1) \u0026#34;bet man\u0026#34; 2) \u0026#34;start war\u0026#34; 3) \u0026#34;2012\u0026#34; redis\u0026gt; SDIFFSTORE joe_diff_peter joe\u0026#39;s_movies peter\u0026#39;s_movies (integer) 2 redis\u0026gt; SMEMBERS joe_diff_peter 1) \u0026#34;hi, lady\u0026#34; SINTER\r#\r\r返回一个或多个指定集合的交集。\nSINTER key [key ...] 如果 key 不存在，返回的结果集为空。\nredis\u0026gt; SMEMBERS group_1 1) \u0026#34;LI LEI\u0026#34; 2) \u0026#34;TOM\u0026#34; 3) \u0026#34;JACK\u0026#34; redis\u0026gt; SMEMBERS group_2 1) \u0026#34;HAN MEIMEI\u0026#34; 2) \u0026#34;JACK\u0026#34; redis\u0026gt; SINTER group_1 group_2 1) \u0026#34;JACK\u0026#34; SINTERSTORE\r#\r\r和 SINTER 类似，但是 SINTERSTORE 是将指定的一个或多个集合的交集存储到集合 destination 中。\nSDIFFSTORE destination key [key ...] 如果 destination 已存在，则覆盖。返回交集成员数量。\nredis\u0026gt; SMEMBERS songs 1) \u0026#34;good bye joe\u0026#34; 2) \u0026#34;hello,peter\u0026#34; redis\u0026gt; SMEMBERS my_songs 1) \u0026#34;good bye joe\u0026#34; 2) \u0026#34;falling\u0026#34; redis\u0026gt; SINTERSTORE song_interset songs my_songs (integer) 1 redis\u0026gt; SMEMBERS song_interset 1) \u0026#34;good bye joe\u0026#34; SUNION\r#\r\r返回一个或多个指定集合的并集。\nSUNION key [key ...] 如果 key 不存在，返回的结果集为空。\nredis\u0026gt; SMEMBERS songs 1) \u0026#34;Billie Jean\u0026#34; redis\u0026gt; SMEMBERS my_songs 1) \u0026#34;Believe Me\u0026#34; redis\u0026gt; SUNION songs my_songs 1) \u0026#34;Billie Jean\u0026#34; 2) \u0026#34;Believe Me\u0026#34; SUNIONSTORE\r#\r\r和 SUNION 类似，但是 SUNIONSTORE 是将指定的一个或多个集合的并集存储到集合 destination 中。\nSDIFFSTORE destination key [key ...] 如果destination已存在，则覆盖。返回并集成员数量。\nredis\u0026gt; SMEMBERS NoSQL 1) \u0026#34;MongoDB\u0026#34; 2) \u0026#34;Redis\u0026#34; redis\u0026gt; SMEMBERS SQL 1) \u0026#34;sqlite\u0026#34; 2) \u0026#34;MySQL\u0026#34; redis\u0026gt; SUNIONSTORE db NoSQL SQL (integer) 4 redis\u0026gt; SMEMBERS db 1) \u0026#34;MySQL\u0026#34; 2) \u0026#34;sqlite\u0026#34; 3) \u0026#34;MongoDB\u0026#34; 4) \u0026#34;Redis\u0026#34; SMOVE\r#\r\r将指定的 member 元素从 source 集合删除并移动到 destination 集合。该命令原子操作。\nSMOVE source destination member 如果 source 不存在或者没有指定的 member 元素，则不执行任何操作，并返回 0。 如果 source 和 destination 是同一个集合，就会把尾元素移动至开头，这叫做列表的旋转(rotation)操作。 如果 destination 已经存在该 member 元素，只删除 source 集合中的 member 元素。 如果 source 或 destination 不是集合类型，返回错误。\nredis\u0026gt; SMEMBERS songs 1) \u0026#34;Billie Jean\u0026#34; 2) \u0026#34;Believe Me\u0026#34; redis\u0026gt; SMEMBERS my_songs (empty list or set) redis\u0026gt; SMOVE songs my_songs \u0026#34;Believe Me\u0026#34; (integer) 1 redis\u0026gt; SMEMBERS songs 1) \u0026#34;Billie Jean\u0026#34; redis\u0026gt; SMEMBERS my_songs 1) \u0026#34;Believe Me\u0026#34; 其他\r#\r\rSSCAN\r#\r\r参考 SCAN 命令。\nSSCAN key cursor [MATCH pattern] [COUNT count] "});index.add({'id':21,'href':'/db-learn/docs/redis/06_redis-sortedset/','title':"Redis 数据类型 Sorted Set",'content':"Redis 的有序集合和 Set 一样也是 String 类型元素的集合,且不允许重复的成员。 Redis 提供的最为特色的数据结构。不同的是每个元素都 会关联一个 double 类型的分数。redis 正是通过分数来为集合中的成员进行从小到大的排序。zset 的成员是唯一的,但分数(score)却可以重复。\nRedis 的 ZSET 类似 Java 的 SortedSet 和 HashMap 的结合体，既保证了内部 value 的唯一性，还可以给每个 value 赋予 一个 score，代表这个 value 的排序权重。当集合移除了最后一个元素之后，该 key 会被自动被删除，内存被回收。\nZSET可以用来存粉丝列表，value值是粉丝的用户ID，score是关注时间。我们可以对粉丝列表按关注时间进行排序。 ZSET还可以用来存储学生的成绩，value值是学生的ID，score是考试成绩。可以按分数对名次进行排序就。\n存取\r#\r\rZADD\r#\r\r将一个或多个 member 元素及其 score 值添加到有序集合 key 中。\nZADD key score member [[score member] [score member] ...] score 可以是整数或双精度浮点数。 如果 key 不存在，创建 key 并执行 ZADD。如果 key 不是有序集合类型，返回一个错误。 如果 member 已经存在，则更新其 score 值，并重新插入，排序。\n# 添加单个元素 redis\u0026gt; ZADD page_rank 10 google.com (integer) 1 # 添加多个元素 redis\u0026gt; ZADD page_rank 9 baidu.com 8 bing.com (integer) 2 redis\u0026gt; ZRANGE page_rank 0 -1 WITHSCORES 1) \u0026#34;bing.com\u0026#34; 2) \u0026#34;8\u0026#34; 3) \u0026#34;baidu.com\u0026#34; 4) \u0026#34;9\u0026#34; 5) \u0026#34;google.com\u0026#34; 6) \u0026#34;10\u0026#34; # 添加已存在元素，且 score 值不变 redis\u0026gt; ZADD page_rank 10 google.com (integer) 0 redis\u0026gt; ZRANGE page_rank 0 -1 WITHSCORES # 没有改变 1) \u0026#34;bing.com\u0026#34; 2) \u0026#34;8\u0026#34; 3) \u0026#34;baidu.com\u0026#34; 4) \u0026#34;9\u0026#34; 5) \u0026#34;google.com\u0026#34; 6) \u0026#34;10\u0026#34; # 添加已存在元素，但是改变 score 值 redis\u0026gt; ZADD page_rank 6 bing.com (integer) 0 redis\u0026gt; ZRANGE page_rank 0 -1 WITHSCORES # bing.com 元素的 score 值被改变 1) \u0026#34;bing.com\u0026#34; 2) \u0026#34;6\u0026#34; 3) \u0026#34;baidu.com\u0026#34; 4) \u0026#34;9\u0026#34; 5) \u0026#34;google.com\u0026#34; 6) \u0026#34;10\u0026#34; ZCARD\r#\r\r返回有序集合 key 的基数。\nZCARD key 如果 key 不存在，返回 0。\n# 添加一个元素 redis \u0026gt; ZADD salary 2000 tom (integer) 1 redis \u0026gt; ZCARD salary (integer) 1 # 再添加一个元素 redis \u0026gt; ZADD salary 5000 jack (integer) 1 redis \u0026gt; ZCARD salary (integer) 2 # 对不存在的有序集合进行 ZCARD 操作 redis \u0026gt; EXISTS non_exists_key (integer) 0 redis \u0026gt; ZCARD non_exists_key (integer) 0 ZRANK\r#\r\r返回有序集合 key 中的指定 member 元素的排名。\nZRANK key member 元素成员按 score 值递增，相同 score 值的成员按字典排序。注意元素排名从 0 开始计数，也就是说第一名返回的是 0，以此类推。 如果 key 不是有序集合类型，返回nil。\n# 显示所有成员及其 score 值 redis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;peter\u0026#34; 2) \u0026#34;3500\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;4000\u0026#34; 5) \u0026#34;jack\u0026#34; 6) \u0026#34;5000\u0026#34; # tom 排名第二 redis\u0026gt; ZRANK salary tom (integer) 1 ZSCORE\r#\r\r返回有序集合 key 中的指定 member 元素的 score 值。\nZRANK key member 如果 key 或 member 不存在，返回 nil。\nredis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;tom\u0026#34; 2) \u0026#34;2000\u0026#34; 3) \u0026#34;peter\u0026#34; 4) \u0026#34;3500\u0026#34; 5) \u0026#34;jack\u0026#34; 6) \u0026#34;5000\u0026#34; # 返回值是字符串形式 redis\u0026gt; ZSCORE salary peter \u0026#34;3500\u0026#34; ZCOUNT\r#\r\r返回有序集合 key 中 score 值在 min 和 max 之间的元素个数，注意包含值为 min 和 max的元素。\nZCOUNT key min max 如果 key 不存在，返回 0。\n# 测试数据 redis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;jack\u0026#34; 2) \u0026#34;2000\u0026#34; 3) \u0026#34;peter\u0026#34; 4) \u0026#34;3500\u0026#34; 5) \u0026#34;tom\u0026#34; 6) \u0026#34;5000\u0026#34; # 计算薪水在 2000-5000 之间的人数 redis\u0026gt; ZCOUNT salary 2000 5000 (integer) 3 # 计算薪水在 3000-5000 之间的人数 redis\u0026gt; ZCOUNT salary 3000 5000 (integer) 2 ZLEXCOUNT\r#\r\r返回有序集合 key 中指定元素 min 和 max 之间的元素个数，注意包含值为 min 和 max 的元素。\nZLEXCOUNT key min max ZLEXCOUNT key min max 相当于 ZLEXCOUNT key [min-member [max-member 成员名称前需要加 [ 符号作为开头,[ 符号与成员之间不能有空格。 可以使用 - 和 + 表示 score 的最小值和最大值。 max 放前面，min 放后面会导致返回结果为0。 如果 key 不存在，返回 0。\nredis\u0026gt; ZADD myzset 0 a 0 b 0 c 0 d 0 e (integer) 5 redis\u0026gt; ZADD myzset 0 f 0 g (integer) 2 redis\u0026gt; ZLEXCOUNT myzset - + (integer) 7 redis\u0026gt; ZLEXCOUNT myzset [b [f (integer) 5 ZRANGE\r#\r\r返回有序集合 key 指定区间内的元素。WITHSCORES 用于指定是否同时返回元素的 score。\nZRANGE key start stop [WITHSCORES] start 和 stop都是从 0 开始。可以是负数，如，-1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。。 元素成员按 score 值递增，相同 score 值的成员按字典排序。\n# 显示有序集合所有成员 redis \u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;jack\u0026#34; 2) \u0026#34;3500\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;5000\u0026#34; 5) \u0026#34;boss\u0026#34; 6) \u0026#34;10086\u0026#34; # 返回有序集下标区间 1 至 2 的成员 redis \u0026gt; ZRANGE salary 1 2 WITHSCORES 1) \u0026#34;tom\u0026#34; 2) \u0026#34;5000\u0026#34; 3) \u0026#34;boss\u0026#34; 4) \u0026#34;10086\u0026#34; # end 超出最大下标时 redis \u0026gt; ZRANGE salary 0 200000 WITHSCORES 1) \u0026#34;jack\u0026#34; 2) \u0026#34;3500\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;5000\u0026#34; 5) \u0026#34;boss\u0026#34; 6) \u0026#34;10086\u0026#34; # 当指定区间超出有序集合范围时 redis \u0026gt; ZRANGE salary 200000 3000000 WITHSCORES (empty list or set) ZREVRANGE\r#\r\rZREVRANGE 和 ZRANGE 类似，区别在于排序，ZREVRANGE 的 score 值按倒序(从大到小)顺序排序。WITHSCORES 用于指定是否同时返 回元素的 score。\nZREVRANGE key start stop [WITHSCORES] # 递增排列 redis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;peter\u0026#34; 2) \u0026#34;3500\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;4000\u0026#34; 5) \u0026#34;jack\u0026#34; 6) \u0026#34;5000\u0026#34; # 递减排列 redis\u0026gt; ZREVRANGE salary 0 -1 WITHSCORES 1) \u0026#34;jack\u0026#34; 2) \u0026#34;5000\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;4000\u0026#34; 5) \u0026#34;peter\u0026#34; 6) \u0026#34;3500\u0026#34; ZRANGEBYLEX\r#\r\r返回有序集合 key 指定区间内的元素。\nZRANGEBYLEX key min max [LIMIT offset count] ZRANGEBYLEX key min max 相当于 ZRANGEBYLEX key [min-member [max-member\n不要在分数不一致的有序集合中去使用 ZRANGEBYLEX,因为获取的结果并不准确。\nredis\u0026gt; zadd zset 0 a 0 aa 0 abc 0 apple 0 b 0 c 0 d 0 d1 0 dd 0 dobble 0 z 0 z1 (integer) 12 redis\u0026gt; ZRANGEBYLEX zset - + 1) \u0026#34;a\u0026#34; 2) \u0026#34;aa\u0026#34; 3) \u0026#34;abc\u0026#34; 4) \u0026#34;apple\u0026#34; 5) \u0026#34;b\u0026#34; 6) \u0026#34;c\u0026#34; 7) \u0026#34;d\u0026#34; 8) \u0026#34;d1\u0026#34; 9) \u0026#34;dd\u0026#34; 10) \u0026#34;dobble\u0026#34; 11) \u0026#34;z\u0026#34; 12) \u0026#34;z1\u0026#34; ZRANGEBYSCORE\r#\r\r返回有序集合 key 中 score 值位于 max 和 min(默认包含 max 和 min)区间内的成员的元素。\nZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 可选参数：\n LIMIT：用于指定返回元素数量， offset：用于指定偏移量(类似 SQL 中的 SELECT LIMIT offset, count)。  -inf 和 +inf 可以表示最小值和最大值。\n# 测试数据 redis\u0026gt; ZADD salary 2500 jack (integer) 0 redis\u0026gt; ZADD salary 5000 tom (integer) 0 redis\u0026gt; ZADD salary 12000 peter (integer) 0 # 显示整个有序集合 redis\u0026gt; ZRANGEBYSCORE salary -inf +inf 1) \u0026#34;jack\u0026#34; 2) \u0026#34;tom\u0026#34; 3) \u0026#34;peter\u0026#34; # 显示整个有序集合及成员的 score 值 redis\u0026gt; ZRANGEBYSCORE salary -inf +inf WITHSCORES 1) \u0026#34;jack\u0026#34; 2) \u0026#34;2500\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;5000\u0026#34; 5) \u0026#34;peter\u0026#34; 6) \u0026#34;12000\u0026#34; # 工资 \u0026lt;=5000 的成员 redis\u0026gt; ZRANGEBYSCORE salary -inf 5000 WITHSCORES 1) \u0026#34;jack\u0026#34; 2) \u0026#34;2500\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;5000\u0026#34; # 工资大于 5000 小于等于 400000 的成员 redis\u0026gt; ZRANGEBYSCORE salary 5000 400000 1) \u0026#34;peter\u0026#34; ZREVRANGEBYLEX\r#\r\rZREVRANGEBYLEX 和 ZRANGEBYLEX 类似，区别在于排序，ZREVRANGEBYLEX 的 score 值按倒序(从大到小)顺序排序。\nZREVRANGEBYLEX key max min [WITHSCORES] [LIMIT offset count] redis\u0026gt; zadd zset 0 a 0 aa 0 abc 0 apple 0 b 0 c 0 d 0 d1 0 dd 0 dobble 0 z 0 z1 (integer) 12 redis\u0026gt; ZREVRANGEBYLEX zset + - 1) \u0026#34;z1\u0026#34; 2) \u0026#34;z\u0026#34; 3) \u0026#34;dobble\u0026#34; 4) \u0026#34;dd\u0026#34; 5) \u0026#34;d1\u0026#34; 6) \u0026#34;d\u0026#34; 7) \u0026#34;c\u0026#34; 8) \u0026#34;b\u0026#34; 9) \u0026#34;apple\u0026#34; 10) \u0026#34;abc\u0026#34; 11) \u0026#34;aa\u0026#34; 12) \u0026#34;a\u0026#34; ZREVRANGEBYSCORE\r#\r\rZREVRANGEBYSCORE 和 ZRANGEBYSCORE 类似，区别在于排序，ZREVRANGEBYSCORE 的 score 值按倒序(从大到小)顺序排序。\nZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count] redis \u0026gt; ZADD salary 10086 jack (integer) 1 redis \u0026gt; ZADD salary 5000 tom (integer) 1 redis \u0026gt; ZADD salary 7500 peter (integer) 1 redis \u0026gt; ZADD salary 3500 joe (integer) 1 # 倒序返回所有成员 redis \u0026gt; ZREVRANGEBYSCORE salary +inf -inf 1) \u0026#34;jack\u0026#34; 2) \u0026#34;peter\u0026#34; 3) \u0026#34;tom\u0026#34; 4) \u0026#34;joe\u0026#34; # 倒序返回salary于 10000 和 2000 之间的成员 redis \u0026gt; ZREVRANGEBYSCORE salary 10000 2000 1) \u0026#34;peter\u0026#34; 2) \u0026#34;tom\u0026#34; 3) \u0026#34;joe\u0026#34; 自增\r#\r\rZINCRBY\r#\r\r为有序集合 key 中的 member 元素的 score 值增加增量 increment。\nZINCRBY key increment member score 可以是整数或双精度浮点数。 如果 key 不存在或者 member 不存在，执行 ZINCRBY key increment member 相当于执行 ZADD key increment member。 如果 key 不是有序集合类型，返回一个错误。\nredis\u0026gt; ZSCORE salary tom \u0026#34;2000\u0026#34; redis\u0026gt; ZINCRBY salary 2000 tom \u0026#34;4000\u0026#34; 删除\r#\r\rZREM\r#\r\r移除有序集合 key 中的一个或多个元素 member。\nZREM key member [member ...] 如果 key 不是有序集类型，返回一个错误。\n# 测试数据 redis\u0026gt; ZRANGE page_rank 0 -1 WITHSCORES 1) \u0026#34;bing.com\u0026#34; 2) \u0026#34;8\u0026#34; 3) \u0026#34;baidu.com\u0026#34; 4) \u0026#34;9\u0026#34; 5) \u0026#34;google.com\u0026#34; 6) \u0026#34;10\u0026#34; # 移除单个元素 redis\u0026gt; ZREM page_rank google.com (integer) 1 redis\u0026gt; ZRANGE page_rank 0 -1 WITHSCORES 1) \u0026#34;bing.com\u0026#34; 2) \u0026#34;8\u0026#34; 3) \u0026#34;baidu.com\u0026#34; 4) \u0026#34;9\u0026#34; # 移除多个元素 redis\u0026gt; ZREM page_rank baidu.com bing.com (integer) 2 redis\u0026gt; ZRANGE page_rank 0 -1 WITHSCORES (empty list or set) # 移除不存在元素 redis\u0026gt; ZREM page_rank non-exists-element (integer) 0 ZREMRANGEBYLEX\r#\r\r移除有序集合 key 中指定区间内的元素。\nZREMRANGEBYLEX key min max ZREMRANGEBYLEX key min max 相当于 ZREMRANGEBYLEX key [min-member [max-member 不要在分数不一致的有序集合中去使用 ZREMRANGEBYLEX,因为获取的结果并不准确。\nredis\u0026gt; zadd zset 0 a 0 aa 0 abc 0 apple 0 b 0 c 0 d 0 d1 0 dd 0 dobble 0 z 0 z1 (integer) 12 redis\u0026gt; ZRANGEBYLEX zset + - 1) \u0026#34;a\u0026#34; 2) \u0026#34;aa\u0026#34; 3) \u0026#34;abc\u0026#34; 4) \u0026#34;apple\u0026#34; 5) \u0026#34;b\u0026#34; 6) \u0026#34;c\u0026#34; 7) \u0026#34;d\u0026#34; 8) \u0026#34;d1\u0026#34; 9) \u0026#34;dd\u0026#34; 10) \u0026#34;dobble\u0026#34; 11) \u0026#34;z\u0026#34; 12) \u0026#34;z1\u0026#34; redis\u0026gt; ZREMRANGEBYLEX zset - + (integer) 7 redis\u0026gt; ZRANGEBYLEX zset - + (empty list or set) ZREMRANGEBYRANK\r#\r\r移除有序集合 key 中指定排名(rank)区间内的元素。\nZREMRANGEBYRANK key start stop start 和 stop 包含在区间内，可以使用负数，如 -1 表示最后一个元素，依次类推。\n# 测试数据 redis\u0026gt; ZADD salary 2000 jack (integer) 1 redis\u0026gt; ZADD salary 5000 tom (integer) 1 redis\u0026gt; ZADD salary 3500 peter (integer) 1 # 移除 0 至 1 区间内的成员 redis\u0026gt; ZREMRANGEBYRANK salary 0 1 (integer) 2 # 有序集只剩下一个成员 redis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;tom\u0026#34; 2) \u0026#34;5000\u0026#34; ZREMRANGEBYSCORE\r#\r\r移除有序集合 key 中指定 score 区间内的元素。\nZREMRANGEBYRANK key min max # 有序集合内的所有成员及其 score 值 redis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;tom\u0026#34; 2) \u0026#34;2000\u0026#34; 3) \u0026#34;peter\u0026#34; 4) \u0026#34;3500\u0026#34; 5) \u0026#34;jack\u0026#34; 6) \u0026#34;5000\u0026#34; # 移除所有salary 在 1500 到 3500 内的员工 redis\u0026gt; ZREMRANGEBYSCORE salary 1500 3500 (integer) 2 # 剩余的成员 redis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;jack\u0026#34; 2) \u0026#34;5000\u0026#34; 合并\r#\r\rZUNIONSTORE\r#\r\r计算一或多个有序集合的并集，并将结果存储到 destination 中。\nZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX] 可选参数：\n WEIGHTS，乘法因子，所有有序集合值在传递聚合函数前，都要乘以该因子。默认值为1 AGGREGATE，指定结果集的聚合方式。  SUM，求合，默认值 MAX，计算最大值 MIN，计算最小值    redis\u0026gt; ZRANGE programmer 0 -1 WITHSCORES 1) \u0026#34;peter\u0026#34; 2) \u0026#34;2000\u0026#34; 3) \u0026#34;jack\u0026#34; 4) \u0026#34;3500\u0026#34; 5) \u0026#34;tom\u0026#34; 6) \u0026#34;5000\u0026#34; redis\u0026gt; ZRANGE manager 0 -1 WITHSCORES 1) \u0026#34;herry\u0026#34; 2) \u0026#34;2000\u0026#34; 3) \u0026#34;mary\u0026#34; 4) \u0026#34;3500\u0026#34; 5) \u0026#34;bob\u0026#34; 6) \u0026#34;4000\u0026#34; # 除 programmer 外，其它成员增加 salary redis\u0026gt; ZUNIONSTORE salary 2 programmer manager WEIGHTS 1 3 (integer) 6 redis\u0026gt; ZRANGE salary 0 -1 WITHSCORES 1) \u0026#34;peter\u0026#34; 2) \u0026#34;2000\u0026#34; 3) \u0026#34;jack\u0026#34; 4) \u0026#34;3500\u0026#34; 5) \u0026#34;tom\u0026#34; 6) \u0026#34;5000\u0026#34; 7) \u0026#34;herry\u0026#34; 8) \u0026#34;6000\u0026#34; 9) \u0026#34;mary\u0026#34; 10) \u0026#34;10500\u0026#34; 11) \u0026#34;bob\u0026#34; 12) \u0026#34;12000\u0026#34; ZINTERSTORE\r#\r\r计算一或多个有序集合的交集，并将结果存储到 destination 中。\nZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX] 可选参数：\n WEIGHTS，乘法因子，所有有序集合值在传递聚合函数前，都要乘以该因子。默认值为1 AGGREGATE，指定结果集的聚合方式。  SUM，求合，默认值 MAX，计算最大值 MIN，计算最小值    redis \u0026gt; ZADD mid_test 70 \u0026#34;Li Lei\u0026#34; (integer) 1 redis \u0026gt; ZADD mid_test 70 \u0026#34;Han Meimei\u0026#34; (integer) 1 redis \u0026gt; ZADD mid_test 99.5 \u0026#34;Tom\u0026#34; (integer) 1 redis \u0026gt; ZADD fin_test 88 \u0026#34;Li Lei\u0026#34; (integer) 1 redis \u0026gt; ZADD fin_test 75 \u0026#34;Han Meimei\u0026#34; (integer) 1 redis \u0026gt; ZADD fin_test 99.5 \u0026#34;Tom\u0026#34; (integer) 1 # 保存交集 redis \u0026gt; ZINTERSTORE sum_point 2 mid_test fin_test (integer) 3 redis \u0026gt; ZRANGE sum_point 0 -1 WITHSCORES 1) \u0026#34;Han Meimei\u0026#34; 2) \u0026#34;145\u0026#34; 3) \u0026#34;Li Lei\u0026#34; 4) \u0026#34;158\u0026#34; 5) \u0026#34;Tom\u0026#34; 6) \u0026#34;199\u0026#34; 其他\r#\r\rZSCAN\r#\r\r参考 SCAN。\nZSCAN key cursor [MATCH pattern] [COUNT count] "});index.add({'id':22,'href':'/db-learn/docs/redis/03_redis-string/','title':"Redis 数据类型 String",'content':"Redis 数据类型 String\r#\r\rString 类型是最常用，也是最简单的的一种类型，string 类型是二进制安全的。也就是说 string 可以包含任何数据。比如 jpg图片 或者 序列化的对象 。一个键最大能存储 512MB。Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯 一 key 值来获取相应的 value 数据。不同类型的数据结构的差异就在于 value 的结构不一样。\n字符串结构使用非常广泛，不仅限于字符串，通常会使用 JSON 序列化成字符串，然后将序列化后的字符串塞进 Redis 来缓存。\n键值对存取\r#\r\rredis\u0026gt; set testkey hello OK redis\u0026gt; get testkey \u0026#34;hello\u0026#34; //EX redis\u0026gt; set testkey hello2 EX 60 OK redis\u0026gt; get testkey \u0026#34;hello2\u0026#34; redis\u0026gt; TTL testkey (integer) 55 //PX redis\u0026gt; SET testkey hello3 PX 60000 OK redis\u0026gt; GET testkey \u0026#34;hello3\u0026#34; redis\u0026gt; PTTL testkey (integer) 55000 //NX redis\u0026gt; SET testkey hello4 NX OK # 键不存在，设置成功 redis\u0026gt; GET testkey \u0026#34;hello4\u0026#34; redis\u0026gt; SET testkey hello4 NX (nil) # 键已经存在，设置失败 redis\u0026gt; GET testkey \u0026#34;hello4\u0026#34; //XX redis\u0026gt; SET testkey hello5 XX OK # 键已经存在，设置成功 redis\u0026gt; GET testkey \u0026#34;hello5\u0026#34; redis\u0026gt; SET testkey2 hello XX (nil) # 键不存在，设置失败 //EX 和 PX 同时使用，后面的选项会覆盖前面设置的选项 redis\u0026gt; set testkey hello2 EX 10 PX 50000 OK redis\u0026gt; TTL testkey (integer) 45000 # PX 参数覆盖了 EX redis\u0026gt; set testkey hello2 PX 50000 EX 10 OK redis\u0026gt; TTL testkey (integer) 8 # EX 参数覆盖了 PX SET\r#\r\rSET [key] [value] [EX seconds] [PX milliseconds] [NX|XX]  EX seconds - 设置过期时间，单位为秒。 PX millisecond - 设置过期时间，单位毫秒。 NX - 只在 key 不存在时才进行设置。 XX - 只在 key 存在时才进行设置。  SETEX\r#\r\r设置 key 值并指定过期时间，单位秒。 SETEX key second value 等同于 SET key value EX second\nredis\u0026gt; SETEX name 60 xiaoming OK redis\u0026gt; GET name \u0026#34;10086\u0026#34; redis\u0026gt; TTL name (integer) 49 PSETEX\r#\r\r设置 key 值并指定过期时间，单位毫秒。 SET key value PX millisecond 等同于 PSETEX key millisecond value\nredis\u0026gt; PSETEX mykey 1000 \u0026#34;Hello\u0026#34; OK redis\u0026gt; PTTL mykey (integer) 999 redis\u0026gt; GET mykey \u0026#34;Hello\u0026#34; SETNX\r#\r\r如果 key 不存在，则设置其值。设置成功，返回 1。失败，返回 0。 SET key value NX 等同于 SETNX key value\nGET\r#\r\r获取 key 对应的 value。如果 key 不存在，则返回 nil；如果 key 不是字符串类型，则返回错误。\nGETSET\r#\r\r设置 key 的值，并返回其旧值。也就是执行了 set 操作和 get 操作。如果 key 不是字符串类型，则返回错误。\nredis\u0026gt; GETSET testkey3 hello3 (nil) # 没有旧值，返回 nil redis\u0026gt; GETSET testkey3 hello4 \u0026#34;hello3\u0026#34; # 返回旧值 批量操作键值对\r#\r\r同时设置或获取多个字符串，可以节省网络耗时开销。\n\u0026gt; SET name xiaoming OK \u0026gt; SET age 18 OK \u0026gt; MGET name age phone 1) \u0026#34;xiaoming\u0026#34; 2) \u0026#34;18\u0026#34; 3) (nil) \u0026gt; MSET name xiaoming age 18 phone 17235617235 \u0026gt; MGET name age phone 1) \u0026#34;xiaoming\u0026#34; 2) \u0026#34;18\u0026#34; 3) \u0026#34;17235617235\u0026#34; MSET\r#\r\rMSET 操作具有原子性，所有 key 设置要么全成功，要么全部失败。\nMSETNX\r#\r\rMSETNX 和 SETNX 类似，当 key 不存在时，才会设置其值。MSETNX 一样具有原子性。\n# 对不存在的 key 进行 MSETNX redis\u0026gt; MSETNX rmdbs \u0026#34;MySQL\u0026#34; nosql \u0026#34;MongoDB\u0026#34; key-value-store \u0026#34;redis\u0026#34; (integer) 1 redis\u0026gt; MGET rmdbs nosql key-value-store 1) \u0026#34;MySQL\u0026#34; 2) \u0026#34;MongoDB\u0026#34; 3) \u0026#34;redis\u0026#34; # MSET 的给定 key 当中有已存在的 key redis\u0026gt; MSETNX rmdbs \u0026#34;Sqlite\u0026#34; language \u0026#34;python\u0026#34; (integer) 0 # 因为 MSET 是原子性操作，language 没有被设置 redis\u0026gt; EXISTS language (integer) 0 # rmdbs 也没有被修改 redis\u0026gt; GET rmdbs \u0026#34;MySQL\u0026#34; MGET\r#\r\r返回一个或多个 key 值。\n自增/自减\r#\r\r在 Redis 中，数值也会也字符串形式存储。 注意，执行自增或自减时，如果 key 不存在，会被初始化为 0 再执行自增或自减操作。如果 key 值为非数字，那么会返回一个错误。数 字值的有效范围为 64 位(bit)有符号数字。\nredis\u0026gt; SET age 18 OK redis\u0026gt; INCR age (integer) 19 redis\u0026gt; GET age \u0026#34;19\u0026#34; redis\u0026gt; DECR age (integer) 18 redis\u0026gt; INCRBY age 20 (integer) 38 INCR\r#\r\r将 key 的值加 1。\nINCRBY\r#\r\r将 key 的值增加指定的值。\nINCRBYFLOAT\r#\r\r将 key 的值增加指定的浮点值。\nredis\u0026gt; SET floatkey 9.5 OK redis\u0026gt; INCRBYFLOAT floatkey 0.1 \u0026#34;9.6\u0026#34; DECR\r#\r\r将 key 的值减 1。如果 key 不存在，\nredis\u0026gt; DECR count #count 不存在，初始化为 0，再减一 (integer) -1 DECRBY\r#\r\r将 key 的值减去指定的值。\nredis\u0026gt; SET count 100 OK redis\u0026gt; DECRBY count 20 (integer) 80 APPEND\r#\r\r向 key 值字符串的末尾追加指定的 value；如果 key 不存在，则执行 SET 操作，设置 key 值\nredis\u0026gt; APPEND notexistkey hello (integer) 5 redis\u0026gt; APPEND notexistkey \u0026#34; - redis\u0026#34; (integer) 13 redis\u0026gt; GET myphone \u0026#34;hello - redis\u0026#34; STRLEN\r#\r\r返回 key 的值的长度。如果 key 不存在，则返回 0。果 key 的值不是字符串，则返回一个错误。\nredis\u0026gt; SET existkey \u0026#34;hello redis\u0026#34; OK redis\u0026gt; STRLEN existkey (integer) 11 redis\u0026gt; STRLEN notexistkey (integer) 0 SETRANGE\r#\r\r使用 value 覆盖 key 以偏移量 offset 开始的字符串。\nSETRANGE key offset value 如果 key 原来储存的字符串长度比偏移量小，那么原字符和偏移量之间的空白将用零字节(\u0026quot;\\x00\u0026rdquo;)来填充。\n# 对非空字符串进行 SETRANGE redis\u0026gt; SET greeting \u0026#34;hello redis\u0026#34; OK redis\u0026gt; SETRANGE greeting 6 \u0026#34;Redis\u0026#34; (integer) 11 redis\u0026gt; GET greeting \u0026#34;hello Redis\u0026#34; # 对空字符串/不存在的 key 进行 SETRANGE redis\u0026gt; EXISTS notexistkey (integer) 0 redis\u0026gt; SETRANGE notexistkey 5 \u0026#34;Redis\u0026#34; (integer) 10 redis\u0026gt; GET notexistkey \u0026#34;\\x00\\x00\\x00\\x00\\x00Redis\u0026#34; GETRANGE\r#\r\rGETRANGE 类似 javascript 中的 substring。提取字符串中两个指定的索引号之间的字符。\nGETRANGE key start end start 提取字符串的起始位置，end 为结束位置。 如果是负数，那么该参数声明从字符串的尾部开始算起的位置。也就是说，-1 指字符串中最后一个字符，-2 指倒数第二个字符，以此类推。\nredis\u0026gt; SET greeting \u0026#34;hello, redis\u0026#34; OK redis\u0026gt; GETRANGE greeting 0 4 \u0026#34;hello\u0026#34; # 不支持回绕操作 redis\u0026gt; GETRANGE greeting -1 -5 \u0026#34;\u0026#34; redis\u0026gt; GETRANGE greeting -3 -1 \u0026#34;dis\u0026#34; # 从第一个到最后一个 redis\u0026gt; GETRANGE greeting 0 -1 \u0026#34;hello, redis\u0026#34; # 取值范围超过实际字符串长度范围，会被自动忽略 redis\u0026gt; GETRANGE greeting 0 1008611 \u0026#34;hello, redis\u0026#34; "});index.add({'id':23,'href':'/db-learn/docs/redis/25_redis-expire-strategy/','title':"Redis 的过期策略和内存淘汰机制",'content':"Redis 的过期策略和内存淘汰机制\r#\r\r在日常开发中，我们使用 Redis 存储 key 时通常会设置一个过期时间，但是 Redis 是怎么删除过期的 key，而且 Redis 是单线程的， 删除 key 过于频繁会不会造成阻塞。要搞清楚这些，就要了解 Redis 的过期策略和内存淘汰机制。\nRedis 采用的是定期删除加懒惰删除策略。懒惰删除就是在客户端访问这个 key 的时候，redis 对 key 的过期时间进行检查，如果过期了 就立即删除。定时删除是集中处理，惰性删除是零散处理。\n定期删除策略\r#\r\rRedis 会将每个设置了过期时间的 key 放入到一个独立的字典中，默认每 100ms 进行一次过期扫描：\n 随机抽取 20 个 key 删除这 20 个 key 中过期的 key 如果过期的 key 比例超过 1/4，就重复步骤 1，继续删除。  之所以不扫描所有的 key，是因为 Redis 是单线程，全部扫描会导致线程卡死。\n而且为了防止每次扫描过期的 key 比例都超过 1/4，导致不停循环卡死线程，Redis 为每次扫描添加了上限时间，默认是 25ms。\n如果一个大型的 Redis 实例中所有的 key 在同一时间过期了，会出现怎样的结果？\r#\r\r大量的 key 在同一时间过期，那么 Redis 会持续扫描过期字典 (循环多次)，直到过期字典中过期的 key 变得稀疏，才会停止 (循环次数明显下降)。 这会导致线上读写请求出现明显的卡顿现象。导致这种卡顿的另外一种原因是内存管理器需要频繁回收内存页，这也会产生一定的 CPU 消耗。\n而且，如果客户端将请求超时时间设置的比较短，比如 10ms，但是请求以为过期扫描导致至少等待 25ms 后才会进行处理，那么就会出现大量的请求因为 超时而关闭，业务端就会出现很多异常。这时你还无法从 Redis 的 slowlog 中看到慢查询记录，因为慢查询指的是逻辑处理过程慢，不包含等待时间。\n所以要避免大批量的 key 同时过期，可以给过期时间设置一个随机范围，分散过期处理的压力。\n从库的过期策略\r#\r\r从库不会进行过期扫描，从库对过期的处理是被动的。主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行 这条 del 指令来删除过期的 key。\n因为指令同步是异步进行的，所以主库过期的 key 的 del 指令没有及时同步到从库的话，会出现主从数据的不一致。\n内存淘汰机制\r#\r\r当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)。交换会让 Redis 的性能急剧下降，对于 Redis 来说，这样 龟速的存取效率基本上等于不可用。\nRedis 为了限制最大使用内存，提供了配置参数 maxmemory，可以在 redis.conf 中配置。当内存超出 maxmemory，Redis 提供了几种 策略（maxmemory-policy）让用户选择：\n noeviction：当内存超出 maxmemory，写入请求会报错，但是删除和读请求可以继续。（这个可是默认的策略）。 allkeys-lru：当内存超出 maxmemory，在所有的 key 中，移除最少使用的 key。 allkeys-random：当内存超出 maxmemory，在所有的 key 中，随机移除某个 key。（应该没人用吧） volatile-lru：当内存超出 maxmemory，在设置了过期时间 key 的字典中，移除最少使用的 key。 volatile-random：当内存超出 maxmemory，在设置了过期时间 key 的字典中，随机移除某个 key。 volatile-ttl：当内存超出 maxmemory，在设置了过期时间 key 的字典中，优先移除 ttl 小的。  volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。如果只是拿 Redis 做缓存，那应该使 用 allkeys-xxx，客户端写缓存时不必携带过期时间。如果还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没 有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。\n近似 LRU 算法\r#\r\rRedis 使用的并不是完全 LRU 算法。不使用 LRU 算法，是为了节省内存，Redis 采用的是随机 LRU 算法，Redis 为每一个 key 增加了 一个 24 bit 的字段，用来记录这个 key 最后一次被访问的时间戳。注意 Redis 的 LRU 淘汰策略是懒惰处理，也就是不会主动执行淘汰 策略，当 Redis 执行写操作时，发现内存超出 maxmemory，就会执行 LRU 淘汰算法。这个算法就是随机采样出 5 (默认值)个 key，然后移 除最旧的 key，如果移除后内存还是超出 maxmemory，那就继续随机采样淘汰，直到内存低于 maxmemory 为止。\n如何采样就是看 maxmemory-policy 的配置，如果是 allkeys 就是从所有的 key 字典中随机，如果是 volatile 就从带过期时间的 key 字 典中随机。每次采样多少个 key 看的是 maxmemory_samples 的配置，默认为 5。\nLFU\r#\r\rRedis 4.0 里引入了一个新的淘汰策略 —— LFU（Least Frequently Used） 模式，作者认为它比 LRU 更加优秀。\nLFU 表示按最近的访问频率进行淘汰，它比 LRU 更加精准地表示了一个 key 被访问的热度。\n如果一个 key 长时间不被访问，只是刚刚偶然被用户访问了一下，那么在使用 LRU 算法下它是不容易被淘汰的，因为 LRU 算法认为当前这个 key 是 很热的。而 LFU 是需要追踪最近一段时间的访问频率，如果某个 key 只是偶然被访问一次是不足以变得很热的，它需要在近期一段时间内被访问很多 次才有机会被认为很热。\nRedis 对象的热度\r#\r\rRedis 的所有对象结构头中都有一个 24 bit 的字段，这个字段用来记录对象的热度。\n// redis 的对象头 typedef struct redisObject { unsigned type:4; // 对象类型如 zset/set/hash 等等  unsigned encoding:4; // 对象编码如 ziplist/intset/skiplist 等等  unsigned lru:24; // 对象的「热度」  int refcount; // 引用计数  void *ptr; // 对象的 body } robj; LRU 模式\r#\r\r在 LRU 模式下，lru 字段存储的是 Redis 时钟 server.lruclock，Redis 时钟是一个 24 bit 的整数，默认是 Unix 时间戳对 2^24 取 模的结果，大约 97 天清零一次。当某个 key 被访问一次，它的对象头的 lru 字段值就会被更新为 server.lruclock。\nLFU 模式\r#\r\r在 LFU 模式下，lru 字段 24 个 bit 用来存储两个值，分别是 ldt(last decrement time) 和 logc(logistic counter)。\n\rlogc 是 8 个 bit，用来存储访问频次，因为 8 个 bit 能表示的最大整数值为 255，存储频次肯定远远不够，所以这 8 个 bit 存储的 是频次的对数值，并且这个值还会随时间衰减。如果它的值比较小，那么就很容易被回收。为了确保新创建的对象不被回收，新对象的这 8 个 bit 会 初始化为一个大于零的值，默认是 LFU_INIT_VAL=5。\nldt 是 16 个位，用来存储上一次 logc 的更新时间，因为只有 16 位，所以精度不可能很高。它取的是分钟时间戳对 2^16 进行取模，大约每 隔 45 天就会折返。同 LRU 模式一样，我们也可以使用这个逻辑计算出对象的空闲时间，只不过精度是分钟级别的。图中的 server.unixtime 是 当前 redis 记录的系统时间戳，和 server.lruclock 一样，它也是每毫秒更新一次。\n启用 LFU\r#\r\rRedis 4.0 给淘汰策略配置参数 maxmemory-policy 增加了 2 个选项，\n volatile-lfu：对带过期时间的 key 执行 lfu 淘汰算法 allkeys-lfu：对所有的 key 执行 lfu 淘汰算法  使用 object freq 指令获取对象的 lfu 计数值：\n\u0026gt; config set maxmemory-policy allkeys-lfu OK \u0026gt; set codehole yeahyeahyeah OK // 获取计数值，初始化为 LFU_INIT_VAL=5 \u0026gt; object freq codehole (integer) 5 // 访问一次 \u0026gt; get codehole \u0026#34;yeahyeahyeah\u0026#34; // 计数值增加了 \u0026gt; object freq codehole (integer) 6 为什么 Redis 要缓存系统时间戳？\r#\r\r因为每一次获取系统时间戳都是一次系统调用。系统调用相对来说是比较费时间的，作为单线程的 Redis 表示承受不起，所以它需要对时间进行缓存，获 取时间都直接从缓存中直接拿。\n内存回收机制\r#\r\rRedis 并不总是可以将空闲内存立即归还给操作系统。\n如果当前 Redis 内存有 10G，当你删除了 1GB 的 key 后，再去观察内存，你会发现内存变化不会太大。原因是操作系统回收内存是以页为单位， 如果这个页上只要有一个 key 还在使用，那么它就不能被回收。Redis 虽然删除了 1GB 的 key，但是这些 key 分散到了很多页面中，每个页面 都还有其它 key 存在，这就导致了内存不会立即被回收。\n不过，如果你执行 flushdb，然后再观察内存会发现内存确实被回收了。原因是所有的 key 都干掉了，大部分之前使用的页面都完全干净了，会立 即被操作系统回收。\nRedis 虽然无法保证立即回收已经删除的 key 的内存，但是它会重用那些尚未回收的空闲内存。\n懒惰删除\r#\r\rRedis 为什么要懒惰删除(lazy free)？\r#\r\r删除指令 del 会直接释放对象的内存，大部分情况下，这个指令非常快，没有明显延迟。不过如果删除的 key 是一个非常大的对象，比如一个包 含了千万元素的 hash，又或者在使用 FLUSHDB 和 FLUSHALL 删除包含大量键的数据库时，那么删除操作就会导致线程卡顿。\nredis 4.0 引入了 lazyfree 的机制，它可以将删除键或数据库的操作放在后台线程里执行， 从而尽可能地避免服务器阻塞。\nunlink\r#\r\runlink 指令，它能对删除操作进行懒处理，丢给后台线程来异步回收内存。\n\u0026gt; unlink key OK flush\r#\r\rflushdb 和 flushall 指令，用来清空数据库，这也是极其缓慢的操作。Redis 4.0 同样给这两个指令也带来了异步化，在指令后面增 加 async 参数就可以扔给后台线程慢慢处理。\n\u0026gt; flushall async OK 异步队列\r#\r\r主线程将对象的引用从有效数据中删除后，会将这个 key 的内存回收操作包装成一个任务，塞进异步任务队列，后台线程会从这个异步队列中取任务。任 务队列被主线程和异步线程同时操作，所以必须是一个线程安全的队列。\n不是所有的 unlink 操作都会延后处理，如果对应 key 所占用的内存很小，延后处理就没有必要了，这时候 Redis 会将对应的 key 内存立即回收， 跟 del 指令一样。\nAOF Sync\r#\r\rRedis 需要每秒一次(可配置)同步 AOF 日志到磁盘，确保消息尽量不丢失，需要调用 sync 函数，这个操作会比较耗时，会导致主线程的效率下降， 所以 Redis 也将这个操作移到异步线程来完成。执行 AOF Sync 操作的线程是一个独立的异步线程，它也有一个属于自己的任务队列，队列里只用 来存放 AOF Sync 任务。\n更多异步删除点\r#\r\rRedis 回收内存除了 del 指令和 flush 之外，还会存在 key 的过期、LRU 淘汰、rename 指令以及从库全量同步时接受完 rdb 文件后会 立即进行的 flush 操作。\nRedis4.0 为这些删除点也带来了异步删除机制，打开这些点需要额外的配置选项。\n slave-lazy-flush 从库接受完 rdb 文件后的 flush 操作 lazyfree-lazy-eviction 内存达到 maxmemory 时进行淘汰 lazyfree-lazy-expire key 过期删除 lazyfree-lazy-server-del rename 指令删除 destKey  异步删除的实现\r#\r\r"});index.add({'id':24,'href':'/db-learn/docs/mysql/redolog/','title':"redo 日志",'content':"redo 日志\r#\r\r"});index.add({'id':25,'href':'/db-learn/docs/mysql/undolog/','title':"undo 日志",'content':"undo 日志\r#\r\r"});index.add({'id':26,'href':'/db-learn/docs/redis/27_skills/','title':"一些命令行技巧",'content':"一些命令行技巧\r#\r\r直接模式\r#\r\r一般使用 redis-cli 都会进入交互模式，然后一问一答来读写服务器，这是交互模式。还有一种直接模式，通过将命令参数直接 传递给 redis-cli 来执行指令并获取输出结果。\n$ redis-cli incrby foo 5 (integer) 5 # 输出的内容较大，可以将输出重定向到外部文件 $ redis-cli info \u0026gt; info.txt $ wc -l info.txt 120 info.txt # 如果想指向特定的服务器 # -n 2 表示使用第 2 个库，相当于 select 2 $ redis-cli -h localhost -p 6379 -n 2 ping PONG 批量执行命令\r#\r\r$ cat cmds.txt set foo1 bar1 set foo2 bar2 set foo3 bar3 ...... $ cat cmds.txt | redis-cli OK OK OK ... # 或者 $ redis-cli \u0026lt; cmds.txt OK OK OK ... set 多行字符串\r#\r\r如果一个字符串有多行，如何传入 set 指令？使用 -x 选项，该选项会使用标准输入的内容作为最后一个参数。\n$ cat str.txt Ernest Hemingway once wrote, \u0026#34;The world is a fine place and worth fighting for.\u0026#34; I agree with the second part. $ redis-cli -x set foo \u0026lt; str.txt OK $ redis-cli get foo \u0026#34;Ernest Hemingway once wrote,\\n\\\u0026#34;The world is a fine place and worth fighting for.\\\u0026#34;\\nI agree with the second part.\\n\u0026#34; 重复执行指令\r#\r\rredis-cli 还支持重复执行指令多次，每条指令执行之间设置一个间隔时间，如此便可以观察某条指令的输出内容随时间变化。\n// 间隔 1s，执行 5 次，观察 qps 的变化 $ redis-cli -r 5 -i 1 info | grep ops instantaneous_ops_per_sec:43469 instantaneous_ops_per_sec:47460 instantaneous_ops_per_sec:47699 instantaneous_ops_per_sec:46434 instantaneous_ops_per_sec:47216 如果将次数设置为 -1 那就是重复无数次永远执行下去。如果不提供 -i 参数，那就没有间隔，连续重复执行。\n在交互模式下也可以重复执行指令，形式上比较怪异，在指令前面增加次数\n127.0.0.1:6379\u0026gt; 5 ping PONG PONG PONG PONG PONG 监控服务器状态\r#\r\r可以使用 --stat 参数来实时监控服务器的状态，间隔 1s 实时输出一次。\n$ redis-cli --stat ------- data ------ --------------------- load -------------------- - child - keys mem clients blocked requests connections 2 6.66M 100 0 11591628 (+0) 335 2 6.66M 100 0 11653169 (+61541) 335 2 6.66M 100 0 11706550 (+53381) 335 2 6.54M 100 0 11758831 (+52281) 335 2 6.66M 100 0 11803132 (+44301) 335 2 6.66M 100 0 11854183 (+51051) 335 可以使用 -i 参数调整输出间隔。\n扫描大 KEY\r#\r\r遇到 Redis 偶然卡顿问题，第一个想到的就是实例中是否存在大 KEY，大 KEY 的内存扩容以及释放都会导致主线程卡顿。 --bigkeys 参数可以很快扫出内存里的大 KEY，使用 -i 参数控制扫描间隔，避免扫描指令导致服务器的 ops 陡增报警。\n$ ./redis-cli --bigkeys -i 0.01 # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest zset found so far \u0026#39;hist:aht:main:async_finish:20180425:17\u0026#39; with 1440 members [00.00%] Biggest zset found so far \u0026#39;hist:qps:async:authorize:20170311:27\u0026#39; with 2465 members [00.00%] Biggest hash found so far \u0026#39;job:counters:6ya9ypu6ckcl\u0026#39; with 3 fields [00.01%] Biggest string found so far \u0026#39;rt:aht:main:device_online:68:{-4}\u0026#39; with 4 bytes [00.01%] Biggest zset found so far \u0026#39;machine:load:20180709\u0026#39; with 2879 members [00.02%] Biggest string found so far \u0026#39;6y6fze8kj7cy:{-7}\u0026#39; with 90 bytes redis-cli 对于每一种对象类型都会记录长度最大的 KEY，对于每一种对象类型，刷新一次最高记录就会立即输出一次。它能保证输出长度 为 Top1 的 KEY，但是 Top2、Top3 等 KEY 是无法保证可以扫描出来的。一般的处理方法是多扫描几次，或者是消灭了 Top1 的 KEY 之后再扫 描确认还有没有次大的 KEY。\n采样服务器指令\r#\r\r现在线上有一台 Redis 服务器的 OPS 太高，有很多业务模块都在使用这个 Redis，如何才能判断出来是哪个业务导致了 OPS 异常的高。这 时可以对线上服务器的指令进行采样，观察采样的指令大致就可以分析出 OPS 占比高的业务点。这时就要使用 monitor 指令，它会将服务器瞬间执行的指令全 部显示出来。不过使用的时候要注意即使使用 ctrl+c 中断，否则你的显示器会噼里啪啦太多的指令瞬间让你眼花缭乱。\n$ redis-cli --host 192.168.x.x --port 6379 monitor 1539853410.458483 [0 10.100.90.62:34365] \u0026#34;GET\u0026#34; \u0026#34;6yax3eb6etq8:{-7}\u0026#34; 1539853410.459212 [0 10.100.90.61:56659] \u0026#34;PFADD\u0026#34; \u0026#34;growth:dau:20181018\u0026#34; \u0026#34;2klxkimass8w\u0026#34; 1539853410.462938 [0 10.100.90.62:20681] \u0026#34;GET\u0026#34; \u0026#34;6yax3eb6etq8:{-7}\u0026#34; 1539853410.467231 [0 10.100.90.61:40277] \u0026#34;PFADD\u0026#34; \u0026#34;growth:dau:20181018\u0026#34; \u0026#34;2kei0to86ps1\u0026#34; 1539853410.470319 [0 10.100.90.62:34365] \u0026#34;GET\u0026#34; \u0026#34;6yax3eb6etq8:{-7}\u0026#34; 1539853410.473927 [0 10.100.90.61:58128] \u0026#34;GET\u0026#34; \u0026#34;6yax3eb6etq8:{-7}\u0026#34; 1539853410.475712 [0 10.100.90.61:40277] \u0026#34;PFADD\u0026#34; \u0026#34;growth:dau:20181018\u0026#34; \u0026#34;2km8sqhlefpc\u0026#34; 1539853410.477053 [0 10.100.90.62:61292] \u0026#34;GET\u0026#34; \u0026#34;6yax3eb6etq8:{-7}\u0026#34; 诊断服务器时延\r#\r\r平时诊断两台机器的时延一般是使用 Unix 的 ping 指令。Redis 也提供了时延诊断指令，不过它的原理不太一样，它是诊断当前机器和 Redis 服务 器之间的指令(PING 指令)时延，它不仅仅是物理网络的时延，还和当前的 Redis 主线程是否忙碌有关。如果你发现 Unix 的 ping 指令时延很小， 而 Redis 的时延很大，那说明 Redis 服务器在执行指令时有微弱卡顿。\n$ redis-cli --host 192.168.x.x --port 6379 --latency min: 0, max: 5, avg: 0.08 (305 samples) 时延单位是 ms。redis-cli 还能显示时延的分布情况，而且是图形化输出。\n$ redis-cli --latency-dist 远程 rdb 备份\r#\r\r执行下面的命令就可以将远程的 Redis 实例备份到本地机器，远程服务器会执行一次 bgsave 操作，然后将 rdb 文件传输到客户端。\n$ ./redis-cli --host 192.168.x.x --port 6379 --rdb ./user.rdb SYNC sent to master, writing 2501265095 bytes to \u0026#39;./user.rdb\u0026#39; Transfer finished with success. 模拟从库\r#\r\r如果你想观察主从服务器之间都同步了那些数据，可以使用 redis-cli 模拟从库。\n$ ./redis-cli --host 192.168.x.x --port 6379 --slave SYNC with master, discarding 51778306 bytes of bulk transfer... SYNC done. Logging commands from master. ... 从库连上主库的第一件事是全量同步，所以看到上面的指令卡顿这很正常，待首次全量同步完成后，就会输出增量的 aof 日志。\n"});index.add({'id':27,'href':'/db-learn/docs/redis/22_sync/','title':"主从同步",'content':"主从同步\r#\r\rRedis 同步支持主从同步和从从同步。\n增量同步\r#\r\rRedis 同步的是指令流，主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存 buffer 中，然后异步将 buffer 中的指令 同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一边向主节点反馈自己同步到哪里了 (偏移量)。\n因为内存的 buffer 是有限的，所以 Redis 主库不能将所有的指令都记录在内存 buffer 中。Redis 的复制内存 buffer 是一个定长的 环形数组，如果数组内容满了，就会从头开始覆盖前面的内容。\n如果因为网络状况不好，从节点在短时间内无法和主节点进行同步，那么当网络状况恢复时，Redis 的主节点中那些没有同步的指令在 buffer 中有 可能已经被后续的指令覆盖掉了，从节点将无法直接通过指令流来进行同步，这个时候就需要用到更加复杂的同步机制 —— 快照同步。\n快照同步\r#\r\r快照同步是一个非常耗费资源的操作，同步过程：\n 在主库上进行一次 bgsave 将当前内存的数据全部快照到磁盘文件中，然后再将快照文件的内容全部传送到从节点。 从节点将快照文件接受完毕后，立即执行一次全量加载，加载之前先要将当前内存的数据清空。 加载完毕后通知主节点继续进行增量同步。  在整个快照同步进行的过程中，主节点的复制 buffer 还在不停的往前移动，如果快照同步的时间过长或者复制 buffer 太小，都会导致同步期间 的增量指令在复制 buffer 中被覆盖，这样就会导致快照同步完成后无法进行增量复制，然后会再次发起快照同步，如此极有可能会陷入快照同步的 死循环。\n所以务必配置一个合适的复制 buffer 大小参数，避免快照复制的死循环。\n增加从节点\r#\r\r当从节点刚刚加入到集群时，它必须先要进行一次快照同步，同步完成后再继续进行增量同步。\n无盘复制\r#\r\r主节点在进行快照同步时，会进行很重的文件 IO 操作，特别是对于非 SSD 磁盘存储时，快照会对系统的负载产生较大影响。特别是当系统正 在进行 AOF 的 fsync 操作时如果发生快照，fsync 将会被推迟执行，这就会严重影响主节点的服务效率。\n无盘复制是指主服务器直接通过套接字将快照内容发送到从节点，生成快照是一个遍历的过程，主节点会一边遍历内存，一边将序列化的内容发送到从 节点，从节点还是跟之前一样，先将接收到的内容存储到磁盘文件中，再进行一次性加载。\nWait 指令\r#\r\rRedis 的复制是异步进行的，Redis3.0 版本以后提供了 wait 指令可以让异步复制变身同步复制，确保系统的强一致性。\n\u0026gt; set key value OK \u0026gt; wait 1 0 (integer) 1 wait 提供两个参数，第一个参数是从库的数量 N，第二个参数是时间 t，以毫秒为单位。它表示等待 wait 指令之前的所 有写操作同步到 N 个从库 (也就是确保 N 个从库的同步没有滞后)，最多等待时间 t。如果时间 t=0，表示无限等待直到 N 个从库同步完成达成一致。\n假设此时出现了网络分区，wait 指令第二个参数时间 t=0，主从同步无法继续进行，wait 指令会永远阻塞，Redis 服务器将丧失可用性。\n"});index.add({'id':28,'href':'/db-learn/docs/mysql/transaction/','title':"事务",'content':"事务\r#\r\r事务的四个特性：ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性）。\n事务处理是一种机制，用来管理必须成批执行的 MySQL 操作，以保证数据库不包含不完整的操作结果。利用事务处理，可以保证一组操作不会中途停止， 它们或者作为整体执行，或者完全不执行（除非明确指示）。如果没有错误发生，整组语句提交给（写到）数据库表。如果发生错误，则进行回退（撤销） 以恢复数据库到某个已知且安全的状态。\n关于事务处理需要知道的几个术语：\n 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（place- holder），你可以对它发布回退（与回退整个事务处理不同）  语法\r#\r\r控制事务处理\r#\r\r管理事务处理的关键在于将 SQL 语句组分解为逻辑块，并明确规定数据何时应该回退，何时不应该回退。\n下面的语句来标识事务的开始：\nSTART TRANSACTION ROLLBACK\r#\r\rROLLBACK 命令用来回退（撤销）MySQL 语句：\nselect * from ordertotals; start transaction; delete from ordertotals; select * from ordertotals; rollback; select * from ordertotals; 先执行一条 SELECT 以显示该表不为空。然后开始一个事务处理，用一条 DELETE 语句删除 ordertotals 中的所有行。 另一条 SELECT 语句验证 ordertotals 确实为空。这时用一条 ROLLBACK 语句回退 START TRANSACTION 之后的所有语句，最后 一条 SELECT 语句显示该表不为空。\nROLLBACK 只能在一个事务处理内使用（在执行一条 START TRANSACTION 命令之后）。\n哪些语句可以回退？\r#\r\r事务处理用来管理 INSERT、UPDATE 和 DELETE 语句。不能回退 SELECT 语句。（这样做也没有什么意义）不能回退 CREATE 或 DROP 操作。事务处理块中可以使用这两条语句，但如果你执行回退，它们不会被撤销。\nCOMMIT\r#\r\r一般的 MySQL 语句都是直接针对数据库表执行和编写的。这就是所谓的隐含提交（implicit commit），即提交（写或保存）操作是自动进行的。\n在事务处理块中，提交不会隐含地进行。为进行明确的提交，使用 COMMIT 语句：\nstart transaction; delete from orderitems where order_num = 20005; delete from orders where order_num = 20005; commit;  当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭（将来的更改会隐含提交）。\n 保留点\r#\r\r简单的 ROLLBACK 和 COMMIT 语句就可以写入或撤销整个事务处理。复杂的事务处理可能需要部分提交或回退。\n为了支持回退部分事务处理，必须能在事务处理块中合适的位置放置占位符。这样，如果需要回退，可以回退到某个占位符。这些占位符称为保留点。\n创建占位符，可使用 SAVEPOINT 语句：SAVEPOINT delete1;。 每个保留点都取标识它的唯一名字，以便在回退时，MySQL 知道要回退到何处。\n回退到本例给出的保留点，可执行：ROLLBACK TO delete1;\n 保留点在事务处理完成（执行一条 ROLLBACK 或 COMMIT）后自动释放。\n "});index.add({'id':29,'href':'/db-learn/docs/redis/21_transaction/','title':"事务",'content':"事务\r#\r\r为了确保连续多个操作的原子性，一个成熟的数据库通常都会有事务支持，Redis 也支持简单的事务模型。\n使用\r#\r\r每个事务的操作都有 begin、commit 和 rollback 三个指令：\n begin 事务的开始 commit 事务的提交 rollback 事务的回滚  begin(); try { command1(); command2(); .... commit(); } catch(Exception e) { rollback(); } Redis 在形式上看起来也差不多，分别是 multi/exec/discard。\n multi 事务的开始 exec 事务的执行 discard 事务的丢弃  \u0026gt; multi OK \u0026gt; incr books QUEUED \u0026gt; incr books QUEUED \u0026gt; exec (integer) 1 (integer) 2 上面的指令演示了一个完整的事务过程，所有的指令在 exec 之前不执行，而是缓存在服务器的一个事务队列中，服务器一旦收到 exec 指令，才 开执行整个事务队列，执行完毕后一次性返回所有指令的运行结果。因为 Redis 的单线程特性，它不用担心自己在执行队列的时候被其它指令打搅，可 以保证他们能得到的原子性执行。\n原子性\r#\r\r\u0026gt; multi OK \u0026gt; set books iamastring QUEUED \u0026gt; incr books QUEUED \u0026gt; set poorman iamdesperate QUEUED \u0026gt; exec 1) OK 2) (error) ERR value is not an integer or out of range 3) OK \u0026gt; get books \u0026#34;iamastring\u0026#34; \u0026gt; get poorman \u0026#34;iamdesperate QUEUED 是一个简单字符串，同 OK 是一个形式，它表示指令已经被服务器缓存到队列里了。\n上面的例子是事务执行到中间遇到失败了，因为不能对一个字符串进行数学运算，事务在遇到指令执行失败后，后面的指令还继续执行， 所以 poorman 的值能继续得到设置。\n到这里，应该明白 Redis 的事务根本不能算\u0026quot;原子性\u0026rdquo;，而仅仅是满足了事务的\u0026quot;隔离性\u0026rdquo;，隔离性中的串行化 —— 当前执行的事务有着不被其 它事务打断的权利。\ndiscard(丢弃)\r#\r\rdiscard 指令，用于丢弃事务缓存队列中的所有指令，在 exec 执行之前。\n\u0026gt; get books (nil) \u0026gt; multi OK \u0026gt; incr books QUEUED \u0026gt; incr books QUEUED \u0026gt; discard OK \u0026gt; get books (nil) 优化\r#\r\r上面的 Redis 事务在发送每个指令到事务缓存队列时都要经过一次网络读写，当一个事务内部的指令较多时，需要的网络 IO 时间也会线性增长。所以 通常 Redis 的客户端在执行事务时都会结合 pipeline 一起使用，这样可以将多次 IO 操作压缩为单次 IO 操作。比如 Python 的 Redis 客 户端时执行事务时是要强制使用 pipeline 的。\npipe = redis.pipeline(transaction=true) pipe.multi() pipe.incr(\u0026#34;books\u0026#34;) pipe.incr(\u0026#34;books\u0026#34;) values = pipe.execute() Watch\r#\r\r考虑到一个业务场景，Redis 存储了我们的账户余额数据，它是一个整数。现在有两个并发的客户端要对账户余额进行修改操作，这个修改不是一个 简单的 incrby 指令，而是要对余额乘以一个倍数。Redis 可没有提供 multiplyby 这样的指令。我们需要先取出余额然后在内存里乘以倍数，再将 结果写回 Redis。\n这就会出现并发问题，因为有多个客户端会并发进行操作。我们可以通过 Redis 的分布式锁来避免冲突，这是一个很好的解决方案。分布式锁是一 种悲观锁，那是不是可以使用乐观锁的方式来解决冲突？\nRedis 提供了这种 watch 的机制，它就是一种乐观锁。有了 watch 又多了一种可以用来解决并发修改的方法。 watch 的使用方式如下：\nwhile True: do_watch() commands() multi() send_commands() try: exec() break except WatchError: continue watch 会在事务开始之前盯住 1 个或多个关键变量，当事务执行时，也就是服务器收到了 exec 指令要顺序执行缓存的事务队列时，Redis 会 检查关键变量自 watch 之后，是否被修改了 (包括当前事务所在的客户端)。如果关键变量被人动过了，exec 指令就会返回 null 回复告知 客户端事务执行失败，这个时候客户端一般会选择重试。\n\u0026gt; watch books OK \u0026gt; incr books # 被修改了 (integer) 1 \u0026gt; multi OK \u0026gt; incr books QUEUED \u0026gt; exec # 事务执行失败 (nil)  Redis 禁止在 multi 和 exec 之间执行 watch 指令，而必须在 multi 之前做好盯住关键变量，否则会出错。\n "});index.add({'id':30,'href':'/db-learn/docs/mysql/13_isolation-level/','title':"事务的隔离级别",'content':"事务的隔离级别\r#\r\r事务并发执行遇到的问题\r#\r\r当数据库上多个事务并发执行的时候，就可能出现脏写（Dirty Write）、脏读（Dirty Read）、不可重复读（Non-Repeatable Read）、 幻读（Phantom Read）的问题。\n问题按照严重性来排序：\n脏写 \u0026gt; 脏读 \u0026gt; 不可重复读 \u0026gt; 幻读\r脏写\r#\r\r\u0026ldquo;脏写\u0026quot;是指，一个事务修改了另一个未提交事务修改过的数据。\n脏读\r#\r\r\u0026ldquo;脏读\u0026quot;是指，一个事务读到了另一个未提交事务修改过的数据。\n不可重复读\r#\r\r\u0026ldquo;不可重复读\u0026quot;是指，一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。\n\r如上图在 Session B 中提交了几个隐式事务（注意是隐式事务，意味着语句结束事务就提交了），这些事务都修改了 number 列为 1 的记录的列 name 的值，每 次事务提交之后，Session A 中的事务都可以查看到最新的值，这就是不可重复读。\n幻读\r#\r\r\u0026ldquo;幻读\u0026quot;是指。一个事务先根据某些条件查询出了一些记录，然后另一个事务又向表中插入了一些符合这些条件的记录，第一个事务再次使用相同条件查询时，把另一个 事务插入的记录也读出来了。\n\r如上图，Session A 中的事务先根据条件 number \u0026gt; 0 这个条件查询表 hero，得到了 name 列值为\u0026rsquo;刘备\u0026rsquo;的记录；之后 Session B 中提交了一个隐式 事务，该事务向表 hero 中插入了一条新记录；之后 Session A 中的事务再根据相同的条件 number \u0026gt; 0 查询表 hero，得到的结果集中包含了 Session B 中的事务新插入的那条记录，这就是幻读。\n 那对于先前已经读到的记录，之后记录被别的事物删除了，导致又读取不到了，这种情况，算不算幻读？这不属于幻读，幻读只是重点强调了读取到了之前读取没 有获取到的记录。\n 四种隔离级别\r#\r\r为了解决事务并发执行遇到的问题，就有了隔离级别的概念。SQL 标准中设立了 4 个隔离级别：\n READ UNCOMMITTED：读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 READ COMMITTED：读已提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 REPEATABLE READ：可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 SERIALIZABLE：可串行化是指，对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执 行完成，才能继续执行。  MySQL 的默认隔离级别为 REPEATABLE READ。\n事务隔离的实现\r#\r\r"});index.add({'id':31,'href':'/db-learn/docs/redis/12_bitmap/','title':"位图",'content':"位图\r#\r\r位图不是特殊的数据结构，它的内容其实就是普通的字符串，也就是 byte 数组。 在日常开发中，可能会有一些 bool 型数据需要存取，如果使用普通的 key/value 方式存储，会浪费很多存储空间,比如签到记录，签 了是 true，没签是 false，记录 365 天。如果每个用户存储 365 条记录，当用户量很庞大的时候，需要的存储空间是惊人的。\n对于这种操作，Redis 提供了位操作，这样每天的签到记录只占据一个位，用 1 表示已签到，0 表示没签，那么 365 天就是 365 个 bit，46 个字节 就可以完全容纳下，大大节约了存储空间。\nSETBIT\r#\r\rRedis 的位数组是自动扩展，如果设置了某个偏移位置超出了现有的内容范围，就会自动将位数组进行零扩充。\nSETBIT key offset value 设置指定偏移量上的 bit 的值。value 的值是 0 或 1。当 key 不存在时，自动生成一个新的 key。 offset 参数的取值范围为大于等于 0，小于 2^32(bit 映射限制在 512 MB 以内)。\nredis\u0026gt; SETBIT testbit 100 1 (integer) 0 redis\u0026gt; GETBIT testbit 100 (integer) 1 redis\u0026gt; GETBIT testbit 101 (integer) 0 # bit 默认被初始化为 0 GETBIT\r#\r\r获取指定偏移量上的位 bit 的值。\nGETBIT key offset 如果 offset 比字符串值的长度大，或者 key 不存在时，返回 0。\nredis\u0026gt; EXISTS testbit2 (integer) 0 redis\u0026gt; GETBIT testbit2 100 (integer) 0 redis\u0026gt; SETBIT testbit2 100 1 (integer) 0 redis\u0026gt; GETBIT bit 100 (integer) 1 BITPOS\r#\r\r获取 key 里面第一个被设置为 1 或者 0 的 bit 位。 BITPOS 可以用来做查找，例如，查找用户从哪一天开始第一次签到。如果指定了范围参数 start, end，就可以统计在某个时间范围内 用户签到了多少天，用户自某天以后的哪天开始签到。\nBITPOS key value start end value 是 0 或者 1。如果我们在空字符串或者 0 字节的字符串里面查找 bit 为 1 的内容，那么结果将返回 -1。 start 和 end 也可以包含负值，负值将从字符串的末尾开始计算，-1 是字符串的最后一个字节，-2 是倒数第二个，等等。 start 和 end 参数是字节索引，也就是说指定的位范围必须是 8 的倍数，而不能任意指定。 不存在的 key 将会被当做空字符串来处理。\nredis\u0026gt; SET testbit3 hello OK redis\u0026gt; BITPOS testbit3 1 # 第一个 1 位 (integer) 1 redis\u0026gt; BITPOS testbit3 0 # 第一个 0 位 (integer) 0 redis\u0026gt; SET mykey \u0026#34;\\xff\\xf0\\x00\u0026#34; OK redis\u0026gt; BITPOS mykey 0 # 查找字符串里面bit值为0的位置 (integer) 12 redis\u0026gt; SET mykey \u0026#34;\\x00\\xff\\xf0\u0026#34; OK redis\u0026gt; BITPOS mykey 1 0 # 查找字符串里面bit值为1从第0个字节开始的位置 (integer) 8 redis\u0026gt; BITPOS mykey 1 2 # 查找字符串里面bit值为1从第2个字节(12)开始的位置 (integer) 16 redis\u0026gt; set mykey \u0026#34;\\x00\\x00\\x00\u0026#34; OK redis\u0026gt; BITPOS mykey 1 # 查找字符串里面bit值为1的位置 (integer) -1 BITOP\r#\r\r对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。处理不同长度的字符串时，较短的那个字符串所缺少的部分 会被看作 0，空的 key 也被看作是包含 0 的字符串序列。\nBITOP operation destkey key [key ...] operation有四种操作可选：\n AND，对一个或多个 key 值求逻辑并。 OR，对一个或多个 key 值求逻辑或。 XOR，对一个或多个 key 值求逻辑异或。 NOT，对给定 key 值求逻辑非，注意 NOT 操作，只可以接收一个 key 作为输入，其他三个操作一个或多个。  BITOP 的复杂度为 O(N)，当处理大型矩阵(matrix)或者进行大数据量的统计时，最好将任务指派到 slave 进行，避免阻塞主节点。\nredis\u0026gt; SETBIT testbit1 0 1 # testbit1 = 1001 (integer) 0 redis\u0026gt; SETBIT testbit1 3 1 (integer) 0 redis\u0026gt; SETBIT testbit2 0 1 # testbit2 = 1101 (integer) 0 redis\u0026gt; SETBIT testbit2 1 1 (integer) 0 redis\u0026gt; SETBIT testbit2 3 1 (integer) 0 redis\u0026gt; BITOP AND andresult testbit1 testbit2 (integer) 1 redis\u0026gt; GETBIT andresult 0 # andresult = 1001 (integer) 1 redis\u0026gt; GETBIT andresult 1 (integer) 0 redis\u0026gt; GETBIT andresult 2 (integer) 0 redis\u0026gt; GETBIT andresult 3 (integer) 1 BITCOUNT\r#\r\r可以用来做高位统计，例如，统计用户一共签到了多少天。 计算指定 key 中，比特位被设置为 1 的数量。指定可选参数 start 和 end 时只统计指定位上的字符，否则统计全部。\nBITPOS key value start end redis\u0026gt; BITCOUNT testbit4 (integer) 0 redis\u0026gt; SETBIT testbit4 0 1 # 0001 (integer) 0 redis\u0026gt; BITCOUNT testbit4 (integer) 1 redis\u0026gt; SETBIT testbit4 3 1 # 1001 (integer) 0 redis\u0026gt; BITCOUNT testbit4 (integer) 2 BITFIELD\r#\r\rSETBIT 和 GETBIT 都只能操作一个 bit，如果要操作多个 bit 就使用 BITFIELD。\nBITFIELD 有四个子指令：\n GET type offset，返回指定的位域 SET type offset value，设置指定位域的值并返回它的原值 INCRBY type offset increment，自增或自减（如果 increment 为负数）指定位域的值并返回它的新值 OVERFLOW [WRAP|SAT|FAIL]，溢出策略子指令，通过设置溢出行为来改变调用 INCRBY 指令的后序操作  GET，SET，INCRBY 最多只能处理 64 个连续的位，超过 64 位，就得使用多个子指令，BITFIELD 可以一次执行多个子指令。\nGET 子指令\r#\r\rredis\u0026gt; set w hello OK redis\u0026gt; BITFIELD w get u4 0 # 从第一个位开始取 4 个位，结果是无符号数 (u) (integer) 6 redis\u0026gt; BITFIELD w get u3 2 # 从第三个位开始取 3 个位，结果是无符号数 (u) (integer) 5 redis\u0026gt; BITFIELD w get i4 0 # 从第一个位开始取 4 个位，结果是有符号数 (i) 1) (integer) 6 redis\u0026gt; BITFIELD w get i3 2 # 从第三个位开始取 3 个位，结果是有符号数 (i) 1) (integer) -3 # 执行多个子命令 redis\u0026gt; BITFIELD w get u4 0 get u3 2 get i4 0 get i3 2 1) (integer) 6 2) (integer) 5 3) (integer) 6 4) (integer) -3 有符号数最多可以获取 64 位，无符号数只能获取 63 位 (Redis 中的 integer 是有符号数，最大 64 位，不能传递 64 位无符号值)。如果超 出位数限制， Redis 就会告诉你参数错误。\nSET 子指令\r#\r\rSET 子指令将第二个字符 e 改成 a，a 的 ASCII 码是 97：\nredis\u0026gt; BITFIELD w set u8 8 97 # 从第 8 个位开始，将接下来的 8 个位用无符号数 97 替换 1) (integer) 101 redis\u0026gt; get w \u0026#34;hallo\u0026#34; INCRBY 子指令\r#\r\r它用来对指定范围的位进行自增操作。既然提到自增，就有可能出现溢出。如果增加了正数，会出现上溢，如果增加的是负数，就会出现下溢出。 Redis 默认的处理是折返。如果出现了溢出，就将溢出的符号位丢掉。如果是 8 位无符号数 255，加 1 后就会溢出，会全部变零。如果是 8 位有符 号数 127，加 1 后就会溢出变成 -128。\nredis\u0026gt; set w hello OK redis\u0026gt; bitfield w incrby u4 2 1 # 从第三个位开始，对接下来的 4 位无符号数 +1 1) (integer) 11 redis\u0026gt; bitfield w incrby u4 2 1 1) (integer) 12 redis\u0026gt; bitfield w incrby u4 2 1 1) (integer) 13 redis\u0026gt; bitfield w incrby u4 2 1 1) (integer) 14 redis\u0026gt; bitfield w incrby u4 2 1 1) (integer) 15 redis\u0026gt; bitfield w incrby u4 2 1 # 溢出折返 1) (integer) 0 OVERFLOW 子指令\r#\r\r用户可以使用 OVERFLOW 来选择溢出行为，默认是折返 (wrap)。\n WRAP 回环/折返 算法，默认是 WRAP，适用于有符号和无符号整型两种类型。对于无符号整型，回环计数将对整型最大值进行取模 操作（C 语言的标准行为）。对于有符号整型，上溢从最负的负数开始取数，下溢则从最大的正数开始取数，例如，如果 i8 整型的值设为 127，自 加 1 后的值变为 -128。 SAT 饱和截断算法，下溢之后设为最小的整型值，上溢之后设为最大的整数值。例如，i8 整型的值从 120 开始加 10 后，结果是 127，继续增 加，结果还是保持为 127。下溢也是同理，但量结果值将会保持在最负的负数值。 FAIL 失败算法，这种模式下，在检测到上溢或下溢时，不做任何操作。相应的返回值会设为 NULL，并返回给调用者。  redis\u0026gt; set w hello OK redis\u0026gt; bitfield w overflow sat incrby u4 2 1 1) (integer) 11 redis\u0026gt; bitfield w overflow sat incrby u4 2 1 1) (integer) 12 redis\u0026gt; bitfield w overflow sat incrby u4 2 1 1) (integer) 13 redis\u0026gt; bitfield w overflow sat incrby u4 2 1 1) (integer) 14 redis\u0026gt; bitfield w overflow sat incrby u4 2 1 1) (integer) 15 redis\u0026gt; bitfield w overflow sat incrby u4 2 1 # 保持最大值 1) (integer) 15 redis\u0026gt; set w hello OK redis\u0026gt; bitfield w overflow fail incrby u4 2 1 1) (integer) 11 redis\u0026gt; bitfield w overflow fail incrby u4 2 1 1) (integer) 12 redis\u0026gt; bitfield w overflow fail incrby u4 2 1 1) (integer) 13 redis\u0026gt; bitfield w overflow fail incrby u4 2 1 1) (integer) 14 redis\u0026gt; bitfield w overflow fail incrby u4 2 1 1) (integer) 15 redis\u0026gt; bitfield w overflow fail incrby u4 2 1 # 不执行 1) (nil) "});index.add({'id':32,'href':'/db-learn/docs/mongo/03_advance/','title':"使用",'content':"关系\r#\r\rMongoDB 中的关系表示文档之间的逻辑相关方式。关系可以通过内嵌（Embedded）或引用（Referenced）两种方式建模。 这样的关系可能是 1：1、1：N、N：1，也有可能是 N：N。\n例如，一个用户可能有多个地址，这是一个 1：N 的关系。\n// user 文档结构 { \u0026#34;_id\u0026#34;:ObjectId(\u0026#34;52ffc33cd85242f436000001\u0026#34;), \u0026#34;name\u0026#34;: \u0026#34;Tom Hanks\u0026#34;, \u0026#34;contact\u0026#34;: \u0026#34;987654321\u0026#34;, \u0026#34;dob\u0026#34;: \u0026#34;01-01-1991\u0026#34; } // address 文档结构 { \u0026#34;_id\u0026#34;:ObjectId(\u0026#34;52ffc4a5d85242602e000000\u0026#34;), \u0026#34;building\u0026#34;: \u0026#34;22 A, Indiana Apt\u0026#34;, \u0026#34;pincode\u0026#34;: 123456, \u0026#34;city\u0026#34;: \u0026#34;Los Angeles\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;California\u0026#34; } // 内嵌关系的建模 { \u0026#34;_id\u0026#34;:ObjectId(\u0026#34;52ffc33cd85242f436000001\u0026#34;), \u0026#34;contact\u0026#34;: \u0026#34;987654321\u0026#34;, \u0026#34;dob\u0026#34;: \u0026#34;01-01-1991\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Tom Benzamin\u0026#34;, \u0026#34;address\u0026#34;: [ { \u0026#34;building\u0026#34;: \u0026#34;22 A, Indiana Apt\u0026#34;, \u0026#34;pincode\u0026#34;: 123456, \u0026#34;city\u0026#34;: \u0026#34;Los Angeles\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;California\u0026#34; }, { \u0026#34;building\u0026#34;: \u0026#34;170 A, Acropolis Apt\u0026#34;, \u0026#34;pincode\u0026#34;: 456789, \u0026#34;city\u0026#34;: \u0026#34;Chicago\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;Illinois\u0026#34; }] } 该方法会将所有相关数据都保存在一个文档中，从而易于检索和维护。缺点是，如果内嵌文档不断增长，会对读写性能造成影响。\n引用关系的建模\r#\r\r这是一种设计归一化关系的方法。按照这种方法，这种引用关系也被称作手动引用，所有的用户和地址文档都将分别存放，而用户文档会包含一个字段，用来引用地址文档 id 字段。\n{ \u0026#34;_id\u0026#34;:ObjectId(\u0026#34;52ffc33cd85242f436000001\u0026#34;), \u0026#34;contact\u0026#34;: \u0026#34;987654321\u0026#34;, \u0026#34;dob\u0026#34;: \u0026#34;01-01-1991\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Tom Benzamin\u0026#34;, \u0026#34;address_ids\u0026#34;: [ ObjectId(\u0026#34;52ffc4a5d85242602e000000\u0026#34;), ObjectId(\u0026#34;52ffc4a5d85242602e000001\u0026#34;) ] } 数组字段 address_ids 含有相应地址的 ObjectId 对象。利用这些 ObjectId，能够查询地址文档，从而获取地址细节信息。利用这种方法时，需要进行两种查询： 首先从 user 文档处获取 address_ids，其次从 address 集合中获取这些地址。\nvar result = db.users.findOne({\u0026#34;name\u0026#34;:\u0026#34;Tom Benzamin\u0026#34;},{\u0026#34;address_ids\u0026#34;:1}) var addresses = db.address.find({\u0026#34;_id\u0026#34;:{\u0026#34;$in\u0026#34;:result[\u0026#34;address_ids\u0026#34;]}}) 数据库引用\r#\r\r上一节中，我们使用引用关系实现了归一化的数据库结构，这种引用关系也被称作手动引用，即可以手动地将引用文档的 id 保存在其他文档中。 有些情况下，文档包含其他集合的引用时，我们可以使用数据库引用（MongoDB DBRefs）。\n如何用数据库引用代替手动引用。假设一个数据库中存储有多个类型的地址（家庭地址、办公室地址、邮件地址，等等），这些地址保存在不同的集合中 （address_home、address_office、address_mailing，等等）。当 user 集合的文档引用了一个地址时，它还需要按照地址类型来指定所需要查看的集合。 这种情况下，一个文档引用了许多结合的文档，所以就应该使用 DBRef。\n使用数据库引用\r#\r\rDBRef 中有三个字段：\n $ref 该字段指定所引用文档的集合。 $id 该字段指定引用文档的 _id 字段 $db 该字段是可选的，包含引用文档所在数据库的名称。  假如在一个简单的 user 文档中包含着 DBRef 字段 address，如下所示：\n{ \u0026#34;_id\u0026#34;:ObjectId(\u0026#34;53402597d852426020000002\u0026#34;), \u0026#34;address\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;address_home\u0026#34;, \u0026#34;$id\u0026#34;: ObjectId(\u0026#34;534009e4d852427820000002\u0026#34;), \u0026#34;$db\u0026#34;: \u0026#34;tutorialspoint\u0026#34;}, \u0026#34;contact\u0026#34;: \u0026#34;987654321\u0026#34;, \u0026#34;dob\u0026#34;: \u0026#34;01-01-1991\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Tom Benzamin\u0026#34; } 数据库引用字段 address 指定出，引用地址文档位于 tutorialspoint 数据库的 address_home 集合中，并且它的 id 为 534009e4d852427820000002。\n在由 $ref 所指定的集合（本例中为 address_home）中，如何动态查找由 $id 所确定的文档。\nvar user = db.users.findOne({\u0026#34;name\u0026#34;:\u0026#34;Tom Benzamin\u0026#34;}) var dbRef = user.address db[dbRef.$ref].findOne({\u0026#34;_id\u0026#34;:(dbRef.$id)}) 查询分析\r#\r\r对于衡量数据库及索引设计的效率来说，分析查询是一个很重要的衡量方式。经常使用的查询有 $explain 和 $hint。\n$explain\r#\r\r$explain 操作提供的消息包括：查询消息、查询所使用的索引以及其他的统计信息。\n例如：\n// 创建索引 db.users.ensureIndex({gender:1,user_name:1}) // 查询 db.users.find({gender:\u0026#34;M\u0026#34;},{user_name:1,_id:0}).explain() // 输出 { \u0026#34;cursor\u0026#34; : \u0026#34;BtreeCursor gender_1_user_name_1\u0026#34;, \u0026#34;isMultiKey\u0026#34; : false, \u0026#34;n\u0026#34; : 1, \u0026#34;nscannedObjects\u0026#34; : 0, \u0026#34;nscanned\u0026#34; : 1, \u0026#34;nscannedObjectsAllPlans\u0026#34; : 0, \u0026#34;nscannedAllPlans\u0026#34; : 1, \u0026#34;scanAndOrder\u0026#34; : false, \u0026#34;indexOnly\u0026#34; : true, \u0026#34;nYields\u0026#34; : 0, \u0026#34;nChunkSkips\u0026#34; : 0, \u0026#34;millis\u0026#34; : 0, \u0026#34;indexBounds\u0026#34; : { \u0026#34;gender\u0026#34; : [ [ \u0026#34;M\u0026#34;, \u0026#34;M\u0026#34; ] ], \u0026#34;user_name\u0026#34; : [ [ { \u0026#34;$minElement\u0026#34; : 1 }, { \u0026#34;$maxElement\u0026#34; : 1 } ] ] } }  indexOnly 为true代表该查询使用了索引。 cursor 字段指定了游标所用的类型。BTreeCursor 类型代表了使用了索引并且提供了所用索引的名称。BasicCursor 表示进行了完整扫描，没有使用任何索引。 n 代表所返回的匹配文档的数量。 nscannedObjects 表示已扫描文档的总数。 nscanned 所扫描的文档或索引项的总数。  $hint\r#\r\r$hint 操作符强制索引优化器使用指定的索引运行查询。这尤其适用于测试带有多个索引的查询性能。比如，下列查询指定了用于该查询的 gender 和 user_name 字段的索引：\ndb.users.find({gender:\u0026#34;M\u0026#34;},{user_name:1,_id:0}).hint({gender:1,user_name:1}) Map Reduce\r#\r\rMap-Reduce（映射归约）是一种将大量数据压缩成有用的聚合结果的数据处理范式。MongoDB 使用 mapReduce 命令来实现映射归约操作。映射归约通常用来处理大型数据。\nmapReduce 命令的基本格式为：\ndb.collection.mapReduce( function() {emit(key,value);}, //map function  function(key,values) {return reduceFunction}, //reduce function  { out: collection, query: document, sort: document, limit: number } ) mapReduce 函数首先查询集合，然后将结果文档利用 emit 函数映射为键值对，然后再根据有多个值的键来简化。\n map 一个 JavaScript 函数，将一个值与键对应起来，并生成键值对。 reduce 一个 JavaScript 函数，用来减少或组合所有拥有同一键的文档。 out 指定映射归约查询结果的位置。 query 指定选择文档所用的选择标准（可选的）。 sort 指定可选的排序标准。 limit 指定返回的文档的最大数量值（可选的）。  使用\r#\r\r例如，下面这个存储用户发帖的文档结构。该文档存储用户的用户名（user_name）和发帖状态（status）。\n{ \u0026#34;post_text\u0026#34;: \u0026#34;tutorialspoint is an awesome website for tutorials\u0026#34;, \u0026#34;user_name\u0026#34;: \u0026#34;mark\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;active\u0026#34; } 在 posts 集合上使用 mapReduce 函数选择所有的活跃帖子，将它们基于用户名组合起来，然后计算每个用户的发帖量。代码如下：\ndb.posts.mapReduce( function() { emit(this.user_id,1); }, function(key, values) {return Array.sum(values)}, { query:{status:\u0026#34;active\u0026#34;}, out:\u0026#34;post_total\u0026#34; } ) 输出：\n{ \u0026#34;result\u0026#34; : \u0026#34;post_total\u0026#34;, \u0026#34;timeMillis\u0026#34; : 9, \u0026#34;counts\u0026#34; : { \u0026#34;input\u0026#34; : 4, \u0026#34;emit\u0026#34; : 4, \u0026#34;reduce\u0026#34; : 2, \u0026#34;output\u0026#34; : 2 }, \u0026#34;ok\u0026#34; : 1, } 结果显示，只有 4 个文档符合查询条件（status:\u0026quot;active\u0026quot;），于是 map 函数就生成了 4 个带有键值对的文档，而最终 reduce 函数将具有相同键值的映射文档变为了 2 个。\n查看 mapReduce 查询的结果\r#\r\r使用 find 操作符。\ndb.posts.mapReduce( function() { emit(this.user_id,1); }, function(key, values) {return Array.sum(values)}, { query:{status:\u0026#34;active\u0026#34;}, out:\u0026#34;post_total\u0026#34; } ).find() 全文检索\r#\r\r启用文本搜索\r#\r\r最初的文本搜索只是一种试验性功能，但从 2.6 版本起就成为默认功能了。但如果使用的是之前的 MongoDB，则需要使用下列代码启用文本搜索：\ndb.adminCommand({setParameter:true,textSearchEnabled:true}) 创建文本索引\r#\r\rdb.posts.ensureIndex({post_text:\u0026#34;text\u0026#34;}) 上面的代码在 post_text 字段上创建文本索引，以便搜索帖子文本之内的内容。\n在 post_text 字段上创建了文本索引，接下来搜索包含 tutorialspoint 文本内容的帖子。\ndb.posts.find({$text:{$search:\u0026#34;tutorialspoint\u0026#34;}}) 删除文本索引\r#\r\r// 找到索引名称 db.posts.getIndexes() // 删掉 db.posts.dropIndex(\u0026#34;post_text_text\u0026#34;) 正则表达式\r#\r\r正则表达式在所有语言当中都是经常会用到的一个功能，可以用来搜索模式或字符串中的单词。MongoDB 也提供了这一功能，使用 $regex 运算符来匹配字符串模式。 MongoDB 使用 PCRE（可兼容 Perl 的正则表达式）作为正则表达式语言。\n使用正则表达式不需要使用任何配置或命令。\n假如 posts 集合有下面这个文档，它包含着帖子文本及其标签。\n{ \u0026#34;post_text\u0026#34;: \u0026#34;enjoy the mongodb articles on tutorialspoint\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;mongodb\u0026#34;, \u0026#34;tutorialspoint\u0026#34; ] } 使用下列正则表达式来搜索包含 tutorialspoint 的所有帖子。\ndb.posts.find({post_text:{$regex:\u0026#34;tutorialspoint\u0026#34;}}) // 或者 db.posts.find({post_text:/tutorialspoint/}) 不区分大小写\r#\r\r要想使搜索不区分大小写，使用 $options 参数和值 $i。\ndb.posts.find({post_text:{$regex:\u0026#34;tutorialspoint\u0026#34;,$options:\u0026#34;$i\u0026#34;}}) 使用正则表达式来处理数组元素\r#\r\r还可以在数组字段上使用正则表达式。在实现标签的功能时，这尤为重要。假如想搜索标签以 “tutorial” 开始（tutorial、tutorials、tutorialpoint 或 tutorialphp）的帖子，可以使用下列代码：\ndb.posts.find({tags:{$regex:\u0026#34;tutorial\u0026#34;}}) 优化正则表达式查询\r#\r\r 如果文档字段已经设置了索引，查询将使用索引值来匹配正则表达式，从而使查询效率相对于扫描整个集合的正则表达式而言大大提高。 如果正则表达式为前缀表达式，所有的匹配结果都要在前面带有特殊的前缀字符串。比如，如果正则表达式为 ^tut，那么查询将搜索所有以 tut 开始的字符串。  ID自增\r#\r\r默认情况下，MongoDB 将 _id 字段（使用 12 字节的 ObjectId）来作为文档的唯一标识。但在有些情况下，我们希望 _id 字段值能够自动增长，而不是固守在 ObjectId 值上。\n使用 counters 集合来程序化地实现该功能。\n 使用 counters 集合 假设存在下列文档 products，我们希望 _id 字段值是一个能够自动增长的整数序列（1、2、3、4 …… n）。  { \u0026#34;_id\u0026#34;:1, \u0026#34;product_name\u0026#34;: \u0026#34;Apple iPhone\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;mobiles\u0026#34; } 创建一个 counters 集合，其中包含了所有序列字段最后的序列值。 现在，将文档（productid 是它的键）插入到 counters 集合中：\ndb.counters.insert({_id:\u0026#34;productid\u0026#34;,sequence_value:0}) sequence_value 字段保存了序列的最后值。\n创建一个 getNextSequenceValue 函数 创建一个 getNextSequenceValue 函数，该函数以序列名为输入，按照 1 的幅度增加序列数，返回更新的序列数。在该例中，序列名称为 productid。  function getNextSequenceValue(sequenceName){ var sequenceDocument = db.counters.findAndModify( { query:{_id: sequenceName }, update: {$inc:{sequence_value:1}}, new:true }); return sequenceDocument.sequence_value; } 使用 getNextSequenceValue 函数  db.products.insert({\u0026#34;_id\u0026#34;:getNextSequenceValue(\u0026#34;productid\u0026#34;),\u0026#34;product_name\u0026#34;:\u0026#34;Apple iPhone\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;mobiles\u0026#34;}) db.products.insert({\u0026#34;_id\u0026#34;:getNextSequenceValue(\u0026#34;productid\u0026#34;),\u0026#34;product_name\u0026#34;:\u0026#34;Samsung S3\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;mobiles\u0026#34;}) 副本集\r#\r\r生产环境下，把数据库放到一台服务器上是有很大风险的。Mongo 的复制功能可以把数据副本保存到多台服务器上。在 Mongo 中，只需要创建副本集 就可以使用复制功能了。副本集是一组服务器，一个是主服务器，处理客户端请求，还有多个备份服务器，用于保存数据副本。并且备份服务器可以在主服务器崩溃时 自动选举一个新的主服务器。\n副本集中有个比较重要的概念叫做大多数（超过一半以上的成员）：选择主节点时，只有在大多数节点支持时才能继续作为主节点。写操作被复制到大多数成员时， 这个写操作就是安全的。\n如果副本集中有些成员挂了，不会影响“大多数”，“大多数”是基于副本集的配置计算的。\n假设有一个包含 5 个成员的副本集，3 个成员不可用，2 个正常。那么剩余的 2 个成员无法达到“大多数”的要求，就无法选举出主节点。而且如果这 2 个成员中有一个是 主节点，这个主节点也会退位。那么，之后就变成了 2 个备份节点和 3 个不可用节点。\n这样的规则是不是弱爆了？明明有两个可用的节点，却不能选举出主节点。为什么？\n为了避免出现多个主节点。\n因为如果 3 个不可用成员不是挂了，可能因为网络问题，造成通信不可达，这种情况下，3 个不可用的节点可能会选举出一个主节点，2 个正常的节点再选举一个主节点， 那么副本集就会出现 2 个主节点。写入数据时，就会发生混乱。\n推荐的配置\r#\r\r在两个数据中心放置数量相同的成员，在第三个地方放置一个决定胜负的副本集成员。只要任意一个数据中心的服务器可以找到那一台关键的服务器，就可以达到“大多数”的要求。\n选举机制\r#\r\r当一个本分节点无法与主节点连通时，它就会联系并请求其他副本集成员将自己选举为主节点。其他成员会做几个检查：\n 自己是否能与主节点连通 希望被选举为主节点的成员数据是否是最新的 有没有优先级更高的成员可以选举为主节点  每个成员都只能要求自己被选举为主节点。\n仲裁者\r#\r\r可以给予每个副本集成员不一样的配置。\nMongo 支持仲裁者类型的成员。仲裁者只参与选举，不保存数据，因为有时候数据副本保存太多，浪费资源。只是为了帮助其他成员满足“大多数”的条件。\n仲裁者不需要履行 Mongo 服务器的职责，所以可以放在配置比较差的服务器上。可以作为上面说到的那个决定胜负的成员。\n仲裁者只允许有一个。\n尽可能在副本集中使用奇数个数据成员，不适用仲裁者。因为如果有三个成员，2 个数据成员，1 个仲裁者，在主节点挂了之后，只剩下一个数据成员，被选举为主节点， 为了数据备份，只能再加入一个数据成员，并复制所有数据副本，复制数据会对服务器造成很大的压力。\n优先级\r#\r\r一个成员成为主节点的优先级，取值范围 0~100。默认是 1。优先级为 0 的成员永远不能成为主节点，这种成员叫做被动成员。\n优先级越高，就越先被选为主节点。\n延迟备份节点\r#\r\r可以使用 slaveDelay 设置一个延迟备份节点，防止有人不小心删除了主数据库，或者程序 bug 导致数据误删除之类的问题。\nslaveDelay 成员要求优先级为 0。\n创建索引\r#\r\r备份节点一般不需要与主节点相同的索引，甚至没有索引。如果只是用来数据备份或者离线任务。可以指定 builgIndexes: false 这个选项阻止备份节点 创建索引。\n注意指定了 builgIndexes: false 这个选项的成员无法恢复为可以创建索引的成员。builgIndexes: false 这个选项的成员要求优先级为 0。\n同步\r#\r\rMongoDB 的复制功能是使用操作日志 oplog 实现的，操作日志包含了主节点的每一次写操作。oplog 是主节点的 local 数据库中的一个固定集合。备份 节点通过查询这个集合就知道需要进行的复制操作。\n每个备份节点要维护自己 oplog，先复制数据，在写入 oplog，记录每一次从主节点复制数据的操作。这样每个成员都可以作为同步源提供给其他成员使用。 如果某个人备份节点挂了，重启后，会自动从 oplog 中最后一个操作开始同步。\noplog 的大小是固定的。如果执行大量的批量操作，oplog 会很快填满。\n处理陈旧数据\r#\r\r备份节点远落后于同步源当前操作，那么这个备份节点就是陈旧的。比如备份节点曾经停机，或者有太多的读请求，都可能导致节点陈旧。\n一个节点陈旧之后，会找到 oplog 足够详尽的成员来同步，如果没有 复合的 oplog，就需啊哟进行完全同步。\n为了避免陈旧备份节点，主节点的 oplog 要足够大。\n分片\r#\r\r分片（sharding）是指将数据拆分，将其分散存放在不同的机器上的过程。有时也叫做分区（partitioning）。数据分散到不同机器上， 这样服务器可以储存更多的数据，处理更大的负载。\n几乎所有的数据库软件都支持手动分片。MongoDB 支持自动分片，简化系统管理，并且对应用程序，就像在使用一台 MongoDB 服务器。\n分片集群\r#\r\rMongo 的分片机制允许创建一个包含多台机器（分片）的集群。将数据子集分散在集群中，每个分片维护一个数据集合的子集。与单机和副本集相比，使用集群架构可以使应用具有 更大的数据处理能力。\n 复制和分片的概念是不一样的，复制是让多台服务器拥有同样的数据副本，而每一个分片都有不同的数据子集。\n 何时分片\r#\r\r通常不必太早分片，因为分片会增加部署的复杂度。也不要在系统运行太久后再分片，在一个过载的系统上不停机分片是非常困难的。\n分片的作用：\n 增加可用 RAM 增加可用磁盘空间 减轻单台服务器负载 处理更大的吞吐量  分片不可太少，因为由于迁移数据，维护元数据，路由等开销，可能还会造成性能下降。\n组件\r#\r\r mongos，数据库集群请求的入口，所有的请求都通过 mongos 进行协调，不需要在应用程序添加一个路由选择器，mongos 自己就是一个请求分发中心， 它负责把对应的数据请求请求转发到对应的 shard 服务器上。在生产环境通常有多 mongos 作为请求的入口，防止其中一个挂掉所有的 mongodb 请求都没有办法操作。 config server，配置服务器，存储所有数据库元信息（路由、分片）的配置。 Shard，分片服务器。  配置分片\r#\r\r配置服务器\r#\r\r配置服务器保存集群和分片的元数据，即各个分片包含哪些数据的信息。\n创建集群首先要先建立配置服务器，并启用日志功能。每个配置服务器都应该位于单独的物理机上。\n# config-1 mongod --configsvr --dbpath /var/lib/mongodb -f /var/lib/config/mongod.conf # config-2 mongod --configsvr --dbpath /var/lib/mongodb -f /var/lib/config/mongod.conf # config-3 mongod --configsvr --dbpath /var/lib/mongodb -f /var/lib/config/mongod.conf 启动 3 台配置服务器， mongos 会向所有配置服务器发送请求，确保每台配置服务器拥有相同的数据。配置服务器不需要太多的资源，它只保存数据的分布表。 所以可以将其部署在分片服务器或者 mongos 服务器上。\n--configsvr 指定 mongod 为配置服务器，其实只是将 mongod 默认的 27017 端口改为了 27019，把默认的数据目录改为 /data/configdb。\n为了避免配置服务器全部挂掉，通常也应该对配置服务器的数据进行备份。\nmongos\r#\r\r3 个配置服务器已运行，启动一个 mongos 进程来做入口。mongos 必须知道配置服务器地址，使用 --configdb 来指定：\nmongos --configdb config-1:27019,config-2:27019,config-3:27019 -f /var/lib/mongos.conf mongos 运行在 27017 端口。不需要指定数据目录，因为 mongos 不保存数据。可以启动多个 mongos 进程。\n将副本集转化为分片\r#\r\r假设已经有一个副本集，如果没有就初始化一个空的副本集。如果有一个使用中的副本集，该副本集会成为第一个分片。\n接下来，mongos 需要知道副本集的名称和副本集成员列表。例如 server-1、server-2、server-3、server-4、server-5 上有一个名为 spock 的 副本集，连接 mongos，并运行：\n\u0026gt; sh.addShard(\u0026#34;spock/server-1:27017,server-2:27017,server-4:27017\u0026#34;) 上面的语句没有指定所有成员，mongos 可以自动检测副本集的所有成员。比如运行 sh.status()，可以发现 mongos 已经找到可所有副本集成员 spock/server-1:27017,server-2:27017,server-3:27017,server-4:27017,server-5:27017。\n副本集名称会作为分片的名称。副本集作为分片添加到集群之后，应用程序就可以从连接副本集改为连接到 mongos。mongos 会将副本集的所有 数据库注册为分片数据库，所有查询会送到新的分片上。\n数据分片\r#\r\r除非明确指定归罪，否则 MongoDB 不会自动对数据进行拆分。\n假设读数据库 music 中的 artists 集合按照 name 字段分片。\n// 数据库 music 启用分片 db.enableSharding(\u0026#34;music\u0026#34;) // 对集合分片 sh.shardCollection(\u0026#34;music.artists\u0026#34;, {\u0026#34;name\u0026#34;: 1}) **注意，如果对已经存在的集合分片，那么 name 字段必须有索引，否则 shardCollection 会报错。**如果报错，进先创建索引， 再执行 shardCollection。\n部署架构\r#\r\r下面是一个常见的部署架构，可以用作参考，机器数量根据自身情况而定。\n使用三台服务器，部署三个分片，每个分片三副本。实际上分片数量可以是任意个，试主机性能而定。各个分片之间是完全相互独立的，一个 database 的数据只会落在一个分片上。\n 服务器：16.187.189.120、16.187.189.121、16.187.189.122  部署结构如下表所示：\n   16.187.189.120 16.187.189.121 16.187.189.122     mongos: 27017 mongos: 27017 mongos: 27017   config: 27018 config: 27018 config: 27018   shard01: 27101 shard01: 27101 shard01: 27101   shard02: 27102 shard02: 27102 shard02: 27102   shard03: 27103 shard03: 27103 shard03: 27103    \r参考文章\nMongoDB 如何追踪集群数据\r#\r\rMongoDB 将文档分组为块（chunk），每个块由给定片键特定范围内的文档组成。一个块只存在一个分片上，所以只需要一个很小的表就可以维护块和分片的映射。\n比如，一个用户集合的片键是 {\u0026quot;age\u0026quot;: 1}，某个块可能是由 age 值为 3~17 的文档组成。如果有一个 {\u0026quot;age\u0026quot;: 5} 的查询，就可以直接路由到 age 值为 3~17 的块所在的分片。\n如果 age 值为 3~17 （3 ≤ age \u0026lt; 17）的块越来越大，比如我们的应用大部分用户是这个年龄段的学生，那么这个块会被拆分成两个小块， 比如会拆成 3~11 （3 ≤ age \u0026lt; 11）和 12~17 （12 ≤ age \u0026lt; 17）的块。接着变大，就 接着拆分。\n块与块之间的范围不能有交集。\n一个文档只属于一个块。就意味着不能使用数组字段作为片键。因为 MongoDB 会为数组创建多个索引。\n--nosplit 可以关闭块的拆分。\n均衡器\r#\r\r均衡器负责数据的迁移，周期性的检查分片间是否存在 不均衡，存在的话，就会开始块的迁移。mongos 会扮演均衡器的角色。均衡不会影响 mongos 的正常路由 操作。\n选择片键\r#\r\r对集合分片时，要选择一个或两个字段来做**片键，用于数据拆分。一旦拥有多个分片，再修改片键几乎不可能，因此，选择合适的片键 非常重要。\n升序片键\r#\r\r升序片键类似 “date” 字段或者 ObjectId，是一种随着时间稳定增长的字段。\n比如基于 _id 分片，集合会根据 _id 拆分块。如果创建一个新文档，这个文档会分发到 ObjectId(\u0026quot;5112fae0b4a4b396ff9d0ee5\u0026quot;) 到 $maxKey 的块。这个 块叫最大块。接下来再插入的文档都在这个最大快中。因为 _id 的值在不断增长，比之前的文档的 _id 值都大。\n\r这样的分块，有明显的弊端，就是接下来的所有写请求都会路由到一个分片中。这个最大块是唯一一个不断增长和拆分的块，因为只有它能接收到插入请求。然后数据不断增多， 最大块再不断拆分出新的小块。这会导致数据均衡处理困难，因为所有新块都是同一个分片创建。MongoDB 必须不断将一些块迁移至其他的分片。\n随机分发的片键\r#\r\r随机分发的片键可以使用户名，邮件地址，UDID，MD5 散列值，或者是数据集中其他一些没有规律的键。\n数据的随机性意味着，新数据会比较均衡的分发在不同的块中。\n弊端是，MongoDB 在随机访问超出 RAM 大小的数据时效率不高。但是如果 RAM 足够，随机分片的键的性能是很好的。\n基于位置的片键\r#\r\r基于位置的片键可以使IP，经纬度，或者地址。这里的 “位置” 比较抽象，疏忽会根据 “位置” 分组，所有与该健值比较接近的文档都会被保存在一个块中。\n片键策略\r#\r\r散列片键\r#\r\r散列片键的数据加载速度最快。散列片键可使其他任意键随机分发，如果打算使用升序键，又希望数据随机分发，那么就选择散列片键。\n弊端是无法使用散列片键做指定目标的范围查询。\n# 创建散列索引 \u0026gt; db.users.ensureIndex({\u0026#34;username\u0026#34;: \u0026#34;hashed\u0026#34;}) # 集合分片 \u0026gt; sh.shardCollection(\u0026#34;app.users\u0026#34;, {\u0026#34;username\u0026#34;: \u0026#34;hashed\u0026#34;}) 片键限制\r#\r\r 片键不可以是数组。 文档一旦插入，片键无法修改，要修改，必须先删除文档，因此尽量选择不会被改变的字段。 大多数特殊类型的索引不能做片键。特别是地理空间索引。  片键的势\r#\r\r选择一个值会发生变化的键很重要，比如以 \u0026ldquo;logLevel\u0026rdquo; 为键，\u0026ldquo;logLevel\u0026rdquo; 只拥有 \u0026ldquo;DEBUG\u0026rdquo;、\u0026ldquo;WARN\u0026rdquo;、\u0026ldquo;ERROR\u0026rdquo;、\u0026ldquo;INFO\u0026rdquo; 四个值。那么最多只有将数据分为四个 块。\n如果一个键的值比较少，又希望用作片键，可以与另一只键一起创建组合片键，比如和 \u0026ldquo;timestamp\u0026rdquo;。\n"});index.add({'id':33,'href':'/db-learn/docs/redis/26_protect-redis/','title':"保护 Redis",'content':"保护 Redis\r#\r\r指令安全\r#\r\rRedis 有一些非常危险的指令。比如 keys 指令会导致 Redis 卡顿，flushdb 和 flushall 会让 Redis 的所有数据全部清空。 如何避免人为操作失误导致这些灾难性的后果也是运维人员特别需要注意的风险点之一。\nRedis 在配置文件中提供了 rename-command 指令用于将某些危险的指令修改成特别的名称，用来避免人为误操作。比如在配置文 件的 security 块增加下面的内容:\nrename-command keys abckeysabc\r如果还想执行 keys 方法，需要键入 abckeysabc。如果想完全封杀某条指令，将指令 rename 成空串，就无法通过任何字符串指令来执行这 条指令了：\nrename-command flushall \u0026quot;\u0026quot;\r端口安全\r#\r\rRedis 默认会监听 *:6379，Redis 的服务地址一旦可以被外网直接访问，黑客可以通过 Redis 执行 Lua 脚本拿到服务器权限。\n所以，务必在 Redis 的配置文件中指定监听的 IP 地址。更进一步，还可以增加 Redis 的密码访问限制，客户端必须使用 auth 指令传入正 确的密码才可以访问 Redis，这样即使地址暴露出去了，普通黑客也无法对 Redis 进行任何指令操作。\nLua 脚本安全\r#\r\r禁止 Lua 脚本由用户输入的内容 (UGC) 生成，避免黑客利用以植入恶意的攻击代码来得到 Redis 的主机权限。Redis 应该以普通用户的身份启动。\nSSL 代理\r#\r\rRedis 并不支持 SSL 链接，意味着客户端和服务器之间交互的数据不应该直接暴露在公网上传输，否则会有被窃听的风险。如果必须要用在公网上，可 以考虑使用 SSL 代理。\nRedis 官方推荐使用 spiped 工具，可能是因为 spiped 的功能相对比较单一，使用也比较 简单，易于理解。\nspiped 原理\r#\r\r\rspiped 会在客户端和服务器各启动一个 spiped 进程。\nspiped 进程 A 负责接受来自 Redis Client 发送过来的请求数据，加密后传送到右边的 spiped 进程 B。spiped B 将接收到的数据解密后传 递到 Redis Server。然后 Redis Server 再走一个反向的流程将响应回复给 Redis Client。\n"});index.add({'id':34,'href':'/db-learn/docs/mongo/02_getting-started/','title':"入门",'content':" database，和\u0026quot;数据库\u0026quot;一样的概念 (对 Oracle 来说就是 schema)。一个 MongoDB 实例中，可以有零个或多个数据库 collections，数据库中可以有零个或多个 collections (集合)。和传统意义上的table基本一致。 documents，集合是由零个或多个 documents (文档)组成。一个文档可以看成是一 row。 fields，文档是由零个或多个 fields (字段)组成。可以看成是 columns。 Indexes (索引)在 MongoDB 中扮演着和它们在 RDBMS(Relational Database Management System 关系数据库管理系统) 中一样的角色。 Cursors (游标)，游标是，当你问 MongoDB 拿数据的时候，它会给你返回一个结果集的指针而不是真正的数据，这个指针我们叫它游标， 我们可以拿游标做我们想做的任何事情，比如说计数或者跨行之类的，而无需把真正的数据拖下来，在真正的数据上操作。  这些概念和关系型数据中的概念类似，但是还是有差异的。\n核心差异在于，关系型数据库是在 table 上定义的 columns，而面向文档数据库是在 document 上定义的 fields。 也就是说，在 collection 中的每个 document 都可以有它自己独立的 fields。\n要点就是，集合不对存储内容严格限制 (所谓的无模式(schema-less))。\nmongo shell\r#\r\rmongo shell 是一个完整的 JavaScript 解释器。可以运行任意的 JavaScript 程序。比如 db.help() 或者 db.stats()。大多数情况下我们会操作集合而不是数据库， 用 db.COLLECTION_NAME ，比如 db.unicorns.help() 或者 db.unicorns.count()。如果输入 db.help (不带括号), 你会看到 help 方法的内部实现。\n.mongorc.js\r#\r\r如果某些脚本会被频繁加载，可以将它添加到 .mongorc.js 文件中，文件会在启动 shell 时自动运行。\n.mongorc.js 常见的用途是移除那些比较危险的 shell 辅助函数。可以在这个文件中重写那些方法，比如：\nvar no = function() { print(\u0026#34;Not on my watch.\u0026#34;) } // 禁止删除数据库 db.droopDatabase = DB.prototype.dropDatabase = no; // 禁止删除集合 DBCollection.prototype.drop = no; // 禁止删除索引 DBCollection.prototype.dropIndex = no; 禁用 .mongorc.js\r#\r\r启动 shell 时指定 --norc，就可以禁止加载 .mongorc.js 了。\n_id\r#\r\r每个文档都会有一个唯一 _id 字段。你可以自己生成一个，或者让 MongoDB 帮你生成一个 ObjectId 类型的。默认的 _id 字段是已被索引的。 _id 是一个 12 字节长的十六进制数，头 4 个字节代表的是当前的时间戳，接着的后 3 个字节表示的是机器 id 号，接着的 2 个字节表示 MongoDB 服务器进程 id，最后的 3 个字节代表递增值。\nObjectId 是轻量的，不同机器都能用全局的唯一的方法生成，MongoDB 没有采用比较常规的做法（比如自增的主键），因为在多个服务器上同步自增主键费力费时。能够在分片环境中生成唯一的标识符 很重要。\n数据类型\r#\r\r String：字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 Integer：整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 Boolean：布尔值。用于存储布尔值（真/假）。 Double：双精度浮点值。用于存储浮点值。 Min/Max keys：将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。 Arrays：用于将数组或列表或多个值存储为一个键。 Timestamp：时间戳。记录文档修改或添加的具体时间。 Object：用于内嵌文档。 Null：用于创建空值。 Symbol：符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 Date：日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 Object ID：对象 ID。用于创建文档的 ID。 Binary Data：二进制数据。用于存储二进制数据。 Code：代码类型。用于在文档中存储 JavaScript 代码。 Regular expression：正则表达式类型。用于存储正则表达式。  常用命令\r#\r\ruse\r#\r\ruse 会创建一个新的数据库，如果该数据库存在，则返回这个数据库。格式 use DATABASE_NAME。\n删除数据库\r#\r\rdropDatabase()用于删除已有数据库。格式 db.dropDatabase()。 它将删除选定的数据库。如果没有选定要删除的数据库，则它会将默认的 test 数据库删除。\n\u0026gt;use mydb switched to db mydb \u0026gt;db.dropDatabase() \u0026gt;{ \u0026#34;dropped\u0026#34; : \u0026#34;mydb\u0026#34;, \u0026#34;ok\u0026#34; : 1 } 创建集合\r#\r\rdb.createCollection(name, options) 创建集合。name 是所要创建的集合名称。options 是一个用来指定集合配置的文档。\n参数 options 是可选的，可用选项：\n capped，（可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。当该值为 true 时，必须指定 size 参数。 autoIndexID，（可选）如为 true，自动在 _id 字段创建索引。默认为 false。 size，（可选）为固定集合指定一个最大值（以字节计）。如果 capped 为 true，也需要指定该字段。 max，（可选）指定固定集合中包含文档的最大数量。  在插入文档时，MongoDB 首先检查固定集合的 size 字段，然后检查 max 字段。\n固定集合\r#\r\risCapped()\r#\r\risCapped()检查集合是否是固定集合。格式 db.COLLECTION_NAME.isCapped()\n将现有集合转化为固定集合\r#\r\rdb.runCommand({\u0026#34;convertToCapped\u0026#34;:\u0026#34;posts\u0026#34;,size:10000}) 将现有的 posts 集合转化为固定集合。\n值得注意的点\r#\r\r 无法从固定集合中删除文档。 固定集合没有默认索引，甚至在 _id 字段中也没有，可以使用autoIndexID创建索引。 在插入新的文档时，MongoDB 并不需要寻找磁盘空间来容纳新文档。它只是盲目地将新文档插入到集合末尾。这使得固定集合中的插入操作是非常快速的。 同样的，在读取文档时，MongoDB 会按照插入磁盘的顺序来读取文档，从而使读取操作也非常快。 如果要把已有的集合变为固定集合，先执行db.runCommand({\u0026quot;convertToCapped\u0026quot;:\u0026quot;posts\u0026quot;,size:10000})转化，否则程序可能会连接数据库失败。  删除集合\r#\r\rdb.collection.drop() 来删除数据库中的集合。格式 db.COLLECTION_NAME.drop()。\n插入文档\r#\r\r使用 insert() 或 save() 方法。格式 db.COLLECTION_NAME.insert(document)。\n查询\r#\r\r使用 find() 方法。格式 db.COLLECTION_NAME.find()。\npretty() 方法\r#\r\r用格式化方式显示结果，使用的是 pretty() 方法。\nfindOne()\r#\r\rfindOne() 方法，它只返回一个文档。\n类似于 WHERE 子句的语句\r#\r\r$lt小于，$lte小于等于，$gt大于，$gte大于等于，$ne不等于。\ndb.mycol.find({\u0026#34;likes\u0026#34;:{$lt:50}}).pretty() AND 和 OR\r#\r\r// 逗号分隔看成是 AND 条件 db.mycol.find({key1:value1, key2:value2}).pretty() 基于 OR 条件来查询文档，可以使用关键字 $or。\ndb.mycol.find({$or: [{key1: value1}, {key2:value2}]}).pretty() 查询数组\r#\r\r比如：db.food.insert({\u0026quot;fruit\u0026quot;: [\u0026quot;apple\u0026quot;, \u0026quot;banana\u0026quot;, \u0026quot;peach\u0026quot;]})。\n要查询数组使用：db.food.find({\u0026quot;fruit\u0026quot;: \u0026quot;banana\u0026quot;})。\n$all\r#\r\r如果需要通过多个元素来匹配数组，可以使用$all。比如：\ndb.food.insert({\u0026#34;fruit\u0026#34;: [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;peach\u0026#34;]}) db.food.insert({\u0026#34;fruit\u0026#34;: [\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;kumquat\u0026#34;]}) db.food.insert({\u0026#34;fruit\u0026#34;: [\u0026#34;cherry\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34;]}) 要匹配含有 apple 和 banana 的文档：\ndb.food.find({\u0026#34;fruit\u0026#34;: {$all: [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;]}}) $size\r#\r\r$size可以用它查询特定长度的数组。比如：db.food.find({\u0026quot;fruit\u0026quot;: {$size: 3}})。$size 不能与其他查询条件一起使用（比如 $gt）。\n$slice\r#\r\rfind() 的第二个参数是可选的，可以指定需要返回的键。$slice操作符可以返回某个键匹配的数组元素的一个自己。比如：\ndb.posts.findOne(criteria, {\u0026#34;comments\u0026#34;: {$slice: 10}}) 返回前 10 条评论，后 10 条的话使用 -10。\n指定偏移量得到返回的元素：\ndb.posts.findOne(criteria, {\u0026#34;comments\u0026#34;: {$slice: [23, 10]}}) 这里的$slice和 Javascript 中的 slice 函数用法类似。\n查询子文档\r#\r\r比如下面的文档：\n{ \u0026#34;address\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Los Angeles\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;California\u0026#34;, \u0026#34;pincode\u0026#34;: \u0026#34;123\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;Tom Benzamin\u0026#34; } 要查询地址为 Los Angeles 的人可以db.users.find({\u0026quot;address\u0026quot;: {\u0026quot;city\u0026quot;: \u0026quot;Los Angeles\u0026quot;}})。\n$where\r#\r\r在一些场景下，可能一般的键值对查询无法满足，这是可以使用 $where 子句。但是这种方式应该禁止使用，很不安全。\ndb.food.find({$where: function () { for (var current in this) { for (var other in this) { if (current != other \u0026amp;\u0026amp; this[current] == this[other]) { return true; } } } return false; }}) 如果函数返回 true，那么文档会作为结果集中的一部分返回。\n$where 子句查询很慢，而且不能使用索引。\n映射（Projection）\r#\r\r映射（Projection）指的是只选择文档中的必要数据，而非全部数据。如果文档有 5 个字段，而你只需要显示 3 个，则只需选择 3 个字段即可。 执行 find() 方法时，可以利用 0 或 1 来设置字段列表。1 用于显示字段，0 用于隐藏字段。\ndb.mycol.find({},{\u0026#34;title\u0026#34;:1, _id:0}) _id 字段是一直显示的。如果不想显示该字段，则可以将其设为 0。\nlimit()\r#\r\rlimit() 方法接受一个数值类型的参数，其值为想要显示的文档数。\ndb.mycol.find({},{\u0026#34;title\u0026#34;:1, _id:0}).limit(2) skip()\r#\r\rdb.mycol.find({},{\u0026#34;title\u0026#34;:1,_id:0}).skip(1).limit(1) 避免使用 skip 略过大量结果\r#\r\rskip 如果略过大量结果，会变得很慢，因为要找到需要被略过的数据，然后抛弃。可以利用上次的查询结果来计算下一次的查询条件\nsort()\r#\r\rsort() 方法可以通过一些参数来指定要进行排序的字段，并使用 1 和 -1 来指定排序方式，其中1 表示升序，而 -1 表示降序。\ndb.mycol.find({},{\u0026#34;title\u0026#34;:1,_id:0}).sort({\u0026#34;title\u0026#34;:-1}) 更新\r#\r\rupdate() 方法更新已有文档中的值，而 save() 方法则是用传入该方法的文档来替换已有文档。格式 db.COLLECTION_NAME.update(SELECTIOIN_CRITERIA, UPDATED_DATA, UPSERT, MULTI)。\n UPSERT: 为 true 时，如果文档不存在则插入文档 MULTI: 为 true 时，更新多个符合条件的文档  db.mycol.update({\u0026#39;title\u0026#39;:\u0026#39;MongoDB Overview\u0026#39;},{$set:{\u0026#39;title\u0026#39;:\u0026#39;New MongoDB Tutorial\u0026#39;}}) 删除\r#\r\rremove() 方法 清除集合中的文档。格式 db.COLLECTION_NAME.remove(DELLETION_CRITTERIA)。 2 个可选参数：\n deletion criteria：（可选）删除文档的标准。 justOne：（可选）如果设为 true 或 1，则只删除一个文档。  db.mycol.remove({\u0026#39;title\u0026#39;:\u0026#39;MongoDB Overview\u0026#39;}, 1) 索引\r#\r\r数据库索引与书籍的索引类似。有了索引就不需要翻整本书，数据库可以直接在索引中找到条目，直接跳转到目标文档的位置，能使查询速度提高几个数量级。\n如果没有索引，那么数据库就会进行全表扫描，比如一个用于集合有一百万条文档，我们执行db.users.find({username: \u0026quot;user101\u0026quot;}).explain()：\n{ \u0026#34;cursor\u0026#34;: \u0026#34;BasicCursor\u0026#34;, \u0026#34;nscanned\u0026#34;: 1000000, \u0026#34;n\u0026#34;: 1, \u0026#34;millis\u0026#34;: 721, ... } nscanned 扫描的文档数。millis 表示查询耗费的毫秒数。n 表示查询结果的数量。\n由于数据库不知道 username 字段是唯一的，Mongo 会查看集合中的每一个文档。这里我们能想到的优化方法就是使用limit限制查询的文档个数，因为我们知道用户是唯一的，所以limit(1)。\n但是如果查询用户 user99999 呢？使用索引是一个非常好的解决方案。\nMongoDB 中 ensureIndex() 方法创建索引。格式 db.COLLECTION_NAME.ensureIndex({KEY:1})。1 代表按升序排列字段值。-1 代表按降序排列。 创建索引会耗费一些时间，根据机器的性能和集合的大小而不同。\ndb.mycol.ensureIndex({\u0026#34;title\u0026#34;:1}) // 为多个字段创建索引 db.mycol.ensureIndex({\u0026#34;title\u0026#34;:1,\u0026#34;description\u0026#34;:-1}) ensureIndex() 方法也可以接受一些可选参数：\n background，在后台构建索引，从而不干扰数据库的其他活动。取值为 true 时，代表在后台构建索引。默认值为 false unique，创建一个唯一的索引，从而当索引键匹配了索引中一个已存在值时，集合不接受文档的插入。取值为 true 代表创建唯一性索引。默认值为 false。 name，索引名称。如果未指定，MongoDB 会结合索引字段名称和排序序号，生成一个索引名称。 dropDups，在可能有重复的字段内创建唯一性索引。MongoDB 只在某个键第一次出现时进行索引，去除该键后续出现时的所有文档。 sparse，如果为 true，索引只引用带有指定字段的文档。这些索引占据的空间较小，但在一些情况下的表现也不同（特别是排序）。默认值为 false。 expireAfterSeconds，指定一个秒数值，作为 TTL 来控制 MongoDB 保持集合中文档的时间。 v，索引版本号。默认的索引版本跟创建索引时运行的 MongoDB 版本号有关。 weights，文档数值，范围从 1 到 99, 999。表示就字段相对于其他索引字段的重要性。 default_language，对文本索引而言，用于确定停止词列表，以及词干分析器（stemmer）与断词器（tokenizer）的规则。默认值为 english。 language_override，对文本索引而言，指定了文档所包含的字段名，该语言将覆盖默认语言。默认值为 language。  background 这个选项要注意，创建索引可能会非常耗时，尤其是在已有的集合上创建索引，Mongo 为了尽可能快的创建索引，会阻塞对 数据库的读请求和写请求，知道创建完成。这时可以使用 background 这个选项，来避免对数据库操作的干扰。但是还是会影响性能，并且比前台创建索引慢的多。\n唯一索引\r#\r\r唯一索引可以确保结婚额的每一个文档的指定键都有唯一值。比如用户名是唯一的：\ndb.users.ensureIndex({\u0026#34;username\u0026#34;: 1}, {\u0026#34;unique\u0026#34;: true}) 在已有的集合中创建唯一索引可能会失败，因为集合中肯能已经存在重复的值了。\nTTL 索引\r#\r\rTTL 索引允许为每一个文档设置过期时间。文档过期之后会自动删除。可以用来实现缓存。\ndb.cache.ensureIndex({\u0026#34;lastUpdated\u0026#34;: 1}, {\u0026#34;expireAfterSeconds\u0026#34;: 60 * 60 * 24}) 上面的语句在 lastUpdated 字段上建立了 TTL 索引。 如果一个文档的 lastUpdated 字段存在并且它的值是日期类型（注意，必须是日期类型）， 当服务器时间比 lastUpdated 字段的时间晚 expireAfterSeconds 秒时，文档就会删除。\nMongo 每分钟会对 TTL 索引进行一次清理，所以以秒为时间单位保证索引的存活状态是不准确的。\n索引管理\r#\r\r所有数据库索引信息存储在 system.indexes 集合中。这个集合只能使用 ensureIndex 和 dropIndexes 对其操作。\n使用 db.COLLECTION_NAME.getIndexes() 来查看所有索引信息。\n文本索引\r#\r\rdb.posts.ensureIndex({post_text:\u0026#34;text\u0026#34;}) 上面的代码在 post_text 字段上创建文本索引，以便搜索帖子文本之内的内容。\n在 post_text 字段上创建了文本索引，接下来搜索包含 tutorialspoint 文本内容的帖子。\ndb.posts.find({$text:{$search:\u0026#34;tutorialspoint\u0026#34;}}) 删除文本索引\r#\r\r// 找到索引名称 db.posts.getIndexes() // 删掉 db.posts.dropIndex(\u0026#34;post_text_text\u0026#34;) 优化全文本搜索\r#\r\r思路就是使用某些查询条件来是搜索范围变小。比如db.posts.ensureIndex({date: 1, post_text:\u0026quot;text\u0026quot;})，date 先将范围缩小到特定日期的文档，再进行全文本搜索。\n覆盖索引查询\r#\r\r在每一个 MongoDB 官方文档中，覆盖查询都具有以下两个特点：\n 查询中的所有字段都属于一个索引； 查询所返回的所有字段也都属于同一索引内。  既然查询中的所有字段都属于一个索引，MongoDB 就会利用同一索引，匹配查询集合并返回结果，而不需要实际地查看文档。因为索引存在于 RAM 中，从索引中获取数据要比通过扫描文档获取数据快得多。\n使用覆盖查询\r#\r\r假设在一个 users 集合中包含下列文档：\n{ \u0026#34;_id\u0026#34;: ObjectId(\u0026#34;53402597d852426020000002\u0026#34;), \u0026#34;contact\u0026#34;: \u0026#34;987654321\u0026#34;, \u0026#34;dob\u0026#34;: \u0026#34;01-01-1991\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Tom Benzamin\u0026#34;, \u0026#34;user_name\u0026#34;: \u0026#34;tombenzamin\u0026#34; } 为 users 集合中的 gender 和 user_name 字段创建一个复合索引：\ndb.users.ensureIndex({gender:1,user_name:1}) 这一索引将覆盖下列查询：\ndb.users.find({gender:\u0026#34;M\u0026#34;},{user_name:1,_id:0}) 也就是说，对于上面的查询，MongoDB 不会去查看文档，转而从索引数据中获取所需的数据。\n下面的查询就不会被覆盖，因为_id会默认返回，而_id和user_name，gender不在同一个索引。\ndb.users.find({gender:\u0026#34;M\u0026#34;},{user_name:1}) 如果出现下列情况，索引不能覆盖查询：\n 索引字段是数组 索引字段是子文档  高级索引\r#\r\r例如下面的 user 集合文档:\n{ \u0026#34;address\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Los Angeles\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;California\u0026#34;, \u0026#34;pincode\u0026#34;: \u0026#34;123\u0026#34; }, \u0026#34;tags\u0026#34;: [ \u0026#34;music\u0026#34;, \u0026#34;cricket\u0026#34;, \u0026#34;blogs\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;Tom Benzamin\u0026#34; } 上述文档包含一个地址子文档（address sub-document）与一个标签数组（tags array）。\n索引数组字段\r#\r\r假设我们想要根据标签来搜索用户文档。首先在集合中创建一个标签数组的索引。\n反过来说，在标签数组上创建一个索引，也就为每一个字段创建了单独的索引项。因此在该例中，当我们创建了标签数组的索引时， 也就为它的music（音乐）、cricket（板球）以及 blog（博客）值创建了独立的索引。\n// 创建标签数据的索引 db.users.ensureIndex({\u0026#34;tags\u0026#34;:1}) // 搜索集合中的标签字段 db.users.find({tags:\u0026#34;cricket\u0026#34;}) // 使用 explain 命令验证所使用索引的正确性 db.users.find({tags:\u0026#34;cricket\u0026#34;}).explain() 上述 explain 命令的执行结果是 \u0026quot;cursor\u0026quot; : \u0026quot;BtreeCursor tags_1\u0026quot;，表示使用了正确的索引。\n索引子文档字段\r#\r\r假设需要根据市（city）、州（state）、个人身份号码（pincode）字段来搜索文档。因为所有这些字段都属于地址子文档字段的一部分， 所以我们将在子文档的所有字段上创建索引。\n// 在子文档的所有三个字段上创建索引 db.users.ensureIndex({\u0026#34;address.city\u0026#34;:1,\u0026#34;address.state\u0026#34;:1,\u0026#34;address.pincode\u0026#34;:1}) // 搜索子文档字段 db.users.find({\u0026#34;address.city\u0026#34;:\u0026#34;Los Angeles\u0026#34;}) // 查询 db.users.find({\u0026#34;address.city\u0026#34;:\u0026#34;Los Angeles\u0026#34;,\u0026#34;address.state\u0026#34;:\u0026#34;California\u0026#34;}) // 也支持如下这样的查询 db.users.find({\u0026#34;address.city\u0026#34;:\u0026#34;LosAngeles\u0026#34;,\u0026#34;address.state\u0026#34;:\u0026#34;California\u0026#34;,\u0026#34;address.pincode\u0026#34;:\u0026#34;123\u0026#34;}) 查询表达式必须遵循指定索引的顺序。\n索引限制\r#\r\r额外开销\r#\r\r每个索引都会占据一些空间，从而也会在每次插入、更新与删除操作时产生一定的开销。所以如果集合很少使用读取操作，就尽量不要使用索引。\n内存使用\r#\r\r因为索引存储在内存中，所以应保证索引总体的大小不超过内存的容量。如果索引总体积超出了内存容量，就会删除部分索引，从而降低性能。\n查询限制\r#\r\r当查询使用以下元素时，不能使用索引：\n 正则表达式或否定运算符（$nin、$not，等等） 算术运算符（比如 $mod） $where 子句 因此，经常检查查询使用的索引是一个明智的做法。  索引键限制\r#\r\r自 MongoDB 2.6 版本起，如果已有索引字段的值超出了索引键限制，则无法创建索引。\n 插入文档超过索引键限制 如果文档的索引字段值超出了索引键的限制，MongoDB 不会将任何文档插入已索引集合。类似于使用 mongorestore 和 mongoimport 工具时的情况。  最大范围\r#\r\r 集合索引数不能超过 64 个。 索引名称长度不能大于 125 个字符。 复合索引最多能有 31 个被索引的字段。  聚合\r#\r\r聚合的结果必须限制在 16 MB 以内（MongoDB 支持的最大相应消息大小）。\n聚合操作能将多个文档中的值组合起来，对成组数据执行各种操作，返回单一的结果。使用 aggregate() 方法。相当于 SQL 中的 count(*) 组合 group by。\ndb.mycol.aggregate([{$group : {_id : \u0026#34;$by_user\u0026#34;, num_tutorial : {$sum : 1}}}]) 上例使用 by_user 字段来组合文档，每遇到一次 by_user，就递增之前的合计值。\n   表达式 描述 范例     $sum 对集合中所有文档的定义值进行加和操作 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, num_tutorial : {$sum : \u0026quot;$likes\u0026quot;}}}])   $avg 对集合中所有文档的定义值进行平均值 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, num_tutorial : {$avg : \u0026quot;$likes\u0026quot;}}}])   $min 计算集合中所有文档的对应值中的最小值 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, num_tutorial : {$min : \u0026quot;$likes\u0026quot;}}}])   $max 计算集合中所有文档的对应值中的最大值 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, num_tutorial : {$max : \u0026quot;$likes\u0026quot;}}}])   $push 将值插入到一个结果文档的数组中 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, url : {$push: \u0026quot;$url\u0026quot;}}}])   $addToSet 将值插入到一个结果文档的数组中，但不进行复制 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, url : {$addToSet : \u0026quot;$url\u0026quot;}}}])   $first 根据成组方式，从源文档中获取第一个文档。但只有对之前应用过 $sort 管道操作符的结果才有意义。 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, first_url : {$first : \u0026quot;$url\u0026quot;}}}])   $last 根据成组方式，从源文档中获取最后一个文档。但只有对之前进行过 $sort 管道操作符的结果才有意义。 db.mycol.aggregate([{$group : {_id : \u0026quot;$by_user\u0026quot;, last_url : {$last : \u0026quot;$url\u0026quot;}}}])    管道\r#\r\r管道（pipeline）概念指的是能够在一些输入上执行一个操作，然后将输出结果用作下一个命令的输入。MongoDB 的聚合架构也支持这种概念。管道中有很多阶段（stage）， 在每一阶段中，管道操作符都会将一组文档作为输入，产生一个结果文档（或者管道终点所得到的最终 JSON 格式的文档），然后再将其用在下一阶段。\n聚合架构中可能采取的管道操作符有：\n $project 用来选取集合中一些特定字段。 $match 过滤操作。减少用作下一阶段输入的文档的数量。 $group 如上所述，执行真正的聚合操作。 $sort 对文档进行排序。 $skip 在一组文档中，跳过指定数量的文档。 $limit 将查看文档的数目限制为从当前位置处开始的指定数目。 $unwind 解开使用数组的文档。当使用数组时，数据处于预连接状态，通过该操作，数据重新回归为各个单独的文档的状态。利用该阶段性操作可增加下一阶段性操作的文档数量。  db.test.aggregate([ {$match: {uuid: \u0026#39;sfsdfsfd\u0026#39;}}, {$project: {completeNum: 1, failedNum: 1, createTime: 1, _id: 0}}, {$group: { _id: \u0026#39;$createTime\u0026#39;, completeTotal: {$sum: \u0026#39;$completeNum\u0026#39;}, failedTotal: {$sum: \u0026#39;$failedNum\u0026#39;}}} ]); db.test.aggregate([ {$match: {\u0026#34;devSN\u0026#34;: \u0026#39;sdfasdsdfs\u0026#39;}}, // 匹配字段  {$unwind: \u0026#34;$wanList\u0026#34;},//把 wanList 展开，wanList 是一个数组，展开这个数组 例如 wanList:[{dd:1},{dd:2},{ff:3}],展开后得到{dd:1},{dd:2},{ff:3}  {$match: {\u0026#34;wanList.status\u0026#34;:1}},// 展开wanList之后再次匹配wanList.status为1  {\u0026#34;$group\u0026#34;: {\u0026#34;_id\u0026#34;: \u0026#34;$_id\u0026#34;, \u0026#34;wanList\u0026#34;: {\u0026#34;$push\u0026#34;: \u0026#34;$wanList\u0026#34;}}},// 把wanList展开后得到的结果重组为一个新的数组，如 wanList:[{_id:2342342,dd:1},{_id:97687687,dd:2},{_id:876678,ff:3}] ]); "});index.add({'id':35,'href':'/db-learn/docs/mysql/01_getting_started/','title':"入门",'content':"MySQL 服务器的进程也被称为 MySQL 数据库实例，简称数据库实例。MySQL 服务器进程的默认名称为 mysqld， 而 MySQL 客户 端进程的默认名称为 mysql。\n基本概念\r#\r\r表\r#\r\r数据库可以理解是一个文件柜。此文件柜是一个存放数据的物理位置，不管数据是什么以及如何组织的。 在将资料放入文件柜时，并不是随便将它们扔进某个抽屉就完事了，而是在文件柜中创建文件，然后将相关的资料放入特定的文件中。 这种文件称为表。是一种结构化的文件，可用来存储某种特定类型的数据。\n列和数据类型\r#\r\r列是表中的一个字段。所有表都是由一个或多个列组成的。\n数据类型（datatype）所容许的数据的类型。每个表列都有相应的数据类型，它限制（或容许）该列中存储的数据。\n行\r#\r\r行是表中的一个记录。\n主键\r#\r\r主键是表中的一列（或一组列），其值能够唯一区分表中每个行。\n表中的任何列都可以作为主键，只要它满足以下条件：\n 任意两行都不具有相同的主键值 每个行都必须具有一个主键值（主键列不允许NULL值）。  主键通常定义在表的一列上，但这并不是必需的。\n外键\r#\r\r外键为某个表中的一列，它包含另一个表的主键值，定义了两个表之间的关系。\n索引\r#\r\r索引是对数据库表中一列或多列的值进行排序的一种结构，使用索引可快速访问数据库表中的特定信息。\n索引的原理\r#\r\r 对要查询的字段建立索引其实就是把该字段按照一定的方式排序 建立的索引只对该字段有用，如果查询的字段改变，那么这个索引也就无效了，比如图书馆的书是按照书名的第一个字母排序的，那么你想要找作者叫张 三的书就不能用这个索引了  索引是优缺点\r#\r\r首先明白为什么索引会增加速度，DB 在执行一条 SQL 语句的时候，默认的方式是根据搜索条件进行全表扫描，遇到匹配条件的就加入搜索结果集合。 如果对某一字段增加索引，查询时就会先去索引列表中一次定位到特定值的行数，大大减少遍历匹配的行数，所以能明显增加查询的速度。不应该加索引 的场景：\n 如果每次都需要取到所有表记录，无论如何都必须进行全表扫描了，那么是否加索引也没有意义了。 对非唯一的字段，例如“性别”这种大量重复值的字段，增加索引也没有什么意义。 对于记录比较少的表，增加索引不会带来速度的优化反而浪费了存储空间，因为索引是需要存储空间的，而且有个致命缺点是 对于 update/insert/delete 的每次执行，字段的索引都必须重新计算更新。  可伸缩性\r#\r\r可伸缩性（scale）能够适应不断增加的工作量而不失败。设计良好的数据库或应用程序称之为可伸缩性好（scale well）。\n"});index.add({'id':36,'href':'/db-learn/docs/mysql/06_other/','title':"其他",'content':"视图\r#\r\rselect cust_name from customers, orders, orderitems where customers.cust_id = orders.cust_id and orderitems.order_num = orders.order_num and pod_id = \u0026#39;TNT2\u0026#39;; 上面的语句，涉及到三个表，用来检索订购了某个特定产品的客户。任何需要这个数据的人都必须理解相关表的结构， 并且知道如何创建查询和对表进行联结。为了检索其他产品（或多个产品）的相同数据，必须修改最后的 WHERE 子句。\n假如可以把整个查询包装成一个名为 productcustomers 的虚拟表，则可以如下轻松地检索出相同的数据：\nselect cust_name from productcustomers where pod_id = \u0026#39;TNT2\u0026#39;; 这就是视图的作用。productcustomers 是一个视图，作为视图，它不包含表中应该有的任何列或数据，它包含的是一个 SQL 查询（与上 面用以正确联结表的相同的查询）。\n为什么使用视图\r#\r\r 重用 SQL 语句。 简化复杂的 SQL 操作。在编写查询后，可以方便地重用它而不必知道它的基本查询细节。 使用表的组成部分而不是整个表。 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限。 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。  视图创建之后，可以用与表基本相同的方式利用它们。可以对视图执行 SELECT 操作，过滤和排序数据，将视图联结到其他视图或表，甚至能添加 和更新数据。\n视图本身不包含数据，因此它们返回的数据是从其他表中检索出来的。在添加或更改这些表中的数据时，视图将返回改变过的数据。\n性能问题\r#\r\r因为视图不包含数据，所以每次使用视图时，都必须处理查询执行时所需的任一个检索。如果你用多个联结和过滤创建了复杂的视图或者嵌套了视图，可能 会发现性能下降得很厉害。因此，在部署使用了大量视图的应用前，应该进行测试。\n规则和限制\r#\r\r 与表一样，视图必须唯一命名（不能给视图取与别的视图或表相同的名字）。 对于可以创建的视图数目没有限制。 为了创建视图，必须具有足够的访问权限。这些限制通常由数据库管理人员授予。 视图可以嵌套，即可以利用从其他视图中检索数据的查询来构造一个视图。 ORDER BY 可以用在视图中，但如果从该视图检索数据 SELECT 中也含有 ORDER BY，那么该视图中的 ORDER BY 将被覆盖。 视图不能索引，也不能有关联的触发器或默认值。 视图可以和表一起使用。例如，编写一条联结表和视图的 SELECT 语句。  使用视图\r#\r\r CREATE VIEW 语句创建视图。 SHOW CREATE VIEW viewname 查看创建视图的语句。 DROP 删除视图，其语法为 DROP VIEW viewname。 更新视图时，可以先用 DROP 再用 CREATE，也可以直接用 CREATE OR REPLACE VIEW。如果要更新的视图不存在， 则第 2 条更新语句会创建一个视图；如果要更新的视图存在，则第2条更新语句会替换原有视图  用视图重新格式化检索出的数据\r#\r\r视图的另一常见用途是重新格式化检索出的数据。\nselect Concat(RTrim(vend_name)), \u0026#39;(\u0026#39; , RTrim(vend_country), \u0026#39;)\u0026#39;) as vend_title from vendors order by vend_name; 上面的语句，格式化了结果，如果要经常用，可以创建一个视图：\ncreate view vendorlocations as select Concat(RTrim(vend_name)), \u0026#39;(\u0026#39; , RTrim(vend_country), \u0026#39;)\u0026#39;) as vend_title from vendors order by vend_name; 然后可以直接使用：\nselect * from vendorlocations; 视图也可以用来过滤数据，或者计算字段。\n更新视图\r#\r\r并非所有视图都是可更新的。基本上可以说，如果 MySQL 不能正确地确定被更新的基数据，则不允许更新（包括插入和删除）。 意味着，如果视图定义中有以下操作，则不能进行视图的更新：\n 分组（使用 GROUP BY 和HAVING）； 联结； 子查询； 并； 聚集函数（Min()、Count()、Sum() 等）； DISTINCT； 导出（计算）列。  存储过程\r#\r\r以下的情形。\n 为了处理订单，需要核对以保证库存中有相应的物品。 如果库存有物品，这些物品需要预定以便不将它们再卖给别的人，并且要减少可用的物品数量以反映正确的库存量。 库存中没有的物品需要订购，这需要与供应商进行某种交互。 关于哪些物品入库（并且可以立即发货）和哪些物品退订，需要通知相应的客户。  执行这个处理需要针对许多表的多条 MySQL 语句。此外，需要执行的具体语句及其次序也不是固定的，它们可能会（和将）根据哪些物品在库存中哪 些不在而变化。\n可以创建存储过程。存储过程简单来说，就是为以后的使用而保存的一条或多条 MySQL 语句的集合。\n为什么要使用存储过程\r#\r\r 简化复杂的操作 由于不要求反复建立一系列处理步骤，这保证了数据的完整性。所有开发人员和应用程序都使用同一（试验和测试）存储过程，则所使用的代码都是 相同的。这一点的延伸就是防止错误。防止错误保证了数据的一致性。 简化对变动的管理。如果表名、列名或业务逻辑（或别的内容）有变化，只需要更改存储过程的代码。使用它的人员甚至不需要知道这些变化。 提高性能。存储过程比使用单独的 SQL 语句要快。  总结就是，简单、安全、高性能。\n缺陷：\n 存储过程的编写比基本 SQL 语句复杂 你可能没有创建存储过程的安全访问权限。  使用\r#\r\rMySQL 称存储过程的执行为调用，因此 MySQL 执行存储过程的语句为 CALL。CALL 接受存储过程的名字以及需要传递给它的任意参数。\ncall productpricing(@pricelow, @pricehigh, @priceacerage); 执行名为 productpricing 的存储过程，它计算并返回产品的最低、最高和平均价格。\n因为存储过程实际上是一种函数，所以存储过程名后需要有 () 符号（即使不传递参数也需要）。\n创建\r#\r\rcreate procedure productpricing() begin select Avg(prod_price) as priceacerage from products; end; 此存储过程名为 productpricing，用 CREATE PROCEDURE productpricing() 语句定义。如果存储过程接受参数，它们将在 () 中列举 出来。此存储过程没有参数，但后跟的 () 仍然需要。BEGIN 和 END 语句用来限定存储过程体，过程体本身仅是一个简单的 SELECT 语句。\n删除\r#\r\rdrop procedure productpricing; 使用参数\r#\r\rcreate procedure productpricing( out pl DECIMAL(8,2), out ph DECIMAL(8,2), out pa DECIMAL(8,2) ) begin select Min(prod_price) into pl from products; select Max(prod_price) into ph from products; select Avg(prod_price) into pa from products; end; 此存储过程接受 3 个参数：pl 存储产品最低价格，ph 存储产品最高价格，pa 存储产品平均价格。每个参数必须具有指定的类型，这里使用十 进制值。关键字 OUT 指出相应的参数用来从存储过程传出一个值（返回给调用者）。\nMySQL 支持三种类型参数：\n IN 传递给存储过程 OUT 从存储过程传出 INOUT 对存储过程传入和传出  存储过程的代码位于 BEGIN 和 END 语句内，它们是一系列 SELECT 语句，用来检索值，然后保存到相应的变量（通过指定 INTO 关键字）。\n调用此修改过的存储过程，必须指定 3 个变量名，如下所示：\ncall productpricing(@pricelow, @pricehigh, @priceacerage); 此存储过程要求 3 个参数，因此必须正好传递 3 个参数。存储过程将保存结果到这 3 个变量。\n 所有 MySQL 变量都必须以 @ 开始。\n 调用这条语句并不显示任何数据。为了显示检索出的产品平均价格，可使用下面的语句：\nselect @priceacerage; select @pricelow, @pricehigh, @priceacerage; 使用 IN 和 OUT\r#\r\rcreate procedure ordertotal( in onumber int, inout ototal DECIMAL(8,2) ) begin select Sum(item_price*quantity) from orderitems where order_num = onumber into otital; end; onumber 定义为 IN，因为需要传订单号给存储过程。ototal 定义为 OUT，因为要从存储过程返回合计。SELECT 语句使用这两个参 数，WHERE 子句使用 onumber 选择正确的行，INTO 使用 ototal 存储计算出来的合计。\ncall ordertotal(2005, @total); 必须给 ordertotal 传递两个参数；第一个参数为订单号，第二个参数为包含计算出来的合计的变量名。\n显示此合计：\nselect @total; 智能存储过程\r#\r\r存储过程只有在包含业务规则和智能处理时，才真正显现出来他的作用。\n例如，需要获得一份订单合计，但需要对合计增加营业税，不过只针对某些顾客。\n 获得合计（与以前一样） 把营业税有条件地添加到合计 返回合计（带或不带税）  -- Name: ordertotal -- Parameters: onumber = order number -- taxable = 0 if not taxable, 1 if taxable -- ototal = order total variable create procedure odertotal( in onumber int, in taxable boolean, out ototal decimal(8, 2) ) comment \u0026#39;Obtain order total, optionally adding tax\u0026#39; begin -- Declare variable for total declare total decimal(8, 2); -- Declare tax percentage declare taxrate int default 6; -- Get the order total select sum(item_price*quantity) from orderitems where order_num = onumber into total; -- Is this taxable? IF taxable THEN -- Yes, so add taxrate to the total  select total+(total/100*taxrate) into total; END IF; -- And finally, save to out variable select total into ototal; end; -- 表示注释。参数 taxable，它是一个布尔值，表示是否增加税。 DECLARE 语句定义了两个局部变量。DECLARE 要求指定变量名和数据类型，它也支持可选的默认值（这里的 taxrate 的默认被设置为 6%）\nIF 语句检查 taxable 是否为真，如果为真，则用另一 SELECT 语句增加营业税到局部变量 total。 最后，用另一 SELECT 语句将 total 保存到 ototal。\n检查存储过程\r#\r\rSHOW CREATE PROCEDURE name 和 SHOW PROCEDURE STATUS name。\n游标\r#\r\rMySQL 检索操作返回一组称为结果集的行。这组返回的行都是与 SQL 语句相匹配的行（零行或多行）。\n有时，需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因。游标（cursor）是一个存储在 MySQL 服务器上的数据库查询， 它不是一条 SELECT 语句，而是被该语句检索出来的结果集。在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据。\n MySQL 游标只能用于存储过程（和函数）。\n 使用游标\r#\r\r 在能够使用游标前，必须声明（定义）它。这个过程实际上没有检索数据，它只是定义要使用的 SELECT 语句。 一旦声明后，必须打开游标以供使用。这个过程用前面定义的 SELECT 语句把数据实际检索出来。 对于填有数据的游标，根据需要取出（检索）各行。 在结束游标使用时，必须关闭游标。  创建游标\r#\r\rDECLARE 语句创建游标。DECLARE 命名游标，并定义相应的 SELECT 语句，根据需要带 WHERE 和其他子句。\ncreate procedure processorders() begin declare ordernumbers cursor for select order_num from orders; end; 存储过程处理完成后，游标就消失。\n打开关闭\r#\r\r打开使用：OPEN ordernumbers; 关闭使用：CLOSE ordernumbers;\n 使用声明过的游标不需要再次声明，用 OPEN 语句打开它就可以了。 如果你不明确关闭游标，MySQL 将会在到达 END 语句时自动关闭它。\n 使用游标数据\r#\r\r在一个游标被打开后，可以使用 FETCH 语句分别访问它的每一行。FETCH 指定检索什么数据（所需的列），检索出来的数据存储在什么地方。\ncreate procedure processorders() begin -- Declare local variables  declare o int; -- Delare the cursor  declare ordernumbers cursor for select order_num from orders; -- open the cursor  open ordernumbers; -- get order number  fetch ordernumbers into o; -- close the cursor  close ordernumbers; end; 触发器\r#\r\r如果你想要某条语句（或某些语句）在事件发生时自动执行，怎么办呢？使用触发器。\n触发器是 MySQL 响应以下任意语句而自动执行的一条 MySQL 语句（或位于 BEGIN 和 END 语句之间的一组语句）：\n DELETE INSERT UPDATE  创建触发器时，需要给出 4 条信息：\n 唯一的触发器名； 触发器关联的表； 触发器应该响应的活动（DELETE、INSERT 或 UPDATE）； 触发器何时执行（处理之前或之后）  创建\r#\r\r用 CREATE TRIGGER 语句创建。\ncreate trigger newproduct after insert on products for each row select \u0026#39;Procduct added\u0026#39;; 创建名为 newproduct 的新触发器。触发器可在一个操作发生之前或之后执行，这里给出了 AFTER INSERT，所以此触发器将在 INSERT 语句成 功执行后执行。这个触发器还指定 FOR EACH ROW，因此代码对每个插入行执行。在这个例子中，文本 Product added 将对每个插入的行显示一次。 使用 INSERT 语句添加一行或多行到 products 中，你将看到对每个成功的插入，显示 Product added 消息。\n 每个表每个事件每次只允许一个触发器。因此，每个表最多支持 6 个触发器（每条 INSERT、UPDATE 和 DELETE 的之前和之后）。 如果 BEFORE 触发器失败，则 MySQL 将不执行请求的操作。此外，如果 BEFORE 触发器或语句本身失败，MySQL 将不执行 AFTER 触发 器（如果有的话）。 MySQL 触发器中不支持 CALL 语句。也就是不能从触发器内调用存储过程。\n 删除\r#\r\r删除触发器使用：DROP TRIGGER newproduct;。为了修改一个触发器，必须先删除它，然后再重新创建。\n事务\r#\r\r事务处理是一种机制，用来管理必须成批执行的 MySQL 操作，以保证数据库不包含不完整的操作结果。利用事务处理，可以保证一组操作不会中途停止， 它们或者作为整体执行，或者完全不执行（除非明确指示）。如果没有错误发生，整组语句提交给（写到）数据库表。如果发生错误，则进行回退（撤销） 以恢复数据库到某个已知且安全的状态。\n关于事务处理需要知道的几个术语：\n 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（place- holder），你可以对它发布回退（与回退整个事务处理不同）  控制事务处理\r#\r\r管理事务处理的关键在于将 SQL 语句组分解为逻辑块，并明确规定数据何时应该回退，何时不应该回退。\n下面的语句来标识事务的开始：\nSTART TRANSACTION ROLLBACK\r#\r\rROLLBACK 命令用来回退（撤销）MySQL 语句：\nselect * from ordertotals; start transaction; delete from ordertotals; select * from ordertotals; rollback; select * from ordertotals; 先执行一条 SELECT 以显示该表不为空。然后开始一个事务处理，用一条 DELETE 语句删除 ordertotals 中的所有行。 另一条 SELECT 语句验证 ordertotals 确实为空。这时用一条 ROLLBACK 语句回退 START TRANSACTION 之后的所有语句，最后 一条 SELECT 语句显示该表不为空。\nROLLBACK 只能在一个事务处理内使用（在执行一条 START TRANSACTION 命令之后）。\n哪些语句可以回退\r#\r\r事务处理用来管理 INSERT、UPDATE 和 DELETE 语句。不能回退 SELECT 语句。（这样做也没有什么意义）不能回退 CREATE 或 DROP 操作。事务处理块中可以使用这两条语句，但如果你执行回退，它们不会被撤销。\nCOMMIT\r#\r\r一般的 MySQL 语句都是直接针对数据库表执行和编写的。这就是所谓的隐含提交（implicit commit），即提交（写或保存）操作是自动进行的。\n在事务处理块中，提交不会隐含地进行。为进行明确的提交，使用 COMMIT 语句：\nstart transaction; delete from orderitems where order_num = 20005; delete from orders where order_num = 20005; commit;  当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭（将来的更改会隐含提交）。\n 保留点\r#\r\r简单的 ROLLBACK 和 COMMIT 语句就可以写入或撤销整个事务处理。复杂的事务处理可能需要部分提交或回退。\n为了支持回退部分事务处理，必须能在事务处理块中合适的位置放置占位符。这样，如果需要回退，可以回退到某个占位符。这些占位符称为保留点。\n创建占位符，可使用 SAVEPOINT 语句：SAVEPOINT delete1;。 每个保留点都取标识它的唯一名字，以便在回退时，MySQL 知道要回退到何处。\n回退到本例给出的保留点，可执行：ROLLBACK TO delete1;\n 保留点在事务处理完成（执行一条 ROLLBACK 或 COMMIT）后自动释放。\n 用户管理\r#\r\r在现实世界的日常工作中，决不能使用 root。应该创建一系列的账号，有的用于管理，有的供用户使用，有的供开发人员使用，等等。\nMySQL 用户账号和信息存储在名为 mysql 的库中。一般不需要直接访问 mysql 数据库和表，但有时需要直接访问。需要直接访问它 的时机之一是在需要获得所有用户账号列表时。\nmysql 库有一个名为 user 的表，它包含所有用户账号。user 表有一个名为 user 的列，它存储用户登录名。\n"});index.add({'id':37,'href':'/db-learn/docs/mysql/05_write_operation/','title':"写操作",'content':"插入数据\r#\r\r插入完整的行\r#\r\rinsert into customers values(\u0026#39;xiaoming\u0026#39;, \u0026#39;shanghai\u0026#39;, 18); insert into customers(cust_name, cust_address, cust_age) values(\u0026#39;xiaoming\u0026#39;, \u0026#39;shanghai\u0026#39;, 18); 第二条语句更安全。第一种语法不建议使用，因为各个列必须以它们在表定义中出现的次序填充。高度依赖于表中列的定义次序，并且还依赖于其次序容 易获得的信息。\n 如果表的定义允许，则可以在 INSERT 操作中省略某些列。省略的列必须满足以下某个条件。该列定义为允许 NULL 值（无值或空值）。在表定义 中给出默认值。这表示如果不给出值，将使用默认值。 不管使用哪种 INSERT 语法，都必须给出 VALUES 的正确数目。如果不提供列名，则必须给每个表列提供一个值。如果提供列名，则必须对 每个列出的列给出一个值。\n 插入多行\r#\r\rinsert into customers(cust_name, cust_address, cust_age) values(\u0026#39;xiaoming\u0026#39;, \u0026#39;shanghai\u0026#39;, 18), values(\u0026#39;xiaoliang\u0026#39;, \u0026#39;shanghai\u0026#39;, 18); 其中单条 INSERT 语句有多组值，每组值用一对圆括号括起来，用逗号分隔。\n MySQL 用单条 INSERT 语句处理多个插入比使用多条 INSERT 语句快。\n 插入检索出的数据\r#\r\rINSERT 还存在另一种形式，可以利用它将一条 SELECT 语句的结果插入表中。\ninsert into customers(cust_name, cust_address, cust_age) select cust_name, cust_address, cust_age from custnew; 使用 INSERT SELECT 从 custnew 中将所有数据导入 customers。SELECT 语句从 custnew 检索出要插入的值，而不是列出它们。\n INSERT 和 SELECT 语句中使用了相同的列名。但是，不一定要求列名匹配。事实上，MySQL 甚至不关心 SELECT 返回的列名。 它使用的是列的位置，因此 SELECT 中的第一列（不管其列名）将用来填充表列中指定的第一个列\n 更新数据\r#\r\r使用 UPDATE 语句更新（修改）表中的数据。\nUPDATE 语句由 3 部分组成，分别是：\n 要更新的表； 列名和它们的新值 确定要更新行的过滤条件  update customers set cust_email = \u0026#39;111111@demo.com\u0026#39; where cust_id = 1005;  使用 UPDATE时一定要注意添加过滤条件，避免更新所有行。 UPDATE 语句更新多行，并且在更新这些行中的一行或多行时出一个现错误，则整个 UPDATE 操作被取消。为即使是发生错误， 也继续进行更新，可使用 IGNORE 关键字，如：UPDATE IGNORE customers。\n 删除数据\r#\r\rdelete from customers where cust_id = 1005; DELETE 不需要列名或通配符。\n 使用 DELETE 时一定要注意添加过滤条件，避免删除所有行。 删除表中所有行使用 TRUNCATE TABLE 语句，它的速度比使用 delete 快很多（TRUNCATE 实际是删除原来的表并重新创建一个表，而 不是逐行删除表中的数据）。\n 对 UPDATE 或 DELETE 语句使用 WHERE 子句前，应该先用 SELECT 进行测试，保证它过滤的是正确的记录，以防编写的 WHERE 子 句不正确。\n创建和操作表\r#\r\r创建\r#\r\rCREATE TABLE 创建表，必须给出下列信息：\n 新表的名字，在关键字 CREATE TABLE 之后给出； 表列的名字和定义，用逗号分隔。  create table customers ( cust_id int NOT NULL AUTO_INCREMENT, cust_name char(50) NOT_NULL, cust_address char(50) NOT NULL DEFAULT \u0026#39;shanghai\u0026#39;, primary key (cust_id) ) engine=InnoDB; NULL值\r#\r\r允许 NULL 值的列也允许在插入行时不给出该列的值。每个表列或者是 NULL 列，或者是 NOT NULL 列，这种状态在创建时由表的定义规定。\n数据库开发人员应该使用默认值而不是 NULL 列。\n主键\r#\r\r主键值必须唯一。即，表中的每个行必须具有唯一的主键值。如果主键使用单个列，则它的值必须唯一。如果使用多个列，则这些列的组合值必 须唯一。\nAUTO_INCREMENT\r#\r\rAUTO_INCREMENT 告诉 MySQL，本列每当增加一行时自动增量。每次执行一个 INSERT 操作时，MySQL 自动对该列增量，给该列赋予下一个可 用的值。\n每个表只允许一个 AUTO_INCREMENT 列，而且它必须被索引。\nlast_insert_id()，函数返回最后一个 AUTO_INCREMENT 值。\n设置表的存储引擎\r#\r\r可以为不同的表设置不同的存储引擎，也就是说不同的表可以有不同的物理存储结构，不同的提取和写入方式。\n创建表的语句没有指定表的存储引擎，那就会使用默认的存储引擎：\nengine=InnoDB; 上面的的语句表示设置表的存储引擎为 InnoDB。\n修改表的存储引擎\r#\r\rALTER TABLE 表名 ENGINE = 存储引擎名称; 更新表\r#\r\r理想状态下，当表中存储数据以后，该表就不应该再被更新。\n使用 ALTER TABLE 更改表结构，必须给出下面的信息：\n 在 ALTER TABLE 之后给出要更改的表名（该表必须存在，否则将出错）； 所做更改的列表   使用 ALTER TABLE 要极为小心，应该在进行改动前做一个完整的备份（模式和数据的备份）。数据库表的更改不能撤销，如果增加了不需 要的列，可能不能删除它们。类似地，如果删除了不应该删除的列，可能会丢失该列中的所有数据。\n 删除表\r#\r\rdrop table customers 重命名表\r#\r\rrename table customers to customers2 "});index.add({'id':38,'href':'/db-learn/docs/redis/13_distributed-lock/','title':"分布式锁",'content':"分布式锁\r#\r\r分布式锁是用来解决并发问题的。比如一个操作要修改用户的状态，修改状态需要先读出用户的状态，在内存里进行修改，改完了再存回去。如果这样的操 作同时进行了，就会出现并发问题，因为读取和保存状态这两个操作不是原子的。\n分布式锁本质上要实现的目标就是在 Redis 里面占一个坑，当别的进程也要来占时，发现已经有人蹲在那里了，就只好放弃或者稍后再试。\n占坑一般是使用 setnx(set if not exists) 指令，只允许被一个客户端占坑。先来先占， 用完了，再调用 del 指令释放茅坑。\n\u0026gt; setnx lock:codehole true OK ... do something critical ... \u0026gt; del lock:codehole (integer) 1 但是有个问题，如果逻辑执行到中间出现异常了，可能会导致 del 指令没有被调用，这样就会陷入死锁，锁永远得不到释放。\n于是我们在拿到锁之后，再给锁加上一个过期时间，这样即使中间出现异常也可以保证锁会自动释放。\n\u0026gt; set lock:codehole true ex 5 nx OK ... do something critical ... \u0026gt; del lock:codehole 超时问题\r#\r\rRedis 的分布式锁不能解决超时问题，例如：\n 加锁和释放锁之间的逻辑执行的太长，以至于超出了过期时间，导致锁过期了。 这时候第一个线程持有的锁过期了，但是临界区的逻辑还没有执行完。 这个时候第二个线程就提前重新持有了这把锁，导致临界区代码不能得到严格的串行执行。  解决方案\r#\r\rtag = random.nextint() # 随机数 if redis.set(key, tag, nx=True, ex=5): do_something() redis.delifequals(key, tag) # 假想的 delifequals 指令 或者：\nSET key random_value NX PX 30000 为 set 指令的 value 参数设置为一个随机数，释放锁时先匹配随机数是否一致，然后再删除 key，这是为了确保当前线程占有的锁不会被 其它线程释放，除非这个锁是过期了被服务器自动释放的。\n设置一个随机字符串 tag 是很有必要的，它保证了一个客户端释放的锁必须是自己持有的那个锁。假如获取锁时 SET 的不是一个随机字 符串，而是一个固定值，那么可能会发生下面的执行序列：\n 客户端 1 获取锁成功。 客户端 1 在某个操作上阻塞了很长时间。 过期时间到了，锁自动释放了。 客户端 2 获取到了对应同一个资源的锁。 客户端 1 从阻塞中恢复过来，释放掉了客户端 2 持有的锁。  但是匹配 value 和删除 key 不是一个原子操作，Redis 也没有提供类似于 delifequals 这样的指令，这就需要使用 Lua 脚本来处理了，因 为 Lua 脚本可以保证连续多个指令的原子性执行。\n# delifequals if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end 这段 Lua 脚本在执行的时候要把前面的 tag 作为 ARGV[1] 的值传进去，把 key 作为 KEYS[1] 的值传进去。\n释放锁的操作必须使用 Lua 脚本来实现。释放锁其实包含三步操作：GET、判断和 DEL，用 Lua 脚本来实现能保证这三步的原子性。 否则，如果把这三步操作放到客户端逻辑中去执行的话，就有可能发生与前面第三个问题类似的执行序列：\n 客户端 1 获取锁成功。 客户端 1 访问共享资源。 客户端 1 为了释放锁，先执行 GET 操作获取随机字符串的值。 客户端 1 判断随机字符串的值，与预期的值相等。 客户端 1 由于某个原因阻塞住了很长时间。 过期时间到了，锁自动释放了。 客户端 2 获取到了对应同一个资源的锁。 客户端 1 从阻塞中恢复过来，执行 DEL 操纵，释放掉了客户端 2 持有的锁。  如果不是客户端阻塞住了，而是出现了大的网络延迟，也有可能导致类似的执行序列发生。\n这个方案，它只是相对安全一点，因为如果真的超时了，当前线程的逻辑没有执行完，其它线程也会乘虚而入。\nRedlock 算法\r#\r\r上面的加锁方式，是有缺陷的。\nantirez 还指出了一个问题，是由 failover 引起的，却是基于单 Redis 节点的分布式锁无法解决的。正是这个问题催生了 Redlock 的出现。\n这个问题是这样的。假如 Redis 节点宕机了，那么所有客户端就都无法获得锁了，服务变得不可用。为了提高可用性，我们可以给这个 Redis 节点挂 一个 Slave，当 Master 节点不可用的时候，系统自动切到 Slave 上（failover）。但由于 Redis 的主从复制（replication）是异步的，这可能 导致在 failover 过程中丧失锁的安全性。考虑下面的执行序列：\n 客户端 1 从 Master 获取了锁。 Master 宕机了，存储锁的 key 还没有来得及同步到 Slave 上。 Slave 升级为 Master。 客户端 2 从新的 Master 获取到了对应同一个资源的锁。  客户端 1 和客户端 2 同时持有了同一个资源的锁。锁的安全性被打破。\n锁的有效时间\r#\r\r锁的有效时间(lock validity time)，设置成多少合适呢？如果设置太短的话，锁就有可能在客户端完成对于共享资源的访问之前过期，从而失去保 护；如果设置太长的话，一旦某个持有锁的客户端释放锁失败，那么就会导致所有其它客户端都无法获取锁，从而长时间内无法正常工作。\n分布式锁 Redlock\r#\r\rRedlock 分布式锁，它基于 N 个完全独立的 Redis 节点（通常情况下 N 可以设置成 5）。\n运行 Redlock 算法的客户端依次执行下面各个步骤，来完成获取锁的操作：\n 获取当前时间（毫秒数）。 按顺序依次向 N 个 Redis 节点执行获取锁的操作。这个获取操作跟前面基于单 Redis 节点的获取锁的过程相同，包含随机字符串 tag，也包含 过期时间(比如 PX 30000，即锁的有效时间)。为了保证在某个 Redis 节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)， 它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个 Redis 节点获取锁失败以后，应该立即尝试下一个 Redis 节点。这里的失败，应该包含任 何类型的失败，比如该 Redis 节点不可用，或者该 Redis 节点上的锁已经被其它客户端持有（注：Redlock 原文中这里只提到了 Redis 节点不可用的 情况，但也应该包含其它的失败情况）。 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第 1 步记录的时间。如果客户端从大多数 Redis 节点（\u0026gt;= N/2+1）成功获 取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第 3 步计算出来的获取锁消耗的时间。 如果最终获取锁失败了（可能由于获取到锁的 Redis 节点个数少于 N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客 户端应该立即向所有 Redis 节点发起释放锁的操作（即前面介绍的 Redis Lua 脚本）。  释放锁的过程比较简单：客户端向所有 Redis 节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。\n"});index.add({'id':39,'href':'/db-learn/docs/redis/ziplist/','title':"压缩列表",'content':"压缩列表\r#\r\r压缩列表（ziplist）是列表键和哈希键的底层实现之一。\n当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做列表键的 底层实现。\n比如：\nredis\u0026gt; RPUSH lst 1 3 5 10086 \u0026#34;hello\u0026#34; \u0026#34;world\u0026#34; (integer) 6 redis\u0026gt; OBJECT ENCODING lst \u0026#34;ziplist\u0026#34; 因为列表键里面包含的都是 1 、 3 、 5 、 10086 这样的小整数值，以及 \u0026ldquo;hello\u0026rdquo; 、 \u0026ldquo;world\u0026rdquo; 这样的短字符串。\n当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做 哈希键的底层实现。\nredis\u0026gt; HMSET profile \u0026#34;name\u0026#34; \u0026#34;Jack\u0026#34; \u0026#34;age\u0026#34; 28 \u0026#34;job\u0026#34; \u0026#34;Programmer\u0026#34; OK redis\u0026gt; OBJECT ENCODING profile \u0026#34;ziplist\u0026#34; 压缩列表的构成\r#\r\r压缩列表是 Redis 为了节约内存而开发的，由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。\n压缩列表的各个组成部分：  zlbytes，uint32_t，4 字节，记录整个压缩列表占用的内存字节数：在对压缩列表进行内存重分配，或者计算 zlend 的位置时使用。 zltail，uint32_t，4 字节，记录压缩列表表尾节点距离压缩列表的起始地址有多少字节：通过这个偏移量，程序无须遍历整个压缩列表就可以确定表尾节点的地址。 zllen，uint16_t，2 字节，记录了压缩列表包含的节点数量：当这个属性的值小于 UINT16_MAX （65535）时，这个属性的值就是压缩列表包含节点的数量； 当这个值等于 UINT16_MAX 时，节点的真实数量需要遍历整个压缩列表才能计算得出。 entryX，压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend，uint8_t，1 字节，特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。  压缩列表节点的构成\r#\r\r压缩列表节点组成： 每个压缩列表节点可以保存一个字节数组或者一个整数值，其中，字节数组可以是以下三种长度的其中一种：\n 长度小于等于 63 （2^{6}-1）字节的字节数组 长度小于等于 16383 （2^{14}-1） 字节的字节数组 长度小于等于 4294967295 （2^{32}-1）字节的字节数组  而整数值则可以是以下六种长度的其中一种：\n 4 位长，介于 0 至 12 之间的无符号整数； 1 字节长的有符号整数； 3 字节长的有符号整数； int16_t 类型整数； int32_t 类型整数； int64_t 类型整数。  previous_entry_length\r#\r\rprevious_entry_length 属性以字节为单位，记录了压缩列表中前一个节点的长度。长度可以是 1 字节或者 5 字节：\n 如果前一节点的长度小于 254 字节， 那么 previous_entry_length 属性的长度为 1 字节：前一节点的长度就保存在这一个字节里面。 如果前一节点的长度大于等于 254 字节， 那么 previous_entry_length 属性的长度为 5 字节：其中属性的第一字节会被设置 为 0xFE （十进制值 254），而之后的四个字节则用于保存前一节点的长度。  因为节点的 previous_entry_length 属性记录了前一个节点的长度，所以程序可以通过指针运算，根据当前节点的起始地址来计算出前一个节点 的起始地址。\n举个例子，如果我们有一个指向当前节点起始地址的指针 c ，那么我们只要用指针 c 减去当前节点 previous_entry_length 属性的值，就可以 得出一个指向前一个节点起始地址的指针 p 。\n压缩列表的从表尾向表头遍历操作就是使用这一原理实现的：只要我们拥有了一个指向某个节点起始地址的指针，那么通过这个指针以及这个节点 的 previous_entry_length 属性，程序就可以一直向前一个节点回溯， 最终到达压缩列表的表头节点。\nencoding\r#\r\rencoding 属性记录了节点的 content 属性所保存数据的类型以及长度：\n 一字节、两字节或者五字节长，值的最高位为 00 、 01 或者 10 的是字节数组编码：这种编码表示节点的 content 属性保存着字节数组，数 组的长度由编码除去最高两位之后的其他位记录 一字节长，值的最高位以 11 开头的是整数编码：这种编码表示节点的 content 属性保存着整数值，整数值的类型和长度由编码除去最高两位 之后的其他位记录     编码 编码长度 content属性保存的值     00bbbbbbbb 1 字节 长度小于等于 63 字节的字节数组。   01bbbbbbbb xxxxxxxx 2 字节 长度小于等于 16383 字节的字节数组。   10______ aaaaaaaa bbbbbbbb cccccccc dddddddd 5 字节 长度小于等于 4294967295 的字节数组。   11000000 1 字节 int16_t 类型的整数。   11010000 1 字节 int32_t 类型的整数。   11100000 1 字节 int64_t 类型的整数。   11110000 1 字节 24 位有符号整数。   11111110 1 字节 8 位有符号整数。   1111xxxx 1 字节 长使用这一编码的节点没有相应的 content 属性， 因为编码本身的 xxxx 四个位已经保存了一个介于 0 和 12 之间的值，所以它无须 content 属性。    content\r#\r\rcontent 属性负责保存节点的值，节点值可以是一个字节数组或者整数。\n连锁更新\r#\r\r考虑这样一种情况： 在一个压缩列表中， 有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN ： e1 至 eN 的所有节点的长度都小于 254 字节，所以记录这些节点的长度只需要 1 字节长的 previous_entry_length 属性， 换句话说， e1 至 eN 的所有节点的 previous_entry_length 属性都是 1 字节长的。\n如果将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点，那么 new 将成为 e1 的前置节点，因为 e1 的 previous_entry_length 属 性仅长 1 字节，它没办法保存新节点 new 的长度 所以程序将对压缩列表执行空间重分配操作，并将 e1 节点的 previous_entry_length 属性从 原来的 1 字节长扩展为 5 字节长。\ne1 原本的长度介于 250 字节至 253 字节之间， 在为 previous_entry_length 属性新增四个字节的空间之后，e1 的长度就变成了介于 254 字 节至 257 字节之间，而这种长度使用 1 字节长的 previous_entry_length 属性是没办法保存的。e2 节点的 previous_entry_length 属性 也要从原来的 1 字节长扩展为 5 字节长。\n扩展 e1 引发了对 e2 的扩展一样，扩展 e2 也会引发对 e3 的扩展，直到 eN 为止。\nRedis 将这种在特殊情况下产生的连续多次空间扩展操作称之为连锁更新。\n连锁更新的复杂度较高， 但它真正造成性能问题的几率是很低的：\n 首先， 压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点， 连锁更新才有可能被引发， 在实际中， 这种情况并不多见； 其次， 即使出现连锁更新， 但只要被更新的节点数量不多， 就不会对性能造成任何影响： 比如说， 对三五个节点进行连锁更新是绝对不会影响 性能的；  "});index.add({'id':40,'href':'/db-learn/docs/mysql/08_config/','title':"启动选项和配置文件",'content':"MySQL 服务端设置项一般都有各自的默认值，例如，服务端客户端做大连接数默认是 151，默认存储引擎是 InnoDB，这些选项叫做启动选项，可以在 程序启动的时候去修改。\n不论是服务端相关的程序（mysqld、mysqld_safe）还是客户端相关的程序（mysql、mysqladmin），在启动的时候基本都可以指定启动参数。 这些启动参数可以放在命令行中指定，也可以把它们放在配置文件中指定。\n在命令行上使用选项\r#\r\r --skip-networking 选项禁止各客户端使用 TCP/IP 网络进行通信。（也可以使用 skip_networking，当选项名由多个单词构成时，可以 - 连接， 也可以 _ 连接） --default-storage-engine=\u0026lt;engine\u0026gt; 改变默认存储引擎。  示例：\nmysqld --default-storage-engine=MyISAM 查看更多启动选项：\nmysqld --verbose --help mysqld_safe --help 配置文件中使用选项\r#\r\r在配置文件中使用选项，就不需要每次都在命令行中添加参数。\n配置文件的路径\r#\r\rWindows 操作系统的配置文件\r#\r\rMySQL 会按照下列路径来寻找配置文件：\n   路径 描述     %WINDIR%\\my.ini， %WINDIR%\\my.cnf    C:\\my.ini， C:\\my.cnf    BASEDIR\\my.ini， BASEDIR\\my.cnf    defaults-extra-file 命令行指定的额外配置文件路径   %APPDATA%\\MySQL\\.mylogin.cnf 登录路径选项（仅限客户端）     %WINDIR% 一般是 C:\\WINDOWS，可以用 echo %WINDIR% 来查看。 BASEDIR 指的是 MySQL 安装目录的路径 defaults-extra-file 在命令行上可以这么写 mysqld --defaults-extra-file=C:\\Users\\xiaohaizi\\my_extra_file.txt %APPDATA% 表示 Windows 应用程序数据目录的值 .mylogin.cnf 中只能包含一些用于启动客户端连接服务端的一些选项，包括 host、user、password、port 和 socket。  类 UNIX 操作系统的配置文件\r#\r\rMySQL 会按照下列路径来寻找配置文件：\n   路径 描述     /etc/my.cnf    /etc/mysql/my.cnf    SYSCONFDIR/my.cnf    $MYSQL_HOME/my.cnf 特定于服务器的选项（仅限服务器）   defaults-extra-file 命令行指定的额外配置文件路径   ~/.my.cnf 用户特定选项   ~/.mylogin.cnf 登录路径选项（仅限客户端）     SYSCONFDIR 表示在使用 CMake 构建 MySQL 时使用 SYSCONFDIR 选项指定的目录。默认情况下，这是位于编译安装目录下的 etc 目录。 MYSQL_HOME 变量的值是我们自己设置的，也可以不设置。  配置文件的内容\r#\r\r配置文件中的启动选项被划分为若干个组，每个组有一个组名，用中括号 [] 扩起来：\n[server]\r(具体的启动选项...)\r[mysqld]\r(具体的启动选项...)\r[mysqld_safe]\r(具体的启动选项...)\r[client]\r(具体的启动选项...)\r[mysql]\r(具体的启动选项...)\r[mysqladmin]\r(具体的启动选项...)\r每个组下边可以定义若干个启动选项，以 [server] 组为例：\n[server]\roption1 # option1，该选项不需要选项值\roption2 = value2 # option2，该选项需要选项值\r...\r配置文件中只能使用长形式的选项。上面的文件转成命令行格式就是 --option1 --option2=value2。\n不同的选项组是给不同的启动命令使用的，如 [mysqld] 和 [mysql] 组分别应用于 mysqld 服务端程序和 mysql 客户端程序。但是注意：\n [server] 组下边的启动选项将作用于所有的服务器程序。 [client] 组下边的启动选项将作用于所有的客户端程序。 mysqld_safe 和 mysql.server 这两个程序在启动时都会读取 [mysqld] 选项组中的选项。  特定 MySQL 版本的专用选项组\r#\r\r可以在选项组的名称后加上特定的 MySQL 版本号，比如对于 [mysqld] 选项组来说，我们可以定义一个 [mysqld-5.7] 的选项组，它的含义 和 [mysqld] 一样，只不过只有版本号为 5.7 的 mysqld 程序才能使用这个选项组中的选项。\n配置文件的优先级\r#\r\r如果在多个配置文件中设置了相同的启动选项，那以最后一个配置文件中的为准。文件的顺序按照上面的表格从上到下的顺序。\n同一个配置文件中多个组的优先级\r#\r\r比如 mysqld 可以访问 [mysqld]、[server] 组，如果在同一个配置文件中，比如 ~/.my.cnf，在这些组里出现了同样的配置项，比如这样：\n[server]\rdefault-storage-engine=InnoDB\r[mysqld]\rdefault-storage-engine=MyISAM\r以最后一组中的启动选项为准，比如上面的文件 default-storage-engine 就以 [mysqld] 组中的配置项为准。\ndefaults-file\r#\r\r如果不想让 MySQL 到默认的路径下搜索配置文件（就是上表中列出的那些），可以在命令行指定 defaults-file 选项 mysqld --defaults-file=/tmp/myconfig.txt。 这样，在程序启动的时候将只在 /tmp/myconfig.txt 路径下搜索配置文件。如果文件不存在或无法访问，则会发生错误。\ndefaults-file 和 defaults-extra-file 的区别，使用 defaults-extra-file 可以指定额外的配置文件搜索路径。\n命令行和配置文件的优先级\r#\r\r如果同一个启动选项既出现在命令行中，又出现在配置文件中，那么以命令行中的启动选项为准。\n系统变量\r#\r\rSHOW VARIABLES [LIKE 匹配的模式]; 查看 MySQL 服务器程序支持的系统变量以及它们的当前值。 对于大部分系统变量来说，它们的值可以在服务端程序运行过程中进行动态修改而无需停止并重启服务器。\n设置系统变量\r#\r\r通过启动选项设置\r#\r\r通过命令行添加启动选项和配置文件。例如：\nmysqld --default-storage-engine=MyISAM --max-connections=10 或者\n[server]\rdefault-storage-engine=MyISAM\rmax-connections=10\rmax-connections 表示允许同时连入的客户端数量。\n服务器程序运行过程中设置\r#\r\r设置不同作用范围的系统变量\r#\r\r了针对不同的客户端设置不同的系统变量：\n GLOBAL：全局变量，影响服务器的整体操作。 SESSION：会话变量，影响某个客户端连接的操作。（SESSION 有个别名叫 LOCAL）  通过启动选项设置的系统变量的作用范围都是 GLOBAL 的，也就是对所有客户端都有效的，因为在系统启动的时候还没有客户端程序连接进来。\n通过客户端程序设置系统变量的语法：\nSET [GLOBAL|SESSION] 系统变量名 = 值;\r# 或者\rSET [@@(GLOBAL|SESSION).]var_name = XXX;\r比如在服务端进程运行过程中把作用范围为 GLOBAL 的系统变量 default_storage_engine 的值修改为 MyISAM，也就是想让之后新连接到 服务器的客户端都用 MyISAM 作为默认的存储引擎：\n语句一：SET GLOBAL default_storage_engine = MyISAM;\r语句二：SET @@GLOBAL.default_storage_engine = MyISAM;\r如果只想对本客户端生效：\n语句一：SET SESSION default_storage_engine = MyISAM;\r语句二：SET @@SESSION.default_storage_engine = MyISAM;\r语句三：SET default_storage_engine = MyISAM;\r如果在设置系统变量的语句中省略了作用范围，那么默认的作用范围就是 SESSION。\n 如果某个客户端改变了某个系统变量在 GLOBAL 作用范围的值，并不会影响该系统变量在当前已经连接的客户端作用范围为 SESSION 的值，只会影响 后续连入的客户端在作用范围为 SESSION 的值。\n 注意：\n 有一些系统变量只具有 GLOBAL 作用范围，如 max_connections。 有一些系统变量只具有 SESSION 作用范围，如 insert_id，表示在对某个包含 AUTO_INCREMENT 列的表进行插入时，该列初始的值。 有些系统变量是只读的，并不能设置值。如 version，表示当前 MySQL 的版本。  状态变量\r#\r\rMySQL 服务器程序中维护了很多关于程序运行状态的变量，它们被称为状态变量。它们的值只能由服务器程序自己来设置。状态变量也有 GLOBAL 和 SESSION 两个作用范围的，所以查看状态变量的语句可以这么写：\nSHOW [GLOBAL|SESSION] STATUS [LIKE 匹配的模式]; "});index.add({'id':41,'href':'/db-learn/docs/redis/10_dict/','title':"字典",'content':"字典\r#\r\rRedis 中除了 hash 结构的数据会用到字典外，整个 Redis 数据库的所有 key 和 value 也组成了一个全局字典，还有带过期时间的 key 集合 也是一个字典。zset 集合中存储 value 和 score 值的映射关系也是通过字典实现的。\nstruct RedisDb { dict* dict; // all keys key=\u0026gt;value  dict* expires; // all expired keys key=\u0026gt;long(timestamp)  ... } struct zset { dict *dict; // all values value=\u0026gt;score  zskiplist *zsl; } 字典的结构\r#\r\r\rdict 结构内部包含两个 hashtable，通常情况下只有一个 hashtable 是有值的。但是在 dict 扩容缩容时，需要分配新的 hashtable，然后 进行渐进式搬迁，这时候两个 hashtable 存储的分别是旧的 hashtable 和新的 hashtable。待搬迁结束后，旧的 hashtable 被删除，新 的 hashtable 取而代之。\n字典就是基于\r散列表的原理实现的。\n渐进式 rehash\r#\r\rrehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。\n原因在于，Redis 是单线程的，如果哈希表里保存的键值对数量非常庞大，一次性 rehash 庞大的计算量会导致服务器一段时间内停止服务。\n渐进式 rehash 的详细步骤：\n 为 ht[1] 分配空间，让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ，并将它的值设置为 0 ， 表示 rehash 工作正式开始。 在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将 ht[0] 哈希表 在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ，当 rehash 工作完成之后，程序将 rehashidx 属性的值增一。 随着字典操作的不断执行，最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ，这时程序将 rehashidx 属性的值 设为 -1，表示 rehash 操作已完成。  渐进式 rehash 的过程中，字典会同时使用 ht[0] 和 ht[1] 两个哈希表，所以在渐进式 rehash 进行期间， 字典的删除（delete）、查 找（find）、更新（update）等操作会在两个哈希表上进行。\n扩容条件\r#\r\r/* Expand the hash table if needed */ static int _dictExpandIfNeeded(dict *d) { /* Incremental rehashing already in progress. Return. */ if (dictIsRehashing(d)) return DICT_OK; /* If the hash table is empty expand it to the initial size. */ if (d-\u0026gt;ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE); /* If we reached the 1:1 ratio, and we are allowed to resize the hash * table (global setting) or we should avoid it but the ratio between * elements/buckets is over the \u0026#34;safe\u0026#34; threshold, we resize doubling * the number of buckets. */ if (d-\u0026gt;ht[0].used \u0026gt;= d-\u0026gt;ht[0].size \u0026amp;\u0026amp; (dict_can_resize || d-\u0026gt;ht[0].used/d-\u0026gt;ht[0].size \u0026gt; dict_force_resize_ratio)) { return dictExpand(d, d-\u0026gt;ht[0].used*2); } return DICT_OK; } 正常情况下，当 hash 表中元素的个数等于数组的长度时，就会开始扩容，扩容的新数组是原数组大小的 2 倍。不过如果 Redis 正在做 bgsave，为 了减少内存页的过多分离 (Copy On Write)，Redis 尽量不去扩容 (dict_can_resize)，但是如果 hash 表已经非常满了，元素的个数已经达 到了数组长度的 5 倍 (dict_force_resize_ratio)，说明 hash 表已经过于拥挤了，这个时候就会强制扩容。\n缩容条件\r#\r\rint htNeedsResize(dict *dict) { long long size, used; size = dictSlots(dict); used = dictSize(dict); return (size \u0026gt; DICT_HT_INITIAL_SIZE \u0026amp;\u0026amp; (used*100/size \u0026lt; HASHTABLE_MIN_FILL)); } 当 hash 表因为元素的逐渐删除变得越来越稀疏时，Redis 会对 hash 表进行缩容来减少 hash 表的数组空间占用。缩容的条件是元素个数 低于数组长度的 10%。缩容不会考虑 Redis 是否正在做 bgsave。\n"});index.add({'id':42,'href':'/db-learn/docs/mysql/09_character/','title':"字符集和比较规则",'content':"字符集和比较规则\r#\r\r字符集和比较规则简介\r#\r\r字符集简介\r#\r\r计算机中只能存储二进制数据，如何存储字符串？当然是建立字符与二进制数据的映射关系，建立这个关系要搞清楚两件事儿：\n 要把哪些字符映射成二进制数据？也就是界定清楚字符范围。 怎么映射？将一个字符映射成一个二进制数据的过程也叫做编码，将一个二进制数据映射到一个字符的过程叫做解码。  字符集就是来描述某个字符范围的编码规则。\n比较规则简介\r#\r\r怎么比较两个字符？最容易的就是直接比较这两个字符对应的二进制编码的大小。如，字符 \u0026lsquo;a\u0026rsquo; 的编码为 0x01，字符 \u0026lsquo;b\u0026rsquo; 的编码为 0x02，所 以 \u0026lsquo;a\u0026rsquo; 小于 \u0026lsquo;b\u0026rsquo;。\n二进制比较规则很简单，但有时候并不符合现实需求，比如在有些场景对于英文字符不区分大小写。这时候可以这样指定比较规则：\n 将两个大小写不同的字符全都转为大写或者小写。 再比较这两个字符对应的二进制数据。  但是实际生活中的字符不止英文字符一种，比如汉字有几万之多，同一种字符集可以有多种比较规则。\n一些重要的字符集\r#\r\r ASCII 字符集，共收录 128 个字符，包括空格、标点符号、数字、大小写字母和一些不可见字符。 ISO 8859-1 字符集，共收录 256 个字符，是在 ASCII 字符集的基础上又扩充了 128 个西欧常用字符。别名 latin1。 GB2312 字符集，收录了汉字以及拉丁字母、希腊字母、日文平假名及片假名字母、俄语西里尔字母。其中收录汉字 6763 个，其他文字符号 682 个。同时这 种字符集又兼容ASCII字符集。 GBK 字符集，GBK 字符集只是在收录字符范围上对 GB2312 字符集作了扩充，编码方式上兼容 GB2312。 utf8 字符集，收录地球上能想到的所有字符，而且还在不断扩充。这种字符集兼容 ASCII 字符集，采用变长编码方式，编码一个字符需要使用 1～4 个字节。   utf8 只是 Unicode 字符集的一种编码方案，Unicode 字符集可以采用 utf8、utf16、utf32 这几种编码方案，utf8 使用 1～4 个字 节编码一个字符，utf16 使用 2 个或 4 个字节编码一个字符，utf32 使用 4 个字节编码一个字符。\n MySQL 中支持的字符集和排序规则\r#\r\rMySQL 中的 utf8 和 utf8mb4\r#\r\rutf8 字符集表示一个字符需要使用 1～4 个字节，但是常用的一些字符使用 1～3 个字节就可以表示了。\n而在 MySQL 中字符集表示一个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以 MySQL 定义了两个概念：\n utf8mb3：阉割过的 utf8 字符集，只使用 1～3个 字节表示字符。 utf8mb4：正宗的 utf8 字符集，使用 1～4 个字节表示字符。  在 MySQL 中 utf8 是 utf8mb3 的别名，所以之后在 MySQL 中提到 utf8 就意味着使用 1~3 个字节来表示一个字符，如果大家有使用 4 字节 编码一个字符的情况，比如存储一些 emoji 表情啥的，那请使用 utf8mb4。\n字符集的查看\r#\r\r查看当前 MySQL 中支持的字符集 SHOW (CHARACTER SET|CHARSET) [LIKE 匹配的模式];。CHARACTER SET 和 CHARSET 是同义词，用任意 一个都可以。\n比较规则查看\r#\r\r查看 MySQL 中支持的比较规则 SHOW COLLATION [LIKE 匹配的模式];。\n查看 utf8 字符集下的比较规则：\nmysql\u0026gt; SHOW COLLATION LIKE \u0026#39;utf8\\_%\u0026#39;; +--------------------------+---------+-----+---------+----------+---------+ | Collation | Charset | Id | Default | Compiled | Sortlen | +--------------------------+---------+-----+---------+----------+---------+ | utf8_general_ci | utf8 | 33 | Yes | Yes | 1 | | utf8_bin | utf8 | 83 | | Yes | 1 | | utf8_unicode_ci | utf8 | 192 | | Yes | 8 | | utf8_icelandic_ci | utf8 | 193 | | Yes | 8 | | utf8_latvian_ci | utf8 | 194 | | Yes | 8 | | utf8_romanian_ci | utf8 | 195 | | Yes | 8 | | utf8_slovenian_ci | utf8 | 196 | | Yes | 8 | | utf8_polish_ci | utf8 | 197 | | Yes | 8 | | utf8_estonian_ci | utf8 | 198 | | Yes | 8 | | utf8_spanish_ci | utf8 | 199 | | Yes | 8 | | utf8_swedish_ci | utf8 | 200 | | Yes | 8 | | utf8_turkish_ci | utf8 | 201 | | Yes | 8 | | utf8_czech_ci | utf8 | 202 | | Yes | 8 | | utf8_danish_ci | utf8 | 203 | | Yes | 8 | | utf8_lithuanian_ci | utf8 | 204 | | Yes | 8 | | utf8_slovak_ci | utf8 | 205 | | Yes | 8 | | utf8_spanish2_ci | utf8 | 206 | | Yes | 8 | | utf8_roman_ci | utf8 | 207 | | Yes | 8 | | utf8_persian_ci | utf8 | 208 | | Yes | 8 | | utf8_esperanto_ci | utf8 | 209 | | Yes | 8 | | utf8_hungarian_ci | utf8 | 210 | | Yes | 8 | | utf8_sinhala_ci | utf8 | 211 | | Yes | 8 | | utf8_german2_ci | utf8 | 212 | | Yes | 8 | | utf8_croatian_ci | utf8 | 213 | | Yes | 8 | | utf8_unicode_520_ci | utf8 | 214 | | Yes | 8 | | utf8_vietnamese_ci | utf8 | 215 | | Yes | 8 | | utf8_general_mysql500_ci | utf8 | 223 | | Yes | 1 | +--------------------------+---------+-----+---------+----------+---------+ 27 rows in set (0.00 sec) 比较规则名称都是以 utf8 开头的，后边紧跟着该比较规则主要作用于哪种语言，名称后缀意味着该比较规则是否区分语言中的重音、大小写啥的：\n _ai，accent insensitive，不区分重音 _as，accent sensitive，区分重音 _ci，case insensitive，不区分大小写 _cs，case sensitive，区分大小写 _bin，binary，以二进制方式比较  每种字符集对应若干种比较规则，每种字符集都有一种默认的比较规则（Default 列的值为 YES 的）。\n字符集和比较规则的应用\r#\r\rMySQL 有 4 个级别的字符集和比较规则，分别是：\n 服务器级别 数据库级别 表级别 列级别  服务器级别\r#\r\rMySQL 提供了两个系统变量来表示服务器级别的字符集和比较规则：\n character_set_server\t服务器级别的字符集 collation_server 服务器级别的比较规则  数据库级别\r#\r\r创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下：\nCREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; DEFAULT可以省略，并不影响语句的语义。比如新创建一个名叫 charset_demo_db 的数据库，在创建的时候指定它使用的字符集为 gb2312，比较规 则为 gb2312_chinese_ci：\nmysql\u0026gt; CREATE DATABASE charset_demo_db -\u0026gt; CHARACTER SET gb2312 -\u0026gt; COLLATE gb2312_chinese_ci; Query OK, 1 row affected (0.01 sec) 如果想查看当前数据库使用的字符集和比较规则，可以查看下面两个系统变量的值（前提是使用 USE 语句选择当前默认数据库，如果没有默认数据库， 则变量与相应的服务器级系统变量具有相同的值）：\n character_set_database 当前数据库的字符集 collation_database 当前数据库的比较规则   character_set_database 和 collation_database 这两个系统变量是只读的，不能通过修改这两个变量的值而改变当前数据库的字 符集和比较规则。\n 如果数据库的创建或修改语句中没有指定字符集和比较规则，那么会使用服务器级别的字符集和比较规则。\n表级别\r#\r\r可以在创建和修改表的时候指定表的字符集和比较规则，语法如下：\nCREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]] ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 比如创建一个名为 t 的表，并指定这个表的字符集和比较规则：\nmysql\u0026gt; CREATE TABLE t( -\u0026gt; col VARCHAR(10) -\u0026gt; ) CHARACTER SET utf8 COLLATE utf8_general_ci; Query OK, 0 rows affected (0.03 sec) 如果表的创建或修改语句中没有指定字符集和比较规则，那么会使用该表所在数据库的字符集和比较规则。\n列级别\r#\r\r同一个表中的不同的列也可以有不同的字符集和比较规则。在创建和修改列定义的时候可以指定该列的字符集和比较规则，语法如下：\nCREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列... ); ALTER TABLE 表名 MODIFY 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称]; 比如我们修改一下表 t 中列 col 的字符集和比较规则可以这么写：\nmysql\u0026gt; ALTER TABLE t MODIFY col VARCHAR(10) CHARACTER SET gbk COLLATE gbk_chinese_ci; Query OK, 0 rows affected (0.04 sec) Records: 0 Duplicates: 0 Warnings: 0 如果表的创建或修改语句中没有指定该列的字符集和比较规则，那么会使用该列所在表的字符集和比较规则。\n 在转换列的字符集时需要注意，如果转换前列中存储的数据不能用转换后的字符集进行表示会发生错误。比方说原先列使用的字 符集是 utf8，列中存储了一些汉字，现在把列的字符集转换为 ascii 的话就会出错，因为 ascii 字符集并不能表示汉字字符。\n 仅修改字符集或仅修改比较规则\r#\r\r由于字符集和比较规则是互相有联系的，如果只修改了字符集，比较规则也会跟着变化，如果只修改了比较规则，字符集也会跟着变化，具体规则如下：\n 只修改字符集，则比较规则将变为修改后的字符集默认的比较规则。 只修改比较规则，则字符集将变为修改后的比较规则对应的字符集。  不论哪个级别的字符集和比较规则，这两条规则都适用。\n客户端和服务器通信中的字符集\r#\r\r编码和解码使用的字符集不一致的后果\r#\r\r比如字符 '我' 在 utf8 字符集编码下的字节串长这样：0xE68891，把这个字节串发送到另一个程序里，另一个程序用不同的字符集去解码这个字节串， 假设使用的 是 gbk 字符集来解释这串字节，解码过程就是这样的：\n 首先看第一个字节 0xE6，它的值大于 0x7F（十进制：127，一个字节表示的最大的有符号整数），说明是两字节编码，继续读一字节后是 0xE688，然 后从 gbk 编码表中查找字节为 0xE688 对应的字符，发现是字符'鎴'。 继续读一个字节 0x91，它的值也大于 0x7F，再往后读一个字节发现木有了，所以这是半个字符。 所以 0xE68891 被 gbk 字符集解释成一个字符 '鎴' 和半个字符。  字符集转换\r#\r\r如果接收 0xE68891 这个字节串的程序按照 utf8 字符集进行解码，然后又把它按照 gbk 字符集进行编码，最后编码后的字节串就是 0xCED2，把这 个过程称为字符集的转换，也就是字符串 '我' 从 utf8 字符集转换为 gbk 字符集。\nMySQL 中字符集的转换\r#\r\r从客户端发往服务端的请求本质上就是一个字符串，服务端向客户端返回的结果本质上也是一个字符串，而字符串其实是使用某种字符集编码的二进制数据。这个字符 串可不是使用一种字符集的编码方式一条道走到黑的，从发送请求到返回结果这个过程中伴随着多次字符集的转换，在这个过程中会用到 3 个系统变量：\n character_set_client 服务端解码请求时使用的字符集 character_set_connection 服务端处理请求时会把请求字符串从 character_set_client 转为 character_set_connection character_set_results 服务端向客户端返回数据时使用的字符集  这三个系统变量的值可能默认都是 utf-8。为了体现出字符集在请求处理过程中的变化，这里特意修改一个系统变量的值：\nmysql\u0026gt; set character_set_connection = gbk; Query OK, 0 rows affected (0.00 sec) 所以现在系统变量 character_set_clien t和 character_set_results 的值还是 utf8，而 character_set_connection 的值为 gbk。 现在假设客户端发送的请求是下边这个字符串：\nSELECT * FROM t WHERE s = \u0026#39;我\u0026#39;; 分析字符 '我' 在这个过程中字符集的转换。请求从发送到结果返回过程中字符集的变化：\n 客户端发送请求所使用的字符集 一般情况下客户端所使用的字符集和当前操作系统一致，不同操作系统使用的字符集可能不一样，如下：   类 Unix 系统使用的是 utf8 Windows 使用的是 gbk  例如在使用的 macOS 操作系统时，客户端使用的就是 utf8 字符集。所以字符 '我' 在发送给服务端的请求中的字节形式就是：0xE68891\n 服务端接收到客户端发送来的请求其实是一串二进制的字节，它会认为这串字节采用的字符集是 character_set_client，然后把这串字节转换 为 character_set_connection 字符集编码的字符。由于计算机上 chacharacter_set_client 的值是 utf8，首先会按照 utf8 字符集 对字节串 0xE68891 进行解码，得到的字符串就是 '我'，然后按照 character_set_connection代表的字符集，也就是 gbk 进行编码，得 到的结果就是字节串 0xCED2。\n  因为表 t 的列 col 采用的是 gbk 字符集，与 character_set_connection 一致，所以直接到列中找字节值为 0xCED2 的记录，最后找 到了一条记录。\n  如果某个列使用的字符集和 character_set_connection 代表的字符集不一致的话，还需要进行一次字符集转换。\n 上一步骤找到的记录中的 col 列其实是一个字节串 0xCED2，col 列是采用 gbk 进行编码的，所以首先会将这个字节串使用 gbk 进行解码， 得到字符串'我'，然后再把这个字符串使用 character_set_results 代表的字符集，也就是 utf8 进行编码，得到了新的字节串：0xE68891，然 后发送给客户端。\n  由于客户端是用的字符集是 utf8，所以可以顺利的将 0xE68891 解释成字符我，从而显示到我们的显示器上，所以我们人类也读懂了返回的结果。\n  几点需要注意的地方：\n 假设你的客户端采用的字符集和 character_set_client 不一样的话，这就会出现意想不到的情况。 假设你的客户端采用的字符集和 character_set_results 不一样的话，这就可能会出现客户端无法解码结果集的情况   通常都把 character_set_client、character_set_connection、character_set_results 这三个系统变量设置成和客户端使用的 字符集一致的情况，这样减少了很多无谓的字符集转换。\n MySQL 提供了一条非常简便的语句:\nSET NAMES 字符集名; 效果与下面的三条语句的执行效果一样：\nSET character_set_client = 字符集名; SET character_set_connection = 字符集名; SET character_set_results = 字符集名; 也可以写到配置文件里：\n[client]\rdefault-character-set=utf8\r 如果你使用的是 Windows 系统，那应该设置成 gbk。\n 比较规则的应用\r#\r\r比较规则的作用通常体现在比较字符串大小的表达式以及对某个字符串列进行排序中，所以有时候也称为排序规则。比方说表 t 的列 col 使 用的字符集是gbk，使用的比较规则是 gbk_chinese_ci，我们向里边插入几条记录：\nmysql\u0026gt; INSERT INTO t(col) VALUES(\u0026#39;a\u0026#39;), (\u0026#39;b\u0026#39;), (\u0026#39;A\u0026#39;), (\u0026#39;B\u0026#39;); Query OK, 4 rows affected (0.00 sec) Records: 4 Duplicates: 0 Warnings: 0 mysql\u0026gt; 查询的时候按照 col 列排序一下：\nmysql\u0026gt; SELECT * FROM t ORDER BY col; +------+ | col | +------+ | a | | A | | b | | B | | 我 | +------+ 5 rows in set (0.00 sec) 可以看到在默认的比较规则 gbk_chinese_ci 中是不区分大小写的，我们现在把列 col 的比较规则修改为 gbk_bin：\nmysql\u0026gt; ALTER TABLE t MODIFY col VARCHAR(10) COLLATE gbk_bin; Query OK, 5 rows affected (0.02 sec) Records: 5 Duplicates: 0 Warnings: 0 gbk_bin是直接比较字符的编码，所以是区分大小写的，我们再看一下排序后的查询结果：\nmysql\u0026gt; SELECT * FROM t ORDER BY col; +------+ | s | +------+ | A | | B | | a | | b | | 我 | +------+ 5 rows in set (0.00 sec) mysql\u0026gt; "});index.add({'id':43,'href':'/db-learn/docs/redis/11_redis-object/','title':"对象",'content':"对象\r#\r\rRedis 用到的所有主要数据结构，比如简单动态字符串（SDS）、双端链表、字典、压缩列表、整数集合，等等。\nRedis 并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希 对象、集合对象和有序集合对象这五种类型的对象。\n对象的类型与编码\r#\r\rRedis 使用对象来表示数据库中的键和值，每次在 Redis 的数据库中新创建一个键值对时，至少会创建两个对象，一个对象用作键值对的键（键对象）， 另一个对象用作键值对的值（值对象）。\n比如：\nredis\u0026gt; SET msg \u0026#34;hello world\u0026#34; OK 每个对象都由一个 redisObject 结构表示:\ntypedef struct redisObject { // 类型  unsigned type:4; // 编码  unsigned encoding:4; // 指向底层实现数据结构的指针  void *ptr; // ...  } robj; type\r#\r\r对象的 type 属性记录了对象的类型，这个属性的值可以是下面列表中的任意一个：\n REDIS_STRING，字符串对象 REDIS_LIST，列表对象 REDIS_HASH，哈希对象 REDIS_SET，集合对象 REDIS_ZSET，有序集合对象  对于 Redis 数据库保存的键值对来说，键总是一个字符串对象，而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象的其中 一种，因此：\n 当我们称呼一个数据库键为“字符串键”时， 我们指的是“这个数据库键所对应的值为字符串对象” 当我们称呼一个键为“列表键”时， 我们指的是“这个数据库键所对应的值为列表对象”  编码和底层实现\r#\r\r对象的 ptr 指针指向对象的底层实现数据结构，而这些数据结构由对象的 encoding 属性决定。\nencoding 属性记录了对象所使用的编码，也即是说这个对象使用了什么数据结构作为对象的底层实现：\n   编码 底层实现     REDIS_ENCODING_INT（int） long 类型的整数   REDIS_ENCODING_EMBSTR（embstr） embstr 编码的简单动态字符串   REDIS_ENCODING_RAW（raw） 简单动态字符串   REDIS_ENCODING_HT（hashtable） 字典   REDIS_ENCODING_LINKEDLIST（linkedlist） 双端链表   REDIS_ENCODING_ZIPLIST（ziplist） 压缩列表   REDIS_ENCODING_INTSET（intset） 整数集合   REDIS_ENCODING_SKIPLIST（skiplist） 跳跃表和字典    每种类型的对象都至少使用了两种不同的编码:\n   类型 编码 对象     REDIS_STRING REDIS_ENCODING_INT 使用整数值实现的字符串对象。   REDIS_STRING REDIS_ENCODING_EMBSTR 使用 embstr 编码的简单动态字符串实现的字符串对象。   REDIS_STRING REDIS_ENCODING_RAW 使用简单动态字符串实现的字符串对象。   REDIS_LIST REDIS_ENCODING_ZIPLIST 使用压缩列表实现的列表对象。   REDIS_LIST REDIS_ENCODING_LINKEDLIST 使用双端链表实现的列表对象。   REDIS_HASH REDIS_ENCODING_ZIPLIST 使用压缩列表实现的哈希对象。   REDIS_HASH REDIS_ENCODING_HT 使用字典实现的哈希对象。   REDIS_SET REDIS_ENCODING_INTSET 使用整数集合实现的集合对象。   REDIS_SET REDIS_ENCODING_HT 使用字典实现的集合对象。   REDIS_ZSET REDIS_ENCODING_ZIPLIST 使用压缩列表实现的有序集合对象。   REDIS_ZSET REDIS_ENCODING_SKIPLIST 使用跳跃表和字典实现的有序集合对象。    字典遍历\r#\r\rRedis 对象树的主干是一个字典，如果对象很多，这个主干字典也会很大。当我们使用 keys 命令搜寻指定模式的 key 时，它会遍历整个主干字典。\n重复遍历\r#\r\r字典在扩容的时候要进行渐进式迁移，会存在新旧两个 hashtable。遍历需要对这两个 hashtable 依次进行，先遍历完旧的 hashtable，再继续 遍历新的 hashtable。如果在遍历的过程中进行了 rehashStep，将已经遍历过的旧的 hashtable 的元素迁移到了新的 hashtable 中，那么会不 会出现元素的重复？\n为了解决这个问题，Redis 为字典的遍历提供了 2 种迭代器，一种是安全迭代器，另一种是不安全迭代器。\n迭代器\r#\r\rtypedef struct dictIterator { dict *d; // 目标字典对象  long index; // 当前遍历的槽位置，初始化为-1  int table; // ht[0] or ht[1]  int safe; // 这个属性非常关键，它表示迭代器是否安全  dictEntry *entry; // 迭代器当前指向的对象  dictEntry *nextEntry; // 迭代器下一个指向的对象  long long fingerprint; // 迭代器指纹，放置迭代过程中字典被修改 } dictIterator; // 获取非安全迭代器，只读迭代器，允许 rehashStep dictIterator *dictGetIterator(dict *d) { dictIterator *iter = zmalloc(sizeof(*iter)); iter-\u0026gt;d = d; iter-\u0026gt;table = 0; iter-\u0026gt;index = -1; iter-\u0026gt;safe = 0; iter-\u0026gt;entry = NULL; iter-\u0026gt;nextEntry = NULL; return iter; } // 获取安全迭代器，允许触发过期处理，禁止 rehashStep dictIterator *dictGetSafeIterator(dict *d) { dictIterator *i = dictGetIterator(d); i-\u0026gt;safe = 1; return i; }  安全的迭代器，指的是在遍历过程中可以对字典进行查找和修改，会触发过期判断，删除内部元素。迭代过程中不会出现元素重复，为了保证不重 复，就会禁止 rehashStep。 不安全的迭代器，是指遍历过程中字典是只读的，不可以修改，只能调用 dictNext 对字典进行持续遍历，不得调用任何可能触发过期判断的函数。 不过好处是不影响 rehash，代价就是遍历的元素可能会出现重复。  安全迭代器在刚开始遍历时，会给字典打上一个标记，有了这个标记，rehashStep 就不会执行，遍历时元素就不会出现重复。\ntypedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; // 这个就是标记，它表示当前加在字典上的安全迭代器的数量  unsigned long iterators; } dict; // 如果存在安全的迭代器，就禁止rehash static void _dictRehashStep(dict *d) { if (d-\u0026gt;iterators == 0) dictRehash(d,1); } 迭代器的选择\r#\r\r除了 keys 指令使用了安全迭代器，因为结果不允许重复。那还有其它的地方使用了安全迭代器么，什么情况下遍历适合使用非安全迭代器？\n如果遍历过程中不允许出现重复，那就使用 SafeIterator：\n bgaofrewrite 需要遍历所有对象转换称操作指令进行持久化，绝对不允许出现重复 bgsave 也需要遍历所有对象来持久化，同样不允许出现重复  如果遍历过程中需要处理元素过期，需要对字典进行修改，那也必须使用 SafeIterator，因为非安全的迭代器是只读的。\n其它情况下，也就是允许遍历过程中出现个别元素重复，不需要对字典进行结构性修改的情况下一律使用非安全迭代器。\n"});index.add({'id':44,'href':'/db-learn/docs/redis/redis-ziplist/','title':"小对象压缩",'content':"小对象压缩\r#\r\r如果 Redis 内部管理的集合数据结构很小，它就会使用紧凑存储形式压缩存储。\n比如 HashMap 结构，如果内部元素比较少，使用散列表反而浪费空间，不如直接使用数组进行存储，需要查找时，因为元素少进行遍历也很快，甚至 可以比 HashMap 本身的查找还要快。\nziplist\r#\r\rRedis 的 ziplist 是一个紧凑的 byte 数组结构，如下图，每个元素之间都是紧挨着的。\n\r如果 ziplist 存储的是 hash 结构，那么 key 和 value 会作为两个 entry 相邻存在一起。\n127.0.0.1:6379\u0026gt; hset hello a 1 (integer) 1 127.0.0.1:6379\u0026gt; hset hello b 2 (integer) 1 127.0.0.1:6379\u0026gt; hset hello c 3 (integer) 1 127.0.0.1:6379\u0026gt; object encoding hello \u0026#34;ziplist\u0026#34; 如果 ziplist 存储的是 zset，那么 value 和 score 会作为两个 entry 相邻存在一起。\n127.0.0.1:6379\u0026gt; zadd world 1 a (integer) 1 127.0.0.1:6379\u0026gt; zadd world 2 b (integer) 1 127.0.0.1:6379\u0026gt; zadd world 3 c (integer) 1 127.0.0.1:6379\u0026gt; object encoding world \u0026#34;ziplist\u0026#34; intset\r#\r\r当 set 集合容纳的元素都是整数并且元素个数较小时，Redis 会使用 intset 来存储结合元素。intset 是紧凑的数组结构，同时 支持 16 位、32 位和 64 位整数。\nstruct intset\u0026lt;T\u0026gt; { int32 encoding; // 决定整数位宽是 16 位、32 位还是 64 位  int32 length; // 元素个数  int\u0026lt;T\u0026gt; contents; // 整数数组，可以是 16 位、32 位和 64 位 } \r如果整数可以用 uint16 表示，那么 intset 的元素就是 16 位的数组，如果新加入的整数超过了 uint16 的表示范围，那么就使用 uint32 表示， 如果新加入的元素超过了 uint32 的表示范围，那么就使用 uint64 表示，Redis 支持 set 集合动态从 uint16 升级到 uint32，再升级 到 uint64。\n127.0.0.1:6379\u0026gt; sadd hello 1 2 3 (integer) 3 127.0.0.1:6379\u0026gt; object encoding hello \u0026#34;intset\u0026#34; 如果 set 里存储的是字符串，那么 sadd 立即升级为 hashtable 结构。\n127.0.0.1:6379\u0026gt; sadd hello yes no (integer) 2 127.0.0.1:6379\u0026gt; object encoding hello \u0026#34;hashtable\u0026#34; 存储界限 当集合对象的元素不断增加，或者某个 value 值过大，这种小对象存储也会被升级为标准结构。Redis 规定在小对象存储结构的限制 条件如下：\nhash-max-ziplist-entries 512 # hash 的元素个数超过 512 就必须用标准结构存储 hash-max-ziplist-value 64 # hash 的任意元素的 key/value 的长度超过 64 就必须用标准结构存储 list-max-ziplist-entries 512 # list 的元素个数超过 512 就必须用标准结构存储 list-max-ziplist-value 64 # list 的任意元素的长度超过 64 就必须用标准结构存储 zset-max-ziplist-entries 128 # zset 的元素个数超过 128 就必须用标准结构存储 zset-max-ziplist-value 64 # zset 的任意元素的长度超过 64 就必须用标准结构存储 set-max-intset-entries 512 # set 的整数元素个数超过 512 就必须用标准结构存储 ziplist 内部实现\r#\r\rRedis 为了节约内存空间使用，zset 和 hash 容器对象在元素个数较少的时候，采用压缩列表 (ziplist) 进行存储。\nstruct ziplist\u0026lt;T\u0026gt; { int32 zlbytes; // 整个压缩列表占用字节数  int32 zltail_offset; // 最后一个元素距离压缩列表起始位置的偏移量，用于快速定位到最后一个节点  int16 zllength; // 元素个数  T[] entries; // 元素内容列表，挨个挨个紧凑存储  int8 zlend; // 标志压缩列表的结束，值恒为 0xFF } ztail_offset 这个字段是为了支持双向遍历，用来快速定位到最后一个元素，然后倒着遍历。\nentry 块随着容纳的元素类型不同，也会有不一样的结构。\nstruct entry { int\u0026lt;var\u0026gt; prevlen; // 前一个 entry 的字节长度  int\u0026lt;var\u0026gt; encoding; // 元素类型编码  optional byte[] content; // 元素内容 }  prevlen 字段 记录了压缩列表中前一个 entry 节点的长度 ，当压缩列表倒着遍历时，需要通过这个字段来快速定位到下一个元素的位置。 它是一个变长的整数，当字符串长度小于 254(0xFE) 时，使用一个字节表示；如果达到或超出 254(0xFE) 那就使用 5 个字节来表示。第一个字节 是 0xFE(254)，剩余四个字节表示字符串长度。你可能会觉得用 5 个字节来表示字符串长度，是不是太浪费了。我们可以算一下，当字符串长度比 较长的时候，其实 5 个字节也只占用了不到 (5/(254+5))\u0026lt;2% 的空间。 encoding 字段存储了元素内容的编码类型信息，ziplist 通过这个字段来决定后面的 content 内容的形式。Redis 通过这个字段的前缀位来 识别具体存储的数据形式：  00xxxxxx 最大长度位 63 的短字符串，后面的 6 个位存储字符串的位数，剩余的字节就是字符串的内容。 01xxxxxx xxxxxxxx 中等长度的字符串，后面 14 个位来表示字符串的长度，剩余的字节就是字符串的内容。 10000000 aaaaaaaa bbbbbbbb cccccccc dddddddd 特大字符串，需要使用额外 4 个字节来表示长度。第一个字节前缀是10，剩余 6 位 没有使用，统一置为零。后面跟着字符串内容。不过这样的大字符串是没有机会使用的，压缩列表通常只是用来存储小数据的。 11000000 表示 int16，后跟两个字节表示整数。 11010000 表示 int32，后跟四个字节表示整数。 11100000 表示 int64，后跟八个字节表示整数。 11110000 表示 int24，后跟三个字节表示整数。 11111110 表示 int8，后跟一个字节表示整数。 11111111 表示 ziplist 的结束，也就是 zlend 的值 0xFF。 1111xxxx 表示极小整数，xxxx 的范围只能是 (0001~1101), 也就是 1~13，因为 0000、1110、1111 都被占用了。读取到的 value 需 要将 xxxx 减 1，也就是整数 0~12 就是最终的 value。   content 字段在结构体中定义为 optional 类型，表示这个字段是可选的，对于很小的整数而言，它的内容已经内联到 encoding 字段的尾部了。  增加元素\r#\r\r因为 ziplist 都是紧凑存储，没有冗余空间 (对比一下 Redis 的字符串结构)。意味着插入一个新的元素就需要调用 realloc 扩展内存。取决 于内存分配器算法和当前的 ziplist 内存大小，realloc 可能会重新分配新的内存空间，并将之前的内容一次性拷贝到新的地址，也可能在原有的地 址上进行扩展，这时就不需要进行旧内容的内存拷贝。\n如果 ziplist 占据内存太大，重新分配内存和拷贝内存就会有很大的消耗。所以 ziplist 不适合存储大型字符串，存储的元素也不宜过多。\n连锁更新\r#\r\r考虑这样一种情况： 在一个压缩列表中， 有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN ： e1 至 eN 的所有节点的长度都小于 254 字节，所以记录这些节点的长度只需要 1 字节长的 prevlen 属性， 换句话说， e1 至 eN 的所有节点的 prevlen 属性都是 1 字节长的。\n如果将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点，那么 new 将成为 e1 的前置节点，因为 e1 的 prevlen 属 性仅长 1 字节，它没办法保存新节点 new 的长度，所以程序将对压缩列表执行空间重分配操作，并将 e1 节点的 prevlen 属性从原来的 1 字节 长扩展为 5 字节长。\ne1 原本的长度介于 250 字节至 253 字节之间， 在为 prevlen 属性新增四个字节的空间之后，e1 的长度就变成了介于 254 字节至 257 字节 之间，由于 e1 的长度变长了，长度导致 e2 节点的 prevlen 属性也要从原来的 1 字节长扩展为 5 字节长，否则 1 字节长的 prevlen 属性 是没办法保存。\n扩展 e1 引发了对 e2 的扩展，扩展 e2 也会引发对 e3 的扩展，直到 eN 为止。\nRedis 将这种在特殊情况下产生的连续多次空间扩展操作称之为连锁更新。\n连锁更新的复杂度较高， 但它真正造成性能问题的几率是很低的：\n 首先， 压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点， 连锁更新才有可能被引发， 在实际中， 这种情况并不多见； 其次， 即使出现连锁更新， 但只要被更新的节点数量不多， 就不会对性能造成任何影响： 比如，对三五个节点进行连锁更新是绝对不会影响 性能的；  "});index.add({'id':45,'href':'/db-learn/docs/redis/16_bloom-filter/','title':"布隆过滤器",'content':"布隆过滤器\r#\r\r我们在使用新闻客户端看新闻时，它会给我们不停地推荐新的内容，它每次推荐时要去重，去掉那些已经看过的内容。问题来了，新闻客户端推荐系统 如何实现推送去重的？\nHyperLogLog 结构就无能为力了，它只提供了 pfadd 和 pfcount 方法，没有提供 pfcontains 这种方法。\n你会想到服务器记录了用户看过的所有历史记录，当推荐系统推荐新闻时会从每个用户的历史记录里进行筛选，过滤掉那些已经存在的记录。 问题是当用户量很大，每个用户看过的新闻又很多的情况下，这种方式，推荐系统的去重工作在性能上跟的上么？\n布隆过滤器 (Bloom Filter) 就是专门用来解决这种去重问题的。它在起到去重的同时，在空间上还能节省 90% 以上，只是稍微有那么点不精 确，也就是有一定的误判概率。\n布隆过滤器是什么？\r#\r\r布隆过滤器可以理解为一个不怎么精确的 set 结构，当你使用它的 contains 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不 是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率。\n当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在。\n布隆过滤器能准确过滤掉那些已经看过的内容，那些没有看过的新内容，它也会过滤掉极小一部分 (误判)，但是绝大多数新内容它都能准确识别。\nRedis 中的布隆过滤器\r#\r\rRedis 官方提供的布隆过滤器到了 Redis 4.0 提供了插件功能之后才正式登场。布隆过滤器作为一个插件加载到 Redis Server 中，给 Redis 提供 了强大的布隆去重功能。\n\u0026gt; docker pull redislabs/rebloom # 拉取镜像 \u0026gt; docker run -p6379:6379 redislabs/rebloom # 运行容器 \u0026gt; redis-cli # 连接容器中的 redis 服务 使用\r#\r\r布隆过滤器有二个基本指令，bf.add 添加元素，bf.exists 查询元素是否存在，它的用法和 set 集合的 sadd 和 sismember 差不多。 注意 bf.add 只能一次添加一个元素，如果想要一次添加多个，就需要用到 bf.madd 指令。同样如果需要一次查询多个元素是否存在，就 需要用到 bf.mexists 指令。\n127.0.0.1:6379\u0026gt; bf.add codehole user1 (integer) 1 127.0.0.1:6379\u0026gt; bf.add codehole user2 (integer) 1 127.0.0.1:6379\u0026gt; bf.add codehole user3 (integer) 1 127.0.0.1:6379\u0026gt; bf.exists codehole user1 (integer) 1 127.0.0.1:6379\u0026gt; bf.exists codehole user2 (integer) 1 127.0.0.1:6379\u0026gt; bf.exists codehole user3 (integer) 1 127.0.0.1:6379\u0026gt; bf.exists codehole user4 (integer) 0 127.0.0.1:6379\u0026gt; bf.madd codehole user4 user5 user6 1) (integer) 1 2) (integer) 1 3) (integer) 1 127.0.0.1:6379\u0026gt; bf.mexists codehole user4 user5 user6 user7 1) (integer) 1 2) (integer) 1 3) (integer) 1 4) (integer) 0 布隆过滤器只会对已经存在的元素误判，如果布隆过滤器判断不存在那就肯定不存在。\n自定义参数的布隆过滤器\r#\r\rRedis 其实还提供了自定义参数的布隆过滤器，需要我们在 add 之前使用 bf.reserve 指令显式创建。如果对应的 key 已经存在， bf.reserve 会报错。\nbf.reserve 有三个参数：\n key error_rate 错误率越低，需要的空间越大。 initial_size 参数表示预计放入的元素数量，当实际数量超出这个数值时，误判率会上升。  initial_size 需要提前设置一个较大的数值避免超出导致误判率升高。如果不使用 bf.reserve，默认的 error_rate 是 0.01， 默认的 initial_size 是 100。\n注意事项\r#\r\r布隆过滤器的 initial_size 估计的过大，会浪费存储空间，估计的过小，就会影响准确率，用户在使用之前一定要尽可能地精确估计好元素数 量，还需要加上一定的冗余空间以避免实际元素可能会意外高出估计值很多。\n布隆过滤器的 error_rate 越小，需要的存储空间就越大，对于不需要过于精确的场合，error_rate 设置稍大一点也无伤大雅。比如在 新闻去重上而言，误判率高一点只会让小部分文章不能让合适的人看到，文章的整体阅读量不会因为这点误判率就带来巨大的改变。\n布隆过滤器的原理\r#\r\r\r每个布隆过滤器对应到 Redis 的数据结构里面就是一个大型的位数组和几个不一样的无偏 hash 函数。所谓无偏就是能够把元素的 hash 值算得比较均匀。\n向布隆过滤器中添加 key 时，会使用多个 hash 函数对 key 进行 hash 算得一个整数索引值然后对位数组长度进行取模运算得到一个位置，每个 hash 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 1 就完成了 add 操作。\n向布隆过滤器询问 key 是否存在时，跟 add 一样，也会把 hash 的几个位置都算出来，看看位数组中这几个位置是否都为 1，只要有一个位为 0，那 么说明布隆过滤器中这个 key 不存在。如果都是 1，这并不能说明这个 key 就一定存在，只是极有可能存在，因为这些位被置为 1 可能是因为其它 的 key 存在所致。如果这个位数组比较稀疏，判断正确的概率就会很大，如果这个位数组比较拥挤，判断正确的概率就会降低。\n布隆过滤器的其它应用\r#\r\r在爬虫系统中，我们需要对 URL 进行去重，已经爬过的网页就可以不用爬了。但是 URL 太多了，几千万几个亿，如果用一个集合装下这些 URL 地址 那是非常浪费空间的。这时候就可以考虑使用布隆过滤器。它可以大幅降低去重存储消耗，只不过也会使得爬虫系统错过少量的页面。\n布隆过滤器在 NoSQL 数据库领域使用非常广泛，我们平时用到的 HBase、Cassandra 还有 LevelDB、RocksDB 内部都有布隆过滤器结构，布 隆过滤器可以显著降低数据库的 IO 请求数量。当用户来查询某个 row 时，可以先通过内存中的布隆过滤器过滤掉大量不存在的 row 请求，然后再去磁 盘进行查询。\n邮箱系统的垃圾邮件过滤功能也普遍用到了布隆过滤器，因为用了这个过滤器，所以平时也会遇到某些正常的邮件被放进了垃圾邮件目录中，这个就是误 判所致，概率很低。\n"});index.add({'id':46,'href':'/db-learn/docs/redis/14_queue/','title':"延时队列",'content':"延时队列\r#\r\rRabbitMQ 和 Kafka 是常用的消息队列中间件，但是这种专业的消息队列中间件使用复杂。\n对于那些只有一组消费者的消息队列，使用 Redis 就可以轻松搞定。Redis 的消息队列没有 ACK 保证，如果追求消息的可靠性，还是使用专业 的消息队列。\n异步消息队列\r#\r\r使用 Redis 的 list(列表) 数据结构来实现异步消息队列，使用 rpush/lpush 操作入队列，使用 lpop/rpop 来出队列。\n\u0026gt; rpush notify-queue apple banana pear (integer) 3 \u0026gt; llen notify-queue (integer) 3 \u0026gt; lpop notify-queue \u0026#34;apple\u0026#34; \u0026gt; llen notify-queue (integer) 2 \u0026gt; lpop notify-queue \u0026#34;banana\u0026#34; \u0026gt; llen notify-queue (integer) 1 \u0026gt; lpop notify-queue \u0026#34;pear\u0026#34; \u0026gt; llen notify-queue (integer) 0 \u0026gt; lpop notify-queue (nil) 上面是 rpush 和 lpop 结合使用的例子。还可以使用 lpush 和 rpop 结合使用，效果是一样的。这里不再赘述。\n空队列处理\r#\r\r客户端是通过队列的 pop 操作来获取消息，然后进行处理。可是如果队列空了，客户端就会陷入 pop 的死循环，不停地 pop，没有数据， 接着再 pop，又没有数据。这就是浪费生命的空轮询。空轮询不但拉高了客户端的 CPU，redis 的 QPS 也会被拉高，如果这样空轮询的客户端 有几十来个，Redis 的慢查询可能会显著增多。\n用 blpop/brpop 替代前面的 lpop/rpop，就可以解决。这两个指令的前缀字符 b 代表的是 blocking，也就是阻塞读。\n空闲连接自动断开\r#\r\r使用阻塞读还有其他的问题要解决 —— 空闲连接的问题。\n如果线程一直阻塞在那里，Redis 的客户端连接就成了闲置连接，闲置过久，服务器一般会主动断开连接，减少闲置资源占用。这个时 候 blpop/brpop 会抛出异常。\n所以编写客户端消费者的时候要小心，注意捕获异常，还要重试。\n锁冲突处理\r#\r\r对于分布式锁，如果加锁失败，一般有 3 种策略来处理：\n 直接抛出异常，通知用户稍后重试 sleep 一会再重试 异步消息可以使用延时队列，将当前冲突的请求扔到另一个队列延后处理以避开冲突  如何实现延时队列\r#\r\r延时队列可以通过 Redis 的 zset 来实现。我们将消息序列化成一个字符串作为 zset 的 value，这个消息的到期处理时间作为 score，然 后用多个线程轮询 zset 获取到期的任务进行处理，多个线程是为了保障可用性，万一挂了一个线程还有其它线程可以继续处理。因为有多个线程，所以 需要考虑并发争抢任务，确保任务不能被多次执行。\ndef delay(msg): msg.id = str(uuid.uuid4()) # 保证 value 值唯一 value = json.dumps(msg) retry_ts = time.time() + 5 # 5 秒后重试 redis.zadd(\u0026#34;delay-queue\u0026#34;, retry_ts, value) def loop(): while True: # 最多取 1 条 values = redis.zrangebyscore(\u0026#34;delay-queue\u0026#34;, 0, time.time(), start=0, num=1) if not values: time.sleep(1) # 延时队列空的，休息 1s continue value = values[0] # 拿第一条，也只有一条 success = redis.zrem(\u0026#34;delay-queue\u0026#34;, value) # 从消息队列中移除该消息 if success: # 因为有多进程并发的可能，最终只会有一个进程可以抢到消息 msg = json.loads(value) handle_msg(msg) # 异常捕获，避免因为个别任务处理问题导致循环异常退出 Redis 的 zrem 方法是多线程多进程争抢任务的关键，它的返回值决定了当前实例有没有抢到任务，因为 loop 方法可能会被多个线程、多个进 程调用，同一个任务可能会被多个进程线程抢到，通过 zrem 来决定唯一的属主。\n"});index.add({'id':47,'href':'/db-learn/docs/redis/redis-quiicklist/','title':"快速列表",'content':"快速列表\r#\r\rRedis 早期版本存储 list 列表数据结构使用的是压缩列表 ziplist 和普通的双向链表 linkedlist，当元素少时用 ziplist，元素多时 用 linkedlist。\n考虑到链表的附加空间相对太高，prev 和 next 指针就要占去 16 个字节 (64bit 系统的指针是 8 个字节)，另外每个节点的内存都是单独分配，会 加剧内存的碎片化，影响内存管理效率。后续版本对列表数据结构进行了改造，使用 quicklist 代替了 ziplist 和 linkedlist。\n\u0026gt; rpush codehole go java python (integer) 3 \u0026gt; debug object codehole Value at:0x7fec2dc2bde0 refcount:1 encoding:quicklist serializedlength:31 lru:6101643 lru_seconds_idle:5 ql_nodes:1 ql_avg_node:3.00 ql_ziplist_max:-2 ql_compressed:0 ql_uncompressed_size:29 quicklist 是 ziplist 和 linkedlist 的混合体，它将 linkedlist 按段切分，每一段使用 ziplist 来紧凑存储，多个 ziplist 之间使用 双向指针串接起来。\n\rstruct ziplist { ... } struct ziplist_compressed { int32 size; byte[] compressed_data; } struct quicklistNode { quicklistNode* prev; quicklistNode* next; ziplist* zl; // 指向压缩列表  int32 size; // ziplist 的字节总数  int16 count; // ziplist 中的元素数量  int2 encoding; // 存储形式 2bit，原生字节数组还是 LZF 压缩存储  ... } struct quicklist { quicklistNode* head; quicklistNode* tail; long count; // 元素总数  int nodes; // ziplist 节点的个数  int compressDepth; // LZF 算法压缩深度  ... } Redis 还会对 ziplist 进行压缩存储，使用 LZF 算法压缩，可以选择压缩深度。\n每个 ziplist 存多少元素\r#\r\rquicklist 内部默认单个 ziplist 长度为 8k 字节，超出了这个字节数，就会新起一个 ziplist。ziplist 的长度由配置 参数 list-max-ziplist-size 决定。\n# Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb \u0026lt;-- not recommended for normal workloads # -4: max size: 32 Kb \u0026lt;-- not recommended # -3: max size: 16 Kb \u0026lt;-- probably not recommended # -2: max size: 8 Kb \u0026lt;-- good # -1: max size: 4 Kb \u0026lt;-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 压缩深度\r#\r\r\rquicklist 默认的压缩深度是 0，也就是不压缩。压缩的深度由配置参数 list-compress-depth 决定。为了支持快速的 push/pop 操作， quicklist 的首尾两个 ziplist 不压缩，此时深度就是 1。如果深度为 2，就表示 quicklist 的首尾第一个 ziplist 以及首尾第二个 ziplist 都不压缩。\n"});index.add({'id':48,'href':'/db-learn/docs/redis/28_slowlog/','title':"慢查询日志",'content':"慢查询日志\r#\r\rRedis 的慢查询日志功能用于记录执行时间超过给定时长的命令请求，用户可以通过这个功能产生的日志来监视和优化查询速度。\n服务器配置有两个和慢查询日志相关的选项：\n slowlog-log-slower-than 选项指定执行时间超过多少微秒（1 秒等于 1,000,000 微秒）的命令请求会被记录到日志上。 slowlog-max-len 选项指定服务器最多保存多少条慢查询日志。当服务器储存的慢查询日志数量等于 slowlog-max-len 选项的值时， 服务器在添加一条新的慢查询日志之前， 会先将最旧的一条慢查询日志删除。  查看慢查询日志\r#\r\r使用 SLOWLOG GET 命令查看服务器所保存的慢查询日志：\nredis\u0026gt; SLOWLOG GET 1) 1) (integer) 4 # 日志的唯一标识符（uid） 2) (integer) 1378781447 # 命令执行时的 UNIX 时间戳 3) (integer) 13 # 命令执行的时长，以微秒计算 4) 1) \u0026#34;SET\u0026#34; # 命令以及命令参数 2) \u0026#34;database\u0026#34; 3) \u0026#34;Redis\u0026#34; "});index.add({'id':49,'href':'/db-learn/docs/redis/19_persistence/','title':"持久化",'content':"持久化\r#\r\rRedis 的数据全部在内存里，如果突然宕机，数据就会全部丢失，因此必须有一种机制来保证 Redis 的数据不会因为故障而丢失， 这种机制就是 Redis 的持久化机制。\nRedis 的持久化机制有两种：\n 快照，快照是一次全量备份。 AOF 日志，AOF 日志是连续的增量备份。  RDB\r#\r\r快照（RDB）是内存数据的二进制序列化形式，在存储上非常紧凑。\nRedis 是单线程程序，这个线程要同时负责多个客户端套接字的并发读写操作和内存数据结构的逻辑读写。内存快照要求 Redis 必须进行文件 IO 操作， 可文件 IO 操作是不能使用多路复用 API。\n这意味着单线程同时在服务线上的请求还要进行文件 IO 操作，文件 IO 操作会严重拖垮服务器请求的性能。\n还有个重要的问题是为了不阻塞线上的业务，就需要边持久化边响应客户端请求。持久化的同时，内存数据结构还在改变，比 如一个大型的 hash 字典正在持久化，结果一个请求过来把它给删掉了，还没持久化完呢，这尼玛要怎么搞？\nRedis 使用操作系统的多进程写时复制(Copy On Write，简称 COW) 机制来实现快照持久化。\nSAVE 和 BGSAVE\r#\r\rSAVE 和 BGSAVE 指令可以用来生成 rdb 文件，区别在于 SAVE 会阻塞 Redis 服务器进程，客户端的所有请求都会被拒绝，直到 SAVE 执行 完成。而 BGSAVE 指令会派生一个子进程，由子进程负责创建 rdb 文件，父进程继续处理客户端请求。\n创建 rdb 文件实际上是调用了 rdb.c/rdbSave 函数，SAVE 和 BGSAVE 指令会以不同的方式调用这个函数：\ndef SAVE(): # 创建 rdb 文件  rdbSave() def BGDAVE(): # 创建子进程  pid = fork() if pid == 0: # 子进程创建 rdb 文件  rdbSave() # 完成后想父进程发送信号  singnal_parent() elif pid \u0026gt; 0: # 父进程继续处理命令 并轮询等待子进程的信号  handle_request_and_wait_signal() else: handle_fork_error() 在 BGSAVE 指令执行期间要注意：\n 客户端的 SAVE 指令会被拒绝。SAVE 和 BGSAVE 指令禁止同时调用，避免父子进程同时执行两个 rdbSave 调用，防止竞争条件。 客户端的 BGSAVE 指令会被拒绝。 BGREWRITEAOF 和 BGSAVE 不能同时执行：  如果 BGSAVE 正在执行，客户端的 BGREWRITEAOF 请求会被延迟，直到 BGSAVE 执行完毕。 如果 BGREWRITEAOF 正在执行，客户端的 BGSAVE 请求会被拒绝。    多进程\r#\r\rRedis 在执行 BGSAVE 持久化时会调用 glibc 的函数 fork 产生一个子进程，快照持久化完全交给子进程来处理，父进程继续处理客户端 请求。子进程刚刚产生时，它和父进程共享内存里面的代码段和数据段。这是 Linux 操作系统的机制，为了节约内存资源，所以尽可能让它们 共享起来。在进程分离的一瞬间，内存的增长几乎没有明显变化。\n子进程做数据持久化，它不会修改现有的内存数据结构，它只是对数据结构进行遍历读取，然后序列化写到磁盘中。但是父进程不一样，它必须持续服 务客户端请求，然后对内存数据结构进行不间断的修改。\n这个时候就会使用操作系统的 COW 机制来进行数据段页面的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数 据进行修改时，会将被共享的页面复制一份分离出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生时那一瞬 间的数据。\n子进程因为数据没有变化，它能看到的内存里的数据在进程产生的一瞬间就凝固了，再也不会改变，这也是为什么 Redis 的持久化叫快照的原因。\nAOF 日志\r#\r\rAOF 日志存储的是 Redis 服务器的顺序指令序列，AOF 日志只记录对内存进行修改的指令记录。\n假设 AOF 日志记录了自 Redis 实例创建以来所有的修改性指令序列，那么就可以通过对一个空的 Redis 实例顺序执行所有的指令，也就是重放， 来恢复 Redis 当前实例的内存数据结构的状态。\nRedis 会在收到客户端修改指令后，进行参数校验进行逻辑处理后，如果没问题，就立即将该指令文本存储到 AOF 日志中，也就是先执行指令再 将日志存盘。这点不同于 leveldb、hbase 等存储引擎，它们都是先存储日志再做逻辑处理。\nAOF 日志在长期的运行过程中会变的无比庞大，数据库重启时需要加载 AOF 日志进行指令重放，这个时间就会无比漫长。所以需要对 AOF 日志瘦身。\nAOF 重写\r#\r\rRedis 提供了 bgrewriteaof 指令用于对 AOF 日志进行瘦身。其原理就是开辟一个子进程对内存进行遍历转换成一系列 Redis 的操作指令（比如 添加一个 key，但是这时已经失效了，就不需要再添加到 AOF 日志中区），序列化到一个新的 AOF 日志文件中。序列化完毕后再将操作期间发生的 增量 AOF 日志追加到这个新的 AOF 日志文件中，追加完毕后就立即替代旧的 AOF 日志文件了，瘦身工作就完成了。\nfsync\r#\r\rAOF 日志是以文件的形式存在的，当程序对 AOF 日志文件进行写操作时，实际上是将内容写到了内核为文件描述符分配的一个内存缓存中，然后内核 会异步将脏数据刷回到磁盘的。\n这就意味着如果机器突然宕机，AOF 日志内容可能还没有来得及完全刷到磁盘中，这个时候就会出现日志丢失。那该怎么办？\nLinux 的 glibc 提供了fsync(int fd) 函数可以将指定文件的内容强制从内核缓存刷到磁盘。只要 Redis 进程实时调用 fsync 函数就 可以保证 aof 日志不丢失。但是 fsync 是一个磁盘 IO 操作，它很慢！\n所以在生产环境的服务器中，Redis 通常是每隔 1s 左右执行一次 fsync 操作，这个时间是可以配置的。这是在数据安全性和性能之间做了一个折 中，在保持高性能的同时，尽可能使得数据少丢失。\nRedis 同样也提供了另外两种策略：\n 永不 fsync，由操作系统来决定何时同步磁盘，不安全。 执行一个指令就 fsync 一次，这是非常慢的操作。生产环境下不应该使用。  运维\r#\r\r 快照遍历整个内存，大块写磁盘会加重系统负载 AOF 的 fsync 是一个耗时的 IO 操作，它会降低 Redis 性能，同时也会增加系统 IO 负担  所以通常 Redis 的主节点是不会进行持久化操作，持久化操作主要在从节点进行。从节点是备份节点，没有来自客户端请求的压力，它的操作系统 资源往往比较充沛。\n但是如果出现网络分区，从节点长期连不上主节点，就会出现数据不一致的问题，特别是在网络分区出现的情况下又不小心主节点宕机了，那么数据就 会丢失，所以在生产环境要做好实时监控工作，保证网络畅通或者能快速修复。另外还应该再增加一个从节点以降低网络分区的概率，只要有一个从节 点数据同步正常，数据也就不会轻易丢失。\n混合持久化\r#\r\r重启 Redis 时，我们很少使用 rdb 来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 rdb 来说 要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。\nRedis 4.0 为了解决这个问题，带来了一个新的持久化选项——混合持久化。将 rdb 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日 志不再是全量的日志，而是自持久化开始到持久化结束的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小。\n于是在 Redis 重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。\n"});index.add({'id':50,'href':'/db-learn/docs/redis/data-structure/','title':"数据结构",'content':"数据结构\r#\r\r链表\r#\r\r链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。\n作为一种常用数据结构，链表内置在很多高级的编程语言里面，因为 Redis 使用的 C 语言并没有内置这种数据结构，所以 Redis 构建了自己的链表 实现。\n链表节点的实现\r#\r\r每个链表节点使用一个 adlist.h/listNode 结构：\ntypedef struct listNode { // 前置节点  struct listNode *prev; // 后置节点  struct listNode *next; // 节点的值  void *value; } listNode; 使用 adlist.h/list 来持有链表：\ntypedef struct list { // 表头节点  listNode *head; // 表尾节点  listNode *tail; // 链表所包含的节点数量  unsigned long len; // 节点值复制函数  void *(*dup)(void *ptr); // 节点值释放函数  void (*free)(void *ptr); // 节点值对比函数  int (*match)(void *ptr, void *key); } list; list 结构为链表提供了表头指针 head 、表尾指针 tail ，以及链表长度计数器 len ，而 dup 、 free 和 match 成员则是用 于实现多态链表所需的类型特定函数：\n dup 函数用于复制链表节点所保存的值； free 函数用于释放链表节点所保存的值； match 函数则用于对比链表节点所保存的值和另一个输入值是否相等。  \rRedis 的链表实现的特性：\n 双向：链表节点带有 prev 和 next 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1)。 无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ，对链表的访问以 NULL 为终点。 带表头指针和表尾指针：通过 list 结构的 head 指针和 tail 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1)。 带链表长度计数器：程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1)。 多态：链表节点使用 void* 指针来保存节点值， 并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特 定函数，所以链表可以用于保存各种不同类型的值。  跳跃表\r#\r\r跳跃表（skiplist）是一种有序数据结构，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。\nRedis 只在两个地方用到了跳跃表 一个是实现有序集合键，另一个是在集群节点中用作内部数据结构。\n跳跃表的简单解释\r#\r\r想想一个创业公司，刚开始只有几个人，团队成员之间人人平等，都是联合创始人。随着公司的成长，人数渐渐变多，团队沟通成本随之增加。这时候就 会引入组长制，对团队进行划分。每个团队会有一个组长。开会的时候分团队进行，多个组长之间还会有自己的会议安排。公司规模进一步扩展，需要再增 加一个层级 —— 部门，每个部门会从组长列表中推选出一个代表来作为部长。部长们之间还会有自己的高层会议安排。\n跳跃列表就是类似于这种层级制，最下面一层所有的元素都会串起来。然后每隔几个元素挑选出一个代表来，再将这几个代表使用另外一级指针串起来。然 后在这些代表里再挑出二级代表，再串起来。最终就形成了金字塔结构。\n跳跃表之所以跳跃，是因为内部的元素可能身兼数职，比如下节图中的第一个节点，同时处于 L1、L2、L3 和 L4 层，可以快速在不同层次之间进行跳跃。\n跳跃表的实现\r#\r\rRedis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义，其中 zskiplistNode 结构用于表示跳跃表节点，而 zskiplist 结构则用于保存跳跃表 节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针等等。\n\rzskiplist 结构， 该结构包含以下属性：\n header：指向跳跃表的表头节点。 tail：指向跳跃表的表尾节点。 level：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内）。 length：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内）。  zskiplistNode 结构， 该结构包含以下属性：\n 层（level）：节点中用 L1 、 L2 、 L3 等字样标记节点的各个层， L1 代表第一层， L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。 前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进 指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。 后退（backward）指针：节点中用 BW 字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值（score）：各个节点中的 1.0 、 2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。 成员对象（obj）：各个节点中的 o1 、 o2 和 o3 是节点所保存的成员对象。  跳跃表节点\r#\r\rredis.h/zskiplistNode 结构定义：\ntypedef struct zskiplistNode { // 后退指针  struct zskiplistNode *backward; // 分值  double score; // 成员对象  robj *obj; // 层  struct zskiplistLevel { // 前进指针  struct zskiplistNode *forward; // 跨度  unsigned int span; } level[]; } zskiplistNode; 层\r#\r\rlevel 数组可以包含多个元素，每次创建一个新跳跃表节点的时候，程序都根据幂次定律 （power law，越大的数出现的概率越小） 随机生成一个介 于 1 和 32 之间的值作为 level 数组的大小， 这个大小就是层的“高度”。\n程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。\n前进指针：\n每个层都有一个指向表尾方向的前进指针（level[i].forward 属性），用于从表头向表尾方向访问节点。\n跨度：\n层的跨度（level[i].span 属性）用于记录两个节点之间的距离：\n 两个节点之间的跨度越大， 它们相距得就越远。 指向 NULL 的所有前进指针的跨度都为 0 ，因为它们没有连向任何节点。  初看上去，很容易以为跨度和遍历操作有关，但实际上并不是这样 —— 遍历操作只使用前进指针就可以完成了，跨度实际上是用来计算排位（rank） 的：在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，得到的结果就是目标节点在跳跃表中的排位。\n例如，前面途中在跳跃表中查找分值为 3.0 、成员对象为 o3 的节点时，沿途经历的层：查找的过程只经过了一个层，并且层的跨度为 3 ，所以 目标节点在跳跃表中的排位为 3 。\n后退指针：\n节点的后退指针（backward 属性）用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以 每次只能后退至前一个节点。\n分值和成员：\n 节点的分值（score 属性）是一个 double 类型的浮点数， 跳跃表中的所有节点都按分值从小到大来排序。 节点的成员对象（obj 属性）是一个指针， 它指向一个字符串对象， 而字符串对象则保存着一个 SDS 值。  同一个跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值却可以是相同的**：分值相同的节点将按照成员对象在字典 序中的大小来进行排序，成员对象较小的节点会排在前面（靠近表头的方向），而成员对象较大的节点则会排在后面（靠近表尾的方向）。\n跳跃表\r#\r\r多个跳跃表节点就可以组成一个跳跃表，通过使用一个 zskiplist 结构来持有这些节点，程序可以更方便地对整个跳跃表进行处理，比如快速访问 跳跃表的表头节点和表尾节点，又或者快速地获取跳跃表节点的数量（也即是跳跃表的长度）等信息。\ntypedef struct zskiplist { // 表头节点和表尾节点  struct zskiplistNode *header, *tail; // 表中节点的数量  unsigned long length; // 表中层数最大的节点的层数  int level; } zskiplist; header 和 tail 指针分别指向跳跃表的表头和表尾节点，通过这两个指针，程序定位表头节点和表尾节点的复杂度为 O(1)。\n通过使用 length 属性来记录节点的数量，程序可以在 O(1) 复杂度内返回跳跃表的长度，注意表头节点不计算在内。\nlevel 属性则用于在 O(1) 复杂度内获取跳跃表中层高最大的那个节点的层数量，注意表头节点的层高并不计算在内。\n整数集合\r#\r\r整数集合（intset）是集合键的底层实现之一：当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis 就会使用整数集合作为集 合键的底层实现。\n整数集合的实现\r#\r\r整数集合它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值，并且保证集合中不会出现重复元素。\nintset.h/intset 结构：\ntypedef struct intset { // 编码方式  uint32_t encoding; // 集合包含的元素数量  uint32_t length; // 保存元素的数组  int8_t contents[]; } intset; contents 数组是整数集合的底层实现：整数集合的每个元素在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复项。\n虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类 型的值 —— contents 数组的真正类型取决于 encoding 属性的值：\n 如果 encoding 属性的值为 INTSET_ENC_INT16 ，那么 contents 就是一个 int16_t 类型的数组，int16_t 类型的整 数值（最小值为 -32,768，最大值为 32,767 ）。 如果 encoding 属性的值为 INTSET_ENC_INT32 ，那么 contents 就是一个 int32_t 类型的数组，int32_t 类型的整 数值（最小值为 -2,147,483,648，最大值为 2,147,483,647 ）。 如果 encoding 属性的值为 INTSET_ENC_INT64 ，那么 contents 就是一个 int64_t 类型的数组，int64_t 类型的 整数值（最小值为 -9,223,372,036,854,775,808，最大值为 9,223,372,036,854,775,807）。  升级\r#\r\r当我们要将一个新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级（upgrade），然后 才能将新元素添加到整数集合里面。\n升级整数集合并添加新元素共分为三步进行：\n 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层 数组的有序性质不变。 将新元素添加到底层数组里面。  例如，一个 INTSET_ENC_INT16 编码的整数集合，包含三个 int16_t 类型的元素（1，2，3），每个元素都占用 16 位空间， 所以整数集合 底层数组的大小为 3 * 16 = 48 位。\n现在要将类型为 int32_t 的整数值 65535 添加到整数集合里面，因为 65535 的类型 int32_t 比整数集合当前所有元素的类型都要长，所以在 将 65535 添加到整数集合之前，程序需要先对整数集合进行升级。\n 升级首先，对底层数组进行空间重分配。每个 int32_t 整数值占用 32 位空间，四个元素就是 32 * 4 = 128 位。 空间重分配之后，数组原有的三个元素仍然是 int16_t 类型， 还保存在数组的前 48 位里面，接下来要做的就是将这三个元素转换成 int32_t 类型，并将转换后的元素放置到正确的位上面。 将新元素添加到底层数组里面。  升级的好处\r#\r\r升级策略有两个好处：\n 是提升整数集合的灵活性，整数集合可以通过自动升级底层数组来适应新元素，所以我们可以随意地将 int16_t 、 int32_t 或者 int64_t 类型的整数添加到集合中，而不必担心出现类型错误，这种做法非常灵活。 节约内存，要让一个数组可以同时保存 int16_t 、 int32_t 、 int64_t 三种类型的值，最简单的做法就是直接使用 int64_t 类型的数 组作为整数集合的底层实现。不过这样一来，即使添加到整数集合里面的都是 int16_t 类型或者 int32_t 类型的值，数组都需要使用 int64_t 类型的空间去保存它们，从而出现浪费内存的情况。整数集合现在的做法既可以让集合能同时保存三种不同类型的值，又可以确保升级操作只会在有需要的 时候进行，这可以尽量节省内存。  不支持降级\r#\r\r整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。\n"});index.add({'id':51,'href':'/db-learn/docs/mongo/04_migrate/','title':"数据迁移",'content':"简单迁移\r#\r\r如何将一台 linux A 上的 mongodb 数据迁移到另外一台 linux B 上？\n两个命令即可完成任务：\n 数据的导出：mongoexport 数据的导入：mongoimport  具体步骤：\n 找到 A 的 mongodb 的mongoexport所在目录，一般在/usr/bin或者/usr/local/mongodb/bin下。  cd /usr/local/mongodb/bin 将数据导出，执行命令：./mongoexport -d dbname -c collectionname -o xxx.dat dbname为数据库名称，collectionname为集合名称，xxx.dat为导出后的数据的名称。导出后的xxx.dat在mongoexport所在的目录下。  ./mongoexport -d moregold -c logs -o logdata.dat 将数据库moregold下的集合logs导出到mongoexport所在的目录下，并将其命名为logdata.dat\n将导出的集合数据移动到 B 服务器上。 找到 B 的mongoimport所在的目录：cd /db/mongo/bin 将数据导入，执行命令./mongoimport -h 127.0.0.1:port -u xxx -p xxx -d dbname -c collectionname /path/xxx.dat：  ./mongoimport -h 127.0.0.1:27017 -u zhangsan -p zhangsan -d moregold -c /root/logdata.dat  -h 127.0.0.1:27017：连接到本地，端口号为27017 -u zhangsan：用户名为zhangsan -p zhangsan：密码为zhangsan  迁移完毕。\n停机迁移\r#\r\r对于生产环境下，上面的方式可能并不适用。\n采用停机迁移的好处是流程操作简单，工具成本低；然而缺点也很明显， 迁移过程中业务是无法访问的，因此只适合于规格小、允许停服的场景。\n 准备迁移工具 原系统下线 全量迁移 新系统上线  业务双写\r#\r\r业务双写是指对现有系统先进行改造升级，支持同时对新库和旧库进行写入。 之后再通过数据迁移工具对旧数据做全量迁移，待所有数据迁移转换完成后切换到新系统。\n业务双写的方案是平滑的，对线上业务影响极小；在出现问题的情况下可重新来过，操作压力也会比较小。 但实现该方案比较复杂，需要对现有的代码进行改造并完成新数据的转换及写入，对于开发人员的要求较高。 在业务逻辑清晰、团队对系统有足够的把控能力的场景下适用。\n增量迁移\r#\r\r增量迁移的基本思路是先进行全量的迁移转换，待完成后持续进行增量数据的处理，直到数据追平后切换系统。\n关键点：\n  要求系统支持增量数据的记录。 对于MongoDB可以利用oplog实现这点，为避免全量迁移过程中oplog被冲掉， 在开始迁移前就必须开始监听oplog，并将变更全部记录下来。 如果没有办法，需要从应用层上考虑，比如为所有的表(集合)记录下updateTime这样的时间戳， 或者升级应用并支持将修改操作单独记录下来。\n  增量数据的回放是持续的。 在所有的增量数据回放转换过程中，系统仍然会产生新的增量数据，这要求迁移工具 能做到将增量数据持续回放并将之追平，之后才能做系统切换。\n   MongoDB 3.6版本开始便提供了Change Stream功能，支持对数据变更记录做监听。 这为实现数据同步及转换处理提供了更大的便利。\n "});index.add({'id':52,'href':'/db-learn/docs/mysql/query-optimize/','title':"查询优化",'content':"查询优化\r#\r\r"});index.add({'id':53,'href':'/db-learn/docs/redis/09_sds/','title':"简单动态字符串",'content':"简单动态字符串\r#\r\rRedis 没有直接使用 C 语言传统的字符串表示（以空字符结尾的字符数组，以下简称 C 字符串），而是自己构建了一种名为简单 动态字符串（simple dynamic string，SDS）的抽象类型， 并将 SDS 用作 Redis 的默认字符串表示。内部结构实现上类 似于 Java 的 ArrayList。\nRedis 中， C 字符串只会作为字符串字面量（string literal），用在一些无须对字符串值进行修改的地方，比如打印日志：\nredisLog(REDIS_WARNING,\u0026#34;Redis is now ready to exit, bye bye...\u0026#34;); 一个可以被修改的字符串值时，Redis 就会使用 SDS 来表示字符串值：比如在 Redis 的数据库里面，包含字符串值的键值对在底层都是由 SDS 实现的。\nredis\u0026gt; SET msg \u0026#34;hello world\u0026#34; OK 其中：\n 键值对的键是一个字符串对象，对象的底层实现是一个保存着字符串 \u0026ldquo;msg\u0026rdquo; 的 SDS 。 键值对的值也是一个字符串对象，对象的底层实现是一个保存着字符串 \u0026ldquo;hello world\u0026rdquo; 的 SDS。  又比如：\nredis\u0026gt; RPUSH fruits \u0026#34;apple\u0026#34; \u0026#34;banana\u0026#34; \u0026#34;cherry\u0026#34; (integer) 3 其中：\n 键值对的键是一个字符串对象，对象的底层实现是一个保存了字符串 \u0026ldquo;fruits\u0026rdquo; 的 SDS 。 键值对的值是一个列表对象，列表对象包含了三个字符串对象，这三个字符串对象分别由三个 SDS 实现： 第一个 SDS 保存着字符串 \u0026ldquo;apple\u0026rdquo; ， 第二个 SDS 保存着字符串 \u0026ldquo;banana\u0026rdquo; ， 第三个 SDS 保存着字符串 \u0026ldquo;cherry\u0026rdquo;。  AOF 模块中的 AOF 缓冲区， 以及客户端状态中的输入缓冲区， 都是由 SDS 实现的。\nSDS 与 C 字符串的区别\r#\r\r常数复杂度获取字符串长度\r#\r\rC 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串 结尾的空字符为止，这个操作的复杂度为 O(N)。\nSDS 在 len 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1)。\nstruct SDS\u0026lt;T\u0026gt; { T capacity; // 数组容量  T len; // 数组长度  byte flags; // 特殊标识位，不理睬它  byte[] content; // 数组内容 } 使用 SDS 确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。\n比如说，对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1)。\n杜绝缓冲区溢出\r#\r\rC 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）。\n例如，\u0026lt;string.h\u0026gt;/strcat 函数可以将 src 字符串中的内容拼接到 dest 字符串的末尾：\nchar *strcat(char *dest, const char *src); 因为 C 字符串不记录自身的长度，所以 strcat 假定用户在执行这个函数时，已经为 dest 分配了足够多的内存， 可以容纳 src 字符串中的 所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。\nSDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：当 SDS API 需要对 SDS 进行修改时， API 会先检查 SDS 的空间是否满足修改所需的要求， 如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间 大小，也不会出现前面所说的缓冲区溢出问题。\n比如 SDS 的 API 中的 sdscat 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，sdscat 会 先检查给定 SDS 的空间是否足够，如果不够的话，sdscat 就会先扩展 SDS 的空间， 然后才执行拼接操作。\n减少修改字符串时带来的内存重分配次数\r#\r\rC 字符串并不记录自身的长度，所以对于一个包含了 N 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 N+1 个字符长的数 组（额外的一个字符空间用于保存空字符）。\n因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行 一次内存重分配操作：\n 如果程序执行的是增长字符串的操作，比如拼接操作（append），那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空 间大小 —— 如果忘了这一步就会产生缓冲区溢出。 如果程序执行的是缩短字符串的操作，比如截断操作（trim），那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部 分空间 —— 如果忘了这一步就会产生内存泄漏。  在 SDS 中，byte 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，数组的容量由 SDS 的 capacity 属性记录。\n\rSDS 实现了空间预分配和惰性空间释放两种优化策略。\n空间预分配\r#\r\r空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配 修改所必须要的空间，还会为 SDS 分配额外的未使用空间。\n 注意，创建字符串时 len 和 capacity 一样长，不会额外分配的未使用空间，因为绝大多数场景下我们不会使用 append 操作来修改字符串。\n 额外分配的未使用空间数量由以下公式决定：\n 如果对 SDS 进行修改之后， SDS 的长度（也即是 len 属性的值）将小于 1 MB ，那么程序分配和 len 属性同样大小的未使用空间， 举个例子，如果进行修改之后， SDS 的 len 将变成 13 字节，那么程序也会分配 13 字节的未使用空间， SDS 的 byte 数组的实际长度 将变成 13 + 13 + 1 = 27 字节（额外的一字节用于保存空字符）。 如果对 SDS 进行修改之后， SDS 的长度将大于等于 1 MB ，那么程序会分配 1 MB 的未使用空间。例如，如果进行修改之后， SDS 的 len 将变成 30 MB，那么程序会分配 1 MB 的未使用空间， SDS 的 byte 数组的实际长度将为 30 MB + 1 MB + 1 byte。  在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。\n通过这种预分配策略，SDS 将连续增长 N 次字符串所需的内存重分配次数从必定 N 次降低为最多 N 次。\n惰性空间释放\r#\r\r惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出 来的字节，而是使用 free 属性将这些字节的数量记录起来，并等待将来使用。\n通过惰性空间释放策略， SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。\nSDS 也提供了相应的 API ，让我们可以在有需要时，真正地释放 SDS 里面的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。\n二进制安全\r#\r\rC 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的 空字符将被误认为是字符串结尾 —— 这些限制使得 C 字符串只能保存文本数据， 而不能保存像图片、音频、视频、压缩文件这样的二进制数据。\n为了确保 Redis 可以适用于各种不同的使用场景， SDS 的 API 都是二进制安全的（binary-safe）：所有 SDS API 都会以处理二进制的方 式来处理 SDS 存放在 buf 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 —— 数据在写入时是什么样的，它被读取时就是 什么样。\n这也是我们将 SDS 的 buf 属性称为字节数组的原因 —— Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。\n兼容部分 C 字符串函数\r#\r\rSDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在 为 buf 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 \u0026lt;string.h\u0026gt; 库定义的函数。\nembstr vs raw\r#\r\rRedis 的字符串有两种存储方式，在长度特别短时，使用 emb 形式存储 (embeded)，当长度超过 44 时，使用 raw 形式存储。 这两种类型有什么区别呢？为什么分界线是 44 呢？\n\u0026gt; set codehole abcdefghijklmnopqrstuvwxyz012345678912345678 OK \u0026gt; debug object codehole Value at:0x7fec2de00370 refcount:1 encoding:embstr serializedlength:45 lru:5958906 lru_seconds_idle:1 \u0026gt; set codehole abcdefghijklmnopqrstuvwxyz0123456789123456789 OK \u0026gt; debug object codehole Value at:0x7fec2dd0b750 refcount:1 encoding:raw serializedlength:46 lru:5958911 lru_seconds_idle:1 注意上面 debug object 输出中的 encoding 字段，一个字符的差别，存储形式就发生了变化。\n来了解一下 Redis 对象头结构体：\nstruct RedisObject { int4 type; // 4bits  int4 encoding; // 4bits  int24 lru; // 24bits  int32 refcount; // 4bytes  void *ptr; // 8bytes，64-bit system } robj;  type(4bit) 对象类型 ， encoding(4bit) 同一个类型的 type 会有不同的存储形式 lru(24bit) 记录对象的 LRU 信息 refcount 每个对象都有个引用计数，当引用计数为零时，对象就会被销毁，内存被回收。 ptr 指针将指向对象内容 (body) 的具体存储位置。  一个 RedisObject 对象头需要占据 16 字节的存储空间。\nSDS 对象头的大小是 capacity+3，至少是 3。意味着分配一个字符串的最小空间占用为 19 字节 (16+3)。\nstruct SDS { int8 capacity; // 1byte  int8 len; // 1byte  int8 flags; // 1byte  byte[] content; // 内联数组，长度为 capacity } \r embstr 存储形式是将 RedisObject 对象头和 SDS 对象连续存在一起，使用 malloc 方法一次分配。 raw 存储需要两次 malloc，两个对象头在内存地址上一般是不连续的。  内存分配器 jemalloc/tcmalloc 等分配内存大小的单位都是 2、4、8、16、32、64 等等，为了能容纳一个完整的 embstr 对象，jemalloc 最少 会分配 32 字节的空间，如果字符串再稍微长一点，那就是 64 字节的空间。如果总体超出了 64 字节，Redis 认为它是一个大字符串，不再 使用 emdstr 形式存储，而该用 raw 形式。\n当内存分配器分配了 64 空间时，那这个字符串的长度最大就是 44。\n为什么是 44 字节？\r#\r\rSDS 结构体中的 content 中的字符串是以字节 \\0 结尾的字符串，之所以多这 1 个字节，是为了便于直接使用 glibc 的字符串处理函数，以及 为了便于字符串的调试打印输出。\n\r上图中可以看出，内存分配器分配的 64 个字节中，content 的长度最多只有 45(64-19) 字节了。再减去 \\0 结尾的一个字节，就是 44 字节。\n 注意，不同版本的 redis，SDS 的结构可能不一样，可能不是 44 字节。\n "});index.add({'id':54,'href':'/db-learn/docs/redis/20_pipeline/','title':"管道",'content':"管道\r#\r\rRedis 管道 (Pipeline) 这个技术本质上是由客户端提供的，跟服务器没有什么直接的关系。\nRedis 的消息交互\r#\r\r当我们使用客户端对 Redis 进行一次操作时，客户端将请求传送给服务器，服务器处理完毕后，再将响应回复给客户端。这要花费一个网络数据 包来回的时间。\n如果连续执行多条指令，那就会花费多个网络数据包来回的时间。\n管道操作的本质，服务器根本没有任何区别对待，还是收到一条消息，执行一条消息，回复一条消息的正常的流程。客户端通过对管道中的指令 列表改变读写顺序，合并 write 和 read 操作，就可以大幅节省 IO 时间。\n管道压力测试\r#\r\rRedis 自带了一个压力测试工具 redis-benchmark，使用这个工具就可以进行管道测试。\n首先我们对一个普通的 set 指令进行压测，QPS 大约 5w/s。\n\u0026gt; redis-benchmark -t set -q SET: 51975.05 requests per second 加入管道选项 -P 参数，它表示单个管道内并行的请求数量，看下面 P=2，QPS 达到了 9w/s。\n\u0026gt; redis-benchmark -t set -P 2 -q SET: 91240.88 requests per second 再看看 P=3，QPS 达到了 10w/s。\n\u0026gt; redis-benchmark -t set -P 3 -q SET: 102354.15 requests per second 但如果再继续提升 P 参数，发现 QPS 已经上不去了。因为这里 CPU 处理能力已经达到了瓶颈，无法再继续提升了。\n管道本质\r#\r\r\r上图就是一个完整的请求交互流程图。我用文字来仔细描述一遍：\n 客户端进程调用 write 将消息写到操作系统内核为套接字分配的发送缓冲 send buffer。 客户端操作系统内核将发送缓冲的内容发送到网卡，网卡硬件将数据通过「网际路由」送到服务器的网卡。 服务器操作系统内核将网卡的数据放到内核为套接字分配的接收缓冲 recv buffer。 服务器进程调用 read 从接收缓冲中取出消息进行处理。 服务器进程调用 write 将响应消息写到内核为套接字分配的发送缓冲 send buffer。 服务器操作系统内核将发送缓冲的内容发送到网卡，网卡硬件将数据通过「网际路由」送到客户端的网卡。 客户端操作系统内核将网卡的数据放到内核为套接字分配的接收缓冲 recv buffer。 客户端进程调用 read 从接收缓冲中取出消息返回给上层业务逻辑进行处理。 结束。  write 操作只负责将数据写到本地操作系统内核的发送缓冲然后就返回了。剩下的事交给操作系统内核异步将数据送到目标机器。但是如果发 送缓冲满了，那么就需要等待缓冲空出空闲空间来，这个就是写操作 IO 操作的真正耗时。\nread 操作只负责将数据从本地操作系统内核的接收缓冲中取出来就了事了。但是如果缓冲是空的，那么就需要等待数据到来，这个就是读操 作 IO 操作的真正耗时。\n所以对于 value = redis.get(key) 这样一个简单的请求来说，write 操作几乎没有耗时，直接写到发送缓冲就返回，而 read 就会比 较耗时了，因为它要等待消息经过网络路由到目标机器处理后的响应消息,再回送到当前的内核读缓冲才可以返回。这才是一个网络来回的真正开销。\n而对于管道来说，连续的 write 操作根本就没有耗时，之后第一个 read 操作会等待一个网络的来回开销，然后所有的响应消息就都已经回送 到内核的读缓冲了，后续的 read 操作直接就可以从缓冲拿到结果，瞬间就返回了。\n"});index.add({'id':55,'href':'/db-learn/docs/redis/redis-listpack/','title':"紧凑列表",'content':"紧凑列表\r#\r\rRedis 5.0 引入了数据结构 listpack，它是对 ziplist 结构的改进，在存储空间上会更加节省，而且结构上也比 ziplist 要精简。 listpack 的设计的目的是用来取代 ziplist，但是 ziplist 在 Redis 数据结构中使用太广泛了，替换起来复杂度会非常之高。目前只使用在了新增 加的 Stream 数据结构中。\nstruct listpack\u0026lt;T\u0026gt; { int32 total_bytes; // 占用的总字节数  int16 size; // 元素个数  T[] entries; // 紧凑排列的元素列表  int8 end; // 同 zlend 一样，恒为 0xFF } \rlistpack 跟 ziplist 的结构几乎一摸一样，只是少了一个 zltail_offset 字段。因为 listpack 可以是通过其它方式来定位出最后一个元素 的位置的。\nstruct lpentry { int\u0026lt;var\u0026gt; encoding; optional byte[] content; int\u0026lt;var\u0026gt; length; } 元素的结构和 ziplist 的元素结构也很类似，都是包含三个字段。不同的是 length 字段放在了元素的尾部，而且存储的不是上一个元素的长度，是 当前元素的长度。正是因为长度放在了尾部，所以可以省去了 zltail_offset 字段来标记最后一个元素的位置，这个位置可以 通过 total_bytes 字段和最后一个元素的长度字段计算出来。\n不同于 skiplist 元素长度的编码为 1 个字节或者 5 个字节，listpack 元素长度的编码可以是 1、2、3、4、5 个字节。同 UTF8 编码一样，它 通过字节的最高为是否为 1 来决定编码的长度。\nencoding 字段：\n 0xxxxxxx 表示非负小整数，可以表示0~127。 10xxxxxx 表示小字符串，长度范围是0~63，content字段为字符串的内容。 110xxxxx yyyyyyyy 表示有符号整数，范围是-2048~2047。 1110xxxx yyyyyyyy 表示中等长度的字符串，长度范围是0~4095，content字段为字符串的内容。 11110000 aaaaaaaa bbbbbbbb cccccccc dddddddd 表示大字符串，四个字节表示长度，content字段为字符串内容。 11110001 aaaaaaaa bbbbbbbb 表示 2 字节有符号整数。 11110010 aaaaaaaa bbbbbbbb cccccccc 表示 3 字节有符号整数。 11110011 aaaaaaaa bbbbbbbb cccccccc dddddddd 表示 4 字节有符号整数。 11110011 aaaaaaaa ... hhhhhhhh 表示 8 字节有符号整数。 11111111 表示 listpack 的结束符号，也就是 0xFF。  级联更新\r#\r\rlistpack 的设计彻底消灭了 ziplist 存在的级联更新行为，元素与元素之间完全独立，不会因为一个元素的长度变长就导致后续的元素内容会受到 影响。\n"});index.add({'id':56,'href':'/db-learn/docs/mysql/table-space/','title':"表空间",'content':"表空间\r#\r\r"});index.add({'id':57,'href':'/db-learn/docs/redis/redis-skiplist/','title':"跳表",'content':"跳表\r#\r\rzset 的内部实现是一个 hash 字典加一个跳跃列表 (skiplist)。\nstruct zset { dict *dict; // all values value=\u0026gt;score  zskiplist *zsl; } dict 结构存储 value 和 score 值的映射关系。\n参考 跳表。\n"});index.add({'id':58,'href':'/db-learn/docs/mysql/14_lock/','title':"锁",'content':"锁\r#\r\r在处理并发读或者写时，可以通过实现两种类型的锁组成的锁系统来解决。这两种类型分别是共享锁（shared lock）和排他锁（exclusive lock）， 也叫读锁（read lock）和写锁（write lock）\n读锁是共享的，相互不阻塞。写锁是排他的，一个写锁会阻塞其他的写锁和读锁，只有这样，才能保证同一时间只有一个用户可以执行写入，并防止其他用户读取正 在写入的同一资源。\n锁粒度\r#\r\r提高共享资源并发性的方式就是让锁定对象更有选择性。尽量只锁定需要修改的部分数据，而不是多有的资源。只对会修改的数据进行精确的锁定。 在给定的资源上，锁定的数据量越少，则系统的并发程度越高，只要不发生冲突即可。\n但是锁也会消耗资源\n"});index.add({'id':59,'href':'/db-learn/docs/redis/17_current-limit/','title':"限流",'content':"限流\r#\r\r限流算法在分布式领域是一个经常被提起的话题，当系统的处理能力有限时，如何阻止计划外的请求继续对系统施压，这是一个需要重视的问题。\n除了控制流量，限流还有一个应用目的是用于控制用户行为，避免垃圾请求。比如在 UGC 社区，用户的发帖、回复、点赞等行为都要严格受控，一般要 严格限定某行为在规定时间内允许的次数，超过了次数那就是非法行为。\nRedis 实现简单限流\r#\r\r系统要限定用户的某个行为在指定的时间里只能允许发生 N 次，如何使用 Redis 的数据结构来实现这个限流的功能？\n# 指定用户 user_id 的某个行为 action_key 在特定的时间内 period 只允许发生一定的次数 max_count def is_action_allowed(user_id, action_key, period, max_count): return True # 调用这个接口 , 一分钟内只允许最多回复 5 个帖子 can_reply = is_action_allowed(\u0026#34;laoqian\u0026#34;, \u0026#34;reply\u0026#34;, 60, 5) if can_reply: do_reply() else: raise ActionThresholdOverflow() 解决方案\r#\r\r这个限流需求中存在一个滑动时间窗口，想想 zset 数据结构的 score 值，是不是可以通过 score 来圈出这个时间窗口来。 而且我们只需要保留这个时间窗口，窗口之外的数据都可以砍掉。那这个 zset 的 value 填什么比较合适？它只需要保证唯一性即可， 用 uuid 会比较浪费空间，那就改用毫秒时间戳吧。\n一个 zset 结构记录用户的行为历史，每一个行为都会作为 zset 中的一个 key 保存下来。同一个用户同一种行为用一个 zset 记录。\n为节省内存，我们只需要保留时间窗口内的行为记录，同时如果用户是冷用户，滑动时间窗口内的行为是空记录，那么这个 zset 就可以从内存 中移除，不再占用空间。\n通过统计滑动窗口内的行为数量与阈值 max_count 进行比较就可以得出当前的行为是否允许。\nimport time import redis client = redis.StrictRedis() def is_action_allowed(user_id, action_key, period, max_count): key = \u0026#39;hist:%s:%s\u0026#39; % (user_id, action_key) now_ts = int(time.time() * 1000) # 毫秒时间戳 with client.pipeline() as pipe: # client 是 StrictRedis 实例 # 记录行为 pipe.zadd(key, now_ts, now_ts) # value 和 score 都使用毫秒时间戳 # 移除时间窗口之前的行为记录，剩下的都是时间窗口内的 pipe.zremrangebyscore(key, 0, now_ts - period * 1000) # 获取窗口内的行为数量 pipe.zcard(key) # 设置 zset 过期时间，避免冷用户持续占用内存 # 过期时间应该等于时间窗口的长度，再多宽限 1s pipe.expire(key, period + 1) # 批量执行 _, _, current_count, _ = pipe.execute() # 比较数量是否超标 return current_count \u0026lt;= max_count for i in range(20): print is_action_allowed(\u0026#34;laoqian\u0026#34;, \u0026#34;reply\u0026#34;, 60, 5) 缺点\r#\r\r因为它要记录时间窗口内所有的行为记录，如果这个量很大，比如限定 60s 内操作不得超过 100w 次这样的参数，它是不适合做这样的限流的，因为 会消耗大量的存储空间。\n漏斗限流\r#\r\r漏斗的容量是有限的，如果将漏嘴堵住，然后一直往里面灌水，它就会变满，直至再也装不进去。如果将漏嘴放开，水就会往下流，流走一部分之后，就又 可以继续往里面灌水。如果漏嘴流水的速率大于灌水的速率，那么漏斗永远都装不满。如果漏嘴流水速率小于灌水的速率，那么一旦漏斗满了，灌水就需 要暂停并等待漏斗腾空。\n所以，漏斗的剩余空间就代表着当前行为可以持续进行的数量，漏嘴的流水速率代表着系统允许该行为的最大频率。\nRedis-Cell\r#\r\rRedis 4.0 提供了一个限流 Redis 模块，它叫 redis-cell。该模块也使用了漏斗算法，并提供了原子的限流指令。有了这个模块，限流问题 就非常简单了。\n该模块只有 1 条指令 cl.throttle，它的参数和返回值都略显复杂：\n\u0026gt; cl.throttle laoqian:reply 15 30 60 1 ▲ ▲ ▲ ▲ ▲ | | | | └───── need 1 quota (可选参数，表示本次要申请的数量，默认值也是 1) | | └──┴─────── 30 operations / 60 seconds 这是漏水速率 | └───────────── 15 capacity 这是漏斗容量 └─────────────────── key laoqian 上面这个指令的意思是允许「用户老钱回复行为」的频率为每 60s 最多 30 次(漏水速率)，漏斗的初始容量为 15，也就是说一开始可以连续回 复 15 个帖子，然后才开始受漏水速率的影响。\n\u0026gt; cl.throttle laoqian:reply 15 30 60 1) (integer) 0 # 0 表示允许，1 表示拒绝 2) (integer) 15 # 漏斗容量 capacity 3) (integer) 14 # 漏斗剩余空间 left_quota 4) (integer) -1 # 如果拒绝了，需要多长时间后再试(漏斗有空间了，单位秒) 5) (integer) 2 # 多长时间后，漏斗完全空出来(left_quota==capacity，单位秒) "});index.add({'id':60,'href':'/db-learn/docs/redis/23_cluster/','title':"集群",'content':"集群\r#\r\rSentinel\r#\r\r可以将 Redis Sentinel 集群看成是一个 ZooKeeper 集群，它是集群高可用的心脏，它一般是由 3～5 个节点组成，这样挂了个别节点集群还可 以正常运转。\n\r它负责持续监控主从节点的健康，当主节点挂掉时，自动选择一个最优的从节点切换为主节点。客户端来连接集群时，会首先连接 sentinel，通 过 sentinel 来查询主节点的地址，然后再去连接主节点进行数据交互。当主节点发生故障时，客户端会重新向 sentinel 要地址，sentinel 会将 最新的主节点地址告诉客户端。\n\r从这张图中我们能看到主节点挂掉了，原先的主从复制也断开了，客户端和损坏的主节点也断开了。从节点被提升为新的主节点，其它从节点开始和新的 主节点建立复制关系。客户端通过新的主节点继续进行交互\nSentinel 会持续监控已经挂掉了主节点，待它恢复后原先挂掉的主节点现在变成了从节点，从新的主节点那里建立复制关系。\n消息丢失\r#\r\rRedis 主从采用异步复制，意味着当主节点挂掉时，从节点可能没有收到全部的同步消息，这部分未同步的消息就丢失了。如果主从延迟特别大，那么 丢失的数据就可能会特别多。Sentinel 无法保证消息完全不丢失，但是也尽可能保证消息少丢失。它有两个选项可以限制主从延迟过大。\nmin-slaves-to-write 1 # 表示主节点必须至少有一个从节点在进行正常复制，否则就停止对外写服务\rmin-slaves-max-lag 10 # 单位是秒，表示如果 10s 没有收到从节点的反馈，就意味着从节点同步异常\rCluster\r#\r\rRedisCluster 是 Redis 提供的分布式数据库方案，集群通过分片来进行数据共享，并提供复制和故障转移功能。RedisCluster 是去中心化的， 如下图，该集群有三个 Redis 节点组成，每个节点负责整个集群的一部分数据，每个节点负责的数据多少可能不一样。这三个节点相互连接组成 一个对等的集群，它们之间通过一种特殊的二进制协议相互交互集群信息。\n\r槽\r#\r\rRedis 通过分片的方式来保存数据：集群的整个数据库被分为 16384 个槽（slot），集群中的每个节点可以处理 0 个或最多 16384 个槽。\n16384 个槽都有节点在处理时，集群处于上线状态，相反的，如果有任何一个槽没有得到处理，那么集群处于下线状态。\n槽位的信息存储于每个节点中，当 Redis Cluster 的客户端来连接集群时，它也会得到一份集群的槽位配置信息。这样当客户端要查找某个 key 时， 可以直接定位到目标节点。\n客户端为了可以直接定位某个具体的 key 所在的节点，它就需要缓存槽位相关信息，这样才可以准确快速地定位到相应的节点。同时因 为槽位的信息可能会存在客户端与服务器不一致的情况，还需要纠正机制来实现槽位信息的校验调整。\nRedisCluster 的每个节点会将集群的配置信息持久化到配置文件中，所以必须确保配置文件是可写的，而且尽量不要依靠人工修改配置文件。\n槽位定位算法\r#\r\rCluster 默认会对 key 值使用 crc16 算法进行 hash 得到一个整数值，然后用这个整数值对 16384 进行取模来得到具体槽位。\nCluster 还允许用户强制某个 key 挂在特定槽位上，通过在 key 字符串里面嵌入 tag 标记，这就可以强制 key 所挂在的槽位等于 tag 所在的槽位。\n跳转\r#\r\r当客户端向一个错误的节点发出了指令，该节点会发现指令的 key 所在的槽位并不归自己管理，这时它会向客户端发送一个特殊的跳转指令携带目 标操作的节点地址，告诉客户端去连这个节点去获取数据。\nGET x -MOVED 3999 127.0.0.1:6381 MOVED 指令的第一个参数 3999 是 key 对应的槽位编号，后面是目标节点地址。MOVED 指令前面有一个减号，表示该指令是一个错误消息。\n客户端收到 MOVED 指令后，要立即纠正本地的槽位映射表。后续所有 key 将使用新的槽位映射表。\n在集群中执行命令\r#\r\r在对数据库的 16384 个槽都进行了指派之后，集群进入上线状态，客户端可以向集群发送指令了。\n当客户端向节点发送与数据库键有关的命令时，接受命令的节点会计算出这个 key 属于哪个槽，并检查这个槽是否指派给了自己：\n 如果 key 所在的槽在当前节点，那么节点直接执行命令。 如果 key 所在的槽没有指派给当前节点，那么节点会向客户端返回一个 MOVED 错诶，指引客户端转向正确的节点，再次发送之前想要执行的指令。  客户端收到 MOVED 指令后，要立即纠正本地的槽位映射表。后续所有 key 将使用新的槽位映射表。\n迁移\r#\r\rRedis Cluster 提供了工具 redis-trib 可以让运维人员手动调整槽位的分配情况，它使用 Ruby 语言进行开发，通过组合各种原 生的 Redis Cluster 指令来实现。\n\rRedis 迁移的单位是槽，Redis 一个槽一个槽进行迁移，当一个槽正在迁移时，这个槽就处于中间过渡状态。这个槽在原节点的状态为 migrating， 在目标节点的状态为 importing，表示数据正在从源流向目标。\n迁移过程：\n 迁移工具 redis-trib 首先会在源和目标节点设置好中间过渡状态， 一次性获取源节点槽位的所有 key 列表( keysinslot 指令，可以部分获取)，再挨个 key 进行迁移。 每个 key 的迁移过程是以源节点作为目标节点的「客户端」，源节点对当前的 key 执行 dump 指令得到序列化内容， 通过「客户端」向目标节点发送指令 restore 携带序列化的内容作为参数，目标节点再进行反序列化就可以将内容恢复到目标节点的内存中， 返回「客户端」OK，原节点「客户端」收到后再把当前节点的 key 删除掉就完成了单个 key 迁移的整个过程。  迁移过程是同步的，在目标节点执行 restore 指令到原节点删除 key 之间，原节点的主线程会处于阻塞状态，直到 key 被成功删除。\n如果迁移过程中突然出现网络故障，整个 slot 的迁移只进行了一半。这时两个节点依旧处于中间过渡状态。待下次迁移工具重新连上时，会提示用户继 续进行迁移。\n在迁移过程中，客户端访问的流程会有很大的变化：\n 新旧两个节点对应的槽位都存在部分 key 数据。客户端先尝试访问旧节点，如果对应的数据还在旧节点里面，那么旧节点正常处理。 如果对应的数据不在旧节点里面，那么有两种可能，要么该数据在新节点里，要么根本就不存在。旧节点不知道是哪种情况，所以它会向客 户端返回一个 -ASK targetNodeAddr 的重定向指令。客户端收到这个重定向指令后，先去目标节点执行一个不带任何参 数的 asking指令，然后在目标节点再重新执行原先的操作指令。  为什么需要执行一个不带参数的 asking 指令呢？\r#\r\r因为在迁移没有完成之前，按理说这个槽位还是不归新节点管理的，如果这个时候向目标节点发送该槽位的指令，节点是不认的，它会向客户端返回一 个 -MOVED 重定向指令告诉它去源节点去执行。如此就会形成重定向循环。asking 指令的目标就是打开目标节点的选项，告诉它下一条指 令不能不理，而要当成自己的槽位来处理。\n容错\r#\r\rRedis Cluster 可以为每个主节点设置若干个从节点，单主节点故障时，集群会自动将其中某个从节点提升为主节点。如果某个主节点没有从节点，那么 当它发生故障时，集群将完全处于不可用状态。不过 Redis 也提供了一个参数 cluster-require-full-coverage 可以允许部分节点故障，其它节 点还可以继续提供对外访问。\n网络抖动\r#\r\r真实世界的机房网络往往并不是风平浪静的，它们经常会发生各种各样的小问题。比如网络抖动就是非常常见的一种现象，突然之间部分连接变得不可 访问，然后很快又恢复正常。\n为解决这种问题，Redis Cluster 提供了一种选项 cluster-node-timeout，表示当某个节点持续 timeout 的时间失联时，才 可以认定该节点出现故障，需要进行主从切换。如果没有这个选项，网络抖动会导致主从频繁切换 (数据的重新复制)。\n槽位迁移感知\r#\r\r如果 Cluster 中某个槽位正在迁移或者已经迁移完了，client 如何能感知到槽位的变化呢？客户端保存了槽位和节点的映射关系表，它需要即时得 到更新，才可以正常地将某条指令发到正确的节点中。\n前面提到 Cluster 有两个特殊的指令，一个是 moved，一个是 asking：\n moved 是用来纠正槽位的。如果我们将指令发送到了错误的节点，该节点发现对应的指令槽位不归自己管理，就会将目标节点的地址随同 moved 指 令回复给客户端通知客户端去目标节点去访问。这个时候客户端就会刷新自己的槽位关系表，然后重试指令，后续所有打在该槽位的指令都会转到目标节点。 asking 指令是用来临时纠正槽位的。如果当前槽位正处于迁移中，指令会先被发送到槽位所在的旧节点，如果旧节点存在数据，那就直接返回 果了，如果不存在，那么它可能真的不存在也可能在迁移目标节点上。所以旧节点会通知客户端去新节点尝试一下拿数据，看看新节点有没有。这时候就 会给客户端返回一个 asking error 携带上目标节点的地址。客户端收到这个 asking error 后，就会去目标节点去尝试。客户端不会刷新槽位 映射关系表，因为它只是临时纠正该指令的槽位信息，不影响后续指令。  重试多次\r#\r\rmoved 和 asking 指令都是重试指令，客户端会因为这两个指令多重试一次。读者有没有想过会不会存在一种情况，客户端有可能重试 2 次呢？这种 情况是存在的，比如一条指令被发送到错误的节点，这个节点会先给你一个 moved 错误告知你去另外一个节点重试。所以客户端就去另外一个节点重 试了，结果刚好这个时候运维人员要对这个槽位进行迁移操作，于是给客户端回复了一个 asking 指令告知客户端去目标节点去重试指令。所以这里 客户端重试了 2 次。\n在某些特殊情况下，客户端甚至会重试多次，所以客户端的源码里在执行指令时都会有一个循环，然后会设置一个最大重试次数。当重试次数超过这个 值时，客户端会直接向业务层抛出异常。\n集群变更感知\r#\r\r当服务器节点变更时，客户端应该即时得到通知以实时刷新自己的节点关系表。那客户端是如何得到通知的呢？这里要分 2 种情况：\n 目标节点挂掉了，客户端会抛出一个 ConnectionError，紧接着会随机挑一个节点来重试，这时被重试的节点会通过 moved error 告知目 标槽位被分配到的新的节点地址。 运维手动修改了集群信息，将 master 切换到其它节点，并将旧的 master 移除集群。这时打在旧节点上的指令会收到一个 ClusterDown 的错误，告知当前节点所在集群不可用 。这时客户端就会关闭所有的连接，清空槽位映射关系表，然后向上层抛错。待下一条指令过来时，就会重新尝 试初始化节点信息。  "});})();