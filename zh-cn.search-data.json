{"/db-learn/docs/es/":{"data":{"":"","elasticsearch-介绍#Elasticsearch 介绍":"Elasticsearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。\nElasticsearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是一种流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。","elasticsearch-特性#Elasticsearch 特性":" 分布式：可以在多台机器上运行，每台机器上运行一个节点，每个节点都有全部的数据。 高可用：每个节点都可以独立运行，不依赖其他节点。 分片：每个节点都可以存储多个分片，每个分片都是一个 Lucene 索引。 复制：每个分片都可以有多个副本，每个副本都是一个分片的完整拷贝。 实时搜索：可以在很短的时间内返回搜索结果。 全文搜索：可以搜索文本中的单词，也可以搜索短语，也可以搜索通配符。 "},"title":"Elasticsearch"},"/db-learn/docs/mongo/":{"data":{"":"MongoDB 是一个文档数据库（以 JSON 为数据模型），C++ 编写，旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。\nMongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，数据格式是 BSON，一种类似 JSON 的二进制形式的存储格式，简称 Binary JSON，和 JSON 一样支持内嵌的文档对象和数组对象，因此可以存储比较复杂的数据类型。\nMongo 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。原则上 Oracle 和 MySQL 能做的事情，MongoDB 都能做（包括 ACID 事务）。","mongodb-vs-关系型数据库#MongoDB vs 关系型数据库":"MongoDB 概念与关系型数据库（RDBMS）非常类似：\n数据库（database）：最外层的概念，可以理解为逻辑上的名称空间，一个数据库包含多个不同名称的集合。 集合（collection）：相当于 SQL 中的表，一个集合可以存放多个不同的文档。 文档（document）：一个文档相当于数据表中的一行，由多个不同的字段组成。 字段（field）：文档中的一个属性，等同于列（column）。 索引（index）：独立的检索式数据结构，与 SQL 概念一致。 _id：每个文档中都拥有一个唯一的 _id 字段，相当于 SQL 中的主键（primary key）。 视图（view）：可以看作一种虚拟的（非真实存在的）集合，与 SQL 中的视图类似。从 MongoDB 3.4 版本开始 提供了视图功能，其通过聚合管道技术实现。 聚合操作（$lookup）：MongoDB 用于实现“类似”表连接（tablejoin）的聚合操作符。 MongoDB 与传统 RDBMS 仍然存在不少差异：\n半结构化，在一个集合中，文档所拥有的字段并不需要是相同的，而且也不需要对所用的字段进行声明。因此，MongoDB 具有很明显的半结构化特点。除了松散的表结构，文档还可以支持多级的嵌套、数组等灵活的数据类型，非常契合面向对象的编程模型。 弱关系，MongoDB 没有外键的约束，也没有非常强大的表连接能力。类似的功能需要使用聚合管道技术来弥补。 ","mongodb-的优势#MongoDB 的优势":"基于灵活的 JSON 文档模型，非常适合敏捷式的快速开发。\n简单直观：从错综复杂的关系模型到一目了然的对象模型。 快速：最简单快速的开发方式。JSON 结构和对象模型接近，开发代码量低。通常后端返回给前端的数据，都是 JSON 格式，基本上可以直接使用，不用再做转换。 灵活：快速响应业务变化，JSON 的动态模型意味着更容易响应新的业务需求。例如，业务里需要实现一个功能，在传统的关系型数据库中，可能需要建很多张表，关联表之类的，然后再关联查询，而在 MongoDB 中，只需要新加一个字段就可以了，这个字段可以是一个内嵌文档。 原生的高可用：复制集提供 99.999% 高可用。 原生的高水平扩展能力：分片架构支持海量数据和无缝扩容。 ℹ️ MongoDB 的集合结构可灵活的修改，可以在集合中添加一个 version 字段，给每个文档添加一个版本号，每当业务变更，对集合结构进行修改时，增加版本号。例如 1.0，1.1，1.2 等等，这样就可以实现版本控制。在后面业务可以根据版本号来进行过滤。 ","应用场景#应用场景":" 游戏场景，使用 MongoDB 存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、更新； 物流场景，使用 MongoDB 存储订单信息，订单状态在运送过程中会不断更新，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来； 社交场景，使用 MongoDB 存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能； 物联网场景，使用 MongoDB 存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多维度的分析； 视频直播，使用 MongoDB 存储用户信息、礼物信息等； 大数据应用，使用云数据库 MongoDB 作为大数据的云存储系统，随时进行数据提取分析。 当前业务是否适合使用 MongoDB 没有某个业务场景必须要使用 MongoDB 才能解决，但使用 MongoDB 通常能让你以更低的成本解决问题。如果你不清楚当前业务是否适合使用 MongoDB,可以通过做几道选择题来辅助决策。\n只要有一项需求满足就可以考虑使用 MongoDB，匹配越多，选择 MongoDB 越合适。基本上 MongoDB 已经可以取代 MySQL 了。\nMongoDB VS MySQL 性能：\nMongoDB 的性能要优于 MySQL，MongoDB 天然支持分片集群，天然支持高可用（复制集）。\nMySQL 的集群架构比较麻烦，做分库分表还需要中间件，而且处理海量数据的性能很差。MySQL 的定位就是一个单机数据库。\n开发：\nMongoDB 灵活的 schema，添加字段，添加内嵌文档无需修改，不会影响其他文档。非常适合敏捷开发。\nMySQL 加一个字段需要修改表结构，如果要像 MongoDB 那样添加一个内嵌文档，可能需要一个关联表，一个新的表来存储内嵌文档。"},"title":"MongoDB"},"/db-learn/docs/mongo/advanced/01_index/":{"data":{"":"MongoDB 索引数据结构是 B-Tree 还是 B+Tree？\n先说结论，是 B+Tree。\n为什么会产生这个问题呢？\n起源于早期的官方文档的一句话 MongoDB indexes use a B-tree data structure.，然后就导致了分歧：有人说 MongoDB 索引数据结构使用的是 B-Tree,有的人又说是 B+Tree。\nMongoDB 从 3.2 开始就默认使用 WiredTiger 作为存储引擎，所以 MongoDB 内部存储的数据结构由 WiredTiger 决定。而 WiredTiger 官方文档明确说了底层用的 B+Tree。\nWiredTiger 官方文档：https://source.wiredtiger.com/3.0.0/tune_page_size_and_comp.html\nWiredTiger maintains a table’s data in memory using a data structure called a B-Tree ( B+ Tree to be specific), referring to the nodes of a B-Tree as pages. Internal pages carry only keys. The leaf pages store both keys and values.","explain-执行计划#Explain 执行计划":"需要关心的问题：\n查询是否使用了索引 索引是否减少了扫描的记录数量 是否存在低效的内存排序 MongoDB 提供了 explain 命令，它可以评估指定查询模型（querymodel）的执行计划，根据实际情况进行调整，然后提高查询效率。\ndb.collection.find().explain(\u003cverbose\u003e) verbose 可选参数，表示执行计划的输出模式，默认 queryPlanner。 queryPlanner：执行计划的详细信息，包括查询计划、集合信息、查询条件、最佳执行计划、查询方式和 MongoDB 服务信息等。 exectionStats：最佳执行计划的执行情况和被拒绝的计划等信息。 allPlansExecution：选择并执行最佳执行计划，并返回最佳执行计划和其他执行计划的执行情况。 queryPlanner // title 字段无索引 db.books.find({title:\"book-1\"}).explain(\"queryPlanner\") 输出结果：\n{ \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.books\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"title\" : { \"$eq\" : \"book-1\" } }, \"winningPlan\" : { \"stage\" : \"COLLSCAN\", // 全表扫描 \"direction\" : \"forward\" }, \"rejectedPlans\" : [] } // ... } 字段名称 描述 plannerVersion 执行计划的版本 namespace 查询的集合 indexFilterSet 是否使用索引 parsedQuery 查询条件 winningPlan 最佳执行计划 stage 查询方式 filter 过滤条件 direction 查询顺序 rejectedPlans 拒绝的执行计划 serverInfo mongodb 服务器信息 exectionStats executionStats 模式的返回信息中包含了 queryPlanner 模式的所有字段，并且还包含了最佳执行计划的执行情况。\n// 创建索引 db.books.createIndex({title:1}) db.books.find({title:\"book-1\"}).explain(\"executionStats\") 运行结果：\n{ \"queryPlanner\" : { // ... \"winningPlan\" : { \"stage\" : \"FETCH\", // 使用索引 \"inputStage\" : { \"stage\" : \"IXSCAN\", // 索引扫描 \"keyPattern\" : { \"title\" : 1 }, \"indexName\" : \"title_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"title\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"title\" : [ \"[\\\"book-1\\\", \\\"book-1\\\"]\" ] } } }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1, // 返回记录数 \"executionTimeMillis\" : 1, // 执行时间 \"totalKeysExamined\" : 1, // 索引扫描的次数 \"totalDocsExamined\" : 1, // 扫描的文档数 // ... } } 字段名称 描述 winningPlan.inputStage 用来描述子 stage，并且为其父 stage 提供文档和索引关键字 winningPlan.inputStage.stage 子查询方式 winningPlan.inputStage.keyPattern 所扫描的 index 内容 winningPlan.inputStage.indexName 索引名称 winningPlan.inputStage.isMultiKey 是否是多键索引。如果索引建立在 array 上，则是 true executionStats.executionSuccess 是否执行成功 executionStats.nReturned 返回记录数 executionStats.executionTimeMillis 语句执行时间 executionStats.executionStages.executionTimeMillisEstimate 检索文档获取数据的时间 executionStats.totalKeysExamined 索引扫描的次数 executionStats.totalDocsExamined 扫描的文档数 executionStats.executionStages.isEOF 是否到达 steam 结尾，1 或者 true 代表已到达结尾 executionStats.executionStages.works 工作单元数，一个查询会分解成小的工作单元 executionStats.executionStages.advanced 优先返回的结果数 executionStats.executionStages.docsExamined 文档检查数 allPlansExecution allPlansExecution 返回的信息包含 executionStats 模式的内容，且包含 allPlansExecution:[] 块\n\"allPlansExecution\" : [ { \"nReturned\" : \u003cint\u003e, \"executionTimeMillisEstimate\" : \u003cint\u003e, \"totalKeysExamined\" : \u003cint\u003e, \"totalDocsExamined\" :\u003cint\u003e, \"executionStages\" : { \"stage\" : \u003cSTAGEA\u003e, \"nReturned\" : \u003cint\u003e, \"executionTimeMillisEstimate\" : \u003cint\u003e, ... } } }, ... ] stage 状态 状态 描述 COLLSCAN 全表扫描 IXSCAN 索引扫描 FETCH 根据索引检索指定文档 SHARD_MERGE 将各个分片返回数据进行合并 SORT 在内存中进行了排序 LIMIT 使用 limit 限制返回数 SKIP 使用 skip 进行跳过 IDHACK 对 _id 进行查询 SHARDING_FILTER 通过 mongos 对分片数据进行查询 COUNTSCAN count 不使用 Index 进行 count 时的 stage 返回 COUNT_SCAN count 使用了 Index 进行 count 时的 stage 返回 SUBPLA 未使用到索引的 $or 查询的 stage 返回 TEXT 使用全文索引进行查询时候的 stage 返回 PROJECTION 限定返回字段时候 stage 的返回 执行计划的返回结果中尽量不要出现以下 stage:\nCOLLSCAN (全表扫描) SORT (使用 sort 但是无 index) 不合理的 SKIP SUBPLA (未用到 index 的 $or) COUNTSCAN (不使用 index 进行 count) ","mongodb-索引与-mysql-索引的对比#MongoDB 索引与 MySQL 索引的对比":"MongoDB 的聚簇索引虽然也是使用的 B+ 树，但是它的叶子节点中只存储主键 ID 和 RecordId（文件偏移量），不存储完整的行记录。通过 RecordId 再直接访问数据文件来获取完整文档。\n这种方式理论上这会导致磁盘随机 I/O。\nMongoDB 的优化策略 MongoDB 比较激进的缓存策略，默认会占用 50% 可用内存，最近访问的文档会保留在内存中，热数据基本不会触发实际磁盘 I/O。 CheckPoint 机制默认 60 秒才会将数据刷入磁盘，写入时合并修改，减少随机写入。 空间重用：删除/更新产生的空闲空间会被记录，新插入文档优先使用这些空间，保持一定程度的物理连续性。 数据文件预分配（默认按 2GB 增量增长），减少文件碎片化。 查询优化，一次索引查询获取多个 RecordId 后，会按照物理位置排序后再读取。 覆盖查询 硬件层优化建议 使用 SSD：完全消除磁头移动开销 增加内存：扩大 WiredTiger 缓存 RAID配置：提高并行 I/O 能力 ","索引实践#索引实践":"为每一个查询建立合适的索引 这个是针对于数据量较大比如说超过几十上百万（文档数目）数量级的集合。如果没有索引 MongoDB 需要把所有的 Document 从盘上读到内存，这会对 MongoDB 服务器造成较大的压力并影响到其他请求的执行。\n创建合适的复合索引，不要依赖于交叉索引 如果你的查询会使用到多个字段，MongoDB 有两个索引技术可以使用：交叉索引和复合索引。交叉索引就是针对每个字段单独建立一个单字段索引，然后在查询执行时候使用相应的单字段索引进行索引交叉而得到查询结果。交叉索引目前触发率较低，所以如果你有一个多字段查询的时候，建议使用复合索引能够保证索引正常的使用。\n// 查找所有年龄小于30岁的深圳市马拉松运动员 db.athelets.find({sport: \"marathon\", location: \"sz\", age: {$lt: 30}}}) // 创建复合索引 db.athelets.createIndex({sport:1, location:1, age:1}) 复合索引字段顺序 复合索引字段顺序：匹配条件在前，范围条件在后（Equality First, Range After）。\n前面的例子，在创建复合索引时如果条件有匹配和范围之分，那么匹配条件 (sport: \"marathon\") 应该在复合索引的前面。范围条件 (age: \u003c 30) 字段应该放在复合索引的后面。\n尽可能使用覆盖索引（Covered Index） 建议只返回需要的字段，同时，利用覆盖索引来提升性能。\n建索引要在后台运行 在对一个集合创建索引时，该集合所在的数据库将不接受其他读写操作。对大数据量的集合建索引，建议使用后台运行选项 {background: true}。\n避免设计过长的数组索引 数组索引是多值的，在存储时需要使用更多的空间。如果索引的数组长度特别长，或者数组的增长不受控制，则可能导致索引空间急剧膨胀。","索引属性#索引属性":"唯一索引（Unique Indexes） 在现实场景中，唯一性是很常见的一种索引约束需求，重复的数据记录会带来许多处理上的麻烦，比如订单的编号、用户的登录名等。通过建立唯一性索引，可以保证集合中文档的指定字段拥有唯一值。\n// 创建唯一索引 db.values.createIndex({title:1},{unique:true}) // 复合索引支持唯一性约束 db.values.createIndex({title:1，type:1},{unique:true}) // 多键索引支持唯一性约束 db.inventory.createIndex( { ratings: 1 },{unique:true} ) 唯一性索引对于文档中缺失的字段，会使用 null 值代替，因此不允许存在多个文档缺失索引字段的情况。 对于分片的集合，唯一性约束必须匹配分片规则。换句话说，为了保证全局的唯一性，分片键必须作为唯一性索引的前缀字段。 部分索引（Partial Indexes） 部分索引仅包含集合中满足指定过滤条件的文档，其他文档不会被包含在索引中。部分索引具有更低的存储需求和更低的索引创建和维护的性能成本。3.2 新版功能。\n部分索引提供了稀疏索引功能的超集，应该优先于稀疏索引。\n例如：\ndb.restaurants.createIndex( { cuisine: 1, name: 1 }, // 索引字段 { partialFilterExpression: { rating: { $gt: 5 } } } // 增加过滤条件，只对符合条件的文档才会加索引 ) partialFilterExpression 选项接受指定过滤条件的文档:\n$eq：等于 $gt：大于 $gte：大于等于 $lt：小于 $lte：小于等于 $type：类型 顶层的 $and // 符合条件，使用索引 db.restaurants.find( { cuisine: \"Italian\", rating: { $gte: 8 } } ) // 不符合条件，不能使用索引 db.restaurants.find( { cuisine: \"Italian\" } ) 案例 准备数据：\ndb.restaurants.insert({ \"_id\" : ObjectId(\"5641f6a7522545bc535b5dc9\"), \"address\" : { \"building\" : \"1007\", \"coord\" : [ -73.856077, 40.848447 ], \"street\" : \"Morris Park Ave\", \"zipcode\" : \"10462\" }, \"borough\" : \"Bronx\", \"cuisine\" : \"Bakery\", \"rating\" : { \"date\" : ISODate(\"2014-03-03T00:00:00Z\"), \"grade\" : \"A\", \"score\" : 2 }, \"name\" : \"Morris Park Bake Shop\", \"restaurant_id\" : \"30075445\" }) // 创建索引 db.restaurants.createIndex( { borough: 1, cuisine: 1 }, { partialFilterExpression: { 'rating.grade': { $eq: \"A\" } } } ) 查询：\n// 符合条件，使用索引 db.restaurants.find( { borough: \"Bronx\", 'rating.grade': \"A\" } ) // 不符合条件，不能使用索引 db.restaurants.find( { borough: \"Bronx\", cuisine: \"Bakery\" } ) 唯一约束结合部分索引使用导致唯一约束失效的问题 如果同时指定了 partialFilterExpression 和唯一约束，那么唯一约束只适用于满足筛选器表达式的文档。如果文档不满足筛选条件，那么带有惟一约束的部分索引不会阻止插入不满足惟一约束的文档。\n准备数据：\ndb.users.insertMany( [ { username: \"david\", age: 29 }, { username: \"amanda\", age: 35 }, { username: \"rajiv\", age: 57 } ] ) // 创建索引，指定 username 字段 // 唯一约束 // 部分过滤器表达式 age: {$gte: 21}，表示只有年龄大于等于 21 的文档才会有索引 // 因此，只有年龄大于等于 21 的文档并且 username 是唯一的，才会有索引 db.users.createIndex( { username: 1 }, { unique: true, partialFilterExpression: { age: { $gte: 21 } } } ) 索引防止了以下文档的插入，因为文档 username 都已经存在，虽然满足了年龄字段大于 21 的条件:\ndb.users.insertMany( [ { username: \"david\", age: 27 }, { username: \"amanda\", age: 25 }, { username: \"rajiv\", age: 32 } ]) 但是，以下具有重复用户名的文档是允许的，因为唯一约束只适用于年龄大于或等于 21 岁的文档。也就是说下面的数据不满足上面的索引的条件，所以不会受到索引的约束：\ndb.users.insertMany( [ { username: \"david\", age: 20 }, { username: \"amanda\" }, { username: \"rajiv\", age: null } ]) 稀疏索引（Sparse Indexes） 索引的稀疏属性确保索引只包含具有索引字段的文档的条目，索引将跳过没有索引字段的文档。\n特性：只对存在字段的文档进行索引（包括字段值为 null 的文档）\n// 不索引不包含 xmpp_id 字段的文档 db.addresses.createIndex( { \"xmpp_id\": 1 }, { sparse: true } ) ℹ️ 如果稀疏索引会导致查询和排序操作的结果集不完整，MongoDB 将不会使用该索引，除非使用 hint() 明确指定索引。 案例 准备数据：\ndb.scores.insertMany([ {\"userid\" : \"newbie\"}, {\"userid\" : \"abby\", \"score\" : 82}, {\"userid\" : \"nina\", \"score\" : 90} ]) // 创建稀疏索引 db.scores.createIndex( { score: 1 } , { sparse: true } ) 查询：\n// 使用稀疏索引 db.scores.find( { score: { $lt: 90 } } ) // 即使排序是通过索引字段，MongoDB 也不会选择稀疏索引来完成查询，以返回完整的结果 db.scores.find().sort( { score: -1 } ) // 要使用稀疏索引，使用 hint() 显式指定索引 db.scores.find().sort( { score: -1 } ).hint( { score: 1 } ) 同时具有稀疏性和唯一性的索引 同时具有稀疏性和唯一性的索引可以防止集合中存在字段值重复的文档，但允许不包含此索引字段的文档插入。\ndb.scores.dropIndex({score:1}) // 创建具有唯一约束的稀疏索引 db.scores.createIndex( { score: 1 } , { sparse: true, unique: true } ) 这个索引将允许插入具有唯一的分数字段值或不包含分数字段的文档。因此，给定 scores 集合中的现有文档，索引允许以下插入操作:\ndb.scores.insertMany( [ { \"userid\": \"AAAAAAA\", \"score\": 50 }, { \"userid\": \"BBBBBBB\", \"score\": 64 }, { \"userid\": \"CCCCCCC\" }, { \"userid\": \"CCCCCCC\" } ] ) 索引不允许添加下列文件，因为已经存在评分为 50 的文档：\ndb.scores.insertMany( [ { \"userid\": \"DDDDDDD\", \"score\": 50 }, ]) TTL 索引（TTL Indexes） 在一般的应用系统中，并非所有的数据都需要永久存储。例如一些系统事件、用户消息等，这些数据随着时间的推移，其重要程度逐渐降低。更重要的是，存储这些大量的历史数据需要花费较高的成本，因此项目中通常会对过期且不再使用的数据进行老化处理。\n通常有两种方案：\n为每个数据记录一个时间戳，应用侧开启一个定时器，按时间戳定期删除过期的数据。 数据按日期进行分表，同一天的数据归档到同一张表，同样使用定时器删除过期的表。 对于数据老化，MongoDB 提供了一种更加便捷的做法：TTL（Time To Live）索引。TTL 索引需要声明在一个日期类型的字段中，TTL 索引是特殊的单字段索引，MongoDB 可以使用它在一定时间或特定时钟时间后自动从集合中删除文档。\n// 创建 TTL 索引，TTL 值为 3600 秒 db.eventlog.createIndex( { \"lastModifiedDate\": 1 }, { expireAfterSeconds: 3600 } ) 对集合创建 TTL 索引之后，MongoDB 会在周期性运行的后台线程中对该集合进行检查及数据清理工作。除了数据老化功能，TTL 索引具有普通索引的功能，同样可以用于加速数据的查询。\nTTL 索引不保证过期数据会在过期后立即被删除。文档过期和 MongoDB 从数据库中删除文档的时间之间可能存在延迟。删除过期文档的后台任务每 60 秒运行一次。因此，在文档到期和后台任务运行之间的时间段内，文档可能会保留在集合中。\n案例 准备数据：\ndb.log_events.insertOne( { \"createdAt\": new Date(), \"logEvent\": 2, \"logMessage\": \"Success!\" }) // 创建 TTL 索引 db.log_events.createIndex( { \"createdAt\": 1 }, { expireAfterSeconds: 20 } ) 可变的过期时间 TTL 索引在创建之后，仍然可以对过期时间进行修改。这需要使用 collMod 命令对索引的定义进行变更：\ndb.runCommand({collMod:\"log_events\",index:{keyPattern:{createdAt:1},expireAfterSeconds:600}}) TTL 索引的限制 TTL 索引的确可以减少开发的工作量，而且通过数据库自动清理的方式会更加高效、可靠，但是在使用 TTL 索引时需要注意以下的限制：\nTTL 索引只能支持单个字段，并且必须是非 _id 字段。 TTL 索引不能用于固定集合。 TTL 索引无法保证及时的数据老化，MongoDB 会通过后台的 TTL Monitor 定时器来清理老化数据，默认的间隔时间是 1 分钟。当然如果在数据库负载过高的情况下，TTL 的行为则会进一步受到影响。 TTL 索引对于数据的清理仅仅使用了 remove 命令，这种方式并不是很高效。因此 TTL Monitor 在运行期间对系统 CPU、磁盘都会造成一定的压力。相比之下，按日期分表的方式操作会更加高效。 隐藏索引（Hidden Indexes） 隐藏索引对查询规划器不可见，不能用于支持查询。通过对规划器隐藏索引，用户可以在不实际删除索引的情况下评估删除索引的潜在影响。如果影响是负面的，用户可以取消隐藏索引，而不必重新创建已删除的索引。4.4 新版功能。\n// 创建隐藏索引 db.restaurants.createIndex({ borough: 1 },{ hidden: true }); // 隐藏现有索引 db.restaurants.hideIndex( { borough: 1} ); db.restaurants.hideIndex( \"索引名称\" ) // 取消隐藏索引 db.restaurants.unhideIndex( { borough: 1} ); db.restaurants.unhideIndex( \"索引名 案例 db.scores.insertMany([ {\"userid\" : \"newbie\"}, {\"userid\" : \"abby\", \"score\" : 82}, {\"userid\" : \"nina\", \"score\" : 90} ]) // 创建隐藏索引 db.scores.createIndex( { userid: 1 }, { hidden: true } ) // 查看索引信息 db.scores.getIndexes() [ // ... { \"v\": 2, \"hidden\": true, \"key\": { \"userid\": 1 } // ... } ] 查询：\n// 不使用索引 db.scores.find({userid:\"abby\"}).explain() // 取消隐藏索引 db.scores.unhideIndex( { userid: 1} ) // 使用索引 db.scores.find({userid:\"abby\"}).explain() ","索引操作#索引操作":"创建索引 db.collection.createIndex(keys, options) keys：指定要创建索引的字段，可以是单个字段或多个字段。如果是多个字段，那么就会创建一个复合索引。1 按升序创建索引， -1 按降序创建索引。 options：可选参数。 可选参数列表：\n参数名 类型 描述 background 布尔值 建索引过程会阻塞其它数据库操作，background 可指定以后台方式创建索引。 background 默认值为 false。 unique 布尔值 是否创建唯一索引，默认为 false。 name 字符串 索引的名称，如果未指定，MongoDB 的通过连接索引的字段名和排序顺序生成一个索引名称。 sparse 布尔值 对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为 true 的话，在索引字段中不会查询出不包含对应字段的文档。默认值为 false。 expireAfterSeconds 数字 指定一个以秒为单位的数值，完成 TTL 设定，设定集合的生存时间。 v 数字 指定索引的版本号，默认的索引版本取决于 mongod 创建索引时运行的版本。 dropDups 布尔值 3.0 版本已废弃。在建立唯一索引时是否删除重复记录，指定 true 创建唯一索引。默认值为 false。 weights 对象 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。 default_language 字符串 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语 language_override 字符串 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language。 3.0.0 版本前创建索引方法为 db.collection.ensureIndex()。\n// 创建索引后台执行 db.values.createIndex({open: 1, close: 1}, {background: true}) // 创建唯一索引 db.values.createIndex({title:1},{unique:true}) 查看索引 // 查看索引信息 db.books.getIndexes() // 查看索引键 db.books.getIndexKeys() 删除索引 // 删除索引 db.collection.dropIndex(indexName) // 删除所有索引，不能删除主键索引 db.collection.dropIndexes() ","索引类型#索引类型":"MongoDB 支持各种丰富的索引类型，包括单键索引、复合索引，唯一索引等一些常用的结构。由于采用了灵活可变的文档类型，因此它也同样支持对嵌套字段、数组进行索引。通过建立合适的索引，可以极大地提升数据的检索速度。在一些特殊应用场景，MongoDB 还支持地理空间索引、文本检索索引、TTL 索引等不同的特性。\n单键索引（Single Field Indexes） 在某一个特定的字段上建立索引。MongoDB 在 ID 上建立了唯一的单键索引,所以经常会使用 id 来进行查询； 在索引字段上进行精确匹配、排序以及范围查找都会使用此索引。\ndb.books.createIndex({title:1}) // 升序索引 // 对内嵌文档的字段进行索引 db.books.createIndex({\"author.name\":1}) 复合索引（Compound Index） 复合索引是多个字段组合而成的索引，其性质和单字段索引类似。但不同的是，复合索引中字段的顺序、字段的升降序对查询性能有直接的影响，因此在设计复合索引时则需要考虑不同的查询场景。\ndb.books.createIndex({type:1,favCount:1}) // 查看执行计划 db.books.find({type:\"novel\",favCount:{$gt:50}}).explain() 多键(数组)索引（Multikey Index） 在数组的属性上建立索引。针对这个数组的任意值的查询都会定位到这个文档,既多个索引入口或者键值引用同一个文档。\n准备数据：\ndb.inventory.insertMany([ { _id: 5, type: \"food\", item: \"aaa\", ratings: [ 5, 8, 9 ] }, { _id: 6, type: \"food\", item: \"bbb\", ratings: [ 5, 9 ] }, { _id: 7, type: \"food\", item: \"ccc\", ratings: [ 9, 5, 8 ] }, { _id: 8, type: \"food\", item: \"ddd\", ratings: [ 9, 5 ] }, { _id: 9, type: \"food\", item: \"eee\", ratings: [ 5, 9, 5 ] } ]) // 创建多键索引 db.inventory.createIndex( { ratings: 1 } ) ℹ️ 多键索引很容易与复合索引产生混淆，复合索引是多个字段的组合，而多键索引则仅仅是在一个字段上出现了多键（multi key）。而实质上，多键索引也可以出现在复合字段上。 // 创建复合多值索引 db.inventory.createIndex( { item:1,ratings: 1 } ) MongoDB 并不支持一个复合索引中同时出现多个数组字段 在包含嵌套对象的数组字段上创建多键索引：\ndb.inventory.insertMany([ { _id: 1, item: \"abc\", stock: [ { size: \"S\", color: \"red\", quantity: 25 }, { size: \"S\", color: \"blue\", quantity: 10 }, { size: \"M\", color: \"blue\", quantity: 50 } ] }, { _id: 2, item: \"def\", stock: [ { size: \"S\", color: \"blue\", quantity: 20 }, { size: \"M\", color: \"blue\", quantity: 5 }, { size: \"M\", color: \"black\", quantity: 10 }, { size: \"L\", color: \"red\", quantity: 2 } ] }, { _id: 3, item: \"ijk\", stock: [ { size: \"M\", color: \"blue\", quantity: 15 }, { size: \"L\", color: \"blue\", quantity: 100 }, { size: \"L\", color: \"red\", quantity: 25 } ] } ]) // 在包含嵌套对象的数组字段上创建多键索引 db.inventory.createIndex( { \"stock.size\": 1, \"stock.quantity\": 1 } ) db.inventory.find({\"stock.size\":\"S\",\"stock.quantity\":{$gt:20}}).explain() Hash 索引（Hashed Index） 不同于传统的 BTree 索引,哈希索引使用 hash 函数来创建索引。在索引字段上进行精确匹配，但不支持范围查询，不支持多键 hash，Hash 索引上的入口是均匀分布的，在分片集合中非常有用。\ndb.users.createIndex({username : 'hashed'}) 对于用户名，电话号码之类的字段，可以使用哈希索引。哈希索引的主要优势在于数据分布更均匀。\n地理空间索引（Geospatial Index） MongoDB 为地理空间检索提供了非常方便的功能。地理空间索引（2dsphereindex）就是专门用于实现位置检索的一种特殊索引。\n案例 如何实现“查询附近商家\"？\n假设商家的数据模型如下：\ndb.restaurant.insert({ restaurantId: 0, restaurantName:\"兰州牛肉面\", location : { type: \"Point\", coordinates: [ -73.97, 40.77 ] } }) 创建一个 2dsphere 索引：\ndb.restaurant.createIndex({location : \"2dsphere\"}) 查询附近 10000 米商家信息：\ndb.restaurant.find( { location:{ $near :{ $geometry :{ type : \"Point\" , coordinates : [ -73.88, 40.78 ] } , $maxDistance: 10000 } } }) $near：查询操作符，用于实现附近商家的检索，返回数据结果会按距离排序。 $geometry：操作符用于指定一个 GeoJSON 格式的地理空间对象，type=Point 表示地理坐标点，coordinates 则是用户当前所在的经纬度位置； $maxDistance：限定了最大距离，单位是米。 全文索引（Text Indexes） MongoDB 支持全文检索功能，可通过建立文本索引来实现简易的分词检索。\ndb.reviews.createIndex( { comments: \"text\" } ) $text 操作符可以在有 text index 的集合上执行文本检索。$text 将会使用空格和标点符号作为分隔符对检索字符串进行分词，并且对检索字符串中所有的分词结果进行一个逻辑上的 OR 操作。\n全文索引能解决快速文本查找的需求，比如有一个博客文章集合，需要根据博客的内容来快速查找，则可以针对博客内容建立文本索引。\n案例 准备数据：\ndb.stores.insertMany( [ { _id: 1, name: \"Java Hut\", description: \"Coffee and cakes\" }, { _id: 2, name: \"Burger Buns\", description: \"Gourmet hamburgers\" }, { _id: 3, name: \"Coffee Shop\", description: \"Just coffee\" }, { _id: 4, name: \"Clothes Clothes Clothes\", description: \"Discount clothing\" }, { _id: 5, name: \"Java Shopping\", description: \"Indonesian goods\" } ] ) // 创建 name 和 description 的全文索引 db.stores.createIndex({name: \"text\", description: \"text\"}) 通过 $text 操作符来查寻数据中所有包含 “coffee”,”shop”，“java” 列表中任何词语的商店：\ndb.stores.find({$text: {$search: \"java coffee shop\"}}) MongoDB 的文本索引功能存在诸多限制，而官方并未提供中文分词的功能，这使得该功能的应用场景十分受限。\n通配符索引（Wildcard Indexes） MongoDB 的文档模式是动态变化的，而通配符索引可以建立在一些不可预知的字段上，以此实现查询的加速。MongoDB 4.2 引入了通配符索引来支持对未知或任意字段的查询。\n案例 准备商品数据，不同商品属性不一样：\ndb.products.insert([ { \"product_name\" : \"Spy Coat\", \"product_attributes\" : { // 每个商品的 product_attributes 中包含的属性是不一样的 \"material\" : [ \"Tweed\", \"Wool\", \"Leather\" ], \"size\" : { \"length\" : 72, \"units\" : \"inches\" } } }, { \"product_name\" : \"Spy Pen\", \"product_attributes\" : { \"colors\" : [ \"Blue\", \"Black\" ], \"secret_feature\" : { \"name\" : \"laser\", \"power\" : \"1000\", \"units\" : \"watts\", } } }, { \"product_name\" : \"Spy Book\" } ]) // 创建通配符索引 // $** 表示任意字段 db.products.createIndex( { \"product_attributes.$**\" : 1 } ) 通配符索引可以支持任意单字段查询 product_attributes 或其嵌入字段：\ndb.products.find( { \"product_attributes.size.length\": { $gt : 60 } } ) db.products.find( { \"product_attributes.material\": \"Leather\" } ) db.products.find( { \"product_attributes.secret_feature.name\": \"laser\" } ) 注意：\n通配符索引不兼容的索引类型或属性：\nCompound TTL Hashed 2dsphere Text Unique 通配符索引不能支持查询字段不存在的文档。因为通配符索引是稀疏的，稀疏索引只索引集合中存在该字段的文档，对于缺少该字段或该字段值为 null 的文档，索引中不会有对应的条目。\n// 通配符索引不能支持以下查询 db.products.find( {\"product_attributes\" : { $exists : false } } ) db.products.aggregate([ { $match : { \"product_attributes\" : { $exists : false } } } ]) 通配符索引会为文档或数组的内容生成条目，而是文档或数组本身。因此通配符索引不能支持精确的文档/数组相等匹配。通配符索引可以支持查询字段等于空文档 {} 的情况。 // 精确匹配查数组中的其中一个条目，这条查询是支持的 db.products.find({ \"product_attributes.colors\" : \"Blue\" } ) // 通配符索引不能支持以下查询 // 匹配数组中的所有条目，这条查询是不支持的 db.products.find({ \"product_attributes.colors\" : [ \"Blue\", \"Black\" ] } ) db.products.aggregate([{ $match : { \"product_attributes.colors\" : [ \"Blue\", \"Black\" ] } }]) "},"title":"索引"},"/db-learn/docs/mongo/advanced/02_cluster_rs/":{"data":{"":"","三节点复制集模式#三节点复制集模式":"","复制集介绍#复制集介绍":"","复制集原理#复制集原理":"高可用 选举 MongoDB 的复制集选举使用 Raft 算法 来实现，选举成功的必要条件是大多数投票节点存活。在具体的实现中，MongoDB 对 Raft 协议添加了一些自己的扩展，这包括：\n支持 chainingAllowed 链式复制，即备节点不只是从主节点上同步数据，还可以选择一个离自己最近（心跳延时最小）的节点来复制数据。因为可能两个从节点是在一个数据中心，而主节点在另一个数据中心，所以可以选择离自己最近的节点来复制数据，这样可以减少网络延时。 增加了预投票阶段，即 preVote，这主要是用来避免网络分区时产生 Term (任期) 值激增的问题（一般使用 Raft 算法的中间件都要优化这个点）。任期激增问题是指： 当网络分区时，如果一个分区只有两个节点，那么这两个节点是无法选举的，因为它们都无法获得大多数的投票。这会导致这两个节点会不停的重新发起选举，Term (任期) 值会不断增加，直到网络恢复。 预投票机制就是指在网络分区时，节点会先发起预投票，先看一下能不能拿到大多数的投票，如果能拿到，那么就会发起正式的投票，否则就不会发起正式的投票。这样就可以避免 Term (任期) 值激增的问题。 支持投票优先级，如果备节点发现自己的优先级比主节点高，则会主动发起投票并尝试成为新的主节点。例如，在同一个节点，如果主节点挂了，通过设置的优先级可以让同一个机房的备节点成为新的主节点。 一个复制集最多可以有 50 个成员，但只有 7 个投票成员。这是因为一旦过多的成员参与数据复制、投票过程，将会带来更多可靠性方面的问题。\n当复制集内存活的成员数量不足大多数时，整个复制集将无法选举出主节点，此时无法提供写服务，这些节点都将处于只读状态。此外，如果希望避免平票结果的产生，最好使用奇数个节点成员，比如 3 个或 5 个。当然，在 MongoDB 复制集的实现中，对于平票问题已经提供了解决方案：\n为选举定时器增加少量的随机时间偏差，这样避免各个节点在同一时刻发起选举，提高成功率。 使用仲裁者角色，该角色不做数据复制，也不承担读写业务，仅仅用来投票。 ℹ️ 在 Raft 协议中，任期是一个关键概念，它代表了一次选举的周期。每个节点都会记录一个任期号，任期号单调递增。当节点参与选举时，任期号较大的节点会被优先接受为领导者，这有助于解决选举过程中的冲突问题‌。 自动故障转移 在故障转移场景中，我们所关心的问题是：\n备节点是怎么感知到主节点已经发生故障的？ 如何降低故障转移对业务产生的影响？ 一个影响检测机制的因素是心跳，在复制集组建完成之后，各成员节点会开启定时器，持续向其他成员发起心跳，这里涉及的参数为 heartbeatIntervalMillis，即心跳间隔时间，默认值是 2s。如果心跳成功，则会持续以 2s 的频率继续发送心跳；如果心跳失败，则会立即重试心跳，一直到心跳恢复成功。\n另一个重要的因素是选举超时检测，一次心跳检测失败并不会立即触发重新选举。实际上除了心跳，成员节点还会启动一个选举超时检测定时器，该定时器默认以 10s 的间隔执行，具体可以通过 electionTimeoutMillis 参数指定：\n如果心跳响应成功，则取消上一次的 electionTimeout 调度（保证不会发起选举），并发起新一轮 electionTimeout 调度。 如果心跳响应迟迟不能成功，那么 electionTimeout 任务被触发，进而导致备节点发起选举并成为新的主节点。 在 MongoDB 的实现中，选举超时检测的周期要略大于 electionTimeoutMillis 设定。该周期会加入一个随机偏移量，大约在 10～11.5s，如此的设计是为了错开多个备节点主动选举的时间，提升成功率。\nℹ️ 因此，在 electionTimeout 任务中触发选举必须要满足以下条件：\n当前节点是备节点。 当前节点具备选举权限。 在检测周期内仍然没有与主节点心跳成功。 业务影响评估 在复制集发生主备节点切换的情况下，会出现短暂的无主节点阶段，此时无法接受业务写操作（访问瞬断）。如果是因为主节点故障导致的切换，则对于该节点的所有读写操作都会产生超时。如果使用 MongoDB 3.6 及以上版本的驱动，则可以通过开启 retryWrite 来降低影响。 # MongoDB Drivers 启用可重试写入 mongodb://localhost/?retryWrites=true # mongo shell mongosh --retryWrites 如果主节点属于强制掉电，那么整个 Failover 过程将会变长，很可能需要在 Election 定时器超时后才被其他节点感知并恢复，这个时间窗口一般会在 12s 以内。然而实际上，对于业务呼损的考量还应该加上客户端或 mongos 对于复制集角色的监视和感知行为（真实的情况可能需要长达 30s 以上）。 对于非常重要的业务，建议在业务层面做一些防护策略，比如设计重试机制。 优雅的重启复制集 重启复制集，不能直接把主节点干掉，因为会导致再次选举一个主节点。\n如果想不丢数据重启复制集，更优雅的打开方式应该是这样的：\n逐个重启复制集里所有的 Secondary 节点。 对 Primary 发送 rs.stepDown() 命令，等待 Primary 降级为 Secondary，这之后复制集会自动选出一个新的 Primary。 重启降级后的 Primary。 数据同步机制 在复制集架构中，主节点与备节点之间是通过 oplog 来同步数据的，这里的 oplog 是一个特殊的固定集合，当主节点上的一个写操作完成后，会向 oplog 集合写入一条对应的日志，而备节点则通过这个 oplog 不断拉取到新的日志，在本地进行回放以达到数据同步的目的。\n什么是 oplog MongoDB oplog 是 Local 库下的一个集合，用来保存写操作所产生的增量日志（类似于 MySQL 中 的 Binlog）。 它是一个 Capped Collection（固定集合），即超出配置的最大值后，会自动删除最老的历史数据，MongoDB 针对 oplog 的删除有特殊优化，以提升删除效率。 主节点产生新的 oplog Entry，从节点通过复制 oplog 并应用来保持和主节点的状态一致； 查看 oplog use local db.oplog.rs.find().sort({$natural:-1}).pretty() 查询结果示例：\n{ \"op\": \"i\", \"ns\": \"test.user\", \"ui\": UUID(\"3aa4b72b-2985-4abf-a40f-fce52sjfysjf6\"), \"ts\": Timestamp(1720772421, 1), \"t\": NumberLong(17), \"v\": NumberLong(2), \"wall\": ISODate(\"2024-04-25T09:27:01.000Z\"), \"o\": { \"_id\": ObjectId(\"65f388906f84793487900000\"), \"name\": \"fox\" } } op：操作类型。 i：插入。 u：更新。 d：删除。 c：执行命令，如 createDatabse、dropDatabse。 n：无操作。 ns：操作的数据库和集合。 o：操作的文档。 o2：操作查询条件 ts：操作的时间戳。当前 timestamp + 计数器，计数器每秒都被重置。 v：oplog 的版本号。 ts 字段描述了 oplog 产生的时间戳，可称之为 optime。optime 是备节点实现增量日志同步的关键，它保证了 oplog 是节点有序的，其由两部分组成：\n当前的系统时间，即 UNIX 时间至现在的秒数，32 位。 整数计时器，不同时间值会将计数器进行重置，32 位。 optime 属于 BSON 的 Timestamp 类型，这个类型一般在 MongoDB 内部使用。既然 oplog 保证了节点级有序，那么备节点便可以通过轮询的方式进行拉取，这里会用到可持续追踪的游标（tailable cursor）技术。\n每个备节点都分别维护了自己的一个 offset，也就是从主节点拉取的最后一条日志的 optime，在执行同步时就通过这个 optime 向主节点的 oplog 集合发起查询。为了避免不停地发起新的查询链接，在启动第一次查询后可以将 cursor 挂住（通过将 cursor 设置为 tailable）。这样只要 oplog 中产生了新的记录，备节点就能使用同样的请求通道获得这些数据。tailable cursor 只有在查询的集合为固定集合时才允许开启。\noplog 集合的大小 oplog 集合的大小可以通过参数 replication.oplogSizeMB 设置，对于 64 位系统来说，oplog 的默认值为：\noplogSizeMB = min(磁盘可用空间*5%，50GB) 对于大多数业务场景来说，很难在一开始评估出一个合适的 oplogSize，所幸的是 MongoDB 在 4.0 版本之后提供了 replSetResizeOplog 命令，可以实现动态修改 oplogSize 而不需要重启服务器。\n# 将复制集成员的 oplog 大小修改为 60GB db.adminCommand({replSetResizeOplog: 1, size: 60000}) # 查看 oplog 大小 use local db.oplog.rs.stats().maxSize oplog 幂等性 每一条 oplog 记录都描述了一次数据的原子性变更，对于 oplog 来说，必须保证是幂等性的。也就是说，对于同一个 oplog，无论进行多少次回放操作，数据的最终状态都会保持不变。\n某文档 x 字段当前值为 100，用户向 Primary 发送一条 {$inc: {x: 1}}，记录 oplog 时会转化为一条 {$set: {x: 101} 的操作，才能保证幂等性。\n幂等性的代价\n简单元素的操作，$inc 转化为 $set 并没有什么影响，执行开销上也差不多，但当遇到数组元素操作时，情况就不一样了。\ndb.coll.insert({_id:1,x:[1,2,3]}) 在数组尾部 push 2 个元素，查看 oplog 发现 $push 操作被转换为了 $set 操作（设置数组指定位置的元素为某个值）\nrs0:PRIMARY\u003e db.coll.update({_id: 1}, {$push: {x: { $each: [4, 5] }}}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 }) rs0:PRIMARY\u003e db.coll.find() { \"_id\" : 1, \"x\" : [ 1, 2, 3, 4, 5 ] } rs0:PRIMARY\u003e use local switched to db local rs0:PRIMARY\u003e db.oplog.rs.find({ns:\"test.coll\"}).sort({$natural:-1}).pretty() { \"op\" : \"u\", \"ns\" : \"test.coll\", \"ui\" : UUID(\"69c871e8-8f99-4734-be5f-c9c5d8565198\"), \"o\" : { \"$v\" : 1, \"$set\" : { \"x.3\" : 4, \"x.4\" : 5 } }, \"o2\" : { \"_id\" : 1 }, \"ts\" : Timestamp(1646223051, 1), \"t\" : NumberLong(4), \"v\" : NumberLong(2), \"wall\" : ISODate(\"2022-03-02T12:10:51.882Z\") } $push 转换为带具体位置的 $set 开销上也差不多，但接下来再看看往数组的头部添加 2 个元素：\nrs0:PRIMARY\u003e use test switched to db test rs0:PRIMARY\u003e db.coll.update({_id: 1}, {$push: {x: { $each: [6, 7], $position: 0 }}}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 }) rs0:PRIMARY\u003e db.coll.find() { \"_id\" : 1, \"x\" : [ 6, 7, 1, 2, 3, 4, 5 ] } rs0:PRIMARY\u003e use local switched to db local rs0:PRIMARY\u003e db.oplog.rs.find({ns:\"test.coll\"}).sort({$natural:-1}).pretty() { \"op\" : \"u\", \"ns\" : \"test.coll\", \"ui\" : UUID(\"69c871e8-8f99-4734-be5f-c9c5d8565198\"), \"o\" : { \"$v\" : 1, \"$set\" : { \"x\" : [ 6, 7, 1, 2, 3, 4, 5 ] } }, \"o2\" : { \"_id\" : 1 }, \"ts\" : Timestamp(1646223232, 1), \"t\" : NumberLong(4), \"v\" : NumberLong(2), \"wall\" : ISODate(\"2022-03-02T12:13:52.076Z\") } 可以发现，当向数组的头部添加元素时，oplog 里的 $set 操作不再是设置数组某个位置的值（因为基本所有的元素位置都调整了），而是 $set 数组最终的结果，即整个数组的内容都要写入 oplog。当 push 操作指定了 $slice 或者 $sort 参数时，oplog 的记录方式也是一样的，会将整个数组的内容作为 $set 的参数。$pull,$addToSet 等更新操作符也是类似，更新数组后，oplog 里会转换成 $set 数组的最终内容，才能保证幂等性。\noplog 的写入被放大，导致同步追不上 - 大数组更新 当数组非常大时，对数组的一个小更新，可能就需要把整个数组的内容记录到 oplog 里，我遇到一个实际的生产环境案例，用户的文档内包含一个很大的数组字段，1000 个元素总大小在 64KB 左右（数组元素是很大的文档），这个数组里的元素按时间反序存储，新插入的元素会放到数组的最前面 ($position: 0)，然后保留数组的前 1000 个元素（$slice: 1000）。\n上述场景导致，Primary 上的每次往数组里插入一个新元素(请求大概几百字节)，oplog 里就要记录整个数组的内容，Secondary 同步时会拉取 oplog 并重放，Primary 到 Secondary 同步 oplog 的流量是客户端到 Primary 网络流量的上百倍，导致主备间网卡流量跑满，而且由于 oplog 的量太大，旧的内容很快被删除掉，最终导致 Secondary 追不上，转换为 RECOVERING 状态。\n在文档里使用数组时，一定得注意上述问题，避免数组的更新导致同步开销被无限放大的问题。使用数组时，尽量注意：\n数组的元素个数不要太多，总的大小也不要太大。 尽量避免对数组进行更新操作。 如果一定要更新，尽量只在尾部插入元素，复杂的逻辑可以考虑在业务层面上来支持。 复制延迟 由于 oplog 集合是有固定大小的，因此存放在里面的 oplog 随时可能会被新的记录冲掉。如果备节点的复制不够快，就无法跟上主节点的步伐，从而产生复制延迟（replication lag）问题。这是不容忽视的，一旦备节点的延迟过大，则随时会发生复制断裂的风险，这意味着备节点的 optime（最新一条同步记录）已经被主节点老化掉，于是备节点将无法继续进行数据同步。\n为了尽量避免复制延迟带来的风险，我们可以采取一些措施，比如：\n增加 oplog 的容量大小，并保持对复制窗口的监视。 通过一些扩展手段降低主节点的写入速度。 优化主备节点之间的网络。 避免字段使用太大的数组（可能导致 oplog 膨胀）。 主从复制数据丢失问题 由于复制延迟是不可避免的，这意味着主备节点之间的数据无法保持绝对的同步。当复制集中的主节点宕机时，备节点会重新选举成为新的主节点。那么，当旧的主节点重新加入时，必须回滚掉之前的一些“脏日志数据”，以保证数据集与新的主节点一致（因为旧的主节点宕机时，它的数据可能是比从节点新的）。主备复制集合的差距越大，发生大量数据回滚的风险就越高。\n对于写入的业务数据来说，如果已经被复制到了复制集的大多数节点，则可以避免被回滚的风险。应用上可以通过设定更高的写入级别 writeConcern：majority（数据写入到大多数节点后才返回成功，这些节点最好在同一个机房，优先级高一点）来保证数据的持久性。类似于 Redis 的 min-slaves-to-write 配置。\n这些由旧主节点回滚的数据会被写到单独的 rollback 目录下，必要的情况下仍然可以恢复这些数据。\n当 rollback 发生时，MongoDB 将把 rollback 的数据以 BSON 格式存放到 dbpath 路径下 rollback 文件夹中，BSON 文件的命名格式如下： \u003cdatabase\u003e.\u003ccollection\u003e.\u003ctimestamp\u003e.bson。\nmongorestore --host 192.168.192:27018 --db test --collection emp -ufox -pfox --authenticationDatabase=admin rollback/emp_rollback.bson 同步源选择 MongoDB 是允许通过备节点进行复制的，这会发生在以下的情况中：\n在 settings.chainingAllowed 开启的情况下，备节点自动选择一个最近的节点（ping 命令时延最小）进行同步。settings.chainingAllowed 选项默认是开启的，也就是说默认情况下备节点并不一定会选择主节点进行同步，这个副作用就是会带来延迟的增加，你可以通过下面的操作进行关闭： cfg = rs.config() cfg.settings.chainingAllowed = false rs.reconfig(cfg) 使用 replSetSyncFrom 命令临时更改当前节点的同步源，比如在初始化同步时将同步源指向备节点来降低对主节点的影响。 db.adminCommand({ replSetSyncFrom: \"hostname:port\" }) ","复制集常用命令#复制集常用命令":"","复制集成员角色#复制集成员角色":"MongoDB 有两种集群架构，分别是：\n主从复制集 分片集群 复制集介绍 复制集是 MongoDB 最基本的集群架构，它是由一组 MongodB 实例组成的集群，包含一个 Primary 节点，多个 Secondary 节点。\n所有数据都写入 Primary，Secondary 从 Primary 同步写入的数据，以保持复制集内所有成员存储相同的数据集，提供数据的高可用。\n复制集的高可用依赖于两个方面：\n数据写入时将数据迅速复制到另一个独立节点上。 在接受写入的节点发生故障时自动选举出一个新的替代节点。 复制集其他几个附加作用：\n数据分发: 将数据从一个区域复制到另一个区域，减少另一个区域的读延迟。 读写分离: 不同类型的压力分别在不同的节点上执行。 异地容灾: 在数据中心故障时候快速切换到异地。 早期版本的 MongoDB 使用了一种 Master-Slave 的主从架构，该做法在 MongoDB 3.4 版本之后已经废弃。\nℹ️ MySQL 和 Redis 都实现了 Master-Slave 的主从架构，普通的主从架构是没有自动故障切换的能力的，一旦 Master 节点发生故障，需要手动切换 Slave 节点为 Master 节点。 三节点复制集模式 常见的复制集架构由 3 个成员节点组成，官方提供了两种方案。\nPSS 模式（官方推荐） PSS模式由一个 Primary 节点和两个 Secondary 节点所组成。\n此模式始终提供两个完整的副本，如果 Primary 节点不可用，则复制集会自动选择一个 Secondary 节点作为新的 Primary 节点。旧的 Primary 节点在可用时重新加入复制集。\nPSA 模式 PSA 模式由一个 Primary 节点、一个 Secondary 节点和一个仲裁者节点（Arbiter）组成。\n其中，Arbiter 节点不存储数据副本，也不提供业务的读写操作。Arbiter 节点发生故障不影响业务，仅影响选举投票。此模式仅提供数据的一个完整副本，如果主节点不可用，则复制集将选择备节点作为主节点。\n典型三节点复制集环境搭建 环境准备\n安装 MongoDB 并配置好环境变量 确保有 10GB 以上的硬盘空间 准备配置文件 复制集的每个 mongod 进程应该位于不同的服务器。现在是在一台机器上运行 3 个进程来模拟，因此要为它们各自配置：\n不同的端口（28017/28018/28019） 不同的数据目录，mkdir -p /data/db{1,2,3} 不同日志文件路径 (例如：/data/db1/mongod.log) 如果是在三台不同的机器上，那就不用创建不同的数据目录了和使用不同的端口了。\n创建配置文件 /data/db1/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db1/mongod.log # log path logAppend: true storage: dbPath: /data/db1 # data directory net: bindIp: 0.0.0.0 port: 28017 # port replication: replSetName: rs0 processManagement: fork: true 参考上面配置修改端口，路径，依次配置 db2，db3。注意必须是 yaml 格式。\n启动 MongoDB 进程：\nmongod -f /data/db1/mongod.conf mongod -f /data/db2/mongod.conf mongod -f /data/db3/mongod.conf 如果启用了 SELinux，可能阻止上述进程启动。简单起见关闭 SELinux：\n# 永久关闭,将 SELINUX=enforcing 改为 SELINUX=disabled,设置后需要重启才能生效 vim /etc/selinux/config # 查看 SELINUX /usr/sbin/sestatus -v 配置复制集 复制集通过 mongosh 的 rs.initiate() 进行初始化，初始化后各个成员间开始发送心跳消息，并发起 Priamry 选举操作，获得大多数成员投票支持的节点，会成为 Primary，其余节点成为 Secondary。\n# mongosh --port 28017 # 初始化复制集 \u003e rs.initiate({ _id: \"rs0\", # 复制集的名称，必须唯一 members: [{ _id: 0, # 成员的编号，必须唯一，不能重复 host: \"192.168.65.174:28017\" },{ _id: 1, host: \"192.168.65.174:28018\" },{ _id: 2, host: \"192.168.65.174:28019\" }] }) 也可以是用 rs.add() 来添加成员。可以使用 rs.help() 除了当前节点角色信息，是一个更精简化的信息，也返回整个复制集的成员列表。\n验证主从节点读写操作 rs.status() 可以查看复制集的状态。db.isMaster()\nMongoDB 主节点进行写入 // mongosh --port 28017 db.user.insertMany([{name:\"fox\"},{name:\"monkey\"}]) 切换到从节点写入 // mongosh --port 28018 db.user.insertMany([{name:\"fox\"},{name:\"monkey\"}]) 抛出异常 MongoBulkWriteError: not primary，从节点是不能写入的。\nMongoDB 从节点进行读 # mongo --port 28018 rs0:SECONDARY\u003e db.user.find() 抛出异常 MongoBulkWriteError: not primary and secondaryOk=false，从节点默认是不可读的。\n# 设置从节点可读 rs0:SECONDARY\u003e rs.secondaryOk() rs0:SECONDARY\u003e db.user.find() 复制集常用命令 命令 描述 rs.add() 为复制集新增节点 rs.addArb() 为复制集新增一个仲裁者（arbiter） rs.conf() 返回复制集配置信息 rs.freeze() 防止当前节点在一段时间内选举成为主节点 rs.help() 返回 replica set 的命令帮助 rs.initiate() 初始化一个新的复制集 rs.printReplicationInfo() 以主节点的视角返回复制的状态报告 rs.printSecondaryReplicationInfo() 以从节点的视角返回复制状态报告 rs.reconfig() 通过重新应用复制集配置来为复制集更新配置 rs.remove() 从复制集中移除一个节点 rs.secondaryOk() 为当前的连接设置从节点可读 rs.status() 返回复制集状态信息 rs.stepDown() 让当前的 primary 变为从节点并触发选举 rs.syncFrom() 设置复制集节点从哪个节点处同步数据，将会覆盖默认选取逻辑 安全认证 复制集集群就不需要使用 --auth 参数了，直接创建 keyFile 文件：\nkeyFile 文件的作用：集群之间的安全认证，增加安全认证机制 KeyFile（开启 keyfile 认证就默认开启了 auth 认证了）。\n# mongo.key 采用随机算法生成，用作节点内部通信的密钥文件。 openssl rand -base64 756 \u003e /data/mongo.key # 权限必须是 600 chmod 600 /data/mongo.key 注意：创建 keyFile 前，需要先停掉复制集中所有主从节点的 mongod 服务，然后再创建，否则有可能出现服务启动不了的情况。\n将主节点中的 keyfile 文件拷贝到复制集其他从节点服务器中，路径地址对应 mongo.conf 配置文件中的 keyFile 字段地址，并设置 keyfile 权限为 600。\n启动 mongod：\nmongod -f /data/db1/mongod.conf --keyFile /data/mongo.key mongod -f /data/db2/mongod.conf --keyFile /data/mongo.key mongod -f /data/db3/mongod.conf --keyFile /data/mongo.key # 进入主节点 mongosh --port 28017 -u fox -p fox --authenticationDatabase=admin 复制集连接方式 方式一：直接连接 Primary 节点，正常情况下可读写 MongoDB，但主节点故障切换后，无法正常访问。\n# 这种写死了 host 和 port 的方式，一旦主节点故障切换，就无法正常访问了。 mongosh -u fox -p fox 192.168.65.206:28018 方式二（强烈推荐）：通过高可用 Uri 的方式连接 MongoDB，当 Primary 故障切换后，MongoDB Driver 可自动感知并把流量路由到新的 Primary 节点。\nmongosh mongodb://fox:fox@192.168.65.206:28017,192.168.65.206:28018,192.168.65.206:28019/admin?replicaSet=rs0 复制集成员角色 复制集里面有多个节点，每个节点拥有不同的职责。\n在看成员角色之前，先了解两个重要属性：\nPriority = 0 当 Priority 等于 0 时，它不可以被复制集选举为主，Priority 的值越高，则被选举为主的概率更大。通常，在跨机房方式下部署复制集可以使用该特性。假设使用了机房 A 和机房 B，由于主要业务与机房 A 更近，则可以将机房 B 的复制集成员 Priority 设置为 0，这样主节点就一定会是 A 机房的成员。\nVote = 0 Vote 等于 0 时，不可以参与选举投票。priority=0 + vote=0：节点永不参与选举，仅作备份。由于一个复制集中最多只有 7 个投票成员，因此多出来的成员则必须将其 vote 属性值设置为 0，即这些成员将无法参与投票。\n成员角色 Primary：主节点，其接收所有的写请求，然后把修改同步到所有备节点。一个复制集只能有一个主节点，当主节点“挂掉”后，其他节点会重新选举出来一个主节点。 Secondary：备节点，与主节点保持同样的数据集。当主节点“挂掉”时，参与竞选主节点。分为以下三个不同类型： Hidden = false：正常的只读节点，是否可选为主，是否可投票，取决于 Priority，Vote 的值； Hidden = true：隐藏节点，对客户端不可见，可以参与选举，但是 Priority 必须为 0，即不能被提升为主。由于隐藏节点不会接受业务访问，因此可通过隐藏节点做一些数据备份、离线计算的任务，这并不会影响整个复制集。 Delayed：延迟节点，必须同时具备隐藏节点和 Priority = 0 的特性，会延迟一定的时间（secondaryDelaySecs 配置决定）从上游复制增量，常用于快速回滚场景。 Arbiter：仲裁节点，只用于参与选举投票，本身不承载任何数据，只作为投票角色。比如你部署了 2 个节点的复制集，1 个 Primary，1 个 Secondary，任意节点宕机，复制集将不能提供服务了（无法选出 Primary），这时可以给复制集添加⼀个 Arbiter 节点，即使有节点宕机，仍能选出 Primary。 Arbiter 本身不存储数据，是非常轻量级的服务，当复制集成员为偶数时，最好加入⼀个 Arbiter 节点，以提升复制集可用性。 配置隐藏节点 很多情况下将节点设置为隐藏节点是用来协助 delayed members 的。如果仅仅需要防止该节点成为主节点，可以通过 priority 0 member 来实现。\ncfg = rs.conf() cfg.members[1].priority = 0 cfg.members[1].hidden = true rs.reconfig(cfg) 设置完毕后，该从节点的优先级将变为 0 来防止其升职为主节点，同时其也是对应用程序不可见的。在其他节点上执行 db.isMaster() 将不会显示隐藏节点。\n配置延时节点 当配置一个延时节点的时候，复制过程与该节点的 oplog 都将延时。延时节点中的数据集将会比复制集中主节点的数据延后。\ncfg = rs.conf() cfg.members[1].priority = 0 cfg.members[1].hidden = true // 延迟 1 分钟 cfg.members[1].secondaryDelaySecs = 60 rs.reconfig(cfg) 查看复制延迟：\n在节点上执行 rs.printSecondaryReplicationInfo() 命令，可以一并列出所有备节点成员的同步延迟情况：\n\u003e rs.printSecondaryReplicationInfo() source: 192.168.65.174:28019 { syncedTo: 'Fri May 19 2023 15:27:36 GMT+0800 (中国标准时间)', replLag: '-53 secs (-0.01 hrs) behind the primary' } 延时节点通常用于数据保护和灾难恢复场景：\n人为错误防护 当管理员误删除数据或错误更新时，延时节点可以提供\"时间缓冲\"。在延迟时间内发现问题，可以从延时节点恢复数据。\n数据回滚点 提供一个人为设置的\"数据快照点\"，相当于一个时间机器。在应用逻辑错误导致数据污染时特别有用。\n灾难恢复 防范逻辑损坏（如 Bug 导致的数据错误）传播到所有节点。比常规备份更快速的恢复方式。\n添加投票节点 # 为仲裁节点创建数据目录，存放配置数据。该目录将不保存数据集 mkdir /data/arb # 启动仲裁节点，指定数据目录和复制集名称 mongod --port 30000 --dbpath /data/arb --replSet rs0 # 进入 mongo shell,添加仲裁节点到复制集 rs.addArb(\"ip:30000\") 如果添加节点遇到下面的错误：\nMongoServerError: Reconfig attempted to install a config that would change the implicit default write concern. Use the setDefaultRWConcern command to set a cluster-wide write concern and try the reconfig again. # 执行命令 db.adminCommand( {\"setDefaultRWConcern\" : 1, \"defaultWriteConcern\" : { \"w\" : 2 } } ) 移除复制集节点 使用 rs.remove() 来移除节点\n# 1.关闭节点实例 # 2.连接主节点，执行下面命令 rs.remove(\"ip:port\") 通过 rs.reconfig() 来移除节点\n# 1.关闭节点实例 # 2.连接主节点，执行下面命令 cfg = rs.conf() cfg.members.splice(2,1) #从2开始移除1个元素 rs.reconfig(cfg) 更改复制集节点 cfg = rs.conf() cfg.members[0].host = \"ip:port\" rs.reconfig(cfg) ","复制集连接方式#复制集连接方式":"","安全认证#安全认证":""},"title":"集群架构 - 复制集"},"/db-learn/docs/mongo/advanced/03_cluster_shared/":{"data":{"":"","mongodb-高级集群架构设计#MongoDB 高级集群架构设计":"两地三中心集群架构设计 容灾级别 RPO\u0026RTO RPO（Recovery Point Objective）：即数据恢复点目标，主要指的是业务系统所能容忍的数据丢失量。 RTO（Recovery Time Objective）：即恢复时间目标，主要指的是所能容忍的业务停止服务的最长时间，也就是从灾难发生到业务系统恢复服务功能所需要的最短时间周期。 MongoDB 两地三中心方案：复制集跨中心部署 两地三中心方案\n双中心双活＋异地热备=两地三中心：\nMongoDB 集群两地三中心部署的考量点\n节点数量建议要 5 个，2+2+1 模式 主数据中心的两个节点要设置高一点的优先级，减少跨中心换主节点 同城双中心之间的网络要保证低延迟和频宽，满足 writeConcern: Majority 的双中心写需求 使用 Retryable Writes and Retryable Reads 来保证零下线时间 用户需要自行处理好业务层的双中心切换 两地三中心复制集搭建 环境准备\n3 台 Linux 虚拟机，准备 MongoDB 环境，配置环境变量。 一定要版本一致（重点） 配置域名解析\n在 3 台虚拟机上分别执行以下 3 条命令，注意替换实际 IP 地址：\necho \"192.168.65.97 mongo1 mongo01.com mongo02.com\" \u003e\u003e /etc/hosts echo \"192.168.65.190 mongo2 mongo03.com mongo04.com\" \u003e\u003e /etc/hosts echo \"192.168.65.200 mongo3 mongo05.com \" \u003e\u003e /etc/hosts 启动 5 个 MongoDB 实例：\n# mongo1 mkdir -p /data/member1/db /data/member1/log /data/member2/db /data/member2/log mongod --dbpath /data/member1/db --replSet demo --bind_ip 0.0.0.0 --port 10001 --fork --logpath /data/member1/log/member1.log mongod --dbpath /data/member2/db --replSet demo --bind_ip 0.0.0.0 --port 10002 --fork --logpath /data/member2/log/member2.log # mongo2 mkdir -p /data/member3/db /data/member3/log /data/member4/db /data/member4/log mongod --dbpath /data/member3/db --replSet demo --bind_ip 0.0.0.0 --port 10001 --fork --logpath /data/member3/log/member3.log mongod --dbpath /data/member4/db --replSet demo --bind_ip 0.0.0.0 --port 10002 --fork --logpath /data/member4/log/member4.log # mongo3 mkdir -p /data/member5/db /data/member5/log mongod --dbpath /data/member5/db --replSet demo --bind_ip 0.0.0.0 --port 10001 --fork --logpath /data/member5/log/member5.log 初始化复制集：\nmongo mongo01.com:10001 # 初始化复制集 rs.initiate({ \"_id\" : \"demo\", \"version\" : 1, \"members\" : [ { \"_id\" : 0, \"host\" : \"mongo01.com:10001\" }, { \"_id\" : 1, \"host\" : \"mongo02.com:10002\" }, { \"_id\" : 2, \"host\" : \"mongo03.com:10001\" }, { \"_id\" : 3, \"host\" : \"mongo04.com:10002\" }, { \"_id\" : 4, \"host\" : \"mongo05.com:10001\" } ] }) # 查看复制集状态 rs.status() 配置选举优先级\n把 mongo1 上的 2 个实例的选举优先级调高为 5 和 10（默认为1），给主数据中心更高的优先级：\nmongosh mongo01.com:10001 conf = rs.conf() conf.members[0].priority = 5 conf.members[1].priority = 10 rs.reconfig(conf) 启动持续写脚本（每 2 秒写一条记录）:\n# mongo3 mongosh --retryWrites mongodb://mongo01.com:10001,mongo02.com:10002,mongo03.com:10001,mongo04.com:10002,mongo05.com:10001/test?replicaSet=demo ingest-script # vim ingest-script db.test.drop() for(var i=1;i\u003c1000;i++){ db.test.insert({item: i}); inserted = db.test.findOne({item: i}); if(inserted) print(\" Item \"+ i +\" was inserted \" + new Date().getTime()/1000); else print(\"Unexpected \"+ inserted) sleep(2000); } 总结\n搭建简单，使用复制集机制，无需第三方软件 使用 Retryable Writes 以后，即使出现数据中心故障，对前端业务没有任何中断 （Retryable Writes 在 4.2 以后就是默认设置） 全球多写集群架构设计 全球多写集群方案，必须用到分片集群。","分片策略#分片策略":"通过分片功能，可以将一个非常大的集合分散存储到不同的分片上，如图：\n假设这个集合大小是 1TB，那么拆分到 4 个分片上之后，每个分片存储 256GB 的数据。这个当然是最理想化的场景，实际上很难做到如此绝对的平衡。一个集合在拆分后如何存储、读写，与该集合的分片策略设定是息息相关的。每个分片不可能直接存储一个 256GB 的数据，这些数据会分成多个 chunk，每个 chunk 存储一部分数据，保证每一个分片一共是 256GB 的数据。\n也就是说一个集合会分成多个 chunk，分布在多个分片上，每个 chunk 存储一部分数据。\n什么是 chunk chunk 的意思是数据块，一个 chunk 代表了集合中的“一段数据”（类似 Redis 的槽位），例如，用户集合（db.users）在切分成多个 chunk 之后如图所示：\nchunk 所描述的是范围区间，例如，db.users 使用了 userId 作为分片键，那么 chunk 就是 userId 的各个值（或哈希值）的连续区间。集群在操作分片集合时，会根据分片键找到对应的 chunk，并向该 chunk 所在的分片发起操作请求，而 chunk 的分布在一定程度上会影响数据的读写路径，这由以下两点决定：\nchunk 的切分方式，决定如何找到数据所在的 chunk chunk 的分布状态，决定如何找到 chunk 所在的分片 分片算法 chunk 切分是根据分片策略进行实施的，分片策略的内容包括分片键和分片算法。当前，MongoDB支持两种分片算法。\n范围分片 假设集合根据 x 字段来分片，x 的完整取值范围为 [minKey, maxKey]（x 为整数，这里的 minKey、maxKey 为整型的最小值和最大值），其将整个取值范围划分为多个 chunk，例如：\nchunk1 包含 x 的取值在 [minKey，-75) 的所有文档。 chunk2 包含 x 取值在 [-75，25) 之间的所有文档，依此类推。 范围分片能很好的满足范围查询的需求，比如要查询 x 在 [-30, 10] 之间的所有文档，这是 mongos 直接将请求定位到 chunk2 所在的分片服务器上，就能查询出所有符合条件的文档。范围分片的缺点在于，如果 Shard Key 有明显的递增或递减的趋势，则新插入的文档会分布到同一个 chunk，此时写压力会集中到一个节点，从而导致单点的性能瓶颈。\n一些常见的导致递增 Key 的场景：\n时间值 ObjectId UUID，包含系统时间、时钟序列。 自增整数 哈希分片 哈希分片会实现根据分片键 Shard Key 计算出一个新的哈希值（64 位整数），再根据哈希值按照范围分片的策略进行 chunk 切分。适用于日志，物联网等高并发场景。\n哈希分片与范围分片是互补的，由于哈希算法保证了随机性，所以文档可以更加离散的分布到不同的 chunk 上，这避免了范围分片的集中写问题。然而，在执行一些范围查询时，哈希分片的效率不如范围分片。因为所有的范围查询都必然导致对多有的 chunk 进行检索，如果集群有 10 个分片，那么 mongos 将需要对 10 个分片进行查询。哈希分片与范围分片的另一个区别是，哈希分片只能选择单个字段，而范围分片允许采用组合式的多个字段作为分片键。\n哈希分片仅支持单个字段的哈希分片：\n{x : \"hashed\" } {x : 1 , y : \"hashed\"} // 4.4 new 4.4 以后的版本，可以将单个字段的哈希分片和一个到多个的范围分片键字段来进行组合，比如指定 x:1,y: \"hashed\" 中 y 是哈希的方式。\n分片标签 MongoDB 允许通过为分片添加标签（tag）的方式来控制数据分发。一个标签可以关联到多个分片区间（TagRange）。均衡器会优先考虑 chunk 是否正处于某个分片区间上（被完全包含），如果是则会将 chunk 迁移到分片区间所关联的分片，否则按一般情况处理。\n分片标签适用于一些特定的场景。例如，集群中可能同时存在 OLTP 和 OLAP 处理，一些系统日志的重要性相对较低，而且主要以少量的统计分析为主。为了便于单独扩展，我们可能希望将日志与实时类的业务数据分开，此时就可以使用标签。\n为了让分片拥有指定的标签，需执行 addShardTag 命令：\nsh.addShardTag(\"shard01\",\"oltp\") sh.addShardTag(\"shard02\",\"oltp\") sh.addShardTag(\"shard03\",\"olap\") 实时计算的集合应该属于 oltp 标签，声明 TagRange：\nsh.addTagRange(\"main.devices\",{shardKey:MinKey},{shardKey:MaxKey},\"oltp\") shardKey 要换成你的分片键。\n而离线计算的集合，则属于 olap 标签：\nsh.addTagRange(\"other.systemLogs\",{shardKey:MinKey},{shardKey:MaxKey},\"olap\") main.devices 集合将被均衡地分发到 shard01、shard02 分片上，而 other.systemLogs 集合将被单独分发到 shard03 分片上。\n分片键选择 在选择分片键时，需要根据业务的需求及范围分片、哈希分片的不同特点进行权衡。一般来说，在设计分片键时需要考虑的因素包括：\n分片键的基数（cardinality），取值基数越大越有利于扩展。 以性别作为分片键：数据最多被拆分为 2 份 以月份作为分片键：数据最多被拆分为 12 份 分片键的取值分布应该尽可能均匀。 业务读写模式，尽可能分散写压力，而读操作尽可能来自一个或少量的分片。 分片键应该能适应大部分的业务操作。 分片键约束 ShardKey 必须是一个索引。非空集合须在 ShardCollection 前创建索引；空集合 ShardCollection 自动创建索引\n4.4 版本之前：\nShardKey 大小不能超过 512 Bytes； 仅支持单字段的哈希分片键； Document 中必须包含 ShardKey； ShardKey 包含的 Field 不可以修改。 4.4 版本之后:\nShardKey 大小无限制； 支持复合哈希分片键； Document 中可以不包含 ShardKey，插入时被当 做 Null 处理； 为 ShardKey 添加后缀 refineCollectionShardKey 命令，可以修改 ShardKey 包含的 Field； 而在 4.2 版本之前，ShardKey 对应的值不可以修改；4.2 版本之后，如果 ShardKey 为非 _id 字段， 那么可以修改 ShardKey 对应的值。","分片简介#分片简介":"分片（shard）是指在将数据进行水平切分之后，将其存储到多个不同的服务器节点上的一种扩展方式。分片在概念上非常类似于应用开发中的“水平分表”。不同的点在于，MongoDB 本身就自带了分片管理的能力，对于开发者来说可以做到开箱即用。\n为什么要使用分片？ MongoDB 复制集实现了数据的多副本复制及高可用，但是一个复制集能承载的容量和负载是有限的。在你遇到下面的场景时，就需要考虑使用分片了：\n存储容量需求超出单机的磁盘容量。 活跃的数据集超出单机内存容量，导致很多请求都要从磁盘读取数据，影响性能。 写 IOPS 超出单个 MongoDB 节点的写服务能力。 垂直扩容（Scale Up） VS 水平扩容（Scale Out）：\n垂直扩容 ： 用更好的服务器，提高 CPU 处理核数、内存数、带宽等 水平扩容 ： 将任务分配到多台计算机上 分片集群架构 MongoDB 分片集群（Sharded Cluster）是对数据进行水平扩展的一种方式。MongoDB 使用分片集群来支持大数据集和高吞吐量的业务场景。在分片模式下，存储不同的切片数据的节点被称为分片节点，一个分片集群内包含了多个分片节点。当然，除了分片节点，集群中还需要一些配置节点、路由节点，以保证分片机制的正常运作。\n比 Redis 的集群架构更加复杂，多了路由，配置节点。\n一般生产环境如果有三个分片，每个分片包含三个节点，那么整个集群就有 9 个节点。每个节点的内存需要大一点（32G/64G），因为复制集中每个节点都可能成为主节点。\n配置节点的数量一般为 3 个，组成一个复制集，内存不需要太大（4G/8G），不需要存储数据。\nMongos 至少需要 2 个节点，避免单点故障，内存不需要太大（4G/8G），不需要存储数据。\n核心概念 数据分片：分片用于存储真正的数据，并提供最终的数据读写访问。分片仅仅是一个逻辑的概念，它可以是一个单独的 mongod 实例，也可以是一个复制集。图中的 Shard1、Shard2 都是一个复制集分片。在生产环境中也一般会使用复制集的方式，这是为了防止数据节点出现单点故障。 配置服务器（Config Server）：配置服务器包含多个节点，并组成一个复制集结构，对应于图中的 ConfigReplSet。配置复制集中保存了整个分片集群中的元数据，其中包含各个集合的分片策略，以及分片的路由表等。 查询路由（mongos）：mongos 是分片集群的访问入口，其本身并不持久化数据。mongos 启动后，会从配置服务器中加载元数据。之后 mongos 开始提供访问服务，并将用户的请求正确路由到对应的分片。在分片集群中可以部署多个 mongos 以分担客户端请求的压力。 ","搭建分片集群#搭建分片集群":" 使用 mtools 搭建分片集群，mtools 可以快速搭建一个简单的分片集群，可用于测试。 标准方式搭建分片集群，需要手动搭建分片集群，比较复杂。 使用分片集群 为了使集合支持分片，需要先开启 database 的分片功能：\nuse shop sh.enableSharding(\"shop\") 执行 shardCollection 命令，对集合执行分片初始化：\nsh.shardCollection(\"shop.product\",{productId:\"hashed\"},false,{numInitialChunks:4}) shop.product 集合将 productId 作为分片键，并采用了哈希分片策略，除此以外，numInitialChunks：4 表示将初始化 4 个 chunk。numInitialChunks 必须和哈希分片策略配合使用。而且，这个选项只能用于空的集合，如果已经存在数据则会返回错误。\n向分片集合写入数据 向 shop.product 集合写入一批数据：\ndb=db.getSiblingDB(\"shop\"); var count=0; for(var i=0;i\u003c1000;i++){ var p=[]; for(var j=0;j\u003c100;j++){ p.push({ \"productId\":\"P-\"+i+\"-\"+j, name:\"羊毛衫\", tags:[ {tagKey:\"size\",tagValue:[\"L\",\"XL\",\"XXL\"]}, {tagKey:\"color\",tagValue:[\"蓝色\",\"杏色\"]}, {tagKey:\"style\",tagValue:\"韩风\"} ] }); } count+=p.length; db.product.insertMany(p); print(\"insert \",count) } 查询数据的分布 db.product.getShardDistribution() 输出结果示例：\nShard shard01 at shard01/localhost:27053,localhost:27054,localhost:27055 data : 12.54MiB docs : 50065 chunks: 2 # 该分片有 2 个 chunk estimated data per chunk : 6.27Mib estimated docs per chunk : 25032 Shard shard02 at shard02/localhost:27056,localhost:27057,localhost:27058 data : 12.51MiB docs : 49935 chunks: 2 # 该分片有 2 个 chunk estimated data per chunk : 6.25Mib estimated docs per chunk : 24967 Totals data : 25.06MiB docs : 100000 chunks : 4 Shard shard01 contains 50.06% of data, 50.06% of docs in cluster, avg obj sieze on shard : 262B Shard shard02 contains 49.93% of data, 49.93% of docs in cluster, avg obj sieze on shard : 262B ","数据均衡#数据均衡":"均衡的方式 一种理想的情况是，所有加入的分片都发挥了相当的作用，包括提供更大的存储容量，以及读写访问性能。因此，为了保证分片集群的水平扩展能力，业务数据应当尽可能地保持均匀分布。这里的均匀性包含以下两个方面：\n所有的数据应均匀地分布于不同的 chunk 上。 每个分片上的 chunk 数量尽可能是相近的。 其中，第 1 点由业务场景和分片策略来决定，而关于第 2 点，有两种选择。\n手动均衡 一种做法是，在初始化集合时预分配一定数量的 chunk（仅适用于哈希分片），比如给 10 个分片分配 1000 个 chunk，那么每个分片拥有 100 个 chunk。 另一种做法则是，可以通过 splitAt、moveChunk 命令进行手动切分、迁移。 自动均衡 开启 MongoDB 集群的自动均衡功能。均衡器会在后台对各分片的 chunk 进行监控，一旦发现了不均衡状态就会自动进行 chunk 的搬迁以达到均衡。其中，chunk 不均衡通常来自于两方面的因素：\n一方面，在没有人工干预的情况下，chunk 会持续增长并产生分裂（split），而不断分裂的结果就会出现数量上的不均衡； 另一方面，在动态增加分片服务器时，也会出现不均衡的情况。自动均衡是开箱即用的，可以极大简化集群的管理工作。 chunk 分裂 在默认情况下，一个 chunk 的大小为 64 MB（MongoDB 6.0默认是 128M），该参数由配置的 chunksize 参数指定。如果持续地向该 chunk 写入数据，并导致数据量超过了 chunk 大小，则 MongoDB 会自动进行分裂，将该 chunk 切分为两个相同大小的 chunk。\nchunk 分裂是基于分片键进行的，如果分片键的基数太小，则可能因为无法分裂而会出现 jumbo chunk（超大块）的问题。例如，对 db.users 使用 gender（性别）作为分片键，由于同一种性别的用户数可能达到数千万，分裂程序并不知道如何对分片键（gender）的一个单值进行切分，因此最终导致在一个 chunk 上集中存储了大量的 user 记录（总大小超过 64MB）。\njumbo chunk 对水平扩展有负面作用，该情况不利于数据的均衡，业务上应尽可能避免。一些写入压力过大的情况可能会导致 chunk 多次失败（split），最终当 chunk 中的文档数大于 1.3×avgObjectSize 时会导致无法迁移。此外在一些老版本中，如果 chunk 中的文档数超过 250000 个，也会导致无法迁移。\n自动均衡原理 均衡器运行于 Primary Config Server（配置服务器的主节点）上，而该节点也同时会控制 chunk 数据的搬迁流程。\n流程说明：\n分片 shard0 在持续的写入压力下，产生了 chunk 分裂。 分片服务器通知 Config Server 更新元数据。 Config Server 上的自动均衡器对 chunk 分布进行检查，发现 shard0 和 shard1 上的 chunk 数量差异达到的阈值，于是向 shard0 下发 moveChunk 命令，将 chunk 迁移到 shard1 上。 shard0 收到 moveChunk 命令后，将 chunk 复制到 shard1 上。该阶段会完成索引、chunk 数据的复制，而且在整个过程中业务侧对数据的操作仍然指向 shard0。所以在第一轮复制完毕之后，目标 shard1 会向 shard0 确认是否还存在增量更新的数据，如果存在则继续复制。 迁移完成后，shard0 通知 Config Server 更新元数据，将 chunk 的位置更新为 shard1。在更新元数据之后确保没有关联 cursor 的情况下，shard0 会删除被迁移的 chunk 副本。 Config Server 通知 mongos 服务器更新路由表。新的请求会被路由到 shard1。 迁移阈值 MongoDB 4.4 迁移条件：\n均衡器对数据的不均衡判断是根据分片上的 chunk 个数差异来进行的：\nchunk 个数 迁移阈值 少于 20 个 2 20 ~ 79 4 80 及以上 8 官方文档\nMongoDB 6.0 迁移条件：\n如果碎片之间的数据差异 (对于该集合) 小于该集合配置范围大小的三倍，则认为该集合是平衡的。对于 128MB 的默认范围大小，对于给定的集合，两个分片必须具有至少 384MB 的数据大小差异，才能进行迁移。\n官方文档\n迁移的速度 数据均衡的整个过程并不是很快，影响 MongoDB 均衡速度的几个选项如下：\n_secondaryThrottle：用于调整迁移数据写到目标分片的安全级别。如果没有设定，则会使用 w：2 选项，即至少一个备节点确认写入迁移数据后才算成功。从 MongoDB 3.4 版本开始，_secondaryThrottle 被默认设定为 false, chunk 迁移不再等待备节点写入确认。 _waitForDelete：在 chunk 迁移完成后，源分片会将不再使用的 chunk 删除。如果 _waitForDelete 是 true，那么均衡器需要等待 chunk 同步删除后才进行下一次迁移。该选项默认为 false，这意味着对于旧 chunk 的清理是异步进行的。 并行迁移数量：在早期版本的实现中，均衡器在同一时刻只能有一个 chunk 迁移任务。从 MongoDB 3.4 版本开始，允许 n 个分片的集群同时执行 n/2 个并发任务。 随着版本的迭代，MongoDB 迁移的能力也在逐步提升。从 MongoDB 4.0 版本开始，支持在迁移数据的过程中并发地读取源端和写入目标端，迁移的整体性能提升了约 40%。这样也使得新加入的分片能更快地分担集群的访问读写压力。\n数据均衡带来的问题 数据均衡会影响性能，在分片间进行数据块的迁移是一个“繁重”的工作，很容易带来磁盘 I/O 使用率飙升，或业务时延陡增等一些问题。因此，建议尽可能提升磁盘能力，如使用 SSD。除此之外，我们还可以将数据均衡的窗口对齐到业务的低峰期以降低影响。\n登录 mongos，在 config 数据库上更新配置，代码如下：\nuse config sh.setBalancerState(true) db.settings.update( {_id:\"balancer\"}, {$set:{activeWindow:{start:\"02:00\",stop:\"04:00\"}}}, {upsert:true} ) 在上述操作中启用了自动均衡器，同时在每天的凌晨 2 点到 4 点运行数据均衡操作。\n对分片集合中执行 count 命令可能会产生不准确的结果，mongos 在处理 count 命令时会分别向各个分片发送请求，并累加最终的结果。如果分片上正在执行数据迁移，则可能导致重复的计算。替代办法是使用 db.collection.countDocuments({}) 方法，该方法会执行聚合操作进行实时扫描，可以避免元数据读取的问题，但需要更长时间。\n在执行数据库备份的期间，不能进行数据均衡操作，否则会产生不一致的备份数据。在备份操作之前，可以通过如下命令确认均衡器的状态:\nsh.getBalancerState()：查看均衡器是否开启。 sh.isBalancerRunning()：查看均衡器是否正在运行。 sh.getBalancerWindow()：查看当前均衡的窗口设定。 "},"title":"集群架构 - 分片集群"},"/db-learn/docs/mongo/advanced/04_storage/":{"data":{"":"存储引擎是数据库的组件，负责管理数据如何存储在内存和磁盘上。MongoDB 支持多个存储引擎，因为不同的引擎对于特定的工作负载表现更好。选择合适的存储引擎可以显著影响应用程序的性能。","wiredtiger#WiredTiger":"MongoDB 从 3.0 开始引入可插拔存储引擎的概念，主要有 MMAPV1、WiredTiger 存储引擎可供选择。从 MongoDB 3.2 开始，WiredTiger 存储引擎是默认的存储引擎。从 4.2 版开始，MongoDB 删除了废弃的 MMAPv1 存储引擎。\nWiredTiger 读写模型 读缓存 理想情况下，MongoDB 可以提供近似内存式的读写性能。WiredTiger 引擎实现了数据的二级缓存，第一层是操作系统的页面缓存，第二层则是引擎提供的内部缓存。\nMongoDB 为了尽可能保证业务查询的“热数据”能快速被访问，其内部缓存的默认大小达到了内存的一半，该值由 wiredTigerCacheSize 参数指定，其默认的计算公式如下：\nwiredTigerCacheSize=Math.max(0.5*(RAM-1GB),256MB) 写缓冲 当数据发生写入时，MongoDB 并不会立即持久化到磁盘上，而是先在内存中记录这些变更，之后通过 CheckPoint 机制将变化的数据写入磁盘。这么处理主要有以下两个原因：\n如果每次写入都触发一次磁盘 I/O，那么开销太大，而且响应时延会比较大。 多个变更的写入可以尽可能进行 I/O 合并，降低资源负荷。 MongoDB 会丢数据吗？ MongoDB 单机下保证数据可靠性的机制包括以下两个部分。\nCheckPoint（检查点）机制 快照（snapshot）描述了某一时刻（point-in-time）数据在内存中的一致性视图，而这种数据的一致性是 WiredTiger 通过 MVCC（多版本并发控制）实现的。当建立 CheckPoint 时，WiredTiger 会在内存中建立所有数据的一致性快照，并将该快照覆盖的所有数据变化一并进行持久化（fsync）。成功之后，内存中数据的修改才得以真正保存。默认情况下，MongoDB 每 60s 建立一次 CheckPoint，在检查点写入过程中，上一个检查点仍然是可用的。这样可以保证一旦出错，MongoDB 仍然能恢复到上一个检查点。\nℹ️ 如果只有 CheckPoint 机制，那么在发生宕机时，还没有刷盘，那么这 60s 内的数据就会丢失。为了保证数据的完整性，MongoDB 还引入了 Journal 日志机制。 Journal 日志 Journal 是一种预写式日志（write ahead log）机制，主要用来弥补 CheckPoint 机制的不足。如果开启了 Journal 日志，那么 WiredTiger 会将每个写操作的 redo 日志写入 Journal 缓冲区，该缓冲区会频繁地将日志持久化到磁盘上。默认情况下，Journal 缓冲区每 100ms 执行一次持久化。此外，Journal 日志达到 100MB，或是应用程序指定 journal：true，写操作都会触发日志的持久化。一旦 MongoDB 发生宕机，重启程序时会先恢复到上一个检查点，然后根据 Journal 日志恢复增量的变化。由于 Journal 日志持久化的间隔非常短，数据能得到更高的保障，如果按照当前版本的默认配置，则其在断电情况下最多会丢失 100ms 的写入数据。\nℹ️ 对于类似于订单系统这样的业务场景，由于数据的重要性，插入数据时直接指定 journal：true，这样可以保证数据的可靠性。 WiredTiger 写入数据的流程：\n应用向 MongoDB 写入数据（插入、修改或删除）。 数据库从内部缓存中获取当前记录所在的页块，如果不存在则会从磁盘中加载（Buffer I/O） WiredTiger 开始执行写事务，修改的数据写入页块的一个更新记录表，此时原来的记录仍然保持不变。 如果开启了 Journal 日志，则在写数据的同时会写入一条 Journal 日志（Redo Log）。该日志在最长不超过 100ms 之后写入磁盘。 数据库每隔 60s 执行一次 CheckPoint 操作，此时内存中的修改会真正刷入磁盘。 Journal 日志的刷新周期可以通过参数 storage.journal.commitIntervalMs 指定，MongoDB 3.4 及以下版本的默认值是 50ms，而 3.6 版本之后调整到了 100ms。由于 Journal 日志采用的是顺序 I/O 写操作，频繁地写入对磁盘的影响并不是很大。\nCheckPoint 的刷新周期可以调整 storage.syncPeriodSecs 参数（默认值 60s），在 MongoDB 3.4 及以下版本中，当 Journal 日志达到 2GB 时同样会触发 CheckPoint 行为。如果应用存在大量随机写入，则 CheckPoint 可能会造成磁盘 I/O 的抖动。在磁盘性能不足的情况下，问题会更加显著，此时适当缩短 CheckPoint 周期可以让写入平滑一些。","多文档事务#多文档事务":"事务（transaction）是传统数据库所具备的一项基本能力，其根本目的是为数据的可靠性与一致性提供保障。而在通常的实现中，事务包含了一个系列的数据库读写操作，这些操作要么全部完成，要么全部撤销。例如，在电子商城场景中，当顾客下单购买某件商品时，除了生成订单，还应该同时扣减商品的库存，这些操作应该被作为一个整体的执行单元进行处理，否则就会产生不一致的情况。\n在 MongoDB 中，对单个文档的操作是原子的。由于可以在单个文档结构中使用内嵌文档和数组来获得数据之间的关系，而不必跨多个文档和集合进行范式化，所以这种单文档原子性避免了许多实际场景中对多文档事务的需求。\n对于那些需要对多个文档（在单个或多个集合中）进行原子性读写的场景，MongoDB 支持多文档事务。而使用分布式事务，事务可以跨多个操作、集合、数据库、文档和分片使用。\nMongoDB 虽然已经在 4.2 开始全面支持了多文档事务，但并不代表大家应该毫无节制地使用它。相反，对事务的使用原则应该是：能不用尽量不用。通过合理地设计文档模型，可以规避绝大部分使用事务的必要性。\n使用事务的原则：\n无论何时，事务的使用总是能避免则避免。 模型设计先于事务，尽可能用模型设计规避事务，使用内嵌文档和数组来避免跨表的操作。 不要使用过大的事务（尽量控制在 1000 个文档更新以内）。 当必须使用事务时，尽可能让涉及事务的文档分布在同一个分片上，这将有效地提高效率。 MongoDB 对事务支持 事务属性 MongoDB MySQL A 原子性 单文档支持：1.x 就已经支持。\n复制集多表多行：4.0。\n分片集群多表多行：4.2 undo log C 一致性 writeConcern、readConcern 无损半同步复制/MGR I 隔离性 readConcern MVCC D 持久性 Journal and Replication redo log and binlog 使用 MongoDB 多文档事务的使用方式与关系数据库非常相似：\ntry (ClientSession clientSession = client.startSession()) { clientSession.startTransaction(); collection.insertOne(clientSession, docOne); collection.insertOne(clientSession, docTwo); clientSession.commitTransaction(); } 事务的隔离级别 事务完成前，事务外的操作对该事务所做的修改不可访问 // 主节点 db.tx.insertMany([{ x: 1 }, { x: 2 }]) var session = db.getMongo().startSession() // 开启事务 session.startTransaction() var coll = session.getDatabase(\"test\").getCollection(\"tx\") // 事务内修改 {x:1, y:1} coll.updateOne({x: 1}, {$set: {y: 1}}) // 事务内查询 {x:1} coll.findOne({x: 1}) // {x:1, y:1} // 事务外查询 {x:1} db.tx.findOne({x: 1}) // {x:1} // 提交事务 session.commitTransaction() // 或者回滚事务 session.abortTransaction() 如果事务内使用 {readConcern: \"snapshot\"}，则可以达到可重复读 Repeatable Read。 var session = db.getMongo().startSession() session.startTransaction({ readConcern: {level: \"snapshot\"}, writeConcern: {w: \"majority\"}}) var coll = session.getDatabase('test').getCollection(\"tx\") coll.findOne({x: 1}) db.tx.updateOne({x: 1}, {$set: {y: 2}}) db.tx.findOne({x: 1}) coll.findOne({x: 1}) session.abortTransaction() writeConcern writeConcern 决定一个写操作落到多少个节点上才算成功。MongoDB 支持客户端灵活配置写入策略（writeConcern），以满足不同场景的需求。\n语法格式：\n{ w: \u003cvalue\u003e, j: \u003cboolean\u003e, wtimeout: \u003cnumber\u003e } w：数据写入到 number 个节点才向客户端发送确认 {w: 0} 对客户端的写入不需要发送任何确认，适用于性能要求高，但不关注正确性的场景。 {w: 1} 默认的 writeConcern，数据写入到 Primary 就向客户端发送确认。 {w: \"majority\"} 数据写入到副本集大多数成员后向客户端发送确认，适用于对数据安全性要求比较高的场景，该选项会降低写入性能。 j：写入到 journal 持久化之后才向客户端确认 {j: false}，默认值，如果要求 Primary 写入持久化了才向客户端确认，则指定该选项为 true。 wtimeout: 写入超时时间，仅 w 的值大于 1 时有效。 当指定 w 时，数据需要成功写入 number 个节点才算成功，如果写入过程中有节点故障，可能导致这个条件一直不能满足，从而一直不能向客户端发送确认结果，针对这种情况，客户端可设置 wtimeout 选项来指定超时时间，当写入过程持续超过该时间仍未结束，则认为写入失败。 对于 5 个节点的复制集来说，写操作落到多少个节点上才算是安全的?\n3 个。最好使用 {w: \"majority\"}，这种方式更灵活一点，节点增加，减少不用再修改代码。\n测试，包含延迟节点的 3 个节点 pss 复制集：\ndb.user.insertOne({name:\"李四\"},{writeConcern:{w:\"majority\"}}) // 配置延迟节点 cfg = rs.conf() cfg.members[2].priority = 0 cfg.members[2].hidden = true cfg.members[2].secondaryDelaySecs = 60 rs.reconfig(cfg) // 等待延迟节点写入数据后才会响应 db.user.insertOne({name:\"王五\"},{writeConcern:{w:3}}) // 超时写入失败 db.user.insertOne({name:\"小明\"},{writeConcern:{w:3,wtimeout:3000}}) ℹ️ 虽然多于半数的 writeConcern 都是安全的，但通常只会设置 majority，因为这是等待写入延迟时间最短的选择； 不要设置 writeConcern 等于总节点数，因为一旦有一个节点故障，所有写操作都将失败； writeConcern 虽然会增加写操作延迟时间，但并不会显著增加集群压力，因此无论是否等待，写操作最终都会复制到所有节点上。设置 writeConcern 只是让写操作等待复制后再返回而已； 重要数据应用 {w: \"majority\"}，普通数据可以应用 {w: 1} 以确保最佳性能。 readPreference 在读取数据的过程中我们需要关注以下两个问题：\n从哪里读？（要不要做读写分离，是优先主节点还是优先从节点） 读哪些数据？ readPreference （读偏好）决定使用哪一个节点来满足正在发起的读请求。可选值包括：\nprimary: 只选择主节点，默认模式； primaryPreferred：优先选择主节点，如果主节点不可用则选择从节点； secondary：只选择从节点； secondaryPreferred：优先选择从节点， 如果从节点不可用则选择主节点； nearest：根据客户端对节点的 Ping 值判断节点的远近，选择从最近的节点读取。 合理的 ReadPreference 可以极大地扩展复制集的读性能，降低访问延迟。\nreadPreference 场景举例：\n用户下订单后马上将用户转到订单详情页 ———— primary/primaryPreferred。因为此时从节点可能还没复制到新订单； 用户查询自己下过的订单 ———— secondary/secondaryPreferred。查询历史订单对时效性通常没有太高要求； 生成报表 ———— secondary。报表对时效性要求不高，但资源需求大，可以在从节点单独处理，避免对线上用户造成影响； 将用户上传的图片分发到全世界，让各地用户能够就近读取 ———— nearest。每个地区的应用选择最近的节点读取数据。 readPreference 配置：\n通过 MongoDB 的连接串参数：\nmongodb://host1:27107,host2:27107,host3:27017/?replicaSet=rs0\u0026readPreference=secondary 通过 MongoDB 驱动程序 API：\nMongoCollection.withReadPreference(ReadPreference readPref) Mongo Shell：\ndb.collection.find().readPref(\"secondary\") 从节点读测试 主节点写入 {count:1}, 观察该条数据在各个节点均可见 mongosh --host rs0/localhost:28017 rs0:PRIMARY\u003e db.user.insert({count:3},{writeConcern:{w:1}}) 在 primary 节点中调用 readPref(\"secondary\") 查询从节点用直连方式（mongosh localhost:28017）会查到数据，需要通过 mongosh --host rs0/localhost:28017 方式连接复制集，参考：https://jira.mongodb.org/browse/SERVER-22289 在两个从节点分别执行 db.fsyncLock() 来锁定写入（同步） mongosh localhost:28018 rs0:SECONDARY\u003e rs.secondaryOk() rs0:SECONDARY\u003e db.fsyncLock() ℹ️ db.fsyncLock() 可以用来锁住数据同步，需要使用 db.fsyncUnlock() 来解锁。可以用来模拟数据同步阻塞的场景。 主节点写入 {count:2} rs0:PRIMARY\u003e db.user.insert({count:2},{writeConcern:{w:1}}) rs0:PRIMARY\u003e db.user.find() # 可以读到 {count:2} rs0:PRIMARY\u003e db.user.find().readPref(\"secondary\") # {count:2} 不可见 解除从节点锁定 db.fsyncUnlock() rs0:SECONDARY\u003e db.fsyncUnlock() 主节点中查从节点数据 rs0:PRIMARY\u003e db.user.find().readPref(\"secondary\") # {count:2} 可见 Tag readPreference 只能控制使用一类节点。Tag 则可以将节点选择控制到一个或几个节点。考虑以下场景：\n一个 5 个节点的复制集； 3 个节点硬件较好，专用于服务线上客户； 2 个节点硬件较差，专用于生成报表； 可以使用 Tag 来达到这样的控制目的：\n为 3 个较好的节点打上 {purpose: \"online\"}； 为 2 个较差的节点打上 {purpose: \"analyse\"}； 在线应用读取时指定 online，报表读取时指定 analyse。 // 为复制集节点添加标签 conf = rs.conf() conf.members[1].tags = { purpose: \"online\"} conf.members[4].tags = { purpose: \"analyse\"} rs.reconfig(conf) // 查询 db.collection.find({}).readPref( \"secondary\", [ {purpose: \"online\"} ] ) ℹ️ 指定 readPreference 时也应注意高可用问题。例如将 readPreference 指定 primary，则发生故障转移不存在 primary 期间将没有节点可读。如果业务允许，则应选择 primaryPreferred； 使用 Tag 时也会遇到同样的问题，如果只有一个节点拥有一个特定 Tag，则在这个节点失效时将无节点可读。这在有时候是期望的结果，有时候不是。例如： 如果报表使用的节点失效，即使不生成报表，通常也不希望将报表负载转移到其他节点上，此时只有一个节点有报表 Tag 是合理的选择； 如果线上节点失效，通常希望有替代节点，所以应该保持多个节点有同样的 Tag； Tag 有时需要与优先级、选举权综合考虑。例如做报表的节点通常不会希望它成为主节点，则优先级应为 0。 readConcern readConcern 决定这个节点上的数据哪些是可读的，类似于事务隔离级别。可选值包括：\navailable：读取所有可用的数据。 local：读取所有可用且属于当前分片的数据。 majority：读取在大多数节点上提交完成的数据。数据读一致性的充分保证。 linearizable：可线性化读取文档，仅支持从主节点读。 snapshot：读取最近快照中的数据，仅可用于多文档事务，类似 MySQL 中的串行化隔离级别。 local 和 available 在复制集中 local 和 available 是没有区别的，两者的区别主要体现在分片集上。\n考虑以下场景：\n一个 chunk x 正在从 shard1 向 shard2 迁移； 整个迁移过程中 chunk x 中的部分数据会在 shard1 和 shard2 中同时存在，但源分片 shard1 仍然是 chunk x 的负责方： 所有对 chunk x 的读写操作仍然进入 shard1； config 中记录的信息 chunk x 仍然属于 shard1； 此时如果读 shard2，则会体现出 local 和 available 的区别： local：只取应该由 shard2 负责的数据（不包括 x）； available：shard2 上有什么就读什么（包括 x）； ℹ️ 虽然看上去总是应该选择 local，但毕竟对结果集进行过滤会造成额外消耗。在一些无关紧要的场景（例如统计）下，也可以考虑 available。 MongoDB \u003c=3.6 不支持对从节点使用 {readConcern: \"local\"}。 从主节点读取数据时默认 readConcern 是 local，从从节点读取数据时默认 readConcern 是 available（向前兼容原因）。 majority 读取大多数据节点上都提交了的数据。类似于读已提交隔离级别，不过在集群中多数节点都已提交。\n如何实现？\n节点上维护多个 x 版本（MVCC 机制），MongoDB 通过维护多个快照来链接不同的版本：\n每个 “被大多数节点确认过的版本” 都是一个快照，注意是大多数节点都确认过的版本； 快照持续到没有人使用为止才被删除； 测试 readConcern: majority vs local：\n将复制集中的两个从节点使用 db.fsyncLock() 锁住写入（模拟同步延迟） 测试 rs0:PRIMARY\u003e db.user.insert({count:10},{writeConcern:{w:1}}) rs0:PRIMARY\u003e db.user.find().readConcern(\"local\") # 可以读到 {count:10} rs0:PRIMARY\u003e db.user.find().readConcern(\"majority\") # 都不到 {count:10}，因为其他两个节点阻塞住了，没有同步到数据，不符合 majority 的要求 update 与 remove 与上同理。\nmajority 避免脏读问题 MongoDB 中的回滚：\n写操作到达大多数节点之前都是不安全的，一旦主节点崩溃，而从节点还没复制到该次操作，刚才的写操作就丢失了； 把一次写操作视为一个事务，从事务的角度，可以认为事务被回滚了。 所以从分布式系统的角度来看，事务的提交被提升到了分布式集群的多个节点级别的“提交”，而不再是单个节点上的“提交”。\n在可能发生回滚的前提下考虑脏读问题：\n如果在一次写操作到达大多数节点前读取了这个写操作，然后因为系统故障该操作回滚了，则发生了脏读问题； 使用 {readConcern: \"majority\"} 可以有效避免脏读。\n如何安全的读写分离 考虑如下场景:\n向主节点写入一条数据; 立即从从节点读取这条数据。 如何保证自己能够读到刚刚写入的数据?\n下述方式有可能读不到刚写入的数据：\ndb.orders.insert({oid:101,sku:\"kite\",q:1}) db.orders.find({oid:101}).readPref(\"secondary\") 使用 writeConcern+readConcern majority 来解决：\ndb.orders.insert({oid:101,sku:\"kite\",q:1},{writeConcern:{w:\"majority\"}}) db.orders.find({oid:101}).readPref(\"secondary\").readConcern(\"majority\") linearizable 只读取大多数节点确认过的数据。和 majority 最大差别是保证绝对的操作线性顺序 ：\n在写操作自然时间后面的发生的读，一定可以读到之前的写。 只对读取单个文档时有效。 可能导致非常慢的读，因此总是建议配合使用 maxTimeMS。 snapshot {readConcern: \"snapshot\"} 只在多文档事务中生效。将一个事务的 readConcern 设置为 snapshot，将保证在事务中的读：\n不出现脏读； 不出现不可重复读； 不出现幻读。 因为所有的读都将使用同一个快照，直到事务提交为止该快照才被释放。\n事务超时 在执行事务的过程中，如果操作太多，或者存在一些长时间的等待，则可能会产生异常：Transaction has been aborted。\n原因在于，默认情况下 MongoDB 会为每个事务设置 1 分钟的超时时间，如果在该时间内没有提交，就会强制将其终止。该超时时间可以通过 transactionLifetimeLimitSecond 变量设定。\n事务的错误处理 MongoDB 的事务错误处理机制不同于关系数据库：\n当一个事务开始后，如果事务要修改的文档在事务外部被修改过，则事务修改这个文档时会触发 Abort 错误，因为此时的修改冲突了（这种直接避免了幻读问题）。这种情况下，只需要简单地重做事务就可以了。 如果一个事务已经开始修改一个文档，在事务以外尝试修改同一个文档，则事务以外的修改会等待事务完成才能继续进行。 写冲突测试 开 3 个 mongo shell 均执行下述语句：\nvar session = db.getMongo().startSession() session.startTransaction({ readConcern: {level: \"majority\"}, writeConcern: {w: \"majority\"}}) var coll = session.getDatabase('test').getCollection(\"tx\") 窗口 1： 正常结束\ncoll.updateOne({x: 1}, {$set: {y: 1}}) 窗口 2：异常 – WriteConflict error\n解决方案：重启事务\ncoll.updateOne({x: 1}, {$set: {y: 2}}) 窗口 3：事务外更新，需等待\ndb.tx.updateOne({x: 1}, {$set: {y: 3}}) 注意事项 可以实现和关系型数据库类似的事务场景 必须使用与 MongoDB 4.2及以上 兼容的驱动； 事务默认必须在 60 秒（可调）内完成，否则将被取消； 涉及事务的分片不能使用仲裁节点； 事务会影响 chunk 迁移效率。正在迁移的 chunk 也可能造成事务提交失败（重试 即可）； 多文档事务中的读操作必须使用主节点读； readConcern 只应该在事务级别设置，不能设置在每次读写操作上。 "},"title":"存储原理"},"/db-learn/docs/mongo/advanced/05_optimization/":{"data":{"":"","建模案例分析#建模案例分析":"官方建模示例。\n一对一关系模型 例如，模式包含两个实体，一个 patron 和一个 address：\n// patron document { _id: \"joe\", name: \"Joe Bookreader\" } // address document { street: \"123 Fake Street\", city: \"Faketon\", state: \"MA\", zip: \"12345\" } 嵌入式文档模式 将 address 信息嵌入 patron 文档：\n{ _id: \"joe\", name: \"Joe Bookreader\", address: { street: \"123 Fake Street\", city: \"Faketon\", state: \"MA\", zip: \"12345\" } } 一个人的地址信息不会太多，那么使用嵌入式文档就比较合适。可以一次查询拿到所有信息。\n子集模式 嵌入式文档模式的一个潜在问题是，例如一个文档包含了很多信息字段，但是通常在使用时，只有其中的几个字段才会被用到。如果将所有的字段都放到一个文档中，那么在查询时，返回所有的字段，会导致网络传输和磁盘空间的浪费。\n{ \"_id\": 1, \"title\": \"The Arrival of a Train\", \"year\": 1896, \"runtime\": 1, \"released\": ISODate(\"01-25-1896\"), \"poster\": \"http://ia.media-imdb.com/images/M/MV5BMjEyNDk5MDYzOV5BMl5BanBnXkFtZTgwNjIxMTEwMzE@._V1_SX300.jpg\", \"plot\": \"A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, ...\", \"fullplot\": \"A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, the line dissolves. The doors of the railway-cars open, and people on the platform help passengers to get off.\", \"lastupdated\": ISODate(\"2015-08-15T10:06:53\"), \"type\": \"movie\", \"directors\": [ \"Auguste Lumière\", \"Louis Lumière\" ], \"imdb\": { \"rating\": 7.3, \"votes\": 5043, \"id\": 12 }, \"countries\": [ \"France\" ], \"genres\": [ \"Documentary\", \"Short\" ], \"tomatoes\": { \"viewer\": { \"rating\": 3.7, \"numReviews\": 59 }, \"lastUpdated\": ISODate(\"2020-01-09T00:02:53\") } } movie 只存储常用的基本信息：\n// movie collection { \"_id\": 1, \"title\": \"The Arrival of a Train\", \"year\": 1896, \"runtime\": 1, \"released\": ISODate(\"1896-01-25\"), \"type\": \"movie\", \"directors\": [ \"Auguste Lumière\", \"Louis Lumière\" ], \"countries\": [ \"France\" ], \"genres\": [ \"Documentary\", \"Short\" ], } movie_details 包含每部电影的其他不常访问的数据，通过 movie_id 来建立关联关系：\n// movie_details collection { \"_id\": 156, \"movie_id\": 1, // reference to the movie collection \"poster\": \"http://ia.media-imdb.com/images/M/MV5BMjEyNDk5MDYzOV5BMl5BanBnXkFtZTgwNjIxMTEwMzE@._V1_SX300.jpg\", \"plot\": \"A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, ...\", \"fullplot\": \"A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, the line dissolves. The doors of the railway-cars open, and people on the platform help passengers to get off.\", \"lastupdated\": ISODate(\"2015-08-15T10:06:53\"), \"imdb\": { \"rating\": 7.3, \"votes\": 5043, \"id\": 12 }, \"tomatoes\": { \"viewer\": { \"rating\": 3.7, \"numReviews\": 59 }, \"lastUpdated\": ISODate(\"2020-01-29T00:02:53\") } } 这种方法可以提高读取性能。\n一对多关系模型 嵌入式文档模式 一个人有多个地址：\n// patron document { _id: \"joe\", name: \"Joe Bookreader\" } // address documents { patron_id: \"joe\", // reference to patron document street: \"123 Fake Street\", city: \"Faketon\", state: \"MA\", zip: \"12345\" } { patron_id: \"joe\", street: \"1 Some Other Street\", city: \"Boston\", state: \"MA\", zip: \"12345\" } 将 address 数据嵌入到 patron 中：\n{ _id: \"joe\", name: \"Joe Bookreader\", addresses: [ { street: \"123 Fake Street\", city: \"Faketon\", state: \"MA\", zip: \"12345\" }, { street: \"1 Some Other Street\", city: \"Boston\", state: \"MA\", zip: \"12345\" } ] } 这种方式只适合数据量少的场景。如果数组无限增长，会导致网络拥堵。而且大量的查询请求，肯定会性能下降。\n子集模式 嵌入式文档模式的一个潜在问题是，它可能导致文档过大，尤其是在嵌入式字段没有限制的情况下。\n考虑一个包含产品评论列表的电商站点：\n{ \"_id\": 1, \"name\": \"Super Widget\", \"description\": \"This is the most useful item in your toolbox.\", \"price\": { \"value\": NumberDecimal(\"119.99\"), \"currency\": \"USD\" }, \"reviews\": [ { \"review_id\": 786, \"review_author\": \"Kristina\", \"review_text\": \"This is indeed an amazing widget.\", \"published_date\": ISODate(\"2019-02-18\") } ... { \"review_id\": 777, \"review_author\": \"Pablo\", \"review_text\": \"Amazing!\", \"published_date\": ISODate(\"2019-02-16\") } ] } 将该集合拆分为两个集合，而不存储该产品的所有评论：\nproduct 集合存储每个产品的信息，包括该产品的 10 条最新评论。评论按时间倒序排列，用户访问产品页面时，应用程序会加载最近十条评论：\n{ \"_id\": 1, \"name\": \"Super Widget\", \"description\": \"This is the most useful item in your toolbox.\", \"price\": { \"value\": NumberDecimal(\"119.99\"), \"currency\": \"USD\" }, \"reviews\": [ { \"review_id\": 786, \"review_author\": \"Kristina\", \"review_text\": \"This is indeed an amazing widget.\", \"published_date\": ISODate(\"2019-02-18\") } ... { \"review_id\": 777, \"review_author\": \"Pablo\", \"review_text\": \"Amazing!\", \"published_date\": ISODate(\"2019-02-16\") } ] } review 集合存储所有评论。每条评论都包含对相应产品的引用 product_id：\n{ \"review_id\": 786, \"product_id\": 1, \"review_author\": \"Kristina\", \"review_text\": \"This is indeed an amazing widget.\", \"published_date\": ISODate(\"2019-02-18\") } { \"review_id\": 785, \"product_id\": 1, \"review_author\": \"Trina\", \"review_text\": \"Nice product. Slow shipping.\", \"published_date\": ISODate(\"2019-02-17\") } ... { \"review_id\": 1, \"product_id\": 1, \"review_author\": \"Hans\", \"review_text\": \"Meh, it's okay.\", \"published_date\": ISODate(\"2017-12-06\") } 引用模式 将出版商文档嵌入图书文档会导致出版商数据重复：\n{ title: \"MongoDB: The Definitive Guide\", author: [ \"Kristina Chodorow\", \"Mike Dirolf\" ], published_date: ISODate(\"2010-09-24\"), pages: 216, language: \"English\", publisher: { name: \"O'Reilly Media\", founded: 1980, location: \"CA\" } } { title: \"50 Tips and Tricks for MongoDB Developer\", author: \"Kristina Chodorow\", published_date: ISODate(\"2011-05-06\"), pages: 68, language: \"English\", publisher: { name: \"O'Reilly Media\", founded: 1980, location: \"CA\" } } 为避免出现重复的出版商数据，使用引用将出版商信息保存在图书集合之外的单独集合中。\n使用引用时，关系的增长将决定引用的存储方式。如果每个出版商的图书数量较少且增长有限，则将图书引用存储在出版商文档中有时可能十分有用。\n{ name: \"O'Reilly Media\", founded: 1980, location: \"CA\", books: [123456789, 234567890, ...] } { _id: 123456789, title: \"MongoDB: The Definitive Guide\", author: [ \"Kristina Chodorow\", \"Mike Dirolf\" ], published_date: ISODate(\"2010-09-24\"), pages: 216, language: \"English\" } { _id: 234567890, title: \"50 Tips and Tricks for MongoDB Developer\", author: \"Kristina Chodorow\", published_date: ISODate(\"2011-05-06\"), pages: 68, language: \"English\" } 相反，当每个出版商的图书数量没有限制时，此数据模型将导致可变且不断增长的数组，为避免出现可变且不断增长的数组，可以将出版商的引用存储在图书文档中：\n{ _id: \"oreilly\", name: \"O'Reilly Media\", founded: 1980, location: \"CA\" } { _id: 123456789, title: \"MongoDB: The Definitive Guide\", author: [ \"Kristina Chodorow\", \"Mike Dirolf\" ], published_date: ISODate(\"2010-09-24\"), pages: 216, language: \"English\", publisher_id: \"oreilly\" } { _id: 234567890, title: \"50 Tips and Tricks for MongoDB Developer\", author: \"Kristina Chodorow\", published_date: ISODate(\"2011-05-06\"), pages: 68, language: \"English\", publisher_id: \"oreilly\" } 树状结构模型 这种也不复杂，就是每一个节点就是一个文档，只不过增加了 parent 字段：\ndb.categories.insertMany( [ { _id: \"MongoDB\", parent: \"Databases\" }, { _id: \"dbm\", parent: \"Databases\" }, { _id: \"Databases\", parent: \"Programming\" }, { _id: \"Languages\", parent: \"Programming\" }, { _id: \"Programming\", parent: \"Books\" }, { _id: \"Books\", parent: null } ]) 检索节点的父节点的查询：\ndb.categories.findOne( { _id: \"MongoDB\" } ).parent 如果是父节点找子节点的方式，增加一个 children 字段：\ndb.categories.insertMany( [ { _id: \"MongoDB\", children: [] }, { _id: \"dbm\", children: [] }, { _id: \"Databases\", children: [ \"MongoDB\", \"dbm\" ] }, { _id: \"Languages\", children: [] }, { _id: \"Programming\", children: [ \"Databases\", \"Languages\" ] }, { _id: \"Books\", children: [ \"Programming\" ] } ] ) 朋友圈评论内容管理 社交类的APP需求，一般都会引入“朋友圈”功能，这个产品特性有一个非常重要的功能就是评论体系。\n先整理下需求：\n这个 APP 希望点赞和评论信息都要包含头像信息： 点赞列表，点赞用户的昵称，头像； 评论列表，评论用户的昵称，头像； 数据查询则相对简单： 根据分享 ID，批量的查询出 10 条分享里的所有评论内容； 建模 好的设计：\n{ \"_id\": 41, \"uid\": \"100000\", \"praise_uid_list\": [ \"100010\", \"100011\", \"100012\" ], \"comment_msg_list\": [ { \"100013\": \"good\" }, { \"100014\": \"bad\" } ] } 昵称和头像，通过 uid 去用户表查询。通常头像，用户名等信息可以做一层缓存，甚至存储在 APP 端。\n多列数据结构 需求是基于电影票售卖的不同渠道价格存储。某一个场次的电影，不同的销售渠道对应不同的价格。整理需求为：\n数据字段： 场次信息； 播放影片信息； 渠道信息，与其对应的价格； 渠道数量最多几十个； 业务查询有两种： 根据电影场次，查询某一个渠道的价格； 根据渠道信息，查询对应的所有场次信息； 建模 不好的设计：\n{ \"scheduleId\": \"0001\", \"movie\": \"你的名字\", \"price\": { \"gewala\": 30, \"maoyan\": 50, \"taopiao\": 20 } } 数据表达上基本没有字段冗余，非常紧凑。再来看业务查询能力：\n根据电影场次，查询某一个渠道的价格： 建立 createIndex({scheduleId:1, movie:1}) 索引，虽然对 price 来说没有创建索引优化，但通过前面两个维度，已经可以定位到唯一的文档，查询效率上来说尚可； 根据渠道信息，查询对应的所有场次信息： 为了优化这种查询，需要对每个渠道分别建立索引，例如：createIndex({\"price.gewala\":1}) 、createIndex({\"price.maoyan\":1})。 但渠道会经常变化，并且为了支持此类查询，肯能需要创建几十个索引，维护困难。 { \"scheduleId\": \"0001\", \"movie\": \"你的名字\", \"channel\": \"gewala\", \"price\": 30 } { \"scheduleId\": \"0001\", \"movie\": \"你的名字\", \"channel\": \"maoyan\", \"price\": 50 } { \"scheduleId\": \"0001\", \"movie\": \"你的名字\", \"channel\": \"taopiao\", \"price\": 20 } 与上面的方案相比，把整个存储对象结构进行了平铺展开，变成了一种表结构，传统的关系数据库多数采用这种类型的方案。信息表达上，把一个对象按照渠道维度拆成多个，其他的字段进行了冗余存储。如果业务需求再复杂点，造成的信息冗余膨胀非常巨大。膨胀后带来的副作用会有磁盘空间占用上升，内存命中率降低等缺点。\n根据电影场次，查询某一个渠道的价格： 建立 createIndex({scheduleId:1, movie:1, channel:1}) 索引； 根据渠道信息，查询对应的所有场次信息： 建立 createIndex({channel:1}) 索引； 好的设计：\n{ \"scheduleId\": \"0001\", \"movie\": \"你的名字\", \"provider\": [ { \"channel\": \"gewala\", \"price\": 30 }, { \"channel\": \"maoyan\", \"price\": 50 }, { \"channel\": \"taopiao\", \"price\": 20 } ] } 使用了在 MongoDB 建模中非常容易忽略的结构——数组。查询方面的处理，是可以建立 Multikey Index 索引。\n根据电影场次，查询某一个渠道的价格： 建立 createIndex({scheduleId:1, movie:1, \"provider.channel\":1}) 索引。 根据渠道信息，查询对应的所有场次信息： 建立 createIndex({\"provider.channel\":1}) 索引； 物联网时序数据建模 案例背景是来自真实的业务，美国州际公路的流量统计。数据库需要提供的能力：\n存储事件数据 提供分析查询能力 理想的平衡点： 内存使用 写入性能 读取分析性能 可以部署在常见的硬件平台上 建模 每个事件用一个独立的文档存储：\n{ segId: \"I80_mile23\", speed: 63, ts: ISODate(\"2013-10-16T22:07:38.000-0500\") } 每辆车每秒钟都会写入一条信息。多少的信息，就有多少条数据，数据量增长非常快。数据采集操作全部是 Insert 语句。\n每分钟的信息用一个独立的文档存储（存储平均值）：\n{ segId: \"I80_mile23\", speed_num: 18, speed_sum: 1134, ts: ISODate(\"2013-10-16T22:07:00.000-0500\") } 对每分钟的平均速度计算非常友好（speed_sum/speed_num）； 数据采集操作基本是 Update 语句； 数据精度降为一分钟； 每分钟的信息用一个独立的文档存储（秒级记录）：\n{ segId: \"I80_mile23\", speed: {0: 63, 1: 58, ... , 58: 66, 59: 64}, ts: ISODate(\"2013-10-16T22:07:00.000-0500\") } 每秒的数据都存储在一个文档中； 数据采集操作基本是 Update 语句； 每小时的信息用一个独立的文档存储（秒级记录）：\n{ segId: \"I80_mile23\", speed: {0: 63, 1: 58, ... , 3598: 54, 3599: 55}, ts: ISODate(\"2013-10-16T22:00:00.000-0500\") } 相比上面的方案更进一步，从分钟到小时：\n每小时的数据都存储在一个文档中； 数据采集操作基本是 Update 语句； 更新最后一个时间点（第3599秒），需要3599次迭代（虽然是在同一个文档中） 进一步优化：\n{ segId: \"I80_mile23\", speed: { 0: {0:47, ..., 59:45}, ..., 59: {0:65, ... , 59:56} } ts: ISODate(\"2013-10-16T22:00:00.000-0500\") } 用了嵌套的手法把秒级别的数据存储在小时数据里； 数据采集操作基本是 Update 语句； 更新最后一个时间点（第 3599 秒），需要 59+59 次迭代； 每小时的信息用一个独立的文档存储 VS 每分钟的信息用一个独立的文档存储 从写入上看：因为 WiredTiger 是每分钟进行一次刷盘，所以每小时一个文档的方案，在这一个小时内要被反复的 load 到 PageCache 中，再刷盘；所以，按分钟存储更合理些，可以减少 IO 操作。 从读取上看：前者的数据信息量较大，正常的业务请求未必需要这么多的数据，有很大一部分是浪费的。业务上一般很少一次取一个小时的数据，统计的时候可能是按分钟级来计算的。 从索引上看：前者的索引更小，内存利用率更高。 ","开发规范#开发规范":" 命名原则。数据库、集合命名需要简单易懂，数据库名使用小写字符，集合名称使用统一命名风格，可以统一大小写或使用驼峰式命名。数据库名和集合名称均不能超过 64 个字符。 集合设计。对少量数据的包含关系，使用嵌套模式有利于读性能和保证原子性的写入。对于复杂的关联关系，以及后期可能发生演进变化的情况，建议使用引用模式。 文档设计。避免使用大文档，MongoDB 的文档最大不能超过 16MB。如果使用了内嵌的数组对象或子文档，应该保证内嵌数据不会无限制地增长。在文档结构上，尽可能减少字段名的长度，MongoDB 会保存文档中的字段名，因此字段名称会影响整个集合的大小以及内存的需求。一般建议将字段名称控制在 32 个字符以内。 索引设计。在必要时使用索引加速查询。避免建立过多的索引，单个集合建议不超过 10 个索引。MongoDB 对集合的写入操作很可能也会触发索引的写入，从而触发更多的I/O操作。无效的索引会导致内存空间的浪费，因此有必要对索引进行审视，及时清理不使用或不合理的索引。遵循索引优化原则，如覆盖索引、优先前缀匹配等，使用 explain 命令分析索引性能。 分片设计。对可能出现快速增长或读写压力较大的业务表考虑分片。分片键的设计满足均衡分布的目标，业务上尽量避免广播查询。应尽早确定分片策略，最好在集合达到 256GB 之前就进行分片。如果集合中存在唯一性索引，则应该确保该索引覆盖分片键，避免冲突。为了降低风险，单个分片的数据集合大小建议不超过 2TB。 升级设计。应用上需支持对旧版本数据的兼容性，在添加唯一性约束索引之前，对数据表进行检查并及时清理冗余的数据。新增、修改数据库对象等操作需要经过评审，并保持对数据字典进行更新。 考虑数据老化问题，要及时清理无效、过期的数据，优先考虑为系统日志、历史数据表添加合理的老化策略。 数据一致性方面，非关键业务使用默认的 WriteConcern：1（更高性能写入）；对于关键业务类，使用 WriteConcern：majority 保证一致性（性能下降）。如果业务上严格不允许脏读，则使用 ReadConcern：majority 选项。 使用 update、findAndModify 对数据进行修改时，如果设置了 upsert：true，则必须使用唯一性索引避免产生重复数据。 业务上尽量避免短连接，使用官方最新驱动的连接池实现，控制客户端连接池的大小，最大值建议不超过 200。 对大量数据写入使用 Bulk Write 批量化 API，建议使用无序批次更新。 优先使用单文档事务保证原子性，如果需要使用多文档事务，则必须保证事务尽可能小，一个事务的执行时间最长不能超过 60s。 在条件允许的情况下，利用读写分离降低主节点压力。对于一些统计分析类的查询操作，可优先从节点上执行。 考虑业务数据的隔离，例如将配置数据、历史数据存放到不同的数据库中。微服务之间使用单独的数据库，尽量避免跨库访问。 维护数据字典文档并保持更新，提前按不同的业务进行数据容量的规划。 ","调优#调优":"导致 MongoDB 性能不佳的原因 慢查询 阻塞等待 硬件资源不足 1、2 通常是因为模型/索引设计不佳导致的。\n排查思路：按 1-2-3 依次排查。\n影响 MongoDB 性能的因素 网络问题是第一个要排查的问题，网络没有问题再从客户端、服务端去排查问题。\n当 WiredTiger 开启压缩功能时，压缩效率会随着压缩算法的不同而变化。\n在 MongoDB 中，WiredTiger 支持的压缩算法有 snappy、zlib、zstd 等。\n压缩效率 zstd \u003e zlib \u003e snappy，压缩效率越高，文件体积越小，传输的时候越快。但是压缩效率越高也以为这 CPU 消耗越大。\n当内存中的数据需要持久化到磁盘的时候，WiredTiger 会先将数据压缩后再进行持久化，可以节约磁盘空间。当从磁盘中加载的时候，也需要进行解压缩。\n性能监控工具 mongostat mongostat 是 MongoDB 自带的监控工具，其可以提供数据库节点或者整个集群当前的状态视图。该功能的设计非常类似于 Linux 系统中的 vmstat 命令，可以呈现出实时的状态变化。不同的是，mongostat 所监视的对象是数据库进程。mongostat 常用于查看当前的 QPS/内存使用/连接数，以及多个分片的压力分布。mongostat 采用 Go 语言实现，其内部使用了 db.serverStatus() 命令，执行用户需具备 clusterMonitor 角色权限。\nmongostat -h 192.168.65.174 --port 28017 -ufox -pfox --authenticationDatabase=admin --discover -n 300 2 参数说明:\n-h：指定监听的主机，分片集群模式下指定到一个 mongos 实例，也可以指定单个 mongod，或者复制集的多个节点。 --port：接入的端口，如果不提供则默认为 27017。 -u：接入用户名，等同于 -user。 -p：接入密码，等同于 -password。 --authenticationDatabase：鉴权数据库。 --discover：启用自动发现，可展示集群中所有分片节点的状态。 -n 300 2：表示输出 300 次，每次间隔 2s。也可以不指定 “-n 300”，此时会一直保持输出。 指标说明：\n指标名 说明 inserts 每秒插入数 query 每秒查询数 update 每秒更新数 delete 每秒删除数 getmore 每秒 getmore 数 command 每秒命令数，涵盖了内部的一些操作 dirty WiredTiger 缓存中脏数据百分比 used WiredTiger 正在使用的缓存百分比（如果这个百分比过高，说明内存可能不够用了，可以将内存参数配置的大一点） flushesWiredTiger 执行 CheckPoint 的次数 vsize 虚拟内存使用量 res 物理内存使用量 qrw 客户端读写等待队列数量，高并发时，一般队列值会升高 arw 客户端读写活跃个数 netIn 网络接收数据量 netOut 网络发送数据量 conn 当前连接数 set 所属复制集名称 repl 复制节点状态（主节点/二级节点……） time 时间戳 需要关注的指标：\n插入、删除、修改、查询的速率是否产生较大波动，是否超出预期。 qrw、arw：代表目前正在排队(queue)的 read 和 write 请求数量。队列是否较高，若长时间大于 0 则说明此时读写速度较慢。 conn：连接数是否太多。 dirty：如果这个百分比过高，说明内存中的数据刷盘的效率不高，磁盘 IO 可能存在瓶颈。 netIn、netOut：是否超过网络带宽阈值。 repl：状态是否异常，如 PRI、SEC、RTR 为正常，若出现 REC 等异常值则需要修复。 使用交互模式 mongostat 一般采用滚动式输出，即每一个间隔后的状态数据会被追加到控制台中。从 MongoDB 3.4 开始增加了 --interactive 选项，用来实现非滚动式的监视，非常方便。\nmongostat -h 192.168.65.174 --port 28017 -ufox -pfox --authenticationDatabase=admin --discover --interactive -n 2 mongotop mongotop 命令可用于查看数据库的热点表，通过观察 mongotop 的输出，可以判定是哪些集合占用了大部分读写时间。mongotop 与 mongostat 的实现原理类似，同样需要 clusterMonitor 角色权限。\nmongotop -h 192.168.65.174 --port=28017 -ufox -pfox --authenticationDatabase=admin 默认情况下，mongotop 会持续地每秒输出当前的热点表：\n指标说明：\n指标名 说明 ns 集合名称空间 total 花费在该集合上的时长 read 花费在该集合上的读操作时长 write 花费在该集合上的写操作时长 需要关注的因素主要包括：\n热点表操作耗费时长是否过高。这里的时长是在一定的时间间隔内的统计值，它代表某个集合读写操作所耗费的时间总量。在业务高峰期时，核心表的读写操作一般比平时高一些，通过 mongotop 的输出可以对业务尖峰做出一些判断。 是否存在非预期的热点表。一些慢操作导致的性能问题可以从 mongotop 的结果中体现出来。 mongotop 的统计周期、输出总量都是可以设定的：\n# 最多输出 100 次，每次间隔时间为 2s mongotop -h 192.168.65.174 --port=28017 -ufox -pfox --authenticationDatabase=admin -n 100 2 Profiler 模块 Profiler 模块可以用来记录、分析 MongoDB 的详细操作日志。默认情况下该功能是关闭的，对某个业务库开启 Profiler 模块之后，符合条件的慢操作日志会被写入该库的 system.profile 集合中。Profiler 的设计很像代码的日志功能，其提供了几种调试级别：\n0：日志关闭，无任何输出。 1：部分开启，仅符合条件（时长大于 slowms）的操作日志会被记录。 2：日志全开，所有的操作日志都被记录。 对当前的数据库开启 Profiler 模块：\n# 将 level 设置为 2，此时所有的操作会被记录下来。 db.setProfilingLevel(2) # 检查是否生效 db.getProfilingStatus() \u003e db.setProfilingLevel(2) { \"was\" : 0, \"slowms\" : 100, # 默认 100 ms \"sampleRate\" : 1, # ... } slowms 是慢操作的阈值，单位是毫秒； sampleRate 表示日志随机采样的比例，1.0 则表示满足条件的全部输出。 如果希望只记录时长超过 500ms 的操作，则可以将 level 设置为 1：\ndb.setProfilingLevel(1,500) 进一步设置随机采样的比例：\ndb.setProfilingLevel(1,{slowms:500,sampleRate:0.5}) 查看操作日志 开启 Profiler 模块之后，可以通过 system.profile 集合查看最近发生的操作日志：\ndb.system.profile.find().limit(5).sort({ts:-1}).pretty() 查询结果示例：\n{ \"op\" : \"insert\", \"ns\" : \"test.emp\", \"command\" : { \"insert\" : \"emp\", \"documents\" : [ { \"_id\" : ObjectId(\"642c23309900000000000000\"), \"name\" : \"guanyu\", \"age\" : 20 } ], \"ordered\" : true, lsid: { \"id\" : UUID(\"00000000-0000-0000-0000-000000000000\") }, txnNumber: NumberLong(1), // ... }, } op：操作类型，描述增加、删除、修改、查询。 ns：名称空间，格式为 {db}.{collection}。 command：原始的命令文档。 cursorid：游标ID。 numYield：操作数，大于0表示等待锁或者是磁盘I/O操作。 nreturned：返回条目数。 keysExamined：扫描索引条目数，如果比nreturned大出很多，则说明查询效率不高。docsExamined：扫描文档条目数，如果比nreturned大出很多，则说明查询效率不高。 locks：锁占用的情况。 storage：存储引擎层的执行信息。 responseLength：响应数据大小（字节数），一次性查询太多的数据会影响性能，可以使用limit、batchSize进行一些限制。 millis：命令执行的时长，单位是毫秒。 planSummary：查询计划的概要，如IXSCAN表示使用了索引扫描。 execStats：执行过程统计信息。 ts：命令执行的时间点。 system.profile 是一个集合，可以像查询普通集合一样加上过滤条件，例如查询 shop 库的 user 集合的 update 操作：\ndb.system.profile.find({op: \"update\", ns: \"shop.user\"}) ℹ️ system.profile 是一个 1MB 的固定大小的集合，随着记录日志的增多，一些旧的记录会被滚动删除。 在线上开启 Profiler 模块需要非常谨慎，这是因为其对 MongoDB 的性能影响比较大。建议按需部分开启，同时 slowms 的值不要设置太低。 sampleRate 的默认值是 1.0，该字段可以控制记录日志的命令数比例，但只有在 MongoDB 4.0 版本之后才支持。 Profiler 模块的设置是内存级的，重启服务器后会自动恢复默认状态。 db.currentOp() Profiler 模块所记录的日志都是已经发生的事情，db.currentOp() 命令则与此相反，它可以用来查看数据库当前正在执行的一些操作。想象一下，当数据库系统的 CPU 发生骤增时，我们最想做的无非是快速找到问题的根源，这时 db.currentOp 就派上用场了。\ndb.currentOp() 读取的是当前数据库的命令快照，该命令可以返回许多有用的信息，比如：\n操作的运行时长，快速发现耗时漫长的低效扫描操作。 执行计划信息，用于判断是否命中了索引，或者存在锁冲突的情况。 操作 ID、时间、客户端等信息，方便定位出产生慢操作的源头。 \u003e db.currentOP() { \"inprog\": [ { \"type\": \"op\", \"host\": \"mongodb1.example.com:27017\", \"desc\": \"conn12345\", \"connectionId\": 12345, \"client\": \"192.168.1.100:54216\", \"clientMetadata\": { \"application\": { \"name\": \"MyApp\" }, \"driver\": { \"name\": \"nodejs\", \"version\": \"4.1.0\" } }, \"active\": true, \"currentOpTime\": \"2023-05-15T08:42:17.123Z\", \"opid\": 678910, \"secs_running\": 5, \"microsecs_running\": NumberLong(5123456), \"op\": \"update\", \"ns\": \"mydb.users\", \"command\": { \"q\": { \"value\": { \"$gt\" : 59.32656132664 } }, \"u\": { \"$inc\": { \"value\": 82.3154654541 } }, \"multi\": true, \"upsert\": true }, \"planSummary\": \"COLLSCAN\", \"locks\": { \"Global\": \"r\", \"Database\": \"r\", \"Collection\": \"r\" }, \"waitingForLock\": false, \"numYields\": 12, \"lockStats\": { \"Global\": { \"acquireCount\": { \"r\": NumberLong(13) } }, \"Database\": { \"acquireCount\": { \"r\": NumberLong(13) } }, \"Collection\": { \"acquireCount\": { \"r\": NumberLong(13) } } } }, # ... ], # ... } 对示例操作的解读如下:\n从 ns、op 字段获知，当前进行的操作正在对 mydb.users 集合执行 update 命令。 command 字段显示了其原始信息。其中，command.q 和 command.u 分别展示了 update 的查询条件和更新操作。 \"planSummary\"：\"COLLSCAN\" 说明情况并不乐观，update 没有利用索引而是正在全表扫描。 microsecs_running：NumberLong（5123456）表示操作运行了 5123ms，注意这里的单位是微秒。 优化方向：\nvalue 字段加上索引。 如果更新的数据集非常大，要避免大范围 update 操作，切分成小批量的操作。 opid 表示当前操作在数据库进程中的唯一编号。如果已经发现该操作正在导致数据库系统响应缓慢，则可以考虑将其“杀”死：\ndb.killOp(678910) db.currentOp 默认输出当前系统中全部活跃的操作，由于返回的结果较多，可以指定一些过滤条件：\n查看等待锁的增加、删除、修改、查询操作 db.currentOp({ waitingForLock:true, $or:[ {op:{$in:[\"insert\",\"update\",\"remove\"]}}, {\"query.findandmodify\":{$exists:true}} ] }) 查看执行时间超过 1s 的操作 db.currentOp({ secs_running:{$gt:1} }) 查看 test 数据库中的操作 db.currentOp({ ns:/test/ }) currentOp 命令输出说明\ncurrentOp.type：操作类型，可以是 op、idleSession、idleCursor 的一种，一般的操作信息以 op 表示。其为 MongoDB 4.2 版本新增功能。 currentOp.host：主机的名称。 currentOp.desc：连接描述，包含 connectionId。 currentOp.connectionId：客户端连接的标识符。 currentOp.client：客户端主机和端口。 currentOp.appName：应用名称，一般是描述客户端类型。 currentOp.clientMetadata：关于客户端的附加信息，可以包含驱动的版本。 currentOp.currentOpTime：操作的开始时间。MongoDB 3.6 版本新增功能。 currentOp.lsid：会话标识符。MongoDB 3.6 版本新增功能。 currentOp.opid：操作的标志编号。 currentOp.active：操作是否活跃。如果是空闲状态则为 false。 currentOp.secs_running：操作持续时间（以秒为单位）。 currentOp.microsecs_running：操作持续时间（以微秒为单位）。 currentOp.op：标识操作类型的字符串。可能的值是：“none” “update” “insert” “query” “command” “getmore” “remove” “killcursors”。其中，command 操作包括大多数命令，如 createIndexes 和 findAndModify。 currentOp.ns：操作目标的集合命名空间。 currentOp.command：操作的完整命令对象的文档。如果文档大小超过 1KB，则会使用一种 $truncate 形式表示。 currentOp.planSummary：查询计划的概要信息。 currentOp.locks：当前操作持有锁的类型和模式。 currentOp.waitingForLock：是否正在等待锁。 currentOp.numYields：当前操作执行 yield（让步）的次数。一些锁互斥或者磁盘 I/O 读取都会导致该值大于 0。 currentOp.lockStats：当前操作持有锁的统计。 currentOp.lockStats.acquireCount：操作以指定模式获取锁的次数。 currentOp.lockStats.acquireWaitCount：操作获取锁等待的次数，等待是因为锁处于冲突模式。acquireWaitCount 小于或等于 acquireCount。 currentOp.lockStats.timeAcquiringMicros：操作为了获取锁所花费的累积时间（以微秒为单位）。timeAcquiringMicros 除以 acquireWaitCount 可估算出平均锁等待时间。 currentOp.lockStats.deadlockCount：在等待锁获取时，操作遇到死锁的次数。 ℹ️ db.currentOp 返回的是数据库命令的瞬时状态，因此，如果数据库压力不大，则通常只会返回极少的结果。 如果启用了复制集，那么 currentOp 还会返回一些复制的内部操作（针对 local.oplog.rs），需要做一些筛选。 db.currentOp 的结果是一个 BSON 文档，如果大小超过 16MB，则会被压缩。可以使用聚合操作 $currentOp 获得完整的结果。 性能问题排查 记一次 MongoDB 占用 CPU 过高问题的排查 MongoDB线上案例：一个参数提升16倍写入速度 "},"title":"开发规范和建模优化"},"/db-learn/docs/mongo/advanced/06_stream/":{"data":{"":"Change Stream 指数据的变化事件流，MongoDB 从 3.6 版本开始提供订阅数据变更的功能。","change-stream-的实现原理#Change Stream 的实现原理":"Change Stream 是基于 oplog 实现的，提供推送实时增量的推送功能。它在 oplog 上开启一个 tailable cursor 来追踪所有复制集上的变更操作，最终调用应用中定义的回调函数。\n被追踪的变更事件主要包括：\ninsert/update/delete：插入、更新、删除； drop：集合被删除； rename：集合被重命名； dropDatabase：数据库被删除； invalidate：drop/rename/dropDatabase 将导致 invalidate 被触发，并关闭 change stream； 从 MongoDB 6.0 开始，change stream 支持 DDL 事件的更改通知，如 createIndexes 和 dropIndexes 事件。\n如果只对某些类型的变更事件感兴趣，可以使用聚合管道的过滤步骤过滤事件：\nvar cs = db.user.watch([{ $match:{operationType:{$in:[\"insert\",\"delete\"]}} }]) db.watch() 语法：https://www.mongodb.com/docs/manual/reference/method/db.watch/#example。\nChange Stream 会采用 \"readConcern：majority\" 这样的一致性级别，保证写入的变更不会被回滚。因此：\n未开启 majority readConcern 的集群无法使用 Change Stream； 当集群无法满足 {w: \"majority\"} 时，不会触发 Change Stream（例如 PSA 架构 中的 S 因故障宕机）。 MongoShell 测试 窗口 1：\ndb.user.watch([],{maxAwaitTimeMS:1000000}) 窗口 2：\ndb.user.insert({name:\"xxxx\"}) 变更字段说明：","使用场景#使用场景":" 监控 用户需要及时获取变更信息（例如账户相关的表），ChangeStreams 可以提供监控功能，一旦相关的表信息发生变更，就会将变更的消息实时推送出去。\n分析平台 例如需要基于增量去分析用户的一些行为，可以基于 ChangeStreams 把数据拉出来，推到下游的计算平台， 比如类似 Flink、Spark 等计算平台等等。\n数据同步 基于 ChangeStreams，用户可以搭建额外的 MongoDB 集群，这个集群是从原端的 MongoDB 拉取过来的， 那么这个集群可以做一个热备份，假如源端集群发生网络不通等等之类的变故，备集群就可以接管服务。还可以做一个冷备份，如用户基于 ChangeStreams 把数据同步到文件，万一源端数据库发生不可服务， 就可以从文件里恢复出完整的 MongoDB 数据库，继续提供服务。（当然，此处还需要借助定期全量备份来一同完成恢复）另外数据同步它不仅仅局限于同一地域，可以跨地域，从北京到上海甚至从中国到美国等等。\n消息推送 假如用户想实时了解公交车的信息，那么公交车的位置每次变动，都实时推送变更的信息给想了解的用户，用户能够实时收到公交车变更的数据，非常便捷实用。\nℹ️ Change Stream 依赖于 oplog，因此中断时间不可超过 oplog 回收的最大时间窗； 在执行 update 操作时，如果只更新了部分数据，那么 Change Stream 通知的也是增量部分； 删除数据时通知的仅是删除数据的 _id。 ","故障恢复#故障恢复":"假设在一系列写入操作的过程中，订阅 Change Stream 的应用在接收到 “写3” 之后 于 t0 时刻崩溃，重启后后续的变更怎么办？\n想要从上次中断的地方继续获取变更流，只需要保留上次变更通知中的 _id 即可。Change Stream 回调所返回的的数据带有 _id，这个 _id 可以用于断点恢复。例如：\nvar cs = db.collection.watch([], {resumeAfter: \u003c_id\u003e}) 即可从上一条通知中断处继续获取后续的变更通知。"},"title":"Change Stream 实战"},"/db-learn/docs/mongo/guide/":{"data":{"":"","安装配置#安装配置":"环境准备：\nLinux 系统： centos7 安装 MongoDB 社区版 # 查看 Linux 版本 [root@hadoop01 soft]# cat /etc/redhat-release CentOS Linux release 7.9.2009 (Core) 下载地址：https://www.mongodb.com/try/download/community，其中 MongoDB Atlas 是 MongoDB 的云服务，可以提供一个 512MB 的免费数据库。\nMongoDB Community Edition 是一个开源的、免费的社区版。Tools 里面是一些工具，例如 MongoDB Compass 是一个可视化工具，可以方便的查看数据库中的数据。\n# 下载 MongoDB wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-6.0.5.tgz tar -zxvf mongodb-linux-x86_64-rhel70-6.0.5.tgz 启动 MongoDB Server：\n# 创建 dbpath 和 logpath mkdir -p /mongodb/data /mongodb/log # 进入 mongodb 目录，启动 mongodb 服务 bin/mongod --port=27017 --dbpath=/mongodb/data --logpath=/mongodb/log/mongodb.log \\ --bind_ip=0.0.0.0 --fork --dbpath: 指定数据文件存放目录。 --logpath: 指定日志文件，注意是指定文件不是目录。 --logappend: 使用追加的方式记录日志。 --port: 指定端口，默认为 27017。 --bind_ip: 默认只监听 localhost 网卡。 --fork: 后台启动。 --auth: 开启认证模式。 添加环境变量：\n修改 /etc/profile，添加环境变量,方便执行 MongoDB 命令：\nexport MONGODB_HOME=/usr/local/soft/mongodb PATH=$PATH:$MONGODB_HOME/bin source /etc/profile 重新加载环境变量。\n使用配置文件启动服务，编辑 /mongodb/conf/mongo.conf 文件，必须是 yaml 格式，内容如下：\nsystemLog: destination: file path: /mongodb/log/mongod.log # log path logAppend: true storage: dbPath: /mongodb/data # data directory engine: wiredTiger #存储引擎 journal: #是否启用journal日志 enabled: true net: bindIp: 0.0.0.0 port: 27017 # port processManagement: fork: true 系统日志 systemLog：\nlogAppend：是否启用日志追加模式。 存储 storage：\ndbPath：数据文件存放目录。 engine：存储引擎，默认为 wiredTiger。 journal：是否启用 journal 日志。journal 日志是 MongoDB 的一种日志机制，类似于 MySQL 的 redo log。MongoDB 在写入数据时，先写入缓冲区，默认是 100ms 刷盘一次，这就意味着在这 100ms 内，如果服务器宕机，那么数据就会丢失。如果业务对数据的可靠性要求比较高，那么最好开启 journal 日志。如果是存储日志之类的，那么可以不开启 journal 日志。 # 启动 mongodb 服务 bin/mongod -f /mongodb/conf/mongo.conf 关闭 MongoDB 服务：\nmongod --port=27017 --dbpath=/mongodb/data --shutdown # 或者 # 进入 mongosh bin/mongosh # 关闭服务 \u003e use admin # 关闭 MongoDB server 服务 admin\u003e db.shutdownServer() mongosh 使用 mongosh 是 MongoDB 的交互式 JavaScript Shell 界面，它为系统管理员提供了强大的界面，并为开发人员提供了直接测试数据库查询和操作的方法。\n下载地址：https://www.mongodb.com/try/download/shell\n# centos7 安装 mongosh wget https://downloads.mongodb.com/compass/mongodb-mongosh-1.8.0.x86_64.rpm yum install -y mongodb-mongosh-1.8.0.x86_64.rpm # 连接 mongodb server端 # --port:指定端口，默认为 27017 # --host:连接的主机地址，默认 127.0.0.1 mongosh --host=192.168.65.206 --port=27017 mongosh 192.168.65.206:27017 # 指定 uri 方式连接 mongosh mongodb://192.168.65.206:27017/test 常用命令 show dbs | show databases：显示所有数据库。 use \u003cdbname\u003e：切换到指定数据库。 db.dropDatabase()：删除当前数据库。 show collections | show tables：显示当前数据库中的所有集合。 db.\u003ccollectionName\u003e.stat()：显示集合详情。 db.\u003ccollectionName\u003e.drop()：删除集合。 show users：显示当前数据库中的所有用户。 show roles：显示当前数据库中的所有角色。 show profile：显示最近发生的操作。 load('\u003cfilename\u003e')：加载并执行 JavaScript 文件。 exit | quit：退出 mongosh。 help：显示帮助信息。 db.help()：显示当前数据库的帮助信息。 db.\u003ccollectionName\u003e.help()：显示集合的帮助信息。 db.version()：显示 MongoDB 版本信息。 #查看所有库 show dbs # 切换到指定数据库，不存在则创建 use test # 删除当前数据库 db.dropDatabase() #查看集合 show collections #创建集合 db.createCollection(\"emp\") #删除集合 db.emp.drop() 创建集合语法：\ndb.createCollection(name, options) options 参数：\ncapped：布尔类型，（可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。 autoIndexId：指定是否自动创建 _id 字段的索引。 size：（可选）为固定集合指定一个最大值（以字节计）。如果 capped 为 true，需要指定该字段。 安全认证 用用户名和密码来认证用户身份是 MongoDB 中最常用的安全认证方式。可以通过以下步骤实现：\n创建一个管理员用户（root）并设置密码，具有所有数据库的管理权限。 创建一个或多个普通用户，指定相应的数据库和集合权限，并设置密码。 启用认证后，客户端连接 MongoDB 服务器时需要提供用户名和密码才能成功连接。\n# 设置管理员用户名密码需要切换到 admin 库 use admin # 创建管理员 db.createUser({user:\"fox\",pwd:\"fox\",roles:[\"root\"]}) # 查看当前数据库所有用户信息 show users # 显示可设置权限 show roles # 显示所有用户 db.system.users.find() 常用权限 权限名 描述 read 允许用户读取指定数据库 readWrite 允许用户读写指定数据库 dbAdmin 允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问 system.profile dbOwner 允许用户在指定数据库中执行任意操作，增、删、改、查等 userAdmin 允许用户向 system.users 集合写入，可以在指定数据库里创建、删除和管理用户 clusterAdmin 只在 admin 数据库中可用，赋予用户所有分片和复制集相关函数的管理权限 readAnyDatabase 只在 admin 数据库中可用，赋予用户所有数据库的读权限 readWriteAnyDatabase 只在 admin 数据库中可用，赋予用户所有数据库的读写权限 userAdminAnyDatabase 只在 admin 数据库中可用，赋予用户所有数据库的 userAdmin 权限 dbAdminAnyDatabase 只在 admin 数据库中可用，赋予用户所有数据库的 dbAdmin 权限 root 只在 admin 数据库中可用。超级账号，超级权限 修改用户操作权限：\ndb.grantRolesToUser( \"fox\" , [ { role: \"clusterAdmin\", db: \"admin\" } , { role: \"userAdminAnyDatabase\", db: \"admin\"}, { role: \"readWriteAnyDatabase\", db: \"admin\"} ]) 删除用户：\ndb.dropUser(\"fox\") # 删除当前数据库所有用户 db.dropAllUser() 用户认证，返回 1 表示认证成功：\ndb.auth(\"fox\",\"fox\") # 认证成功返回 1 创建应用数据库用户：\n# 创建数据库 use mydb # 创建用户 db.createUser({user:\"fox\",pwd:\"fox\",roles:[\"readWrite\"]}) # 读写权限 MongoDB 启用鉴权 默认情况下，MongoDB 不会启用鉴权，以鉴权模式启动 MongoDB：\n# 启动 mongodb 服务 bin/mongod -f /mongodb/conf/mongo.conf --auth 启用鉴权之后，连接 MongoDB 的相关操作都需要提供身份认证：\nmongosh 192.168.65.206:27017 -u fox -p fox --authenticationDatabase=admin Docker 安装 # 拉取 mongo 镜像 docker pull mongo:6.0.5 # 运行 mongo 镜像 docker run --name mongo-server -p 29017:27017 \\ -e MONGO_INITDB_ROOT_USERNAME=fox \\ -e MONGO_INITDB_ROOT_PASSWORD=fox \\ -d mongo:6.0.5 --wiredTigerCacheSizeGB 1 # 连接 mongo 容器 mongosh ip:29017 -u fox -p fox ℹ️ --wiredTigerCacheSizeGB 选项用于设置 WiredTiger 引擎的缓存大小。\nWiredTiger 引擎使用缓存来提高读取性能。通过设置 --wiredTigerCacheSizeGB，可以指定 WiredTiger 引擎的缓存大小，以提高读取性能。\n默认情况下，MongoDB 会将 wiredTigerCacheSizeGB 设置为与主机总内存成比例的值（RAM - 1/2，大概占机器一半的运行内存），而不考虑你可能对容器施加的内存限制。\nMONGO_INITDB_ROOT_USERNAME 和 MONGO_INITDB_ROOT_PASSWORD 都存在就会启用身份认证（mongod --auth）","常用工具#常用工具":"官方 GUI COMPASS，MongoDB 图形化管理工具。下载地址：https://www.mongodb.com/zh-cn/products/compass。\nMongoDB Database Tools，下载地址：https://www.mongodb.com/try/download/database-tools。\n文件名称 描述 mongostat 数据库性能监控工具 mongotop 热点表监控工具 mongodump 数据库逻辑备份工具 mongorestore 数据库逻辑恢复工具 mongoimport 数据导入工具 mongoexport 数据导出工具 bsondump BSON 格式转换工具 mongofiles GridFS 文件管理工具 "},"title":"使用指南"},"/db-learn/docs/mongo/guide/01_getting_started/":{"data":{"":"","文档操作#文档操作":"SQL to MongoDB Mapping Chart ：https://www.mongodb.com/docs/manual/reference/sql-comparison/\n插入文档 db.collection.insertOne()：将单个文档插入到集合中。 db.collection.insertMany()：将多个文档插入到集合中。 注意：save 和 insert 方法已被弃用，建议使用 insertOne、insertMany 或者 buldWrite 方法。\ndb.collection.insertOne( \u003cdocument\u003e, { writeConcern: \u003cdocument\u003e } ) db.emps.insertOne( { name: \"fox\", age: 35 } ) // 设置 writeConcern 参数 db.emps.insertOne( { name: \"fox\", age: 35}, { writeConcern: { w: \"majority\", j: true, wtimeout: 5000 } } ) writeConcern 是 MongoDB 中用来控制写入确认的选项。以下是 writeConcern 参数的一些常见选项：\nw：指定写入确认级别。如果指定为数字，则表示要等待写入操作完成的节点数。如果指定为 majority，则表示等待大多数节点完成写入操作。默认为 1，表示等待写入操作完成的节点数为 1。 j：表示写入操作是否要求持久化到磁盘。如果设置为 true，则表示写入操作必须持久化到磁盘后才返回成功。如果设置为 false，则表示写入操作可能在数据被持久化到磁盘之前返回成功。默认为 false。 wtimeout：表示等待写入操作完成的超时时间，单位为毫秒。如果超过指定的时间仍然没有返回确认信息，则返回错误。默认为 0，表示不设置超时时间。 // 插入多条文档数据 db.collection.insertMany( [ \u003cdocument 1\u003e , \u003cdocument 2\u003e, ... ], { writeConcern: \u003cdocument\u003e, ordered: \u003cboolean\u003e } ) db.emps.insertMany( [ { name: \"fox\", age: 35 }, { name: \"cat\", age: 35 } ] ) ordered：指定是否按顺序写入，默认 true，按顺序写入。 var tags = [\"nosql\",\"mongodb\",\"document\",\"developer\",\"popular\"]; var types = [\"technology\",\"sociality\",\"travel\",\"novel\",\"literature\"]; var books=[]; for(var i=0;i\u003c50;i++){ var typeIdx = Math.floor(Math.random()*types.length); var tagIdx = Math.floor(Math.random()*tags.length); var favCount = Math.floor(Math.random()*100); var book = { title: \"book-\"+i, type: types[typeIdx], tag: tags[tagIdx], favCount: favCount, author: \"xxx\"+i }; books.push(book) } db.books.insertMany(books); 进入 mongosh，执行：\n\u003e load(\"books.js\") true \u003e db.bools.countDocuments() 50 查询文档 查询多个文档 db.collection.find(query, projection) query：可选，使用查询操作符指定查询条件。 projection：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值，只需省略该参数即可（默认省略）。投影时，_id 为 1 的时候，其他字段必须是 1；_id 是 0 的时候，其他字段可以是 0；如果没有 _id 字段约束，多个其他字段必须同为 0 或同为 1。 如果查询返回的条目数量较多，mongosh 则会自动实现分批显示。默认情况下每次只显示 20 条，可以输入 it 命令读取下一批。\n查询集合中的第一个文档 db.collection.findOne(query, projection) \u003e db.books.findOne() { _id: ObjectId(\"6470393633826370200\"), title: 'book-0', type: 'technology', tag: 'nosql', favCount: 7, author: 'xxx0' } pretty pretty() 方法以格式化的方式来显示所有文档：\ndb.collection.find().pretty() 条件查询 查询条件对照表：\nSQL MQL a = 1 {a: 1} a \u003c\u003e 1 {a: {$ne: 1}} a \u003e 1 {a: {$gt: 1}} a \u003e= 1 {a: {$gte: 1}} a \u003c 1 {a: {$lt: 1}} a \u003c= 1 {a: {$lte: 1}} 查询逻辑对照表：\nSQL MQL a = 1 AND b = 2 {a: 1, b: 2} 或者 {$and: [{a: 1}, {b: 1}]} a = 1 OR b = 2 {$or: [{a: 1}, {b: 2}]} a IS NULL {a: {$exists: false}} a IS NOT NULL {a: {$exists: true}} a IN (1, 2, 3) {a: {$in: [1, 2, 3]}} a NOT IN (1, 2, 3) {a: {$nin: [1, 2, 3]}} // 查询带有 nosql 标签的 book 文档： db.books.find({tag:\"nosql\"}) // 按照 id 查询单个 book 文档： db.books.find({_id:ObjectId(\"61caa09ee0782536660494d9\")}) // 查询分类为 “travel”，收藏数超过 60 个的 book 文档： db.books.find({type:\"travel\",favCount:{$gt:60}}) 正则表达式匹配查询 使用 $regex 操作符来设置匹配字符串的正则表达式：\n// 使用正则表达式查找 type 包含 so 字符串的 book db.books.find({type:{$regex:\"so\"}}) // 或者 db.books.find({type:/so/}) 排序 使用 sort() 方法对数据进行排序：\n// 指定按收藏数（favCount）降序返回 db.books.find({type:\"travel\"}).sort({favCount:-1}) 1 为升序排列，而 -1 是用于降序排列。 分页 skip 用于指定跳过记录数，limit 则用于限定返回结果数量：\n// 假定每页大小为 8 条，查询第 3 页的 book 文档 // 跳过前面 16 条记录，返回后面 8 条记录 db.books.find().skip(16).limit(8) 分页优化 数据量大的时候，应该避免使用 skip/limit 形式的分页。\n替代方案：使用 查询条件+唯一排序条件。例如：\n第一页：db.books.find({}).sort({_id: 1}).limit(10) 第二页：db.books.find({_id: {$gt: \u003c第一页最后一个_id\u003e}}).sort({_id: 1}).limit(10); 第三页：db.books.find({_id: {$gt: \u003c第二页最后一个_id\u003e}}).sort({_id: 1}).limit(10);\n避免使用 count：\n尽可能不要计算总页数，特别是数据量大和查询条件不能完整命中索引时。\n假设集合总共有 1000w 条数据，在没有索引的情况下考虑以下查询：\ndb.coll.find({x: 100}).limit(50); db.coll.count({x: 100}); 前者只需要遍历前 n 条，直到找到 50 条 x=100 的文档即可结束； 后者需要遍历完 1000w 条找到所有符合要求的文档才能得到结果。为了计算总页数而进行的 count() 往往是拖慢页面整体加载速度的原因。 更新文档 db.collection.updateOne()：即使多个文档可能与指定的筛选器匹配，也只会更新第一个匹配的文档。 db.collection.updateMany()：更新与指定筛选器匹配的所有文档。 更新操作符 操作符 格式 描述 $set {$set: {field: value}} 指定一个键并更新值，若键不存在则创建 $unset {$unset: {field: 1}} 删除一个键 $inc {$inc: {field: value}} 对数值类型进行增减 $rename {$rename: {old_field_name: new_field_name}} 修改字段名称 $push {$push: {field: value}} 向数组末尾添加一个元素 $pushAll {$pushAll: {field: [value1, value2, ...]}} 向数组末尾添加多个元素 $pull {$pull: {field: value}} 从数组中删除指定的元素 $addToSet {$addToSet: {field: value}} 添加元素到数组中，具有排重功能 $pop {$pop: {field: 1}} 删除数组的第一个或最后一个元素 更新单个文档 db.collection.updateOne( \u003cfilter\u003e, \u003cupdate\u003e, { upsert: \u003cboolean\u003e, writeConcern: \u003cdocument\u003e, collation: \u003cdocument\u003e, arrayFilters: [ \u003cfilterdocument1\u003e, ... ], hint: \u003cdocument|string\u003e // Available starting in MongoDB 4.2.1 } ) // favCount 加 1 db.books.updateOne({_id:ObjectId(\"642e62ec933c0dca8f8e9f60\")},{$inc:{favCount:1}}) \u003cfilter\u003e：必选。一个筛选器对象，用于指定要更新的文档。只有与筛选器对象匹配的第一个文档才会被更新。 \u003cupdate\u003e：必选。一个更新操作对象，用于指定如何更新文档。可以使用一些操作符，例如 $set、$inc、$unset 等，以更新文档中的特定字段。 upsert：可选。一个布尔值，用于指定如果找不到与筛选器匹配的文档时是否应插入一个新文档。如果 upsert 为true，则会插入一个新文档。默认值为 false。 writeConcern：可选。一个文档，用于指定写入操作的安全级别。可以指定写入操作需要到达的节点数或等待写入操作的时间。 collation：可选。一个文档，用于指定用于查询的排序规则。例如，可以通过指定 locale 属性来指定语言环境，从而实现基于区域设置的排序。 arrayFilters：可选。一个数组，用于指定要更新的数组元素。数组元素是通过使用更新操作符 $[] 和 $ 来指定的。 hint：一个文档或字符串，用于指定查询使用的索引。该参数仅在 MongoDB 4.2.1 及以上版本中可用。 更新多个文档 updateMany 更新与集合的指定筛选器匹配的所有文档：\n// 给分类为 “novel” 的文档添加发布时间 db.books.updateMany({type:\"novel\"},{$set:{publishedDate:new Date()}}) findAndModify findAndModify 兼容了查询和修改指定文档的功能，findAndModify 只能更新单个文档。\n// 将某个 book 文档的收藏数（favCount）加1 db.books.findAndModify({ query:{_id:ObjectId(\"6457a39c817728350ec83b9d\")}, update:{$inc:{favCount:1}} }) 默认情况下，findAndModify 会返回修改前的“旧”数据。如果希望返回修改后的数据，则可以指定 new 选项：\ndb.books.findAndModify({ query:{_id:ObjectId(\"6457a39c817728350ec83b9d\")}, update:{$inc:{favCount:1}}, new: true }) 与 findAndModify 语义相近的命令如下：\nfindOneAndUpdate：更新单个文档并返回更新前（或更新后）的文档。 findOneAndReplace：替换单个文档并返回替换前（或替换后）的文档。 删除文档 deleteOne() 和 deleteMany() 方法用来删除文档。\ndb.books.deleteOne({type:\"novel\"}) // 删除 type 等于 novel 的第一个文档 db.books.deleteMany({}) // 删除集合下全部文档，最好使用 db.books.drop() 来删除。deleteMany 是一条条的删除，drop 是直接删除集合 db.books.deleteMany({type:\"novel\"}) // 删除 type 等于 novel 的全部文档 findOneAndDelete deleteOne 命令在删除文档后只会返回确认性的信息，如果希望获得被删除的文档，则可以使用 findOneAndDelete 命令：\ndb.books.findOneAndDelete({type:\"novel\"}) findOneAndDelete 命令还允许定义删除的顺序，即按照指定顺序删除找到的第一个文档。利用这个特性，findOneAndDelete 可以实现队列的先进先出。\ndb.books.findOneAndDelete({type:\"novel\"},{sort:{favCount:1}}) 批量操作 bulkwrite() 方法提供了执行批量插入、更新和删除操作的能力。支持以下写操作:\ninsertOne：插入单个文档。 updateOne：更新单个文档。 updateMany：更新多个文档。 replaceOne：替换单个文档。 deleteOne：删除单个文档。 deleteMany：删除多个文档。 每个写操作都作为数组中的文档传递给 bulkWrite()。\ndb.pizzas.insertMany( [ { _id: 0, type: \"pepperoni\", size: \"small\", price: 4 }, { _id: 1, type: \"cheese\", size: \"medium\", price: 7 }, { _id: 2, type: \"vegan\", size: \"large\", price: 8 } ]) db.pizzas.bulkWrite([ { insertOne: { document: { _id: 3, type: \"beef\", size: \"medium\", price: 6 } } }, { insertOne: { document: { _id: 4, type: \"sausage\", size: \"large\", price: 10 } } }, { updateOne: { filter: { type: \"cheese\" }, update: { $set: { price: 8 } } }}, { deleteOne: { filter: { type: \"pepperoni\"} } }, { replaceOne: { filter: { type: \"vegan\" }, replacement: { type: \"tofu\", size: \"small\", price: 4 } }} ]) 可控的执行顺序 有序模式（ordered: true）： 操作按顺序执行，适合有依赖关系的场景（如先删除再插入）。\n无序模式（ordered: false）： 操作并行执行，最大化吞吐量，适合独立操作。\nbulkWrite 是非原子性的 默认情况下：bulkWrite 不是原子性的。\n如果中间某个操作失败，已执行的操作不会回滚（部分成功）。\n例如：批量插入 10 个文档，第 5 个失败时，前 4 个仍会写入。\n有序模式（ordered: true）： 操作按顺序执行，遇到错误会停止后续操作（但已执行的不会回滚）。\n无序模式（ordered: false）： 操作并行执行，失败的操作不影响其他操作。\n批量操作的优势 减少网络开销： 将多个操作合并为一个请求发送到服务器，避免多次网络往返延迟（尤其在高延迟环境中优势明显）。\n批量处理优化： MongoDB 会对批量操作进行内部优化（如顺序写入、减少锁竞争），比单条操作更高效。"},"title":"快速入门"},"/db-learn/docs/mongo/guide/02_type/":{"data":{"":"","bson-协议与数据类型#BSON 协议与数据类型":"为什么会使用 BSON？ JSON 是当今非常通用的一种跨语言 Web 数据交互格式，属于 ECMAScript 标准规范的一个子集。JSON（JavaScript Object Notation, JS 对象简谱）即 JavaScript 对象表示法，它是 JavaScript 对象的一种文本表现形式。\n大多数情况下，使用 JSON 作为数据交互格式已经是理想的选择，但是 JSON 基于文本的解析效率并不是最好的，在某些场景下往往会考虑选择更合适的编/解码格式，一些做法如：\n在微服务架构中，使用 gRPC（基于 Google 的 Protobuf）可以获得更好的网络利用率。 分布式中间件、数据库，使用私有定制的 TCP 数据包格式来提供高性能、低延时的计算能力。 BSON（Binary JSON）是二进制版本的JSON，其在性能方面有更优的表现。BSON 在许多方面和 JSON 保持一致，其同样也支持内嵌的文档对象和数组结构。二者最大的区别在于 JSON 是基于文本的，而 BSON 则是二进制（字节流）编/解码的形式。在空间的使用上，BSON 相比 JSON 并没有明显的优势。\nMongoDB 在文档存储、命令协议上都采用了 BSON 作为编/解码格式，主要具有如下优势：\n类 JSON 的轻量级语义，支持简单清晰的嵌套、数组层次结构，可以实现模式灵活的文档结构。 更高效的遍历，BSON 在编码时会记录每个元素的长度，可以直接通过 seek 操作进行元素的内容读取，相对 JSON 解析来说，遍历速度更快。 更丰富的数据类型，除了 JSON 的基本数据类型，BSON 还提供了 MongoDB 所需的一些扩展类型，比如日期、二进制数据等，这更加方便数据的表示和操作。 代码中通常会将文档转换为一个对象来操作，使用 BSON 就可以很容易的将对象映射为一个文档，或者将文档转换为一个对象。 BOSN 和 JSON 存储结构对比 JSON存储示例（UTF-8 编码）：\n{ \"name\": \"张三\", \"age\": 30, \"scores\": [95.5, 89.0], \"meta\": {\"id\": \"A123\"} } 实际存储（十六进制表示）：\n7B 0A 20 20 22 6E 61 6D 65 22 3A 20 22 E5 BC A0 E4 B8 89 22 2C 0A 20 20 22 61 67 65 22 3A 20 33 30 2C 0A 20 20 22 73 63 6F 72 65 73 22 3A 20 5B 39 35 2E 35 2C 20 38 39 2E 30 5D 2C 0A 20 20 22 6D 65 74 61 22 3A 20 7B 22 69 64 22 3A 20 22 41 31 32 33 22 7D 0A 7D 相同数据的 BSON 编码（十六进制）：\n5A 00 00 00 // 总长度 90 字节 02 // 字符串类型 6E 61 6D 65 00 // 字段名 \"name\" 07 00 00 00 // 字符串长度 7 字节 E5 BC A0 E4 B8 89 00 // UTF-8 值\"张三\" 10 // 32 位整数类型 61 67 65 00 // 字段名 \"age\" 1E 00 00 00 // 值 30 04 // 数组类型 73 63 6F 72 65 73 00 // 字段名\"scores\" 26 00 00 00 // 数组长度 38 字节 01 // 双精度浮点类型 30 00 // 元素索引 \"0\" 00 00 00 00 00 C0 57 40 // 值 95.5 01 // 双精度浮点类型 31 00 // 元素索引\"1\" 00 00 00 00 00 60 56 40 // 值 89.0 00 // 数组结束 03 // 文档类型 6D 65 74 61 00 // 字段名 \"meta\" 12 00 00 00 // 内嵌文档长度 18 字节 02 // 字符串类型 69 64 00 // 字段名 \"id\" 05 00 00 00 // 字符串长度 5 字节 41 31 32 33 00 // 值 \"A123\" 00 // 内嵌文档结束 00 // 主文档结束 BOSN 和 JSON 解析过程对比 JSON 解析流程\n字符解码：将字节流解码为UTF-8字符串 词法分析：识别标记字符和值 遇到 { 开始对象 遇到 \" 开始字符串 遇到 : 分隔键值 语法分析：构建内存数据结构 类型转换：将字符串表示的值转为对应类型 // 伪代码示例 function parseJSON(input) { let index = 0; function parseValue() { skipWhitespace(); const ch = input[index]; if (ch === '{') return parseObject(); if (ch === '[') return parseArray(); if (ch === '\"') return parseString(); if (ch === '-' || isDigit(ch)) return parseNumber(); // ...其他类型 } // 其他解析函数... return parseValue(); } BSON 解析流程\n长度检查：读取前 4 字节获取文档总长度 类型驱动解析：根据类型标记决定解析方式 0x01：双精度浮点（直接读取 8 字节） 0x02：字符串（先读长度再读内容） 0x10：32 位整数（直接读取 4 字节） 直接内存映射：数值类型无需转换直接拷贝 长度前缀跳转：通过长度前缀快速跳过不需要的字段 // 伪代码示例 void parseBSON(const uint8_t* data) { uint32_t length = readInt32(data); data += 4; while (*data != 0x00) { // 直到遇到结束符 uint8_t type = *data++; char* fieldName = readCString(data); switch(type) { case 0x01: // Double double value = readDouble(data); data += 8; break; case 0x02: // String uint32_t strLen = readInt32(data); data += 4; char* str = readString(data, strLen); data += strLen; break; // ...其他类型处理 } } } BSON 的数据类型 MongoDB 中，一个 BSON 文档最大大小为 16M，文档嵌套的级别不超过 100。\nType Number Alias Description Double 1 “double” String 2 “string” Object 3 “object” Array 4 “array” Binary data 5 “binData” 二进制数据 Undefined 6 “undefined” Deprecated ObjectId 7 “objectId” 对象 ID，用于创建文档 ID，这个 ID 是由客户端生成 Boolean 8 “bool” Date 9 “date” Null 10 “null” Regular expression 11 “regex” 正则表达式 DBPointer 12 “dbPointer” Deprecated JavaScript 13 “javascript” Symbol 14 “symbol” Deprecated JavaScript code with scope 15 “javascriptWithScope” Deprecated in MongoDB 4.4. 32-bit integer 16 “int” Timestamp 17 “timestamp” 64-bit integer 18 “long” Decimal128 19 “decimal” 3.4 新类型 Min key -1 “minKey” 表示一个最小值 Max key 127 “maxKey” 表示一个最大值 $type 操作符 $type 操作符基于 BSON 类型来检索集合中匹配的数据类型，并返回结果。\n// 这里的 2 就是类型的 Number，就是 \"string\" db.books.find({\"title\" : {$type : 2}}) // 或者 db.books.find({\"title\" : {$type : \"string\"}}) 日期类型 MongoDB 的日期类型使用 UTC（Coordinated Universal Time，即世界协调时）进行存储，也就是 +0 时区的时间。\ndb.dates.insertMany([{data1:Date()},{data2:new Date()},{data3:ISODate()}]) db.dates.find().pretty() Date() 生成的 JavaScript 的时间字符串。 new Date()与 ISODate() 最终都会生成 ISODate 类型的字段（对应于 UTC 时间）。 ObjectId 生成器 MongoDB 集合中所有的文档都有一个唯一的 _id 字段，作为集合的主键。在默认情况下，_id 字段使用 ObjectId 类型，采用 16 进制编码形式，共 12 个字节。\n为了避免文档的 _id 字段出现重复，ObjectId 被定义为 3 个部分：\n4 字节表示 Unix 时间戳（秒）。 5 字节表示随机数（机器号+进程号唯一，3 个字节机器号，2 个字节进程号）。 3 字节表示计数器（初始化时随机）。 生成一个新的 ObjectId，可以直接调用 ObjectId() 函数。\ndb.books.insertOne({_id: ObjectId(),title:\"MongoDB\"}) db.books.find().pretty() ObjectId 由几个属性方法：\nObjectId.getTimestamp()：将对象的时间戳部分作为日期返回。 ObjectId.toString()：以字符串形式返回 ObjectId。 ObjectId.valueOf()：将对象的表示形式返回为十六进制字符串。返回的字符串是 str 属性。 内嵌文档和数组 一个文档中可以包含作者的信息，包括作者名称、性别、家乡所在地，一个显著的优点是，当我们查询 book 文档的信息时，作者的信息也会一并返回。\ndb.books.insert({ title: \"撒哈拉的故事\", author: { name:\"三毛\", gender:\"女\", hometown:\"重庆\" } }) // 查询三毛的作品 db.books.find({\"author.name\":\"三毛\"}) // 修改三毛的家乡所在地 db.books.update({\"author.name\":\"三毛\"},{$set:{\"author.hometown\":\"北京\"}}) 除了作者信息，文档中还包含了若干个标签，这些标签可以用来表示文档所包含的一些特征：\ndb.books.updateOne({\"author.name\":\"三毛\"},{$set:{tags:[\"旅行\",\"随笔\",\"散文\",\"爱情\",\"文学\"]}}) // 会查询到所有的 tags db.books.find({\"author.name\":\"三毛\"},{title:1,tags:1}) // 利用 $slice 获取最后一个 tag db.books.find({\"author.name\":\"三毛\"},{title:1,tags:{$slice:-1}}) {title:1,tags:1} 表示只返回 title 和 tags 字段。 默认 _id 字段会被返回，如果不想要返回 _id 字段，可以使用 {_id:0,title:1,tags:1}。 {tags:{$slice:-1}} 表示只返回 tags 数组的最后一个元素。-2 表示返回最后两个元素，-3 表示返回最后三个元素，以此类推。 数组末尾追加元素，可以使用 $push 操作符：\ndb.books.updateOne({\"author.name\":\"三毛\"},{$push:{tags:\"科幻\"}}) $push 操作符可以配合其他操作符，一起实现不同的数组修改操作，比如和 $each 操作符配合可以用于添加多个元素：\ndb.books.updateOne({\"author.name\":\"三毛\"},{$push:{tags:{$each:[\"悬疑\",\"推理\"]}}}) 如果加上 $slice 操作符，那么只会保留经过切片后的元素，下面的例子中，只会保留最后三个元素：\ndb.books.updateOne({\"author.name\":\"三毛\"},{$push:{tags:{$each:[\"悬疑\",\"推理\"],$slice: -3}}}) 根据元素查询：\n// 查询 tags 数组中包含科幻的文档 db.books.find({\"tags\":\"科幻\"}) // 查询 tags 数组中同时包含科幻和推理的文档 db.books.find({tags:{$all:[\"悬疑\",\"推理\"]}}) 嵌套型的数组 数组元素可以是基本类型，也可以是内嵌的文档结构：\n{ tags:[ {tagKey:xxx,tagValue:xxxx}, {tagKey:xxx,tagValue:xxxx} ] } 这种结构非常灵活，一个很适合的场景就是商品的多属性，例如一个商品可以同时包含多个维度的属性，比如颜色、尺寸、重量等。\ndb.goods.insertMany([{ name:\"羽绒服\", tags:[ {tagKey:\"size\",tagValue:[\"M\",\"L\",\"XL\",\"XXL\",\"XXXL\"]}, {tagKey:\"color\",tagValue:[\"黑色\",\"宝蓝\"]}, {tagKey:\"style\",tagValue:\"韩风\"} ] },{ name:\"羊毛衫\", tags:[ {tagKey:\"size\",tagValue:[\"L\",\"XL\",\"XXL\"]}, {tagKey:\"color\",tagValue:[\"蓝色\",\"杏色\"]}, {tagKey:\"style\",tagValue:\"韩风\"} ] }]) 当需要根据属性进行检索时，需要用到 $elemMatch 操作符：\n// 筛选出 color=黑色 的商品信息 db.goods.find({ tags:{ $elemMatch:{tagKey:\"color\",tagValue:\"黑色\"} } }) 如果进行组合式的条件检索，可以使用多个 $elemMatch 操作符：\n// 筛选出 color=黑色，style=韩范 的商品信息 db.goods.find({ tags:{ $elemMatch:{tagKey:\"color\",tagValue:\"黑色\"}, $elemMatch:{tagKey:\"style\",tagValue:\"韩范\"} } }) ","固定封顶集合#固定封顶集合":"固定集合（capped collection）是一种限定大小的集合，其中 capped 是覆盖、限额的意思。跟普通的集合相比，数据写入这种集合时遵循 FIFO（先进先出）的原则，即当集合达到最大容量时，最早写入的数据会被自动删除。可以将这种集合理解为一个环形缓冲区，当集合满时，新的数据会覆盖最早写入的数据。通过固定集合的大小，可以保证数据库的存储空间不会无限增长，超过限额的旧数据会被丢弃。\n创建固定集合 db.createCollection(\"logs\",{capped:true,size:4096,max:10}) capped：表示创建的是固定集合。 size：指集合占用空间的最大值，这里是 4096 字节，即 4KB。 max：表示集合的文档数量的最大值，这里是 10 条。 size 是必选的，max 是可选的。如果同时指定了 size 和 max，只要满足其中一个条件，就会认为集合已满。\ncollection.stats() 可以查看文档的占用空间大小：\ndb.logs.stats() 将普通集合转换为固定集合：\ndb.runCommand({\"convertToCapped\": \"mycoll\", size: 100000}) 适用场景 固定集合适合用来存储一些“临时态”的数据，临时态意味着数据在一定程度上可以被丢弃。同时用户还应该更关注最新的数据，随着时间的推移，数据的重要性逐渐降低，直至被淘汰。\n日志数据：比如网站的访问日志、错误日志等。 存储少量文档：比如最新发布的 TopN 文章信息。比如集合就设置 max 为 10 条，这样就可以保持只存储最新的 10 条文档，查询的时候就可以直接使用 find() 方法。 存储股票价格变动信息 在股票实时系统中，大家往往最关心股票的价格变动。而应用系统中也需要根据这些实时的变化数据来分析当前行情。若将股票的价格变化看作是一个事件，而股票交易所则是价格变动的发布者，股票 APP，应用系统则是事件的消费者。这样就可以将股票价格的发布、通知抽象为一种数据的消费行为，此时需要一个消息队列来实现。\n利用固定集合实现存储股票价格变动的消息队列：\n创建 stock_queue 消息队列，可以容纳 10MB 的数据。 db.createCollection(\"stock_queue\",{capped:true,size:10485760}) 定义消息格式： { timestamped:new Date(), // 股票动态消息的产生时间 stock: \"MongoDB Inc\", // 股票名称 price: 20.33 // 股票价格，double 类型 } 为了支持按时间检索，比如查询某个时间点之后的数据，可以为 timestamped 字段添加索引：\ndb.stock_queue.createIndex({timestamped:1}) 构建生产者，发布股票动态： // 每隔 1 秒向队列中插入一条股票价格变动信息 function pushEvent(){ while(true){ db.stock_queue.insert({ timestamped:new Date(), stock: \"MongoDB Inc\", price: 100*Math.random(1000) }); print(\"publish stock changed\"); sleep(1000); } } // 执行 pushEvent 函数 pushEvent(); 构建消费者，消费股票动态 对于消费者来说，更关心的是最新的数据，同时还应该保持持续进行拉取，以便知晓实时发生的变化。\nfunction listen(){ var cursor = db.stock_queue.find({timestamped:{$gte:new Date()}}).tailable(); while(true){ if(cursor.hasNext()){ print(JSON.stringify(cursor.next(),null,2)); } sleep(1000); } } find 方法中使用了 tailable 选项，这个选项表示采用读取游标的方式，如果没有新的数据，那么就会阻塞等待，直到有新的数据插入。类似 Linux 中的 tail -f 命令。一旦发现新的数据 cursor.hasNext() 就会返回 true，然后调用 cursor.next() 方法获取新的数据。"},"title":"数据类型"},"/db-learn/docs/mongo/guide/03_aggregation/":{"data":{"":"聚合操作允许用户处理多个文档并返回计算结果。\n聚合操作分为三类：\n单一作用聚合：一种简单的聚合操作，用于对单个集合中的文档进行操作。常用的 db.collection.estimateDocumentCount()、db.collection.countDocument()、db.collection.distinct() 等这类单一作用的聚合函数。 聚合管道：是一个数据聚合的框架，基于数据处理流水线的概念。文档进入多级管道，每个管道可以对文档进行各种操作，包括过滤、投影、分组、排序等，将文档转为聚合结果。 MapReduce：已经被废弃，不再使用。 ","聚合优化#聚合优化":"官方文档：聚合管道优化\n优化的三个目标：\n尽可能利用索引完成搜索和排序 -\u003e 快速找到数据，快速排序 尽早尽多的减少数据量 -\u003e 减少 CPU 消耗，减少内存消耗 尽可能减少执行步骤 -\u003e 减少内存消耗，缩短响应时间 执行顺序 $match/$sort vs $project/$addFields 为了使查询能够命中索引，$match/$sort 步骤需要在最前面，该原则适用于 MongoDB \u003c=3.4。MongoDB 3.6 开始具备一定的自动优化能力。\n$project + $skip/$limit $skip/$limit 应该尽可能放在 $project 之前，减少 $project 的工作量。3.6 开始自动完成这个优化。\n内存排序 在没有索引支持的情况下，MongoDB 最多只支持使用 100MB 内存进行排序。假设总共可用内存为 16GB，一个请求最多可以使用 100MB 内存排序，总共可以有 16000/ 100= 160 个请求同时执行。\n内存排序消耗的不仅是内存，还有大量 CPU。\n方案一： $sort + $limit：只排 Top N ，只要 N 条记录总和不超过 100MB 即可。 方案二： {allowDiskUse: true}：使用磁盘作为交换空间完成全量，超出 100MB 部分与磁盘交换排序。 方案三： 索引排序：使用索引完成排序，没有内存限制 ","聚合管道#聚合管道":"MongoDB 聚合框架（Aggregation Framework）是一个计算框架，它可以：\n作用在一个或几个集合上 对集合中的数据进行一系列运算 将这些数据转化为期望的形式 类似于 SQL 查询的 GROUP BY、LEFT JOIN、AS 等。\n管道（Pipeline）和阶段（Stage） 整个聚合运算过程称为管道（Pipeline），管道由多个阶段（Stage）组成，每个管道：\n接受一系列文档作为输入（原始数据） 每个阶段对这些文档进行一系列运算 结果文档作为下一个阶段的输入 通过将多个操作符组合到聚合管道中，用户可以构建出足够复杂的数据处理管道以提取数据并进行分析。\n聚合管道的语法：\npipeline = [$stage1, $stage2, ...$stageN]; db.collection.aggregate(pipeline, {options}) pipeline：一组聚合阶段。除了 $out、$merge 和 $geoNear 之外，其他阶段都可以在管道中多次出现。 options：可选参数，包含：查询计划、是否使用临时文件、游标、最大操作时间、读写策略、强制索引等。 例如，对订单做一个聚合操作：\ndb.students.aggregate([ { $match: { status: \"A\" } }, // match stage，过滤订单状态为 A 的 { $group: { _id: \"$cus_id\", total: { $sum: \"$amount\" } } } // group stage，类似 SQL 的 group by，按照 cus_id 分组，对 amount 求和，放到 total 字段中 ]); 常用的聚合阶段运算符 阶段运算符 描述 SQL 等价运算符 $match 过滤文档 WHERE $project 投影 AS $lookup 左连接 LEFT JOIN $sort 排序 ORDER BY $group 分组 GROUP BY $skip/$limit 跳过/限制 LIMIT $unwind 展开数组 $graphLookup 图查询 $facet/$bucket 分面搜索 官方文档：Aggregation Pipeline Stages\n聚合表达式 获取字段信息：\n$\u003cfield\u003e ： 用 $ 指示字段路径 $\u003cfield\u003e.\u003csub field\u003e ： 使用 $ 和 . 来指示内嵌文档的路径 常量表达式：\n$literal :\u003cvalue\u003e ： 指示常量 \u003cvalue\u003e 系统变量表达式：\n$$\u003cvariable\u003e 使用 $$ 指示系统变量 $$CURRENT 指示管道中当前操作的文档 使用聚合 准备数据：\nvar tags = [\"nosql\",\"mongodb\",\"document\",\"developer\",\"popular\"]; var types = [\"technology\",\"sociality\",\"travel\",\"novel\",\"literature\"]; var books=[]; for(var i=0;i\u003c50;i++){ var typeIdx = Math.floor(Math.random()*types.length); var tagIdx = Math.floor(Math.random()*tags.length); var tagIdx2 = Math.floor(Math.random()*tags.length); var favCount = Math.floor(Math.random()*100); var username = \"xx00\"+Math.floor(Math.random()*10); var age = 20 + Math.floor(Math.random()*15); var book = { title: \"book-\"+i, type: types[typeIdx], tag: [tags[tagIdx],tags[tagIdx2]], favCount: favCount, author: {name:username,age:age} }; books.push(book) } db.books.insertMany(books); 可以使用 load() 方法将数据加载到当前数据库中，也可以直接在 MongoDB 客户端中执行上面的代码。\n查看数据：\ndb.books.countDocuments(); // 50 $project 投影操作，将原始字段投影成指定名称，如将集合中的 title 字段投影成 name 字段：\ndb.books.aggregate([{$project:{name:\"$title\"}}]) $project 可以灵活控制输出文档的格式，也可以剔除不需要的字段：\ndb.books.aggregate([{$project:{name:\"$title\",_id:0,type:1,author:1}}]) // 输出文档中不包含 _id 字段，只包含 name、type 和 author 字段 从嵌套文档中排除字段:\ndb.books.aggregate([ {$project:{name:\"$title\",_id:0,type:1,\"author.name\":1}} ]) // 或者 db.books.aggregate([ {$project:{name:\"$title\",_id:0,type:1,author:{name:1}}} ]) $match $match 用于对文档进行过滤，之后可以在得到的文档子集上做聚合，$match 可以使用除了地理空间之外的所有常规查询操作符。\n在实际应用中尽可能将 $match 放在管道的前面位置。这样有两个好处：\n可以快速将不需要的文档过滤掉，减少后续管道操作符要操作的文档数，提升效率。 如果在投射和分组之前执行 $match，查询可以使用索引。 db.books.aggregate([{$match:{type:\"technology\"}}]) // 将 $match 放在管道的前面位置，减少后续管道操作符要操作的文档数，提升效率 db.books.aggregate([ {$match:{type:\"technology\"}}, {$project:{name:\"$title\",_id:0,type:1,author:{name:1}}} ]) $count 计数并返回与查询匹配的结果数：\ndb.books.aggregate([ {$match:{type:\"technology\"}}, // match 阶段筛选出 type 为 technology 的文档 {$count: \"type_count\"} // count 阶段返回聚合管道中剩余文档的计数，，并将该值分配给 type_count 字段 ]) $group 按指定的表达式对文档进行分组，并将每个不同分组的文档输出到下一个阶段。输出文档包含一个 _id 字段，该字段按键包含不同的组。\n输出文档还可以包含计算字段，该字段保存由 $group 的 _id 字段分组的一些 accumulator 表达式的值。$group 不会输出具体的文档而只是统计信息。\n{$group: { _id: \u003cexpression\u003e, \u003cfield1\u003e: { \u003caccumulator1\u003e : \u003cexpression1\u003e }, ... } } _id 字段是必填的。但是，可以指定 _id 值为 null 来为整个输入文档计算累计值。 剩余的计算字段是可选的，并使用 \u003caccumulator\u003e 运算符进行计算。 _id 和 \u003caccumulator\u003e 表达式可以接受任何有效的表达式。 accumulator 操作符 名称 描述 类比 SQL $sum 计算指定表达式的总和。 sum() $avg 计算平均值 AVG $first 返回每组第一个文档，如果有排序，按照排序，如果没有按照默认的存储的顺序的第一个文档。 LIMIT 0,1 $last 返回每组最后一个文档，如果有排序，按照排序，如果没有按照默认的存储的顺序的最后个文档。 $max 根据分组，获取集合中所有文档对应值得最大值。 max() $min 根据分组，获取集合中所有文档对应值得最小值。 min() $push 将指定的表达式的值添加到一个数组中。 array_append() $addToSet 将表达式的值添加到一个集合中（无重复值，无序）。 array_append() $stdDevPop 返回输入值的总体标准偏差（population standard deviation）。 $stdDevSamp 返回输入值的样本标准偏差（the sample standard deviation）。 $group 阶段的内存限制为 100M。默认情况下，如果 stage 超过此限制，$group 将产生错误。但是，要允许处理大型数据集，可以将 allowDiskUse 选项设置为 true 以启用 $group 操作以写入临时文件。\n示例 book 的数量，收藏总数和平均值：\ndb.books.aggregate([ // _id 类似于 SQL 的 count(*)，使用 $group 需要一个分组字段，但是这里不需要分组，所以可以使用 null {$group:{_id:null,count:{$sum:1},pop:{$sum:\"$favCount\"},avg:{$avg:\"$favCount\"}}} ]) 统计每个作者的 book 收藏总数：\ndb.books.aggregate([ {$group:{_id:\"$author.name\",pop:{$sum:\"$favCount\"}}} ]) 统计每个作者的每本 book 的收藏数，多个字段分组：\ndb.books.aggregate([ // _id 可以是多个字段，这里是 author.name 表示每个作者， title 表示每本 book {$group:{_id:{name:\"$author.name\",title:\"$title\"},pop:{$sum:\"$favCount\"}}} ]) 每个作者的 book 的 type 合集：\ndb.books.aggregate([ {$group:{_id:\"$author.name\",types:{$addToSet:\"$type\"}}} ]) $unwind 可以将数组拆分为单独的文档。\n{ $unwind: { // 要指定字段路径，在字段名称前加上$符并用引号括起来。 path: \u003cfield path\u003e, // 可选,一个新字段的名称用于存放元素的数组索引。该名称不能以 $ 开头。 includeArrayIndex: \u003cstring\u003e, // 可选，default: false，若为 true,如果路径为空，缺少或为空数组，则 $unwind 输出文档 preserveNullAndEmptyArrays: \u003cboolean\u003e } } 姓名为 xx006 的作者的 book 的 tag 数组拆分为多个文档：\ndb.books.aggregate([ {$match:{\"author.name\":\"xx006\"}}, {$unwind:\"$tag\"} ]) 每个作者的 book 的 tag 合集：\ndb.books.aggregate([ {$unwind:\"$tag\"}, {$group:{_id:\"$author.name\",types:{$addToSet:\"$tag\"}}} ]) 案例 准备数据：\ndb.books.insert([ { \"title\" : \"book-51\", \"type\" : \"technology\", \"favCount\" : 11, \"tag\":[], \"author\" : { \"name\" : \"fox\", \"age\" : 28 } },{ \"title\" : \"book-52\", \"type\" : \"technology\", \"favCount\" : 15, \"author\" : { \"name\" : \"fox\", \"age\" : 28 } },{ \"title\" : \"book-53\", \"type\" : \"technology\", \"tag\" : [ \"nosql\", \"document\" ], \"favCount\" : 20, \"author\" : { \"name\" : \"fox\", \"age\" : 28 } }]) 测试：\n// 使用 includeArrayIndex 选项来输出数组元素的数组索引 db.books.aggregate([ {$match:{\"author.name\":\"fox\"}}, {$unwind:{path:\"$tag\", includeArrayIndex: \"arrayIndex\"}} ]) 运行结果：\nunwind 按照 tag 数组中的元素拆分文档，每个元素都有一个 arrayIndex 字段，用于指示元素在数组中的位置。\n[ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"title\": \"book-53\", \"type\": \"technology\", \"tag\": \"nosql\", \"favCount\": 20, \"author\": { \"name\": \"fox\", \"age\": 28 }, \"arrayIndex\": 0 }, { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"title\": \"book-53\", \"type\": \"technology\", \"tag\": \"document\", \"favCount\": 20, \"author\": { \"name\": \"fox\", \"age\": 28 }, \"arrayIndex\": 1 } ] 上面的示例拆出了两个文档，分别是 nosql 和 document。并没有都取出来，这种来做统计肯定是由问题的，因为由的文档没有 tag 字段。这时候就可以使用 preserveNullAndEmptyArrays 选项来输出缺少 tag 字段，null 或空数组的文档。\n// 使用 preserveNullAndEmptyArrays 选项在输出中包含缺少 tag 字段，null 或空数组的文档 // 因为 MognoDB 的结构很灵活，有些字段在文档中可能不存在 db.books.aggregate([ {$match:{\"author.name\":\"fox\"}}, {$unwind:{path:\"$tag\", preserveNullAndEmptyArrays: true}} ]) $limit 限制传递到管道中下一阶段的文档数：\ndb.books.aggregate([ {$limit : 5 } ]) ℹ️ 这里 MongoDB 对 $limit 进行了优化，如果 $limit 之前的阶段有 $sort，则 $limit 不会对所有的数据进行排序，$sort 操作只会在过程中维持前 n 个结果，其中 n 是指定的限制，而 MongoDB 只需要将 n 个项存储在内存中。 $skip 跳过进入 stage 的指定数量的文档，并将其余文档传递到管道中的下一个阶段：\ndb.books.aggregate([ {$skip : 5 } ]) 此操作将跳过管道传递给它的前 5 个文档。$skip 对沿着管道传递的文档的内容没有影响。\n$sort 对所有输入文档进行排序，并按排序顺序将它们返回到管道。\n{$sort: { \u003cfield1\u003e: \u003csort order\u003e, \u003cfield2\u003e: \u003csort order\u003e ... } } 要对字段进行排序，请将排序顺序设置为 1 或 -1，以分别指定升序或降序排序，如下例所示：\ndb.books.aggregate([ {$sort : {favCount:-1,\"author.age\":1}} ]) $lookup Mongodb 3.2 版本新增，主要用来实现多表关联查询，相当关系型数据库中多表关联查询。每个输入待处理的文档，经过 $lookup 阶段的处理，输出的新文档中会包含一个新生成的数组（可根据需要命名新 key）。数组列存放的数据是来自被 Join 集合的适配文档，如果没有，集合为空（即 为[]）。\ndb.collection.aggregate([{ $lookup: { from: \"\u003ccollection to join\u003e\", localField: \"\u003cfield from the input documents\u003e\", foreignField: \"\u003cfield from the documents of the from collection\u003e\", as: \"\u003coutput array field\u003e\" } }) from：待连接的集合名称。 localField：源集合中的 match 值，如果输入的集合中，某文档没有 localField 这个 Key（Field），在处理的过程中，会默认为此文档含有 localField：null 的键值对。 foreignField：待连接的集合的 match 值，如果待连接的集合中，文档没有 foreignField 值，在处理的过程中，会默认为此文档含有 foreignField：null 的键值对。 as：为输出文档的新增值命名。如果输入的集合中已存在该值，则会覆盖掉。 其语法功能类似于下面的伪 SQL 语句：\nSELECT *, \u003coutput array field\u003e FROM collection WHERE \u003coutput array field\u003e IN (SELECT * FROM \u003ccollection to join\u003e WHERE \u003cforeignField\u003e= \u003ccollection.localField\u003e); 案例 准备数据：\n// customer 客户集合，customerCode 是客户 id db.customer.insert({customerCode:1,name:\"customer1\",phone:\"13112345678\",address:\"test1\"}) db.customer.insert({customerCode:2,name:\"customer2\",phone:\"13112345679\",address:\"test2\"}) // order 订单集合，orderId 是订单 id，customerCode 是客户 id，用来管来关联 customer 集合 db.order.insert({orderId:1,orderCode:\"order001\",customerCode:1,price:200}) db.order.insert({orderId:2,orderCode:\"order002\",customerCode:2,price:400}) // orderItem 订单详情集合，orderId 是订单 id，用来关联 order 集合 db.orderItem.insert({itemId:1,productName:\"apples\",qutity:2,orderId:1}) db.orderItem.insert({itemId:2,productName:\"oranges\",qutity:2,orderId:1}) db.orderItem.insert({itemId:3,productName:\"mangoes\",qutity:2,orderId:1}) db.orderItem.insert({itemId:4,productName:\"apples\",qutity:2,orderId:2}) db.orderItem.insert({itemId:5,productName:\"oranges\",qutity:2,orderId:2}) db.orderItem.insert({itemId:6,productName:\"mangoes\",qutity:2,orderId:2}) 关联查询：\n// 查询客户信息并包含客户相关的订单信息 db.customer.aggregate([ {$lookup: { from: \"order\", localField: \"customerCode\", foreignField: \"customerCode\", as: \"customerOrder\" } } ]) 运行结果：\n[ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"customerCode\": 1, \"name\": \"customer1\", \"phone\": \"13112345678\", \"address\": \"test1\", \"customerOrder\": [ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"orderId\": 1, \"orderCode\": \"order001\", \"customerCode\": 1, \"price\": 200 } ] }, { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"customerCode\": 2, \"name\": \"customer2\", \"phone\": \"13112345679\", \"address\": \"test2\", \"customerOrder\": [ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"orderId\": 2, \"orderCode\": \"order002\", \"customerCode\": 2, \"price\": 400 } ] } ] // 查询订单信息并包含订单相关的客户信息和订单详情信息 db.order.aggregate([ {$lookup: { from: \"customer\", localField: \"customerCode\", foreignField: \"customerCode\", as: \"curstomer\" } }, {$lookup: { from: \"orderItem\", localField: \"orderId\", foreignField: \"orderId\", as: \"orderItem\" } } ]) 运行结构：\n[ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"orderId\": 1, \"orderCode\": \"order001\", \"customerCode\": 1, \"price\": 200, \"curstomer\": [ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"customerCode\": 1, \"name\": \"customer1\", \"phone\": \"13112345678\", \"address\": \"test1\" } ], \"orderItem\": [ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"itemId\": 1, \"productName\": \"apples\", \"qutity\": 2, \"orderId\": 1 }, { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"itemId\": 2, \"productName\": \"oranges\", \"qutity\": 2, \"orderId\": 1 }, { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"itemId\": 3, \"productName\": \"mangoes\", \"qutity\": 2, \"orderId\": 1 } ] }, { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"orderId\": 2, \"orderCode\": \"order002\", \"customerCode\": 2, \"price\": 400, \"curstomer\": [ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"customerCode\": 2, \"name\": \"customer2\", \"phone\": \"13112345679\", \"address\": \"test2\" } ], \"orderItem\": [ { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"itemId\": 4, \"productName\": \"apples\", \"qutity\": 2, \"orderId\": 2 }, { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"itemId\": 5, \"productName\": \"oranges\", \"qutity\": 2, \"orderId\": 2 }, { \"_id\": ObjectId(\"65776072401f11001831313e\"), \"itemId\": 6, \"productName\": \"mangoes\", \"qutity\": 2, \"orderId\": 2 } ] } ] 聚合操作案例 1 统计每个分类的 book 文档数量：\ndb.books.aggregate([ {$group:{_id:\"$type\",total:{$sum:1}}}, {$sort:{total:-1}} ]) 标签的热度排行，标签的热度则按其关联 book 文档的收藏数（favCount）来计算：\ndb.books.aggregate([ {$match:{favCount:{$gt:0}}}, // match 阶段：用于过滤去除掉 favCount=0 的文档 {$unwind:\"$tag\"}, // unwind 阶段：用于将标签数组进行展开，这样一个包含 3 个标签的文档会被拆解为 3 个条目 {$group:{_id:\"$tag\",total:{$sum:\"$favCount\"}}}, // group 阶段：对拆解后的文档进行分组计算，$sum：\"$favCount\" 表示按 favCount 字段进行累加。 {$sort:{total:-1}} // sort 阶段：接收分组计算的输出，按 total 得分进行排序。 ]) 统计 book 文档收藏数 [0,10),[10,60),[60,80),[80,100),[100,+∞)：\ndb.books.aggregate([{ // $bucket 阶段：用于将文档按照指定的边界值进行分组，将文档的 favCount 字段的值分配到不同的桶中。 $bucket:{ groupBy:\"$favCount\", boundaries:[0,10,60,80,100], default:\"other\", output:{\"count\":{$sum:1}} } }]) 聚合操作案例 2 使用 mongoimport 工具导入数据。\n导入邮政编码数据集: https://media.mongodb.org/zips.json\n使用 mongoimport 工具导入数据：\nmongoimport -h 192.168.65.174 -d test -u fox -p fox --authenticationDatabase=admin -c zips --file D:\\ProgramData\\mongodb\\import\\zips.json h,--host ：代表远程连接的数据库地址，默认连接本地Mongo数据库； --port：代表远程连接的数据库的端口，默认连接的远程端口27017； -u,--username：代表连接远程数据库的账号，如果设置数据库的认证，需要指定用户账号； -p,--password：代表连接数据库的账号对应的密码； -d,--db：代表连接的数据库； -c,--collection：代表连接数据库中的集合； -f, --fields：代表导入集合中的字段； --type：代表导入的文件类型，包括csv和json,tsv文件，默认json格式； --file：导入的文件名称 --headerline：导入csv文件时，指明第一行是列名，不需要导入； 返回人口超过 1000 万的州：\ndb.zips.aggregate( [ { $group: { _id: \"$state\", totalPop: { $sum: \"$pop\" } } }, { $match: { totalPop: { $gte: 10*1000*1000 } } } ]) 等价 SQL 是：\nSELECT state, SUM(pop) AS totalPop FROM zips GROUP BY state HAVING totalPop \u003e= (10*1000*1000) 返回各州平均城市人口：\ndb.zips.aggregate( [ // group 阶段：_id: { state: \"$state\", city: \"$city\" } 用于按州和城市进行分组，cityPop: { $sum: \"$pop\" } 用于计算每个城市的人口总和。 { $group: { _id: { state: \"$state\", city: \"$city\" }, cityPop: { $sum: \"$pop\" } } }, // group 阶段：_id: \"$_id.state\" 用于按州进行分组，avgCityPop: { $avg: \"$cityPop\" } 用于计算每个州的平均城市人口。 $_id.state 表示上一个阶段的 _id 字段中的 state 字段的值。 { $group: { _id: \"$_id.state\", avgCityPop: { $avg: \"$cityPop\" } } } ]) 按州返回最大和最小的城市：\ndb.zips.aggregate( [ // group 阶段：_id: { state: \"$state\", city: \"$city\" } 用于按州和城市进行分组，pop: { $sum: \"$pop\" } 用于计算每个城市的人口总和。 { $group: { _id: { state: \"$state\", city: \"$city\" }, pop: { $sum: \"$pop\" } } }, // sort 阶段：对每个州的城市人口进行排序，pop: 1 表示按人口升序排序。 { $sort: { pop: 1 } }, // group 阶段：_id: \"$_id.state\" 用于按州进行分组，biggestCity: { $last: \"$_id.city\" } 用于获取每个州中人口最大的城市，biggestPop: { $last: \"$pop\" } 用于获取每个州中人口最大的城市的人口。 { $group: { _id : \"$_id.state\", biggestCity: { $last: \"$_id.city\" }, biggestPop: { $last: \"$pop\" }, smallestCity: { $first: \"$_id.city\" }, smallestPop: { $first: \"$pop\" } } }, // project 阶段：选择要返回的字段，_id: 0 表示不返回 _id 字段，state: \"$_id\" 表示将 _id 字段的值作为 state 字段的值，biggestCity: { name: \"$biggestCity\", pop: \"$biggestPop\" } 表示将 biggestCity 和 biggestPop 字段的值作为 biggestCity 字段的值的嵌套对象。 {$project: { _id: 0, state: \"$_id\", biggestCity: { name: \"$biggestCity\", pop: \"$biggestPop\" }, smallestCity: { name: \"$smallestCity\", pop: \"$smallestPop\" } } } ]) 运行结果，最终可以拿到每个州最大和最小的城市：\n[ { \"state\": \"AK\", \"biggestCity\": { \"name\": \"Anchorage\", \"pop\": 29730 }, \"smallestCity\": { \"name\": \"Fairbanks\", \"pop\": 583 } }, { \"state\": \"AL\", \"biggestCity\": { \"name\": \"Mobile\", \"pop\": 1498337 }, \"smallestCity\": { \"name\": \"Birmingham\", \"pop\": 48758 } } // ... ] "},"title":"聚合操作"},"/db-learn/docs/mysql/":{"data":{"":"MySQL 是世界上最流行的开源关系型数据库管理系统(RDBMS)，是由 C 和 C++ 语言编写的。"},"title":"MySQL"},"/db-learn/docs/mysql/advance/01_architecture/":{"data":{"":"一条 SQL 查询语句在 MySQL 内的执行过程，是怎样的？\n上图是 MySQL 的基本架构示意图。MySQL 大致可以分为两部分：Server 层 和 存储引擎层。\nServer 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎。不同的存储引擎共用一个 Server 层。存储引擎向 Server 层提供统一的调用接口（存储引擎 API），包含了几十个底层函数，像\"读取索引第一条内容\"、“读取索引下一条内容”、“插入记录\"等等。\n从 MySQL 5.5.5 版本开始， InnoDB 成为了默认存储引擎。\n接下来看一条 SQL 查询语句的执行过程，如 select * from T where id=10;。","server-层和存储引擎层是如何交互的#Server 层和存储引擎层是如何交互的":"准备数据：\nCREATE TABLE hero ( id INT, name VARCHAR(100), country varchar(100), PRIMARY KEY (id), KEY idx_name (name) ) Engine=InnoDB CHARSET=utf8; INSERT INTO hero VALUES (1, 'l刘备', '蜀'), (3, 'z诸葛亮', '蜀'), (8, 'c曹操', '魏'), (15, 'x荀彧', '魏'), (20, 's孙权', '吴'); name 列创建了一个二级索引。\nmysql\u003e EXPLAIN SELECT * FROM hero WHERE name \u003c 's孙权' AND country = '蜀'; +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+ | 1 | SIMPLE | hero | NULL | range | idx_name | idx_name | 303 | NULL | 2 | 20.00 | Using index condition; Using where | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+ 1 row in set, 1 warning (0.03 sec) key 列值为 idx_name，type 列的值为 range，表明使用 idx_name 二级索引进行一个范围查询。\n那么 InnoDB 是一次性把所有符合条件的二级索引都取出来之后再统一进行回表操作，还是每取出一条符合条件的记录就进行回表一次？\n其实 Server 层和存储引擎层的交互是以记录为单位的（也就是说得到一条二级索引记录后立即去回表，而不是把所有的二级索引记录都拿到后统一去回表）。整个执行过程如下：\nServer 层第一次开始执行查询，把条件 name \u003c 's孙权' 交给存储引擎，让存储引擎定位符合条件的第一条记录。 存储引擎在二级索引 idx_name 中定位 name \u003c 's孙权' 的第一条记录的 name 列的值为 c曹操。判断 ICP（索引下推）条件。然后拿着该二级索引记录中的主键值去回表，把完整的用户记录都取到之后返回给 Server 层。 Extra 列有一个 Using Where，意味着 Server 层在接收到存储引擎层返回的记录之后，接着就要判断其余的 WHERE 条件是否成立（就是再判断一下 country = '蜀' 是否成立）。如果成立的话，就直接发送给客户端。不成立的话，就跳过该条记录。 接着 Server 层向存储引擎层要求继续读刚才那条记录的下一条记录。 因为每条记录的头信息中都有 next_record 的这个属性，所以可以快速定位到下一条记录的位置，然后继续判断 ICP 条件，然后进行回表操作，存储引擎把下一条记录取出后就将其返回给 Server 层。 然后重复第 3 步的过程，直到存储引擎层遇到了不符合 name \u003c 's孙权' 的记录，然后向 Server 层返回了读取完毕的信息，这时 Server 层将结束查询。 MySQL Server 层返回记录时，既不是一次性返回所有结果，也不是纯粹的逐条返回，而是采用了一种分批返回的折中方案。","优化器#优化器":"经过了分析器，MySQL 就知道你要做什么了，要查询的列是哪些，表是哪个，搜索条件是什么等等。在开始执行之前，还要先经过优化器的处理。因为我们写的 SQL 语句执行起来效率可能并不是很高，优化器会对语句做一些优化，如外连接转换为内连接、表达式简化、子查询转为连接、选择索引等等。\n优化器会生成一个执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的。\n使用 EXPLAIN 语句可以查看某个语句的执行计划。","分析器#分析器":"如果查询缓存没有命中，接下来就需要真正执行查询语句。MySQL 首先要对查询语句的文本做分析。\n分析器先会做词法分析。分析文本里面的字符串分别是什么，代表什么。比如 select 可以判断出是一个查询语句。把字符串 “T” 识别成 “表名 T”，把字符串 “id” 识别成 “列 id”。\n接下来要做语法分析，根据词法分析的结果，分析器根据语法规则，判断这个 SQL 语句是否满足 MySQL 语法。如果语句不对，就会收到 “You have an error in your SQL syntax” 的错误提醒。","存储引擎#存储引擎":"MySQL 提供了多种存储引擎，不同存储引擎管理的表具体的存储结构可能不同，采用的存取算法也可能不同。\n常用存储引擎 存储引擎 描述 InnoDB 具备外键支持功能的事务存储引擎 Memory 置于内存的表 MyISAM 主要的非事务处理存储引擎 默认的存储引擎是 InnoDB。\n查看当前服务器程序支持的存储引擎：\nSHOW ENGINES; mysql\u003e SHOW ENGINES; +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ 9 rows in set (0.00 sec) mysql\u003e Support 表示该存储引擎是否可用，如果值为 DEFAULT 则表示是默认的存储引擎。 Transactions 表示该存储引擎是否支持事务处理。 XA 表示着该存储引擎是否支持分布式事务。 Savepoints 表示着该列是否支持部分事务回滚。 ","执行器#执行器":"通过分析器知道了要做什么，通过优化器知道了该怎么做，接下来就进入了执行器阶段，开始执行语句。\n开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限（经过分析器之后才知道要查询、修改的表，所以连接器无法验证表的权限），如果没有，就会返回没有权限的错误（如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证）。\n打开表的时候，执行器就会根据表的引擎定义，调用这个引擎提供的接口。","查询缓存#查询缓存":"MySQL 处理查询请求时，会把刚刚处理过的查询请求和结果以 key-value 的形式，缓存在内存中。如果下一次有一模一样的请求过来，优先从缓存中查找结果。如果命中缓存，就不需要再执行后面的复杂操作，直接返回结果。\n查询缓存可以在不同客户端之间共享。\n查询缓存的弊端 两个查询请求在任何字符上的不同（例如：空格、注释、大小写），都会导致缓存不会命中。 如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema 数据库中的表，那这个请求就不会被缓存。 查询缓存的失效非常频繁。只要表的结构或者数据被修改，如对表使用了 insert、 update、delete、truncate table、alter table、drop table 或 drop database 语句，那这个表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务表，很长时间才会更新一次。 按需使用查询缓存 一般只有在静态表才建议使用查询缓存，静态表极少更新的表。比如，一个系统配置表、字典表，那这张表上的查询才适合使用查询缓存。MySQL 提供了“按需使用”的方式。可以将my.cnf 中的参数 query_cache_type 设置成 DEMAND。那么，默认的 SQL 语句不会使用查询缓存。而对于确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定：\nmy.cnf：\n# 0 代表关闭查询缓存 OFF # 1 代表开启 ON # 2（DEMAND）代表当 sql 语句中有 SQL_CACHE 关键词时才缓存 query_cache_type=2 select SQL_CACHE * from T where ID=10; -- 查看当前 mysql 实例是否开启缓存机制 show global variables like \"%query_cache_type%\"; MySQL 在 8.0 中删除了查询缓存的功能。","连接器#连接器":"第一步就是与服务端建立连接。连接器负责跟客户端建立连接、获取权限、维持和管理连接。\n客户端可以采用 TCP/IP、命名管道或共享内存、Unix 域套接字这几种方式之一来与服务端建立连接。\n连接命令：\nmysql -h$ip -P$port -u$user -p 连接建立后，连接器会进行身份认证，并获取权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。也就是说，一个用户成功建立连接后，修改权限对当前连接是无效的。重新建立连接才会生效。\n每一个连接建立，服务器都会专门创建一个线程来处理与这个客户端的交互，断开连接时，服务器会先把这个线程缓存起来，以便复用。避免频繁创建和销毁线程。\nMySQL 服务器线程分配的太多会影响系统性能，所以要限制连接数量。\n连接完成后，如果没有后续的动作，这个连接就处于空闲状态，可以使用 show processlist 命令查看。Command 列显示为 Sleep 的，就是空闲连接。\n客户端如果太长时间没动静，连接器就会自动将它断开。可以设置 wait_timeout 来控制，默认值是 8 小时。\n连接被断开之后，再次发送请求，就会收到一个错误提醒：“Lost connection to MySQL server during query”。需要重连，再执行请求。\n数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。\n建立连接的过程通常是比较复杂的，所以建议在使用中要尽量减少建立连接的动作，尽量使用长连接。\n长连接的弊端 全部使用长连接后，可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。\n怎么解决这个问题？可以考虑以下两种方案。\n定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 "},"title":"MySQL 基础架构"},"/db-learn/docs/mysql/advance/02_record-and-page-structure/":{"data":{"":"","file-header文件头部#File Header（文件头部）":"不同类型的页都会以 File Header 作为第一个组成部分，描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁等等。 File Header 占用固定的 38 个字节。\n几个重要的部分：\nFIL_PAGE_SPACE_OR_CHKSUM，当前页面的校验和。 FIL_PAGE_OFFSET，每一个页都有一个唯一的页号。 FIL_PAGE_TYPE，页的类型。 FIL_PAGE_PREV 和 FIL_PAGE_NEXT，代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了，而无需这些页在物理上真正连着。并不是所有类型的页都有上一个和下一个页的属性，但是数据页（也就是类型为 FIL_PAGE_INDEX 的页）是有这两个属性的，所以所有的数据页其实是一个双向链表。 ","innodb-页简介#InnoDB 页简介":"InnoDB 数据是存储在磁盘上的。而读写磁盘的速度非常慢，所以 InnoDB 采取的方式是：将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位。页的大小一般为 16 KB。也就是在一般情况下，一次最少从磁盘中读取 16KB 的内容到内存中，一次最少把内存中的 16KB 内容刷新到磁盘中。","page-directory页目录#Page Directory（页目录）":"如何根据主键值查询页中的记录？\n例如：\nSELECT * FROM page_demo WHERE c1 = 3; 因为记录在页中按照主键值由小到大顺序串联成了一个单链表，那么就可以从 Infimum 记录（最小记录）开始遍历链表，当找到主键值大于你想要查找的主键值时，就可以停止了。但是这种方法，如果记录多了，效率及会很差。\nInnoDB 的 Page Directory，是一个类似书籍目录的设计：\n将所有正常的记录（包括最大和最小记录，不包括标记为已删除的记录）划分为几个组。 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的 n_owned 属性 表示该记录拥有多少条记录，也就是该组内共有几条记录。 将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页的尾部的地方，这个地方就是所谓的 Page Directory，也就是页目录。页面目录中的这些地址偏移量被称为槽（Slot），所以这个页目录是由槽组成的。 最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。\nmysql\u003e INSERT INTO page_demo VALUES(5, 500, 'eeee'), (6, 600, 'ffff'), (7, 700, 'gggg'), (8, 800, 'hhhh'), (9, 900, 'iiii'), (10, 1000, 'jjjj'), (11, 1100, 'kkkk'), (12, 1200, 'llll'), (13, 1300, 'mmmm'), (14, 1400, 'nnnn'), (15, 1500, 'oooo'), (16, 1600, 'pppp'); Query OK, 12 rows affected (0.00 sec) Records: 12 Duplicates: 0 Warnings: 0 往表中添加 12 条记录，现在页里边就一共有 18 条记录（包括最小和最大记录），这些记录被分成了 5 个组，如图所示：\n各个槽代表的记录的主键值都是从小到大排序的，所以可以使用二分法来进行快速查找。5 个槽的编号分别是：0、1、2、3、4，所以初始情况下最低的槽就是 low=0，最高的槽就是 high=4。比如找到主键值为 6 的记录，过程是这样的：\n计算中间槽的位置：(0+4)/2=2，所以查看槽 2 对应记录的主键值为 8，又因为 8 \u003e 6，所以设置 high=2，low 保持不变。 重新计算中间槽的位置：(0+2)/2=1，所以查看槽 1 对应的主键值为 4，又因为 4 \u003c 6，所以设置 low=1，high 保持不变。 high - low 的值为 1，所以确定主键值为 6 的记录在槽 2 对应的组中。槽 2 对应的记录是主键值为 8 的记录（改组的最大记录），但是槽 1 对应的记录（主键值为 4），该条记录的下一条记录就是槽 2 中主键值最小的记录，该记录的主键值为 5。所以可以从这条主键值为 5 的记录出发，遍历槽 2 中的各条记录。 ","page-header页面头部#Page Header（页面头部）":"Page Header 占用固定的 56 个字节，专门存储各种状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等。","总结#总结":" 每个记录的头信息中都有一个 next_record 属性，从而使页中的所有记录串联成一个单链表。 InnoDB 会把页中的记录划分为若干个组，每个组的最后一个记录的地址偏移量作为一个槽，存放在 Page Directory 中，所以在一个页中根据主键查找记录是非常快的，分为两步： 通过二分法确定该记录所在的槽。 通过记录的 next_record 属性遍历该槽所在的组中的各个记录。 每个数据页的 File Header 部分都有上一个和下一个页的编号，所以所有的数据页会组成一个双链表。 ","数据页结构#数据页结构":"InnoDB 管理存储空间的基本单位是页，一个页的大小一般是 16KB。InnoDB 设计了多种不同类型的页，比如存放表空间头部信息的页，存放 Insert Buffer 信息的页等等。存放表中记录的页，叫做 索引（INDEX）页。暂叫做 数据页 吧。\n一个 InnoDB 数据页的存储空间大致被划分成了 7 个部分：\n名称 中文名 占用空间 简单描述 File Header 文件头部 38 字节 页的一些通用信息 Page Header 页面头部 56 字节 数据页专有的一些信息 Infimum + Supremum 最小记录和最大记录 26 字节 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中的某些记录的相对位置 File Trailer 文件尾部 8 字节 校验页是否完整 ","行格式#行格式":"MySQL 是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式被称为 行格式 或者 记录格式。InnoDB 存储引擎目前有 4 种行格式，分别是 Compact、Redundant、Dynamic 和 Compressed 行格式。\n指定行格式的语法 create table 表名 (列的信息) row_format=行格式名称 alter table 表名 row_format=行格式名称 例如：\nmysql\u003e use test; Database changed mysql\u003e crate table record_format_demo ( -\u003e c1 VARCHAR(10), -\u003e c2 VARCHAR(10) NOT NULL, -\u003e c3 CHAR(10), -\u003e c4 VARCHAR(10) -\u003e ) CHARSET=ascii ROW_FORMAT=COMPACT; Query OK, 0 rows affected (0.03 sec) mysql\u003e INSERT INTO record_format_demo(c1, c2, c3, c4) VALUES('aaaa', 'bbb', 'cc', 'd'), ('eeee', 'fff', NULL, NULL); Query OK, 2 rows affected (0.02 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u003e SELECT * FROM record_format_demo; +------+-----+------+------+ | c1 | c2 | c3 | c4 | +------+-----+------+------+ | aaaa | bbb | cc | d | | eeee | fff | NULL | NULL | +------+-----+------+------+ 2 rows in set (0.00 sec) mysql\u003e Compact 行格式 一条完整的记录其实可以被分为 记录的额外信息 和 记录的真实数据 两部分。\n记录的额外信息 为了描述这条记录而不得不额外添加的一些信息，分为 3 类，分别是变长字段长度列表，NULL值列表 和 记录头信息。\n变长字段长度列表 MySQL 支持一些变长的数据类型，比如 VARCHAR(M)、VARBINARY(M)、TEXT、BLOB 等类型，这些数据类型的列被称为变长字段。\n变长字段中存储多少字节的数据是不固定的，所以存储真实数据的时候需要把数据占用的字节数也存起来，变长字段占用的存储空间分为两部分：\n真正的数据内容 占用的字节数 在 Compact 行格式中，把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长字段长度列表。\n各变长字段数据占用的字节数按照列的逆序存放，注意是逆序存放。\n假设一个表的 c1、c2、c4 列都是 VARCHAR(10) 类型的，所以这三个列的值的长度都需要保存在记录开头处，表中的各个列都使用的是 ascii 字符集，所以每个字符只需要 1 个字节来进行编码，来看一下第一条记录各变长字段内容的长度：\n列名 存储内容 内容长度（十进制表示） 内容长度（十六进制表示） c1 ‘aaaa’ 4 0x04 c2 ‘bbb’ 3 0x03 c4 ’d' 1 0x01 这些长度值需要按照列的逆序存放。\n由于第一行记录中 c1、c2、c4 列中的字符串都比较短，也就是说内容占用的字节数比较小，用 1 个字节就可以表示，但是如果变长列的内容占用的字节数比较多，可能就需要用 2 个字节来表示。\n注意，变长字段长度列表中只存储值为 非 NULL 的列内容占用的长度，值为 NULL 的列的长度是不储存的。也就是说对于第二条记录来说，因为 c4 列的值为 NULL，所以第二条记录的变长字段长度列表只需要存储 c1 和 c2 列的长度即可。并不是所有记录都有这个 变长字段长度列表 部分，比方说表中所有的列都不是变长的数据类型的话，就不需要有这部分。\nNULL 值列表 表中的某些列可能存储 NULL 值，如果把这些 NULL 值都放到记录的真实数据中存储会很占地方，所以 Compact 行格式把这些值为 NULL 的列统一管理起来，存储到 NULL 值列表中：\n首先统计表中允许存储 NULL 的列有哪些。 主键列、被 NOT NULL 修饰的列都是不可以存储 NULL 值的，所以在统计的时候不会把这些列算进去。例如表 record_format_demo 的 3 个列 c1、c3、c4 都是允许存储 NULL 值的，而 c2 列是被 NOT NULL 修饰，不允许存储 NULL 值。\n如果表中都是 NOT NULL 修饰的列，则 NULL 值列表也不存在了，否则将每个允许存储 NULL 的列对应一个二进制位，二进制位按照列的逆序排列，二进制位表示的意义如下： 二进制位的值为 1 时，代表该列的值为 NULL。 二进制位的值为 0 时，代表该列的值不为 NULL。 MySQL 规定 NULL 值列表必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补 0。表 record_format_demo 只有 3 个值允许为 NULL 的列，对应 3 个二进制位，不足一个字节，所以在字节的高位补 0，效果就是这样： 这两条记录在填充了 NULL 值列表后的示意图就是这样：\n第二条记录的 c3、c4 列的值都为 NULL，所以 NULL 值列表的二进制位为 00000110，也就是 06。\n记录头信息 记录头信息，它是由固定的 5 个字节组成。5 个字节也就是 40 个二进制位，不同的位代表不同的意思：\n名称 大小（单位：bit） 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+ 树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型，0 表示普通记录，1 表示 B+ 树非叶子节点记录，2 表示最小记录，3 表示最大记录 next_record 16 表示下一条记录的位置偏移量 记录的真实数据 对于 record_format_demo 表来说，记录的真实数据除了 c1、c2、c3、c4 这几个自己定义的列的数据以外，MySQL 会为每个记录默认的添加一些列（也称为隐藏列）：\n列名 是否必须 占用空间 描述 row_id 否 6 字节 行 ID，唯一标识一条记录 trx_id 是 6 字节 事务 ID roll_pointer 是 7 字节 回滚指针 实际上这几个列的真正名称其实是：DB_ROW_ID、DB_TRX_ID、DB_ROLL_PTR。\nInnoDB 表对主键的生成策略：优先使用用户自定义主键作为主键，如果用户没有定义主键，则选取一个Unique 键作为主键，如果表中连 Unique 键都没有定义的话，则 InnoDB 会为表默认添加一个名为 row_id 的隐藏列作为主键。\nInnoDB 会为每条记录都添加 trx_id 和 roll_pointer 这两个列。\nCHAR(M) 列的存储格式 record_format_demo 表的 c3 列的类型是 CHAR(10)，Compact 行格式下只会把变长类型的列的长度逆序存到变长字段长度列表中，但是这只是因为 record_format_demo 表采用的是 ascii 字符集，这个字符集是一个定长字符集，也就是说表示一个字符采用固定的一个字节，如果采用变长的字符集（也就是表示一个字符需要的字节数不确定，比如 utf8mb3 表示一个字符要 1~3 个字节等）的话，c3 列的长度也会被存储到变长字段长度列表中。\n对于 CHAR(M) 类型的列来说，当列采用的是定长字符集时，该列占用的字节数不会被加到变长字段长度列表，而如果采用变长字符集时，该列占用的字节数也会被加到变长字段长度列表。\nℹ️ 变长字符集的 CHAR(M) 类型的列要求至少占用 M 个字节，而 VARCHAR(M) 却没有这个要求。比方说对于使用 utf8mb3 字符集的 CHAR(10) 的列来说，该列存储的数据字节长度的范围是 10～30 个字节。即使向该列中存储一个空字符串也会占用 10 个字节。\n这是怕将来更新该列的值的字节长度大于原有值的字节长度而小于 10 个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有的记录空间成为所谓的碎片。\nCompact 行格式的设计既想节省存储空间，又不想更新 CHAR(M) 类型的列产生碎片。\n行溢出数据 VARCHAR(M) 最多能存储的数据 VARCHAR(M) 类型的列最多可以占用 65535 个字节。但是当你创建表时使用 VARCHAR(65535)：\nmysql\u003e CREATE TABLE varchar_size_demo( -\u003e c VARCHAR(65535) -\u003e ) CHARSET=ascii ROW_FORMAT=Compact; ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs mysql\u003e MySQL 对一条记录占用的最大存储空间是有限制的，除了 BLOB 或者 TEXT 类型的列之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。\n由于存储一个 VARCHAR(M) 类型的列，其实需要占用 3 部分存储空间：\n真实数据 真实数据占用字节的长度 NULL 值标识，如果该列有 NOT NULL 属性则可以没有这部分存储空间 如果该 VARCHAR 类型的列没有 NOT NULL 属性，那最多只能存储 65532 个字节的数据，因为真实数据的长度可能占用 2 个字节，NULL 值标识需要占用 1 个字节。\n如果 VARCHAR 类型的列有 NOT NULL 属性，那最多只能存储 65533 字节的数据，因为真实数据的长度可能占用 2 个字节，不需要 NULL 值标识。\n如果 VARCHAR(M) 类型的列使用的不是 ascii 字符集，那 M 的最大取值取决于该字符集表示一个字符最多需要的字节数。在列的值允许为 NULL 的情况下，utf8mb3 字符集表示一个字符最多需要 3 个字节，那在该字符集下，M 的最大取值就是 21844，就是说最多能存储 21844（也就是：65532/3）个字符。\nℹ️ 处理 VARCHAR 类型，并且长度变化较大时，可能会触发页分裂和页合并。\nVARCHAR 类型的字段数据长度增加，且增加的长度使行总长度超过页面剩余空间时，可能需要数据页分裂。 VARCHAR 类型的字段长度减少，导致页中大量空间未使用，可能触发页合并。 记录中的数据太多产生的溢出 MySQL 中磁盘和内存交互的基本单位是页，记录都会被分配到某个页中存储。而一个页的大小一般是 16KB，也就是 16384 字节，而一个 VARCHAR(M) 类型的列就最多可以存储 65532 个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。\nMySQL 中规定一个页中至少存放两行记录。\n在 Compact 行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用 20 个字节存储指向这些页的地址，从而可以找到剩余数据所在的页。这个过程也叫做行溢出。存储超出字节的那些页面也被称为溢出页。\n只需要知道如果在一个行中存储了很大的数据时，可能发生行溢出的现象。\n页分裂 (Page Split) 与行溢出 (Row Overflow) 的区别 页分裂（Page Split） 页分裂是 B+ 树索引结构（如 InnoDB 的主键索引和唯一索引）在插入或更新数据时可能出现的一种情况。\n插入数据时：\n当向表中插入一条新记录时，InnoDB 会根据主键或唯一索引将记录插入到对应的索引页中。如果目标索引页已经满了（即没有足够的空间容纳新记录），InnoDB 会将该索引页分裂为两个新的索引页，每个新页大约包含原页一半的数据。这样可以为新记录腾出空间。\n更新数据时：\n如果更新操作导致记录的大小增加（例如，更新某个字段的值使其占用更多空间），并且当前索引页无法容纳更新后的记录，也可能触发页分裂。\n行溢出（Row Overflow） 行溢出是 InnoDB 存储引擎中处理变长字段（如 VARCHAR、TEXT）时可能出现的一种情况。即使单列不大，多列组合超过限制也会溢出。\nDynamic 和 Compressed 行格式 MySQL 默认的行格式就是 Dynamic。\nDynamic 和 Compressed 行格式和 Compact 行格式挺像，只不过在处理行溢出数据时不同，它们不会在记录的真实数据处存储字段真实数据的前 768 个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址。\nCompressed 行格式和 Dynamic 不同的一点是，Compressed 行格式会采用压缩算法对页面进行压缩，以节省空间。\n定长类型的优势 存储与读取性能更高 固定长度：定长字段（如 CHAR(10)）始终占用预分配的空间，无需计算实际长度。\n直接定位：数据库引擎可以直接通过偏移量访问数据，减少解析时间，适合高频查询。\n减少碎片化：变长字段（如 VARCHAR）可能导致存储碎片。\n内存对齐优化 定长字段在内存中按固定对齐方式存储，CPU 缓存命中率更高，加速排序、聚合等操作。\n避免行溢出 变长字段（如超长 VARCHAR）可能触发行溢出（行数据超出页大小，存储到额外位置），导致额外 I/O 开销。定长字段无此问题。\n简化索引操作 定长字段读取性能高，那自然定长字段的索引（如 CHAR）也更高效，B+ 树节点分裂和合并更可控。","记录在页中的存储#记录在页中的存储":"User Records 部分就是存储实际数据的部分。\n一开始生成页的时候，是没有 User Records 的，每当插入一条记录，都会从 Free Space 部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到 User Records 部分。\n当 Free Space 空间全部划分到 User Records 之后，就意味着这个页使用完了，插入新的记录，就需要申请新的页。\n记录头信息 以 compact 格式为例，先创建一个表：\nmysql\u003e CREATE TABLE page_demo( -\u003e c1 INT, -\u003e c2 INT, -\u003e c3 VARCHAR(10000), -\u003e PRIMARY KEY (c1) -\u003e ) CHARSET=ascii ROW_FORMAT=Compact; Query OK, 0 rows affected (0.03 sec) 插入四条记录：\nmysql\u003e INSERT INTO page_demo VALUES(1, 100, 'aaaa'), (2, 200, 'bbbb'), (3, 300, 'cccc'), (4, 400, 'dddd'); Query OK, 4 rows affected (0.00 sec) Records: 4 Duplicates: 0 Warnings: 0 这些记录下页中的示意图（记录中头信息和实际的列数据其实是一堆二进制位）：\ndelete_mask 这个属性标记着当前记录是否被删除，占用 1 个二进制位，为 1 的时候代表记录被删除掉了。\n被删除的记录还在页中么？\n是的，确实还在磁盘上。这些被删除的记录之所以不立即从磁盘上移除，是因为移除它们之后把其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记而已，所有被删除掉的记录都会组成一个所谓的垃圾链表，在这个链表中的记录占用的空间称之为所谓的可重用空间，之后如果有新记录插入到表中的话，可以把这些被删除的记录占用的存储空间覆盖掉。\nℹ️ Page Header 部分有一个称之为 PAGE_FREE 的属性，它指向由被删除记录组成的垃圾链表中的头节点。\n删除一条记录需要经历两个阶段：\nDelete Mark 阶段，把记录的 delete_mask 属性设置为 1，其他的不做修改（其实会修改记录的 trx_id、roll_pointer 这些隐藏列的值）。但是这个时候还没有被加入到垃圾链表。也就是此时记录处于一个中间状态。 Purge 阶段，当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉。所谓真正的删除就是把该记录从正常记录链表中移除，并且加入到垃圾链表中，然后还要调整一些页面的其他信息，比如页面中可重用的字节数量PAGE_GARBAGE、还有页目录的一些信息等等。 min_rec_mask B+ 树的每层非叶子节点中的最小记录都会添加该标记。\nheap_no 表示当前记录在本页中的位置。注意上面插入 4 条记录的示意图，4 条记录的位置分别是 2、3、4、5。\n0 和 1 去哪了？\n因为 InnoDB 给每个页都自动添加了两个记录。这两个记录称为伪记录或者虚拟记录。这两个分别是最小记录，最大记录。\n它们并不存放在页的 User Records 部分，被单独放在一个称为 Infimum + supremum 的部分。\n小记录和最大记录的 heap_no 值分别是 0 和 1，也就是说它们的位置最靠前。\nrecord_type 表示当前记录的类型，0 表示普通记录，1 表示 B+ 树非叶子节点记录，2 表示最小记录，3 表示最大记录。\nnext_record 表示从当前记录的\"真实数据\"到下一条记录的\"真实数据\"的地址偏移量。如，第一条记录的 next_record 值为 32，意味着从第一条记录的真实数据的地址处向后找 32 个字节便是下一条记录的真实数据。这其实是个链表，可以通过一条记录找到它的下一条记录。“下一条记录” 指得按照主键值由小到大的顺序的下一条记录。下图箭头来替代一下 next_record 中的地址偏移量：\n记录按照主键从小到大的顺序形成了一个单链表。最大记录的 next_record 的值为 0。这也就是说最大记录是没有下一条记录了，它是这个单链表中的最后一个节点。如\n如果删掉第 2 条记录：\n删除第 2 条记录前后主要发生了这些变化：\n第 2 条记录的 delete_mask 值设置为 1。 第 2 条记录的 next_record 值变为了 0，意味着该记录没有下一条记录了。 第 1 条记录的 next_record 指向了第 3 条记录。 最大记录的 n_owned 值从 5 变成了 4。 对页中的记录做任何的增删改操作，InnoDB 始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。\nnext_record 指针为什么要指向记录头信息和真实数据之间的位置 因为这个位置，向左读取就是记录头信息，向右读取就是真实数据。\n并且变长字段长度列表、NULL 值列表中的信息都是逆序存放，这样可以使记录中位置靠前的字段和它们对应的字段长度信息在内存中的距离更近，可能会提高高速缓存的命中率。\nℹ️ CPU 缓存是按“块”读取的，一般一次加载 64 字节。如果长度信息和对应字段数据在同一个缓存块内，CPU 一次缓存加载就能拿到。逆序设计让频繁访问的前几个字段（如主键）和它们的长度信息尽量靠近，减少缓存缺失（Cache Miss）。 如果再次把这条记录插入到表中，会发生什么事？ mysql\u003e INSERT INTO page_demo VALUES(2, 200, 'bbbb'); Query OK, 1 row affected (0.00 sec) InnoDB 没有为它申请新的存储空间，而是直接复用了原来被删除记录的存储空间。"},"title":"记录行格式和页结构"},"/db-learn/docs/mysql/advance/03_b-tree/":{"data":{"":"InnoDB 数据页有 7 个组成部分，各个数据页可以组成一个双向链表，而每个数据页中的记录会按照主键值从小到大的顺序组成一个单向链表，每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。","没有索引的查找#没有索引的查找":"没有索引的时候是怎么查找记录的？比如：\nSELECT [列名列表] FROM 表名 WHERE 列名 = xxx; 在一个页中的查找 如果表中的记录比较少，所有的记录都可以被存放到一个页中，在查找记录的时候可以根据搜索条件的不同分为两种情况：\n以主键为搜索条件 在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。\n以其他列作为搜索条件 对非主键列，数据页中并没有对非主键列建立所谓的页目录，所以无法通过二分法快速定位相应的槽。这种情况下只能从最小记录开始依次遍历单链表中的每条记录，然后对比每条记录是不是符合搜索条件。很显然，这种查找的效率是非常低的。\nℹ️ 每个槽对应的记录都是该组中主键值最大的记录，那么怎么定位一个组中最小的记录？各个槽都是挨着的，拿到上一个槽的最大记录，那么该条记录的下一条记录下一个槽的最小记录。 在很多页中查找 大部分情况下表中存放的记录都是非常多的，需要好多的数据页来存储这些记录。在很多页中查找记录的话可以分为两个步骤：\n定位到记录所在的页。 从所在的页内中查找相应的记录。 由于并不能快速的定位到记录所在的页，所以只能从第一个页沿着双向链表一直往下找，每一个页中再使用上面一个页中的查找方法。非常低效。","索引#索引":"B+ 树 B+ 树是目前为止排序最有效率的数据结构。像二叉树，哈希索引、红黑树、SkipList，在海量数据基于磁盘存储效率方面远不如 B+ 树索引高效。\nB+ 树索引的特点是： 基于磁盘的平衡树，但树非常矮，通常为 3~4 层，能存放千万到上亿的排序数据。树矮意味着访问效率高，从千万或上亿数据里查询一条数据，只需要 3、4 次 I/O。\nB+ 树是 B 树的变种，主要区别：\n非叶子节点不存储 data，只存储索引(冗余)，可以放更多的索引。B 树的非叶子节点也存储 data，会占用更多的磁盘空间，每个非叶子节点存储的记录远少于 B+ 树，这会导致树的高度变高，磁盘 I/O 次数变多，查询效率变低。 叶子节点包含所有索引字段。 叶子节点用指针连接，组成一个双向链表，提高区间访问的性能。B 树的叶子节点由于没有这个指针，一次只能访问一个节点，然后再从根节点开始遍历。 B 树的优势 B 树可以在非叶子节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。\n为什么是 B+ 树 哈希表是一种以键-值（key-value）存储数据的结构，插入和查询都很快，但是适用于只有等值查询的场景，由于 key 是无序的，所以范围查询很慢。也不支持模糊查询。\n有序数组在等值查询（二分法）和范围查询场景中的性能就都非常优秀。如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。\nInnoDB 中的 B+ 树 在一个数据页中，Page Directory 通过二分法可以快速定位一条记录在页中的位置。\n但是，在一个表中，可能有很多个数据页，如何快速的定位到需要查找的记录在哪些数据页中？\n给所有的页建立目录项，目录项记录的 record_type 值是 1，而普通用户记录的 record_type 值是 0。\n目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，可能包含很多列，另外还有 InnoDB 自己添加的隐藏列。\n上图中，如果用户记录的主键值在 [1, 320) 之间，则到页 30 中查找更详细的目录项记录，如果主键值不小于 320 的话，就到页 32 中查找更详细的目录项记录。\n不论是存放用户记录的数据页，还是存放目录项记录的数据页，在 B+ 树这个数据结构中，都叫做节点。从图中可以看出来，实际的用户记录都存放在 B+ 树的最底层的节点上，这些节点也被称为叶子节点或叶节点，其余用来存放目录项的节点称为非叶子节点或者内节点，其中 B+ 树最上边的那个节点也称为根节点。\n聚簇索引 使用记录主键值的大小进行记录和页的排序 B+ 树的叶子节点存储完整的用户记录（记录中存储了所有列的值，包括隐藏列）。 具有这两种特性的 B+ 树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这就是所谓的索引即数据，数据即索引。\n二级索引 非主键列建立的 B+ 树需要一次回表操作才可以定位到完整的用户记录，所以这种 B+ 树也被称为二级索引（secondary index），或者辅助索引。\n二级索引的叶子节点包含的用户记录由 索引列 + 主键 组成。\n为什么二级索引的叶子节点只存储主键？\n节省空间，如果所有的列都存储在二级索引的叶子节点中，那么二级索引的叶子节点就会非常大，占用的空间也会非常大。 一致性问题，如果二级索引的叶子节点中存储的是完整的用户记录，那么当用户记录发生变化时，所有二级索引的叶子节点也需要发生变化。 联合索引 联合索引，本质上也是一个二级索引。例如一个联合索引包含列 a 和 b：\nB+ 树按照 a 和 b 列的大小进行排序：\n先把各个记录和页按照 a 列进行排序。 在记录的 a 列相同的情况下，采用 b 列进行排序。 B+ 树索引的注意事项 一个 B+ 树索引的根节点自诞生之日起，便不会再移动。根节点一旦建立，它的页号便会被记录到某个地方，然后 InnoDB 存储引擎需要用到这个索引的时候，都会从那个固定的地方取出根节点的页号，从而来访问这个索引。\nB+ 树的形成过程：\n当为某个表创建一个 B+ 树索引时，都会为这个索引创建一个根节点页。最开始表中没有数据的时候，每个 B+ 树索引对应的根节点中既没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点中。 当根节点中的可用空间用完时继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页，比如 页 a 中，然后对这个新页进行页分裂的操作，得到另一个新页，比如 页 b。这时新插入的记录根据键值（也就是聚簇索引中的主键值，二级索引中对应的索引列的值）的大小就会被分配到 页 a 或者 页 b 中，而根节点升级为存储目录项记录的页。 B+ 树的同一层内节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的：\n索引列的值 主键值 页号 ℹ️ 把主键值也添加到二级索引内节点中的目录项记录，这样就能保证 B+ 树每一层节点中各条目录项记录除页号这个字段外是唯一的，因为对于二级索引，索引列是会有相同的值的，插入数据时，无法判断插入哪个页。 一个页面最少存储 2 条记录。","索引的代价#索引的代价":" 空间上的代价 每建立一个索引都要为它建立一棵 B+ 树，每一棵 B+ 树的每一个节点都是一个数据页，一个页默认会占用 16KB 的存储空间，一棵很大的 B+ 树会消耗很大的一片存储空间。\n时间上的代价 每次对表中的数据进行增、删、改操作时，都需要去修改各个 B+ 树索引。B+ 树每层节点都是按照索引列的值从小到大的顺序排序而组成了双向链表。不论是叶子节点中的记录，还是非叶节点中的记录都是按照索引列的值从小到大的顺序而形成了一个单向链表。而增、删、改操作可能会对节点和记录的排序造成破坏，所以存储引擎需要额外的时间进行一些记录移位，页面分裂、页面回收等操作来维护好节点和记录的排序。\n一个表上索引建的越多，就会占用越多的存储空间，在增删改记录的时候性能就越差。","索引的使用#索引的使用":"B+ 树索引适用的条件 联合索引的各个排序列的排序顺序必须是一致的\nCREATE TABLE person_info( id INT NOT NULL auto_increment, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name, birthday, phone_number) ); 二级索引 idx_name_birthday_phone_number，它是由 3 个列组成的联合索引。所以在这个索引对应的 B+ 树的叶子节点处存储的用户记录只保留 name、birthday、phone_number 这三个列的值以及主键 id 的值。\n这个 idx_name_birthday_phone_number 索引对应的 B+ 树中页面和记录的排序方式就是这样的：\n先按照 name 列的值进行排序。 如果 name 列的值相同，则按照 birthday 列的值进行排序。 如果 birthday 列的值也相同，则按照 phone_number 的值进行排序。 这个排序方式非常重要，因为只要页面和记录是排好序的，就可以通过二分法来快速定位查找。\n全值匹配 当查询条件中的列与索引中的列完全匹配，并且全部使用等值比较（=）时，称为全值匹配。这种情况下，MySQL 可以最有效地利用索引。\nSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27' AND phone_number = '15123983239'; idx_name_birthday_phone_number 索引包含的 3 个列，查询过程：\nB+ 树的数据页和记录是先按照 name 列的值进行排序的，可以先按照 name 列来查找。 name 列相同的记录又是按照 birthday 进行排序的，可以继续按照 birthday 来查找。 如果 name 和 birthday 都是相同的，会按照 phone_number 列的值排序。 name、birthday、phone_number 这几个搜索列的顺序对查询结果没有影响，因为优化器可以优化语句。\n匹配左边的列 SELECT * FROM person_info WHERE name = 'Ashburn'; SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27'; 没有包含全部联合索引的列，只要包含左边的一列或者多列，也可以使用索引。\n因为 B+ 树的联合索引按照索引从左到右的顺序排序的。\n如果想使用联合索引中尽可能多的列，搜索条件中的各个列必须是联合索引中从最左边连续的列。\n匹配列前缀 字符串排序的本质就是比较哪个字符串大一点儿，哪个字符串小一点，比较字符串大小就用到了该列的字符集和比较规则。\n比较两个字符串的大小的过程其实是这样的：\n先比较字符串的第一个字符，第一个字符小的那个字符串就比较小。 如果两个字符串的第一个字符相同，那就再比较第二个字符，第二个字符比较小的那个字符串就比较小。 如果两个字符串的第二个字符也相同，那就接着比较第三个字符，依此类推。 所以一个排好序的字符串列其实有这样的特点：\n先按照字符串的第一个字符进行排序。 如果第一个字符相同再按照第二个字符进行排序。 如果第二个字符相同再按照第三个字符进行排序，依此类推。 也就是说这些字符串的前 n 个字符，也就是前缀都是排好序的，所以对于字符串类型的索引列来说，我们只匹配它的前缀也是可以快速定位记录的，比方说我们想查询名字以’As’开头的记录，那就可以这么写查询语句：\nSELECT * FROM person_info WHERE name LIKE 'As%'; 但是如果只给出后缀或者中间某个字符串，如 %As%，就没办法利用索引了。\n匹配范围值 所有记录都是按照索引列的值从小到大的顺序排好序的，所以查找某个范围的值的记录是很简单的。\nSELECT * FROM person_info WHERE name \u003e 'Asa' AND name \u003c 'Barlow'; 记录是先按 name 列排序的，所以我们上边的查询过程其实是这样的：\n找到 name 值为 Asa 的记录。 找到 name 值为 Barlow 的记录。 由于所有记录都是由链表连起来的（记录之间用单链表，数据页之间用双链表），所以他们之间的记录都可以很容易的取出来。 找到这些记录的主键值，再到聚簇索引中回表查找完整的记录。 如果对多个列同时进行范围查找的话，只有对索引最左边的那个列进行范围查找的时候才能用到 B+ 树索引。\nSELECT * FROM person_info WHERE name \u003e 'Asa' AND name \u003c 'Barlow' AND birthday \u003e '1980-01-01'; 查询可以分成两个部分：\n通过条件 name \u003e 'Asa' AND name \u003c 'Barlow' 来对 name 进行范围查找，查找的结果可能有多条 name 值不同的记录， 对这些 name 值不同的记录继续通过 birthday \u003e '1980-01-01' 条件继续过滤。 ℹ️ 对于联合索引 idx_name_birthday_phone_number 来说，只能用到 name 列的部分，而用不到 birthday 列的部分，因为只有 name 值相同的情况下才能用 birthday 列的值进行排序。\n由于 name 值不相同，birthday 列的值肯定是无序的，无法再使用二分查找这种方式来定位了。\n精确匹配某一列并范围匹配另外一列 对于同一个联合索引来说，虽然对多个列都进行范围查找时只能用到最左边那个索引列，但是如果左边的列是精确查找，则右边的列可以进行范围查找，比方说这样：\nSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday \u003e '1980-01-01' AND birthday \u003c '2000-12-31' AND phone_number \u003e '15100000000'; name = 'Ashburn'，对 name 列进行精确查找，使用 B+ 树索引。\nbirthday \u003e '1980-01-01' AND birthday \u003c '2000-12-31'，由于 name 列是精确查找，所以通过 name = 'Ashburn'条件查找后得到的结果的 name 值都是相同的，它们会再按照 birthday 的值进行排序。所以此时对 birthday 列进行范围查找是可以用到 B+ 树索引的。\nphone_number \u003e '15100000000'，通过 birthday 的范围查找的记录的 birthday 的值可能不同，所以这个条件无法再利用 B+ 树索引了，只能遍历上一步查询得到的记录。\n用于排序 一般情况下，我们只能把记录都加载到内存中，再用一些排序算法，比如快速排序、归并排序、吧啦吧啦排序等等在内存中对这些记录进行排序，有的时候可能查询的结果集太大以至于不能在内存中进行排序的话，还可能暂时借助磁盘的空间来存放中间结果，排序操作完成后再把排好序的结果集返回到客户端。在 MySQL 中，把这种在内存中或者磁盘上进行排序的方式统称为文件排序。\n如果 ORDER BY 子句里使用到了我们的索引列，就有可能省去在内存或文件中排序的步骤，比如下边这个简单的查询语句：\nSELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10; 这个查询的结果集需要先按照 name 值排序，如果记录的 name 值相同，则需要按照 birthday 来排序，如果 birthday 的值相同，则需要按照 phone_number 排序。大家可以回过头去看我们建立的 idx_name_birthday_phone_number 索引的示意图，因为这个 B+ 树索引本身就是按照上述规则排好序的，所以直接从索引中提取数据，然后进行回表操作取出该索引中不包含的列就好了。\n注意，ORDER BY 的子句后边的列的顺序也必须按照索引列的顺序给出，如果给出 ORDER BY phone_number, birthday, name 的顺序，那也是用不了 B+ 树索引\n同理，ORDER BY name、ORDER BY name, birthday 这种匹配索引左边的列的形式可以使用部分的 B+ 树索引。当联合索引左边列的值为常量，也可以使用后边的列进行排序\nSELECT * FROM person_info WHERE name = 'A' ORDER BY birthday, phone_number LIMIT 10; 不可以使用索引进行排序:\nASC、DESC 混用 排序列包含非同一个索引的列 排序列使用了复杂的表达式 SELECT * FROM person_info ORDER BY UPPER(name) LIMIT 10; 用于分组 SELECT name, birthday, phone_number, COUNT(*) FROM person_info GROUP BY name, birthday, phone_number 先把记录按照 name 值进行分组，所有 name 值相同的记录划分为一组。\n将每个 name 值相同的分组里的记录再按照 birthday 的值进行分组，将 birthday 值相同的记录放到一个小分组里，所以看起来就像在一个大分组里又化分了好多小分组。\n再将上一步中产生的小分组按照 phone_number 的值分成更小的分组，所以整体上看起来就像是先把记录分成一个大分组，然后把大分组分成若干个小分组，然后把若干个小分组再细分成更多的小小分组。\n如果没有索引的话，这个分组过程全部需要在内存里实现，而如果有了索引的话，恰巧这个分组顺序又和我们的 B+ 树中的索引列的顺序是一致的，而我们的 B+ 树索引又是按照索引列排好序的，这不正好么，所以可以直接使用 B+ 树索引进行分组。\n回表的代价 idx_name_birthday_phone_number 索引为例\nSELECT * FROM person_info WHERE name \u003e 'Asa' AND name \u003c 'Barlow'; 索引 idx_name_birthday_phone_number 对应的 B+ 树用户记录中只包含 name、birthday、phone_number、id 这 4 个字段，而查询列表是 *，意味着要查询表中所有字段。这时需要把从上一步中获取到的每一条记录的 id 字段都到聚簇索引对应的 B+ 树中找到完整的用户记录，也就是我们通常所说的回表，然后把完整的用户记录返回给查询用户。\n顺序 I/O 索引 idx_name_birthday_phone_number 对应的 B+ 树中的记录首先会按照 name 列的值进行排序，所以值在 Asa～Barlow 之间的记录在磁盘中的存储是相连的，集中分布在一个或几个数据页中，我们可以很快的把这些连着的记录从磁盘中读出来，这种读取方式我们也可以称为顺序 I/O\n随机 I/O 根据第 1 步中获取到的记录的 id 字段的值可能并不相连，而在聚簇索引中记录是根据 id（也就是主键）的顺序排列的，所以根据这些并不连续的 id 值到聚簇索引中访问完整的用户记录可能分布在不同的数据页中，这样读取完整的用户记录可能要访问更多的数据页，这种读取方式我们也可以称为随机 I/O\n顺序 I/O 比随机 I/O 的性能高很多。\n需要回表的记录越多，使用二级索引的性能就越低。某些查询宁愿使用全表扫描也不使用二级索引。比方说 name 值在 Asa~Barlow 之间的用户记录数量占全部记录数量 90% 以上，那么如果使用 idx_name_birthday_phone_number 索引的话，有 90% 多的 id 值需要回表，这不是吃力不讨好么，还不如直接去扫描聚簇索引（也就是全表扫描）。\n查询优化器会事先对表中的记录计算一些统计数据，然后再利用这些统计数据根据查询的条件来计算一下需要回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用 二级索引 + 回表 的方式。\n回表的记录特别少，优化器就会倾向于使用 二级索引 + 回表 的方式执行查询。\n覆盖索引 为了彻底告别回表操作带来的性能损耗：最好在查询列表里只包含索引列\nSELECT name, birthday, phone_number FROM person_info WHERE name \u003e 'Asa' AND name \u003c 'Barlow' 只查询 name, birthday, phone_number 这三个索引列的值，所以在通过 idx_name_birthday_phone_number 索引得到结果后就不必到聚簇索引中再查找记录的剩余列，这样就省去了回表操作带来的性能损耗。\n不鼓励用 * 号作为查询列表，最好把我们需要查询的列依次标明。\n索引下推 什么是索引下推（Index Condition Pushdown，ICP）？\n索引下推是一种在存储引擎层提前过滤不符合条件的记录的优化手段。没有 ICP 的时候，索引只用来定位数据行的位置，具体 WHERE 条件由 Server 层来判断。即使部分索引已能判断，仍需回表取数据。有 ICP 的时候 InnoDB 在扫描索引时，就会尽量利用 WHERE 子句中的条件直接过滤数据，不必回表就能丢掉无效行，减少回表的次数。\n没有使用 ICP 的情况下，MySQL 的查询：\n存储引擎读取索引记录； 根据索引中的主键值，定位并读取完整的行记录； 存储引擎把记录交给 Server 层去检测该记录是否满足 WHERE 条件。 使用 ICP 的情况下，查询过程：\n存储引擎读取索引记录（不是完整的行记录）； 判断 WHERE 条件部分能否用索引中的列来做检查，条件不满足，则处理下一行索引记录； 条件满足，使用索引中的主键去定位并读取完整的行记录（回表）； 存储引擎把记录交给 Server 层，Server 层检测该记录是否满足 WHERE 条件的其余部分。 -- 联合索引 (name,age,position) SELECT * FROM employees WHERE name like 'LiLei%' AND age = 22 AND position ='manager'; 在 MySQL 5.6 之前的版本没有 ICP，这个查询只能在联合索引里匹配到名字是 'LiLei' 开头的索引，然后拿这些索引对应的主键逐个回表，到主键索引上找出相应的记录，服务器层再根据 age 和 position 的过滤条件进行筛选。这种情况只会走 name 字段索引，无法很好的利用索引。\nMySQL 5.6 引入了索引下推优化 ICP，上面那个查询在联合索引里匹配到名字是 'LiLei' 开头的索引之后，同时还会在索引里过滤 age 和 position 这两个字段，拿着过滤完剩下的索引对应的主键 id 再回表查整行数据。\n为什么范围查找 MySQL 没有用索引下推优化？ 可能是 MySQL 认为索引下推需要额外的判断，范围查找过滤的结果集过大，会导致更多的计算，like KK% 在绝大多数情况来看，过滤后的结果集比较小，所以这里 MySQL 选择给 like KK% 用了索引下推优化，当然这也不是绝对的，有时 like KK% 也不一定就会走索引下推。\n索引下推的使用条件 索引下推优化只对联合索引生效。索引下推的目的是为了减少回表次数，也就是要减少 IO 操作。对聚簇索引来说，数据和索引是在一起的，不存在回表这一说。 索引下推优化只对 InnoDB 存储引擎生效。 百万级别或以上的数据如何删除 删除数据的速度和创建的索引数量是成正比的。百万级别的数据删除速度也会很慢。最快的方案是删除重建索引。\n想要删除百万数据的时候可以先删除索引。 然后删除其中无用数据。 删除完成后重新创建索引(此时数据较少了)创建索引也非常快。 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。 "},"title":"B+ 树索引"},"/db-learn/docs/mysql/advance/04_transaction/":{"data":{"":"事务的四个特性 ACID：\n原子性（Atomicity）：当前事务的操作要么同时成功，要么同时失败。原子性由 undo log 日志来实现。 一致性（Consistent）：使用事务的最终目的，由其它3个特性以及业务代码正确逻辑来实现。 隔离性（Isolation）：在事务并发执行时，他们内部的操作不能互相干扰。隔离性由 MySQL 的各种锁以及 MVCC 机制来实现。 持久性（Durable）：一旦提交了事务，它对数据库的改变就应该是永久性的。持久性由 redo log 日志来实现。 事务处理是一种机制，用来管理必须成批执行的 MySQL 操作，以保证数据库不包含不完整的操作结果。利用事务处理，可以保证一组操作不会中途停止，它们或者作为整体执行，或者完全不执行（除非明确指示）。如果没有错误发生，整组语句提交（写到）数据库表。如果发生错误，则进行回退（撤销）以恢复数据库到某个已知且安全的状态。\n关于事务处理需要知道的几个术语：\n事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（place- holder），你可以对它发布回退（与回退整个事务处理不同） ","自动提交#自动提交":"MySQL 中有一个系统变量 autocommit：\n默认情况下，如果不显式的使用 START TRANSACTION 或者 BEGIN 语句开启一个事务，那么每一条语句都算是一个独立的事务，这种特性称之为事务的自动提交。\n如果想关闭这种自动提交的功能，可以使用下边两种方法：\n显式的的使用 START TRANSACTION 或者 BEGIN 语句开启一个事务。 这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。\n把系统变量 autocommit 的值设置为 OFF，就像这样：\nSET autocommit = OFF; 这样的话，写入的多条语句就算是属于同一个事务了，直到显式的写出 COMMIT 语句来把这个事务提交掉，或者显式的写出 ROLLBACK 语句来把这个事务回滚掉。","语法#语法":"控制事务处理 管理事务处理的关键在于将 SQL 语句组分解为逻辑块，并明确规定数据何时应该回退，何时不应该回退。\n下面的语句来标识事务的开始：\nSTART TRANSACTION BEGIN BEGIN 和 START TRANSACTION 差不多，不过 START TRANSACTION 语句后边可以跟随几个修饰符，就是它们几个，START TRANSACTION READ ONLY;，START TRANSACTION READ ONLY, WITH CONSISTENT SNAPSHOT;：\nREAD ONLY：标识当前事务是一个只读事务，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。 READ WRITE：标识当前事务是一个读写事务，也就是属于该事务的数据库操作既可以读取数据，也可以修改数据。 WITH CONSISTENT SNAPSHOT：启动一致性读。 ROLLBACK ROLLBACK 命令用来回退（撤销）MySQL 语句：\nselect * from ordertotals; start transaction; delete from ordertotals; select * from ordertotals; rollback; select * from ordertotals; 先执行一条 SELECT 以显示该表不为空。然后开始一个事务处理，用一条 DELETE 语句删除 ordertotals 中的所有行。另一条 SELECT 语句验证 ordertotals 确实为空。这时用一条 ROLLBACK 语句回退 START TRANSACTION 之后的所有语句，最后一条 SELECT 语句显示该表不为空。\nROLLBACK 只能在一个事务处理内使用（在执行一条 START TRANSACTION 命令之后）。\n哪些语句不可以回退 CREATE 或 DROP 操作不能回退。事务处理块中可以使用这两条语句，但如果你执行回退，它们不会被撤销。\nCOMMIT 在事务处理块中，提交不会隐式地进行。需要使用 COMMIT 语句显示提交：\nstart transaction; delete from orderitems where order_num = 20005; delete from orders where order_num = 20005; commit; 当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭（将来的更改会隐式提交）。\n保留点 简单的 ROLLBACK 和 COMMIT 语句就可以写入或撤销整个事务处理。复杂的事务处理可能需要部分提交或回退。\n为了支持回退部分事务处理，必须能在事务处理块中合适的位置放置占位符。这样，如果需要回退，可以回退到某个占位符。这些占位符称为保留点。\n创建占位符，可使用 SAVEPOINT 语句：SAVEPOINT delete1;。每个保留点都取标识它的唯一名字，以便在回退时，MySQL 知道要回退到何处。\n回退到本例给出的保留点，可执行：ROLLBACK TO delete1;\n保留点在事务处理完成（执行一条 ROLLBACK 或 COMMIT）后自动释放。\nmysql\u003e SELECT * FROM account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 狗哥 | 11 | | 2 | 猫爷 | 2 | +----+--------+---------+ 2 rows in set (0.00 sec) mysql\u003e BEGIN; Query OK, 0 rows affected (0.00 sec) mysql\u003e UPDATE account SET balance = balance - 10 WHERE id = 1; Query OK, 1 row affected (0.01 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u003e SAVEPOINT s1; # 一个保存点 Query OK, 0 rows affected (0.00 sec) mysql\u003e SELECT * FROM account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 狗哥 | 1 | | 2 | 猫爷 | 2 | +----+--------+---------+ 2 rows in set (0.00 sec) mysql\u003e UPDATE account SET balance = balance + 1 WHERE id = 2; # 更新错了 Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u003e ROLLBACK TO s1; # 回滚到保存点 s1 处 Query OK, 0 rows affected (0.00 sec) mysql\u003e SELECT * FROM account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 狗哥 | 1 | | 2 | 猫爷 | 2 | +----+--------+---------+ 2 rows in set (0.00 sec) ","隐式提交#隐式提交":"当使用 START TRANSACTION 或者 BEGIN 语句开启了一个事务，或者把系统变量 autocommit 的值设置为 OFF 时，事务就不会进行自动提交，但是如果输入了某些语句之后就会悄悄的提交掉，就像输入了 COMMIT 语句了一样，这种因为某些特殊的语句而导致事务提交的情况称为隐式提交。\n隐式提交的语句包括：\n定义或修改数据库对象的数据定义语言（Data definition language，缩写为：DDL）： 所谓的数据库对象，指的就是数据库、表、视图、存储过程等等这些东西。当使用 CREATE、ALTER、DROP 等语句去修改这些所谓的数据库对象时，就会隐式的提交前边语句所属于的事务。\n隐式使用或修改 mysql 数据库中的表： 当使用 ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD 等语句时也会隐式的提交前边语句所属于的事务。\n事务控制或关于锁定的语句： 当在一个事务还没提交或者回滚时就又使用 START TRANSACTION 或者 BEGIN 语句开启了另一个事务时，会隐式的提交上一个事务。\n或者当前的 autocommit 系统变量的值为 OFF，我们手动把它调为 ON 时，也会隐式的提交前边语句所属的事务。\n加载数据的语句 比如使用 LOAD DATA 语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。\n关于 MySQL 复制的一些语句 使用 START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO 等语句时也会隐式的提交前边语句所属的事务。\n其它的一些语句 使用 ANALYZE TABLE、CACHE INDEX、CHECK TABLE、FLUSH、LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET 等语句也会隐式的提交前边语句所属的事务。"},"title":"事务"},"/db-learn/docs/mysql/advance/05_isolation-level/":{"data":{"":"","mvcc#MVCC":"MVCC（Multi-Version Concurrency Control）多版本并发控制，是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问。\nInnoDB 存储引擎实现了 MVCC，用来解决不可重复读和幻读的问题。\n版本链 InnoDB 存储引擎它的聚簇索引记录中都包含两个必要的隐藏列：\ntrx_id：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的事务 id 赋值给 trx_id 隐藏列。 roll_pointer：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 undo log 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 假设插入一条记录，事务 id 为 80，之后另外两个事务 id 分别为 100、200 的事务对这条记录进行更新：\n顺序 trx 100 trx 200 1 begin; 2 begin; 3 update hero set name = '关羽' where number = 1; 4 update hero set name = '张飞' where number = 1; 5 commit; 6 update hero set name = '赵云' where number = 1; 7 update hero set name = '诸葛' where number = 1; 8 commit; 每次对记录进行改动，都会记录一条 undo log，每条 undo log 也都有一个 roll_pointer 属性，可以将这些 undo log 都连起来，串成一个链表。对该记录每次更新后，都会将旧值放到一条 undo log 中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，这个链表称之为版本链。\n版本链的头节点就是当前记录最新的值。\nReadView 对于使用 READ UNCOMMITTED 隔离级别的事务来说，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了。\n对于使用 SERIALIZABLE 隔离级别的事务来说，使用加锁的方式来访问记录。\n对于使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是：需要判断版本链中的哪个版本对于当前事务可见的。\nReadView 中主要包含 4 个比较重要的内容：\nm_ids：表示在生成 ReadView 时当前系统中活跃的读写事务的事务 id 列表。 min_trx_id：表示在生成 ReadView 时当前系统中活跃的读写事务中最小的事务 id，也就是 m_ids 中的最小值。 max_trx_id：表示生成 ReadView 时系统中应该分配给下一个事务的 id 值。 creator_trx_id：表示生成该 ReadView 的事务的事务 id。 ℹ️ 事务执行过程中，只有在第一次真正修改记录时（执行 INSERT、DELETE、UPDATE 这些语句或加排它锁操作比如 select...for update 语句时），才会被分配一个单独的事务 id，否则在一个只读事务中的事务 id 值都默认为 0。 如果被访问版本的 trx_id 与 ReadView 中的 creator_trx_id 相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。 如果被访问版本的 trx_id 小于 ReadView 中的 min_trx_id，表明生成该版本的事务在当前事务生成 ReadView 前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的 trx_id 大于或等于 ReadView 中的 max_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 后才开启，所以该版本不可以被当前事务访问。 如果被访问版本的 trx_id 在 ReadView 的 min_trx_id 和 max_trx_id 之间，那就需要判断一下 trx_id 属性值是不是在 m_ids 列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。 如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性，依此类推，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该条记录对该事务完全不可见，查询结果就不包含该记录。\nREAD COMMITTED 和 REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生成 ReadView 的时机不同。\nREAD COMMITTD 在每一次进行普通 SELECT 操作前都会生成一个 ReadView。 REPEATABLE READ 只在第一次进行普通 SELECT 操作前生成一个 ReadView，之后的查询操作都重复使用这个 ReadView 就好了。 ","事务优化实践原则#事务优化实践原则":" 将查询等数据准备操作放到事务外，对于 RC 隔离级别，可以将查询放到事务之外，对于 RR 隔离级别，将查询放到事务里面，如果放到外面就无法保证 RR。 事务中避免远程调用，远程调用要设置超时，防止事务等待时间太久。 事务中避免一次性处理太多数据，可以拆分成多个事务分次处理。 更新等涉及加锁的操作尽可能放在事务靠后的位置。更新操作应该在插入操作之后，因为更新操作会对已存在的记录加锁，其他事务可能会使用该记录，会造成不必要的等待。而插入操作虽然也会加锁，但是其他事务不会使用该记录，因为还不存在。 能异步处理的尽量异步处理。 应用侧（业务代码）保证数据一致性，非事务执行。如果对性能要求非常高，可以考虑不适用事务。 ","事务并发执行遇到的问题#事务并发执行遇到的问题":"当数据库上多个事务并发执行的时候，就可能出现脏写（Dirty Write）、脏读（Dirty Read）、不可重复读（Non-Repeatable Read）、幻读（Phantom Read）的问题。\n问题按照严重性来排序：\n脏写 \u003e 脏读 \u003e 不可重复读 \u003e 幻读 脏写 “脏写\"是指，一个事务修改了另一个未提交事务修改过的数据。\n脏读 “脏读\"是指，一个事务读到了另一个未提交事务修改过的数据。\n不可重复读 如果一个事务能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值，那就意味着发生了不可重复读。\n幻读 如果一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来，那就意味着发生了幻读。\n不可重复读的重点是修改，幻读的重点在于新增或者删除。","事务问题定位#事务问题定位":" -- 查询执行时间超过 1 秒的事务 SELECT * FROM information_schema.innodb_trx WHERE TIME_TO_SEC( timediff( now( ), trx_started ) ) \u003e 1; -- 强制结束事务 kill 线程 id (就是上面语句查出结果里的 trx_mysql_thread_id 字段的值) ","四种隔离级别#四种隔离级别":"为了解决事务并发执行遇到的问题，就有了隔离级别的概念。SQL 标准中设立了 4 个隔离级别：\nREAD UNCOMMITTED：读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。可能发生脏读、不可重复读和幻读问题。 READ COMMITTED：读已提交是指，一个事务提交之后，它做的变更才会被其他事务看到。可能发生不可重复读和幻读问题，但是不可以发生脏读问题。 REPEATABLE READ：可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。可能发生幻读问题，但是不可以发生脏读和不可重复读的问题。 SERIALIZABLE：可串行化是指，对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 MySQL 的默认隔离级别为 REPEATABLE READ。\n隔离级别越高事务隔离性越好，但性能就越低；隔离级别越低，越严重的问题就越可能发生：\n事务隔离级别 脏读 不可重复读 幻读 读未提交 (read uncommitted) 可能 可能 可能 读已提交 (read committed) 不可能 可能 可能 可重复读 (repeatable read) 不可能 不可能 可能 串行化 (serializable) 不可能 不可能 不可能 MySQL 在 REPEATABLE READ 隔离级别下，是可以禁止幻读问题的发生的（需要配合间隙锁）：\nMVCC (多版本并发控制)：处理快照读。 间隙锁 (Gap Lock)：处理当前读。 设置事务的隔离级别 SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL level; level 可选值有 4 个：REPEATABLE READ，READ COMMITTED，READ UNCOMMITTED，SERIALIZABLE。\n启动参数 transaction-isolation 可以设置事务的默认隔离级别。","大事务的影响#大事务的影响":" 并发情况下，数据库连接池容易被撑爆。 定太多的数据，造成大量的阻塞和锁超时。 执行时间长，容易造成主从延迟。 回滚所需要的时间比较长，如果大事务中有大量的 update 操作，回滚时也需要逐个去找 undo log 进行回滚。 undo log 膨胀，事务未提交，undo log 会一直存在。 容易导致死锁。 ","查询需不需要加事务#查询需不需要加事务":"对于 RC 隔离级别来说，查询操作是不需要加事务的，因为在不管是在事务内还是在事务外查询，没有什么区别，读到的都是已提交的数据。\n对于 RR 隔离级别来说，如果查询操作是不是在事务内执行的话，可能会出现幻读的问题。对于一些生成报表的业务场景来说，需要保证数据是在同一时间维度，那就需要加事务。"},"title":"事务的隔离级别"},"/db-learn/docs/mysql/advance/06_lock/":{"data":{"":"","死锁分析#死锁分析":" ‐‐ 查看死锁，status 字段中的内容可以帮助进行死锁分析 show engine innodb status; 示例：\n-- 插入测试数据 CREATE TABLE hero ( id INT, name VARCHAR(100), country varchar(100), PRIMARY KEY (id), KEY idx_name (name) ) Engine=InnoDB CHARSET=utf8; INSERT INTO hero VALUES (1, 'l刘备', '蜀'), (3, 'z诸葛亮', '蜀'), (8, 'c曹操', '魏'), (15, 'x荀彧', '魏'), (20, 's孙权', '吴'); -- 8.0 之后需要换成变量 transaction_isolation set tx_isolation='repeatable‐read'; -- session 1 执行： begin; select * from hero where id=1 for update; -- session 2 执行： begin; select * from hero where id=3 for update; -- session 1 执行： select * from hero where id=3 for update; -- session 2 执行： select * from hero where id=1 for update; -- 查看近期死锁日志信息，根据 DEADLOCK 关键字搜索，分析： show engine innodb status; 大多数情况 MySQL 可以自动检测死锁并回滚产生死锁的那个事务，但是有些情况 MySQL 没法自动检测死锁，这种情况可以通过日志分析找到对应事务线程 id，可以通过 kill 杀掉。\nmysql\u003e SHOW ENGINE INNODB STATUS\\G ... ------------------------ LATEST DETECTED DEADLOCK ------------------------ 2025-05-14 14:53:26 0x2414 *** (1) TRANSACTION: TRANSACTION 121629, ACTIVE 8 sec starting index read mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 163, OS thread handle 7184, query id 332329 localhost ::1 root statistics select * from hero where id=3 for update *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121629 lock_mode X locks rec but not gap Record lock, heap no 2 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000001; asc ;; 1: len 6; hex 00000001daf9; asc ;; 2: len 7; hex 81000000a10110; asc ;; 3: len 7; hex 6ce58898e5a487; asc l ;; 4: len 3; hex e89c80; asc ;; *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121629 lock_mode X locks rec but not gap waiting Record lock, heap no 3 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000003; asc ;; 1: len 6; hex 00000001daf9; asc ;; 2: len 7; hex 81000000a1011d; asc ;; 3: len 10; hex 7ae8afb8e8919be4baae; asc z ;; 4: len 3; hex e89c80; asc ;; *** (2) TRANSACTION: TRANSACTION 121630, ACTIVE 5 sec starting index read mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 164, OS thread handle 9416, query id 332330 localhost ::1 root statistics select * from hero where id=1 for update *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121630 lock_mode X locks rec but not gap Record lock, heap no 3 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000003; asc ;; 1: len 6; hex 00000001daf9; asc ;; 2: len 7; hex 81000000a1011d; asc ;; 3: len 10; hex 7ae8afb8e8919be4baae; asc z ;; 4: len 3; hex e89c80; asc ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121630 lock_mode X locks rec but not gap waiting Record lock, heap no 2 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000001; asc ;; 1: len 6; hex 00000001daf9; asc ;; 2: len 7; hex 81000000a10110; asc ;; 3: len 7; hex 6ce58898e5a487; asc l ;; 4: len 3; hex e89c80; asc ;; *** WE ROLL BACK TRANSACTION (2) ... 死锁信息，就是 LATEST DETECTED DEADLOCK 这一部分。\n第一句：\n2025-05-14 14:53:26 0x2414 死锁发生的时间是：2025-05-14 14:53:26，后边的一串十六进制 0x2414 表示的操作系统为当前 session 分配的线程的线程 id。\n然后是关于死锁发生时第一个事务的有关信息：\n*** (1) TRANSACTION: # 事务 id 为 121629，事务处于 ACTIVE 状态已经 8 秒了，事务现在正在做的操作就是：“starting index read” TRANSACTION 121629, ACTIVE 8 sec starting index read # 此事务使用了 1 个表，为 1 个表上了锁 mysql tables in use 1, locked 1 # 此事务处于 LOCK WAIT 状态，拥有 3 个锁结构，heap size 是为了存储锁结构而申请的内存大小（可以忽略），其中有 2 个行锁的结构 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) # 本事务所在线程的 id 是 163（MySQL 自己命名的线程 id），该线程在操作系统级别的 id 就是那一长串数字，当前查询的 id 为 332329（MySQL 内部使用，可以忽略），还有用户名主机信息 MySQL thread id 163, OS thread handle 7184, query id 332329 localhost ::1 root statistics # 本事务发生阻塞的语句 select * from hero where id=3 for update # 表示该事务获取到的锁信息 *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121629 lock_mode X locks rec but not gap Record lock, heap no 2 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 # 主键值为 1 0: len 4; hex 80000001; asc ;; 1: len 6; hex 00000001daf9; asc ;; 2: len 7; hex 81000000a10110; asc ;; 3: len 7; hex 6ce58898e5a487; asc l ;; 4: len 3; hex e89c80; asc ;; # 本事务当前在等待获取的锁： *** (1) WAITING FOR THIS LOCK TO BE GRANTED: # 等待获取的表空间 ID 为 26，页号为 4，也就是表 hero 的P RIMAY 索引中的某条记录的锁 # 该锁的类型是 X 锁（rec but not gap） RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121629 lock_mode X locks rec but not gap waiting # 记录在页面中的 heap_no 为 3，具体的记录信息如下： Record lock, heap no 3 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 # 这是主键值 0: len 4; hex 80000003; asc ;; # 这是 trx_id 隐藏列 1: len 6; hex 00000001daf9; asc ;; # 这是 roll_pointer 隐藏列 2: len 7; hex 81000000a1011d; asc ;; # 这是 name 列 3: len 10; hex 7ae8afb8e8919be4baae; asc z ;; # 这是 country 列 4: len 3; hex e89c80; asc ;; 从这个信息中可以看出，Session 1 中的事务为 2 条记录生成了锁结构，但是其中有一条记录上的 X 锁（rec but not gap）并没有获取到，没有获取到锁的这条记录主键值为 80000003，这其实是 InnoDB 内部存储使用的格式，其实就代表数字 3，也就是该事务在等待获取 hero 表聚簇索引主键值为 3 的那条记录的 X 锁。\n然后是关于死锁发生时第二个事务的有关信息：\n*** (2) TRANSACTION: TRANSACTION 121630, ACTIVE 5 sec starting index read mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1128, 2 row lock(s) MySQL thread id 164, OS thread handle 9416, query id 332330 localhost ::1 root statistics select * from hero where id=1 for update # 表示该事务获取到的锁信息 *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121630 lock_mode X locks rec but not gap Record lock, heap no 3 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 # 主键值为 3 0: len 4; hex 80000003; asc ;; 1: len 6; hex 00000001daf9; asc ;; 2: len 7; hex 81000000a1011d; asc ;; 3: len 10; hex 7ae8afb8e8919be4baae; asc z ;; 4: len 3; hex e89c80; asc ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 26 page no 4 n bits 72 index PRIMARY of table `asdb`.`hero` trx id 121630 lock_mode X locks rec but not gap waiting Record lock, heap no 2 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 # 主键值为 1 0: len 4; hex 80000001; asc ;; 1: len 6; hex 00000001daf9; asc ;; 2: len 7; hex 81000000a10110; asc ;; 3: len 7; hex 6ce58898e5a487; asc l ;; 4: len 3; hex e89c80; asc ;; # 回滚事务 *** WE ROLL BACK TRANSACTION (2) Session 2 中的事务获取了 hero 表聚簇索引主键值为 3 的记录的 X 锁，等待获取 hero 表聚簇索引主键值为 1 的记录的 X 锁。\n分析的思路 首先看一下发生死锁的事务等待获取锁的语句是什么。 找到发生死锁的事务中所有的语句之后，对照着事务获取到的锁和正在等待的锁的信息来分析死锁发生过程。 ","死锁检测和锁超时#死锁检测和锁超时":"处理死锁的方式有两种：\n死锁检测 锁超时 即使启用了 innodb_deadlock_detect=ON 死锁检测，仍有可能出现非死锁型的锁等待（例如，一个事务长时间持有锁，其他事务只能一直等待），这时才由 innodb_lock_wait_timeout 锁超时来控制。\n死锁检测 innodb_deadlock_detect=ON innodb_deadlock_detect 的默认值是 on，表示开启死锁检测。死锁检测能够在发生死锁的时候，快速发现并进行处理，但是它也是有额外负担的。\n可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。\n每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 （1000*1000）100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。\n怎么解决由这种热点行更新导致的性能问题？问题的症结在于，死锁检测要耗费大量的 CPU 资源。\n一种就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。\n另一个思路是控制并发度。根据上面的分析，如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。\n可以考虑通过将一行改成逻辑上的多行来减少锁冲突（分段）。以一个影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。\n锁超时 行锁的超时时间可以通过参数 innodb_lock_wait_timeout 来设置。\ninnodb_lock_wait_timeout 的默认值是 50s。当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。\n但是，又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。要根据业务来设置一个合理的值。","锁优化实践#锁优化实践":" 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁， 合理设计索引，尽量缩小锁的范围。 尽可能减少检索条件范围，避免间隙锁。 尽量控制事务大小，减少锁定资源量和时间长度，涉及事务加锁的 SQL 尽量放在事务最后执行。 尽可能用低的事务隔离级别。 ","锁分类#锁分类":" 从性能上分为乐观锁（用版本对比或 CAS 机制）和悲观锁， 乐观锁适合读操作较多的场景。 悲观锁适合写操作较多的场景，如果在写操作较多的场景使用乐观锁会导致大量的重试，长时间占用 CPU，降低性能。 从对数据操作的粒度分，分为表锁、页锁、行锁。 从对数据库操作的类型分，分为读锁和写锁(都属于悲观锁)，还有意向锁。 读锁（共享锁，S 锁（Shared））：针对同一份数据，多个读操作可以同时进行而不会互相影响，比如：select * from T where id=1 lock in share mode 写锁（排它锁，X 锁(eXclusive)）：当前写操作没有完成前，它会阻断其他写锁和读锁，数据修改操作都会加写锁，比如：select * from T where id=1 for update 意向锁（Intention Lock）：又称 I 锁，针对表锁，主要是为了提高加表锁的效率，是 MySQL 数据库自己加的。当有事务给表的数据行加了共享锁或排他锁时，同时也会给表设置一个标识，代表已经有行锁了，其他事务要想对表加表锁时，就不必逐行判断有没有行锁可能跟表锁冲突了，直接读这个标识就可以确定自己该不该加表锁。特别是表中的记录很多时，逐行判断加表锁的方式效率很低。而这个标识就是意向锁。 意向共享锁，IS 锁（Intention Shared）：对整个表加共享锁之前，需要先获取到意向共享锁。 意向排他锁，IX 锁（Intention Xclusive）：对整个表加排他锁之前，需要先获取到意向排他锁。 表锁 每次操作锁住整张表。开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低；一般用在整表数据迁移的场景。\n‐‐手动增加表锁 lock table 表名称 read(write),表名称2 read(write),表名称3 read; ‐‐查看表上加过的锁 show open tables; ‐‐删除表锁 unlock tables; 页锁 只有 BDB 存储引擎支持页锁，页锁就是在页的粒度上进行锁定，锁定的数据资源比行锁要多，因为一个页中可以有多个行记录。当使用页锁的时候，会出现数据浪费的现象，但这样的浪费最多也就是一个页上的数据行。页锁的开销介于表锁和行锁之间，会出现死锁。锁定粒度介于表锁和行锁之间，并发度一般。\n行锁 每次操作锁住一行数据。开销大，加锁慢（行锁需要先到表中找到对应的那行记录，所以说开销大；而表锁是直接在整张表上加一个标记）；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度最高。\nInnoDB 相对于 MYISAM 的最大不同有两点：\nInnoDB 支持事务 InnoDB 支持行级锁 注意，InnoDB 的行锁实际上是针对索引字段加的锁（在索引对应的索引项上做标记），不是针对整个行记录加的锁。并且该索引不能失效（或者不存在索引），否则会从行锁升级为表锁。不管是一级索引还是二级索引，只要更新时使用了索引，就会对索引字段加锁，否则就会升级为表锁。（RR 级别会升级为表锁，RC 级别不会升级为表锁）\n比如在 RR 级别执行如下 SQL：\n‐‐ name 字段无索引 select * from account where name = 'lilei' for update; 由于 name 无索引，升级为表锁，则其它 session 对该表任意一行记录做修改，插入，删除的操作都会被阻塞住。\n关于 RR 级别行锁升级为表锁的原因分析 因为在 RR 隔离级别下，需要解决幻读问题，当字段没有索引时，MySQL 无法有效使用间隙锁精确锁定范围，只能退而求其次使用表锁来保证隔离性。所以在遍历扫描聚簇索引记录时，为了防止扫描过的索引间隙被其它事务插入记录（幻读问题），从而导致数据不一致，MySQL 的解决方案就是把所有扫描过的索引记录和间隙都锁上，这里要注意，并不是直接将整张表加表锁，因为不一定能加上表锁，可能会有其它事务锁住了表里的其它行记录。\nRC 读已提交级别不会升级为表锁，因为 RC 级别下，不需要解决幻读问题，所以不需要加锁。\n间隙锁（Gap Lock） 间隙锁，锁的就是两个值之间的空隙，间隙锁是在可重复读隔离级别下才会生效。MySQL 默认隔离级别是 RR，有幻读问题，间隙锁是可以解决幻读问题的。\n上面的表中，间隙就有 id 为 (1,4)，(4,112)，(115,正无穷) 这三个区间，执行 sql：\nselect * from account where id = 10 for update; 其他 Session 没法在这个 (4,112) 这个间隙范围里插入任何数据。\n也就是说，只要在间隙范围内锁了一条不存在的记录，整个间隙范围都会被锁住，这样就能防止其它 Session 在这个间隙范围内插入数据，就解决了可重复读隔离级别的幻读问题。\n为什么说间隙锁可以解决幻读问题 例如还是下面这些数据：\nset tx_isolation='repeatable‐read'; -- 事务1 START TRANSACTION; SELECT * FROM account; -- 快照读，看到 6 条记录 -- 此时事务 2 插入一条 name=小郭 的新记录并提交 insert into account values(5,'小郭',12256487569,'洛阳'); SELECT * FROM account; -- 仍然看到 6 条记录（快照读） update account set address='信阳' where id = 5; -- 当前读 SELECT * FROM account; -- 看到 7 条记录 COMMIT; 可以看到，事务 1 最后看到了 7 条记录。还是有幻读问题的。\n但是如果在 (4,112) 这个区间加一把 间隙锁，这个间隙就不会允许其他事务再插入数据，就不会出现幻读问题了。\n对于删除的行为：\n事务 A 和事务 B 一起开启：\n如果事务 A 删除了一条记录 1，并已提交。 事务 B 去更新记录 1，事务 B 发现记录已经被删除，更新无效，不会报错，只是匹配不到记录，影响行数为 0。 如果事务 A 删除了一条记录 1，还未提交，那事务 B 会等待锁。 临键锁（Next-Key Lock） 临键锁 = 间隙锁 + 行锁，既能锁住某条记录，又能阻止别的事务将新记录插入被锁记录前边的间隙。\nselect * from account where id \u003e 4 and id \u003c= 122 for update; (4,112) 这个间隙和 112 这条记录都会被锁住，相当于 (4,112]。\n锁定读 采用加锁方式解决脏读、不可重复读、幻读这些问题时，读取一条记录时需要获取一下该记录的 S 锁，其实这是不严谨的，有时候想在读取记录时就获取记录的 X 锁，来禁止别的事务读写该记录，MySQL 提供了两种比较特殊的 SELECT 语句格式：\n对读取的记录加 S 锁： SELECT ... LOCK IN SHARE MODE; 对读取的记录加 X 锁： SELECT ... FOR UPDATE; 也就是在普通的 SELECT 语句后边加 FOR UPDATE，如果当前事务执行了该语句，那么它会为读取到的记录加 X 锁，这样既不允许别的事务获取这些记录的S锁（比方说别的事务使用 SELECT ... LOCK IN SHARE MODE 语句来读取这些记录），也不允许获取这些记录的 X 锁。\nMyISAM 和 InnoDB 的锁 MyISAM 在执行查询语句 SELECT 前，会自动给涉及的所有表加读锁，在执行 update、insert、delete 操作会自动给涉及的表加写锁。MyISAM 没有事务，就算是加的表锁，也只是执行一条 SQL 语句时才会生效，执行完 SQL 语句后，会自动释放锁。\nInnoDB 在执行查询语句 SELECT 时(非串行隔离级别)，不会加锁。但是 update、insert、delete 操作会加行锁。\n读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。\nInnoDB 存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一下，但是在整体并发处理能力方面要远远优于MYISAM的表级锁定的。当系统并发量高的时候，InnoDB 的整体性能和 MYISAM 相比就会有比较明显的优势了。","锁等待分析#锁等待分析":"通过检查 innodb_row_lock 状态变量来分析系统上的行锁的争夺情况\nshow status like 'innodb_row_lock%'; Innodb_row_lock_current_waits：当前正在等待锁定的数量。 Innodb_row_lock_time：从系统启动到现在锁定总时间长度。 Innodb_row_lock_time_avg：每次等待所花平均时间。 Innodb_row_lock_time_max：从系统启动到现在等待最久的一次所花的时间。 Innodb_row_lock_waits：系统启动后到现在总共等待的次数。 在 MySQL 8.0，这些统计变量不再更新或直接消失。\n在 MySQL 8.0 之后，可以通过 performance_schema 数据库的表来查看锁的相关信息。\n‐‐ 查看事务 select * from information_schema.INNODB_TRX; ‐‐ 查看锁，8.0 之后需要换成表 performance_schema.data_locks select * from information_schema.INNODB_LOCKS; ‐‐ 查看锁等待，8.0 之后需要换成表 performance_schema.data_lock_waits select * from information_schema.INNODB_LOCK_WAITS; ‐‐ 释放锁，trx_mysql_thread_id 可以从 INNODB_TRX 表里查看到 kill \u003ctrx_mysql_thread_id\u003e; "},"title":"锁"},"/db-learn/docs/mysql/advance/07_log/":{"data":{"":" ","binlog#binlog":"MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层。\nredo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。\nbinlog 二进制日志记录保存了所有执行过的修改操作语句，不保存查询操作。\n启动 binlog 记录功能，会影响服务器性能，但如果需要恢复数据或主从复制功能，则好处则大于对服务器性能的影响。\nMySQL 5.7 版本中，binlog 默认是关闭的，8.0 版本默认是打开的。\n开启 binlog 记录功能，需要修改配置文件 my.ini(windows) 或 my.cnf(linux)，然后重启数据库。\n[mysqld] ... # 设置 binlog 的存放位置，可以是绝对路径，也可以是相对路径，这里写的相对路径，则 binlog 文件默认会放在 data 数据目录下 log_bin = mysql‐binlog # Server Id 是数据库服务器 id，随便写一个数都可以，这个 id 用来在 MySQL 集群环境中标记唯一 MySQL 服务器，集群环境中每台 MySQL 服务器的 id 不能一样，否则启动会报错 server‐id=1 # 记录 binlog 的格式，有三种格式：STATEMENT、ROW、MIXED，默认是 STATEMENT 格式 binlog_format = ROW # 执行自动删除 binlog 日志文件的天数，也就是每个 binlog 文件最多保存的时间。默认为 0，表示不自动删除。一般情况下，需要根据业务设置一个合理的值，这样可以保证 binlog 日志文件不会占用太多的磁盘空间。 expire_logs_days = 7 # 单个 binlog 日志文件的大小限制，默认为 1GB max_binlog_size = 200M 查看 binlog 配置：\nshow variables like '%log_bin%'; log_bin：binlog 日志是否打开状态 log_bin_basename：是 binlog 日志的基本文件名，后面会追加标识来表示每一个文件，binlog 日志文件会滚动增加 log_bin_index：指定的是 binlog 文件的索引文件，这个文件管理了所有的 binlog 文件的目录。 sql_log_bin：SQL 语句是否写入 binlog 文件，ON 代表需要写入，OFF 代表不需要写入。 sql_log_bin 有什么用？\n既然开启了 binlog 日志，那为什么还要设置 sql_log_bin 呢？\nsql_log_bin 如果修改为 OFF，则代表所有的 SQL 语句都不会写入 binlog 文件，但是 binlog 文件还是有的。这样就可以在主库上执行一些操作，但不复制到 slave 库上。可以用来模拟主从同步复制异常。\nbinlog 的日志格式 binlog_format 可以设置 binlog 日志的记录格式，MySQL 支持三种格式类型：\nSTATEMENT：基于 SQL 语句的复制，每一条会修改数据的 SQL 都会记录到 master 机器的 binlog 中，这种方式日志量小，节约 IO 开销，提高性能，但是对于一些执行过程中才能确定结果的函数，比如 UUID()、SYSDATE() 等函数如果随 SQL 同步到 slave 机器去执行，则结果跟 master 机器执行的不一样。 ROW：基于行的复制，日志中会记录成每一行数据被修改的形式，然后在 slave 端再对相同的数据进行修改记录下每一行数据修改的细节，可以解决函数、存储过程等在 slave 机器的复制问题，但这种方式日志量较大，性能不如 STATEMENT。举个例子，假设 update 语句更新 10 行数据，STATEMENT 方式就记录这条 update 语句，而 Row 方式会记录被修改的 10 行数据。 MIXED：混合模式复制，实际就是前两种模式的结合，在 MIXED 模式下，MySQL 会根据执行的每一条具体的 SQL 语句来区分对待记录的日志形式，也就是在 STATEMENT 和 ROW 之间选择一种，如果 SQL 里有函数或一些在执行时才知道结果的情况，会选择 ROW，其它情况选择 STATEMENT，推荐使用这一种。 当表结构发生变化的时候，会使用 STATEMENT 合适。\nbinlog 的写入机制 binlog 写入磁盘机制主要通过 sync_binlog 参数控制。与 redo log 类似，sync_binlog 也有三种取值：\n设置为 0，默认值，表示每次提交事务都只 write 到 Page Cache，由系统自行判断什么时候执行 fsync 写入磁盘。虽然性能得到提升，但是机器宕机，Page Cache 里面的 binlog 会丢失。 设置为 1，表示每次提交事务都会执行 fsync 写入磁盘，这种方式最安全。 设置为 N （N\u003e1），表示每次提交事务都 write 到 Page Cache，但累积 N 个事务后才 fsync 写入磁盘，这种如果机器宕机会丢失 N 个事务的 binlog。 binlog 日志文件重新生成的时机：\n服务器启动或重新启动。 服务器刷新日志，执行命令 flush logs。 日志文件大小达到 max_binlog_size 值，默认值为 1GB。 删除 binlog 日志文件 -- 删除当前的 binlog 文件 reset master; -- 删除指定日志文件之前的所有日志文件，下面这个是删除 6 之前的所有日志文件，当前这个文件不删除 purge master logs to 'mysql‐binlog.000006'; -- 删除指定日期前的日志索引中binlog日志文件 purge master logs before '2023‐01‐21 14:00:00'; 查看 binlog 日志文件 MySQL 提供了 mysqlbinlog 命令，可以查看 binlog 日志：\n# 查看 binlog（命令行方式，不用登录 MySQL） $ mysqlbinlog ‐‐no‐defaults ‐v \\ ‐‐base64‐output=decode‐rows D:/dev/mysql‐5.7.25‐winx64/data/mysql‐binlog.000007 # 查看 binlog（带查询条件） $ mysqlbinlog ‐‐no‐defaults ‐v \\ ‐‐base64‐output=decode‐rows D:/dev/mysql‐5.7.25‐winx64/data/mysql‐binlog.000007 \\ start‐datetime=\"2023‐01‐21 00:00:00\" \\ stop‐datetime=\"2023‐02‐01 00:00:00\" \\ startposition=\"5000\" \\ stop‐position=\"20000\" 查出来的 binlog 日志文件内容如下：\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/; /*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/; DELIMITER /*!*/; # at 4 #230127 21:13:51 server id 1 end_log_pos 123 CRC32 0x084f390f Start: binlog v 4, server v 5.7.25‐ log created 230127 21:13:51 at startup # Warning: this binlog is either in use or was not closed properly. ROLLBACK/*!*/; # at 123 #230127 21:13:51 server id 1 end_log_pos 154 CRC32 0x672ba207 Previous‐GTIDs # [empty] # at 154 #230127 21:22:48 server id 1 end_log_pos 219 CRC32 0x8349d010 Anonymous_GTID last_committed=0 sequence_number=1 rbr_only=yes /*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/; SET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/; # at 219 #230127 21:22:48 server id 1 end_log_pos 291 CRC32 0xbf49de02 Query thread_id=3 exec_time=0 erro r_code=0 SET TIMESTAMP=1674825768/*!*/; SET @@session.pseudo_thread_id=3/*!*/; SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@s ession.autocommit=1/*!*/; SET @@session.sql_mode=1342177280/*!*/; SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/; /*!\\C utf8 *//*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; BEGIN /*!*/; # at 291 #230127 21:22:48 server id 1 end_log_pos 345 CRC32 0xc4ab653e Table_map: `test`.`account` mapped to number 99 # at 345 #230127 21:22:48 server id 1 end_log_pos 413 CRC32 0x54a124bd Update_rows: table id 99 flags: ST MT_END_F ### UPDATE `test`.`account` ### WHERE ### @1=1 ### @2='lilei' ### @3=1000 ### SET ### @1=1 ### @2='lilei' ### @3=2000 # at 413 #230127 21:22:48 server id 1 end_log_pos 444 CRC32 0x23355595 Xid = 10 COMMIT/*!*/; # at 444 ... 包含具体执行的 SQL 语句以及执行时的相关情况。\nbinlog 日志文件恢复数据 -- 先执行刷新日志的命令生成一个新的 binlog 文件 mysql‐binlog.000008，后面修改操作日志都会记录在最新的这个文件里 flush logs; -- 执行两条插入语句 INSERT INTO `test`.`account` (`id`, `name`, `balance`) VALUES ('4', 'zhuge', '666'); INSERT INTO `test`.`account` (`id`, `name`, `balance`) VALUES ('5', 'zhuge1', '888'); -- 假设现在误操作执行了一条删除语句把刚新增的两条数据删掉了 delete from account where id \u003e 3; 现在需要恢复被删除的两条数据，先查看 binlog 日志文件：\n$ mysqlbinlog ‐‐no‐defaults ‐v \\ ‐‐base64‐output=decode‐rows D:/dev/mysql‐5.7.25‐winx64/data/mysql‐binlog.000008 文件内容：\n... SET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/; # at 219 #230127 23:32:24 server id 1 end_log_pos 291 CRC32 0x4528234f Query thread_id=5 exec_time=0 error_code=0 SET TIMESTAMP=1674833544/*!*/; SET @@session.pseudo_thread_id=5/*!*/; SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/; SET @@session.sql_mode=1342177280/*!*/; SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/; /*!\\C utf8 *//*!*/; SET @@session.character_set_client=33,@@session.collation_connection=33,@@session.collation_server=33/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; BEGIN /*!*/; # at 291 #230127 23:32:24 server id 1 end_log_pos 345 CRC32 0x7482741d Table_map: `test`.`account` mapped to number 99 # at 345 #230127 23:32:24 server id 1 end_log_pos 396 CRC32 0x5e443cf0 Write_rows: table id 99 flags: STMT_END_F ### INSERT INTO `test`.`account` ### SET ### @1=4 ### @2='zhuge' ### @3=666 # at 396 #230127 23:32:24 server id 1 end_log_pos 427 CRC32 0x8a0d8a3c Xid = 56 COMMIT/*!*/; # at 427 #230127 23:32:40 server id 1 end_log_pos 492 CRC32 0x5261a37e Anonymous_GTID last_committed=1 sequence_number=2 rbr_only=yes /*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/; SET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/; # at 492 #230127 23:32:40 server id 1 end_log_pos 564 CRC32 0x01086643 Query thread_id=5 exec_time=0 error_code=0 SET TIMESTAMP=1674833560/*!*/; BEGIN /*!*/; # at 564 #230127 23:32:40 server id 1 end_log_pos 618 CRC32 0xc26b6719 Table_map: `test`.`account` mapped to number 99 # at 618 #230127 23:32:40 server id 1 end_log_pos 670 CRC32 0x8e272176 Write_rows: table id 99 flags: STMT_END_F ### INSERT INTO `test`.`account` ### SET ### @1=5 ### @2='zhuge1' ### @3=888 # at 670 #230127 23:32:40 server id 1 end_log_pos 701 CRC32 0xb5e63d00 Xid = 58 COMMIT/*!*/; # at 701 #230127 23:34:23 server id 1 end_log_pos 766 CRC32 0xa0844501 Anonymous_GTID last_committed=2 sequence_number=3 rbr_only=yes /*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/; SET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/; # at 766 #230127 23:34:23 server id 1 end_log_pos 838 CRC32 0x687bdf88 Query thread_id=7 exec_time=0 error_code=0 SET TIMESTAMP=1674833663/*!*/; BEGIN /*!*/; # at 838 #230127 23:34:23 server id 1 end_log_pos 892 CRC32 0x4f7b7d6a Table_map: `test`.`account` mapped to number 99 # at 892 #230127 23:34:23 server id 1 end_log_pos 960 CRC32 0xc47ac777 Delete_rows: table id 99 flags: STMT_END_F ### DELETE FROM `test`.`account` ### WHERE ### @1=4 ### @2='zhuge' ### @3=666 ### DELETE FROM `test`.`account` ### WHERE ### @1=5 ### @2='zhuge1' ### @3=888 # at 960 #230127 23:34:23 server id 1 end_log_pos 991 CRC32 0x386699fe Xid = 65 COMMIT/*!*/; SET @@SESSION.GTID_NEXT= 'AUTOMATIC' /* added by mysqlbinlog */ /*!*/; DELIMITER ; # End of log file ... 找到两条插入数据的 SQL，每条 SQL 的上下都有 BEGIN 和 COMMIT，找到第一条 SQL BEGIN 前面的文件位置标识 at 219（这是文件的位置标识），再找到第二条 SQL COMMIT 后面的文件位置标识 at 701 。然后可以根据文件位置标识来恢复数据，执行如下 SQL：\nmysqlbinlog --no-defaults --start-position=219 --stop-position=701 --database=test D:/dev/mysql-5.7.25-winx64/data/mysql-binlog.000009 | mysql -uroot -p123456 -v test # 补充一个根据时间来恢复数据的命令，找到第一条 SQL BEGIN 前面的时间戳标记 SET TIMESTAMP=1674833544，再找到第二条 SQL COMMIT 后面的时间戳标记 SET TIMESTAMP=1674833663，转成 datetime 格式 mysqlbinlog --no-defaults --start-datetime=\"2023-1-27 23:32:24\" --stop-datetime=\"2023-1-27 23:34:23\" --database=test D:/dev/mysql-5.7.25-winx64/data/mysql-binlog.000009 | mysql -uroot -p123456 -v test 执行后删除数据被恢复。如果数据后面还有别的更新，那么就接着找到对应 SQL 的位置标识，然后执行恢复数据的命令即可。如果要恢复整个 binlog 日志文件，那么就不需要过滤参数，直接执行恢复数据的命令即可。\nℹ️ 注意，如果要恢复大量数据，假设数据库所有数据都被删除了要怎么恢复，如果数据库之前没有备份，所有的 binlog 日志都在的话，就从 binlog 第一个文件开始逐个恢复每个 binlog 文件里的数据，这种一般不太可能，因为 binlog 日志比较大，早期的 binlog 文件会定期删除的，所以一般不可能用 binlog 文件恢复整个数据库的。 ℹ️ 一般推荐的是每天(在凌晨后)需要做一次全量数据库备份，那么恢复数据库可以用最近的一次全量备份再加上备份时间点之后的 binlog 来恢复数据。备份数据库一般可以用 mysqldump 命令工具。\nmysqldump -u root 数据库名\u003e备份文件名; # 备份整个数据库 mysqldump -u root 数据库名 表名字\u003e备份文件名; # 备份整个表 mysql -u root test \u003c 备份文件名 # 恢复整个数据库，test 为数据库名称，需要自己先建一个数据库 test binlog 的 expire_logs_days 如何设置，一般最好比全量备份的间隔时间长，避免全量备份某天备份失败了，后面的 binlog 日志也丢失了，无法恢复数据。\nredo log 和 binlog 的区别 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如 “给 ID=2 这一行的 c 字段加 1”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 为什么需要两阶段提交？ 如果只有 redo log 或者只有 binlog，那么事务就不需要两阶段提交。但是如果同时使用了 redo log 和 binlog，那么就需要保证这两种日志之间的一致性。否则，在数据库发生异常重启或者主从切换时，可能会出现数据不一致的情况。\n假设我们有一个事务 T，它修改了两行数据 A 和 B，并且同时开启了 redo log 和 binlog。\n如果先写 redo log 再写 binlog，并且在写完 redo log 后数据库发生了宕机，那么在重启后，根据 redo log 可以恢复 A 和 B 的修改，但是 binlog 中没有记录数据 A 和 B 的修改信息，导致备份或者从库中没有 A 和 B 的修改。 如果先写 binlog 再写 redo log，并且在写完 binlog 后数据库发生了宕机，那么在重启后，根据 redo log 无法恢复 A 和 B 的修改，但是 binlog 中有记录 A 和 B 的修改信息，导致备份或者从库中有 A 和 B 的修改。 所以如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。\n为什么会有 redo log 和 binlog 两份日志？ 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力， binlog 只在 Server 层记录逻辑操作，不参与存储引擎的数据页写入和 crash recovery 过程。而 InnoDB 是以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。","redo-log#redo log":"redo log 只是为了系统崩溃后恢复脏页用的，先写 redo log，再写数据库表文件的机制，也被称之为 WAL（Write Ahead Logging）。\nWAL：对数据库做任何修改之前，必须先将对应的修改操作记录到日志文件中，并保证日志已经持久化到磁盘，才能真正对数据页进行修改。\nWAL 的好处：\n保证数据的持久性，只要日志已经写入磁盘，重启后可以通过日志重放恢复数据。 日志是顺序 I/O（追加写入），比随机 I/O 快得多（尤其是机械硬盘时代）。 先顺序 I/O 写入日志文件，数据页可后续批量写入数据表文件，减少 I/O 频率（innodb 中是以页为单位来进行磁盘 IO 的，一个页面默认是 16KB，也就是说在该事务提交时不得不将一个完整的页面从内存中刷新到磁盘，哪怕只修改一个字节也要刷新 16KB 的数据到磁盘上）。 redo log 关键配置 innodb_log_buffer_size 设置 redo log buffer 大小参数，默认 16M，最大值是 4096M，最小值为 1M。\nshow variables like '%innodb_log_buffer_size%'; innodb_log_group_home_dir 设置 redo log 文件存储位置，默认值为 ./，即 innodb 数据文件存储位置，其中的 ib_logfile0 和 ib_logfile1 即为 redo log 文件。\nshow variables like '%innodb_log_group_home_dir%'; innodb_log_files_in_group 磁盘上的 redo log 文件不只一个，而是以一个日志文件组的形式出现的。innodb_log_files_in_group 设置 redo log 文件组中文件个数，默认值为 2，命名方式如: ib_logfile0, iblogfile1 … iblogfileN。\nshow variables like '%innodb_log_files_in_group%'; redo log 写入磁盘过程 在将 redo log 写入日志文件组时，是从 ib_logfile0 开始写，如果 ib_logfile0 写满了，就接着 ib_logfile1 写，同理，ib_logfile1 写满了就去写 ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到 ib_logfile0 继续写。类似环形的写入，假设现在有四个 redo log 文件，如下图，\nwrite pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。 checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的部分就是空着的可写部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示 redo log 写满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。\ninnodb_log_file_size 设置 redo log 文件大小，默认值为 48M，最大值为 512G，注意最大值指的是所有 redo log 文件之和，即 innodb_log_files_in_group * innodb_log_file_size 不能大于最大值 512G。\nshow variables like '%innodb_log_file_size%'; innodb_flush_log_at_trx_commit 这个参数控制 redo log 的写入策略，可选值：\n0：表示每次事务提交时都只是把 redo log 留在 redo log buffer 中，效率最高，但是如果数据库宕机可能会丢失数据。 1：默认值，表示每次事务提交时都将 redo log 直接持久化到磁盘，数据最安全，不会因为数据库宕机丢失数据，但是效率稍微差一点，线上系统推荐这个设置。 2：这是一个折中的选择，表示每次事务提交时都只是把 redo log 写到操作系统的缓存 Page Cache 里，这种情况如果数据库宕机是不会丢失数据的，但是操作系统如果宕机了，Page Cache 里的数据还没来得及写入磁盘文件的话就会丢失数据。 查看 innodb_flush_log_at_trx_commit 的值：\nshow variables like 'innodb_flush_log_at_trx_commit'; 设置 innodb_flush_log_at_trx_commit 的值（也可以在 my.ini 或 my.cnf 文件里配置）：\nset global innodb_flush_log_at_trx_commit = 1; redo 日志刷盘时机 innodb_flush_log_at_trx_commit 是控制事务提交时 redo 日志写入磁盘的策略，还有其他的一些时机也会触发 redo 日志写入磁盘：\nredo log buffer 空间不足时。redo log buffer 的大小是有限的（通过系统变量 innodb_log_buffer_size 指定），如果不停的往这个 buffer 里塞入日志，很快它就会被填满。如果当前写入的 redo log 已经占满了 buffer 总容量的大约一半左右，就会把日志刷新到磁盘上。 事务提交时。 后台线程不停的刷刷刷。后台有一个线程，大约每秒都会刷新一次 redo log buffer 中的 redo 日志到磁盘。 正常关闭服务器时。 ","undo-log#undo log":"事务需要保证原子性，也就是事务中的操作要么全部完成，要么什么也不做。但是有时候就是会在事务执行到一半会出现一些的情况，比如：\n事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。 手动输入 ROLLBACK 语句结束当前的事务的执行。 为了保证事务的原子性，就需要把东西改回原先的样子，这个过程就称之为回滚。\n每当对一条记录做改动时（INSERT、DELETE、UPDATE），都需要把回滚时所需的东西都给记下来。例如：\n插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。 删除了一条记录，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。 修改了一条记录，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。 这些为了回滚而记录的这些日志就称之为 undo log。\n事务 id 分配的时机 事务执行过程中，只有在第一次真正修改记录时（执行 INSERT、DELETE、UPDATE 这些语句或加排它锁操作比如 select...for update 语句时），才会被分配一个单独的事务 id，否则在一个只读事务中的事务 id 值都默认为 0。\ntrx_id 和 roll_pointer 隐藏列 聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为 trx_id、roll_pointer 的隐藏列。如果用户没有在表中定义主键以及 UNIQUE 键，还会自动添加一个名为 row_id 的隐藏列。\ntrx_id 就是某个聚簇索引记录被修改时所在的事务对应的事务 id。 roll_pointer 就是一个指向记录对应的 undo log 的一个指针。 undo log 回滚段 InnoDB 对 undo log 文件的管理采用段的方式，也就是回滚段（rollback segment）。每个回滚段记录了 1024 个 undo log segment，每个事务只会使用一个 undo log segment。\n在 MySQL 5.5 时只有一个回滚段，那么最大同时支持的事务数量为 1024 个。MySQL 5.6 以后，InnoDB 支持最大 128 个回滚段，故其支持同时在线的事务限制提高到了 128*1024。\n查看 undo log 相关参数：\nshow variables like '%innodb_undo%'; show variables like 'innodb_rollback_segments'; innodb_undo_directory：undo log 文件所在的路径。默认值为 ./，即 innodb 数据文件存储位置，目录下 ibdata1 文件就是 undo log 存储的位置。 innodb_undo_tablespaces: undo log 文件的数量，这样回滚段可以较为平均地分布在多个文件中。设置该参数后，会在路径 innodb_undo_directory 看到 undo 为前缀的文件。 innodb_rollback_segments: undo log 文件内部回滚段的个数，默认值为 128。 undo log 日志什么时候删除 新增类型的，在事务提交之后就可以清除掉了。 修改类型的，事务提交之后不能立即清除掉，这些日志会用于 MVCC。只有当没有事务用到该版本信息时才可以清除。 ","慢查询日志#慢查询日志":"慢查询日志是 MySQL 提供的一种日志记录，它用来记录在 MySQL 中响应时间超过阈值的语句，具体指运行时间超过 long_query_time 值的 SQL 才会被记录到慢查询日志中。\n如果不是调优需要的话，不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。\nshow variables like '%slow_query%'; long_query_time：慢查询日志的阈值，默认值为 10，单位为秒。意思是记录运行 10 秒以上的语句。 log_queries_not_using_indexes：未使用索引的查询也被记录到慢查询日志中（可选项）。 log_output：日志存储方式。log_output='FILE' 表示将日志存入文件，默认值是 'FILE'。log_output='TABLE' 表示将日志存入数据库。 log_slow_admin_statements：表示，是否将慢管理语句例如ANALYZE TABLE和ALTER TABLE等记入慢查询日志。 开启慢查询日志：\n-- 1 表示开启，0 表示关闭 SET GLOBAL slow_query_log=1; mysqldumpslow MySQL 提供了日志分析工具 mysqldumpslow，可以用来分析慢查询日志，找出执行时间比较长的 SQL 语句。\n比如，得到返回记录集最多的 10 个 SQL。\nmysqldumpslow -s r -t 10 /database/mysql/mysql06_slow.log 得到访问次数最多的 10 个 SQL：\nmysqldumpslow -s c -t 10 /database/mysql/mysql06_slow.log 得到按照时间排序的前 10 条里面含有左连接的查询语句：\nmysqldumpslow -s t -t 10 -g “left join” /database/mysql/mysql06_slow.log 建议在使用这些命令时结合 | 和 more 使用，否则有可能出现刷屏的情况：\nmysqldumpslow -s r -t 20 /mysqldata/mysql/mysql06-slow.log | more ","通用查询日志#通用查询日志":"通用查询日志记录用户的所有操作，包括启动和关闭 MySQL 服务、所有用户的连接开始时间和截止时间、发给 MySQL 数据库服务器的所有 SQL 指令等，如 select、show 等，无论 SQL 的语法正确还是错误、也无论 SQL 执行成功还是失败，MySQL 都会将其记录下来。\n一般不建议开启，只在需要调试查询问题时开启。因为开启会消耗系统资源并且占用大量的磁盘空间。\nshow variables like '%general_log%'; -- 打开通用查询日志 SET GLOBAL general_log=on; ","错误日志#错误日志":"MySQL 有一个比较重要的日志是错误日志，它记录了数据库启动和停止，以及运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，建议首先查看此日志。在 MySQL 数据库中，错误日志功能是默认开启的，而且无法被关闭。\nshow variables like '%log_error%'; "},"title":"MySQL 日志机制"},"/db-learn/docs/mysql/advance/08_config/":{"data":{"":"MySQL Server 的配置一般都有默认值，可以在命令行中指定启动参数，也可以通过配置文件指定。","在命令行上使用选项#在命令行上使用选项":"示例：\nmysqld --default-storage-engine=MyISAM --skip-networking （或 skip_networking）选项禁止各客户端使用 TCP/IP 网络进行通信。（多个单词之间可以用 - 连接， 也可以用 _ 连接） --default-storage-engine=\u003cengine\u003e 改变默认存储引擎。 查看更多启动选项：\nmysqld --verbose --help mysqld_safe --help ","状态变量#状态变量":"MySQL 服务器程序中维护了很多关于程序运行状态的变量，它们被称为状态变量。它们的值只能由服务器程序自己来设置。状态变量也有 GLOBAL 和 SESSION 两个作用范围的，所以查看状态变量的语句可以这么写：\nSHOW [GLOBAL|SESSION] STATUS [LIKE 匹配的模式]; ","系统变量#系统变量":"SHOW VARIABLES [LIKE 匹配的模式]; 查看 MySQL 服务器程序支持的系统变量以及它们的当前值。\n大部分系统变量的值可以在服务端程序运行过程中进行动态修改而无需停止并重启服务器。\n设置系统变量 通过启动选项设置 通过命令行添加启动选项和配置文件。例如：\nmysqld --default-storage-engine=MyISAM --max-connections=10 或者\n[server] default-storage-engine=MyISAM max-connections=10 max-connections 表示允许同时连入的客户端数量。\n服务器程序运行过程中设置 设置不同作用范围的系统变量 了针对不同的客户端设置不同的系统变量：\nGLOBAL：全局变量，影响服务器的整体操作。 SESSION：会话变量，影响某个客户端连接的操作。（SESSION 有个别名叫 LOCAL） 通过启动选项设置的系统变量的作用范围都是 GLOBAL 的，也就是对所有客户端都有效的，因为在系统启动的时候还没有客户端程序连接进来。\n通过客户端程序设置系统变量的语法：\nSET [GLOBAL|SESSION] 系统变量名 = 值; # 或者 SET [@@(GLOBAL|SESSION).]var_name = XXX; 比如在服务端进程运行过程中把作用范围为 GLOBAL 的系统变量 default_storage_engine 的值修改为 MyISAM，也就是想让之后新连接到服务器的客户端都用 MyISAM 作为默认的存储引擎：\n语句一：SET GLOBAL default_storage_engine = MyISAM; 语句二：SET @@GLOBAL.default_storage_engine = MyISAM; 只对本客户端生效：\n语句一：SET SESSION default_storage_engine = MyISAM; 语句二：SET @@SESSION.default_storage_engine = MyISAM; 语句三：SET default_storage_engine = MyISAM; SESSION 是默认的作用范围。\n如果某个客户端改变了某个系统变量在 GLOBAL 作用范围的值，并不会影响该系统变量在当前已经连接的客户端作用范围为 SESSION 的值，只会影响后续连入的客户端在作用范围为 SESSION 的值。\n注意：\n有一些系统变量只具有 GLOBAL 作用范围，如 max_connections。 有一些系统变量只具有 SESSION 作用范围，如 insert_id，表示在对某个包含 AUTO_INCREMENT 列的表进行插入时，该列初始的值。 有些系统变量是只读的，并不能设置值。如 version，表示当前 MySQL 的版本。 ","配置文件中使用选项#配置文件中使用选项":"配置文件的路径 Windows 操作系统的配置文件 MySQL 会按照下列路径来寻找配置文件：\n路径 描述 %WINDIR%\\my.ini， %WINDIR%\\my.cnf C:\\my.ini， C:\\my.cnf BASEDIR\\my.ini， BASEDIR\\my.cnf defaults-extra-file 命令行指定的额外配置文件路径 %APPDATA%\\MySQL\\.mylogin.cnf 登录路径选项（仅限客户端） %WINDIR% 一般是 C:\\WINDOWS，可以用 echo %WINDIR% 来查看。 BASEDIR 指的是 MySQL 安装目录的路径 defaults-extra-file 在命令行上可以这么写 mysqld --defaults-extra-file=C:\\Users\\xiaohaizi\\my_extra_file.txt %APPDATA% 表示 Windows 应用程序数据目录的值 .mylogin.cnf 中只能包含一些用于启动客户端连接服务端的一些选项，包括 host、user、password、port 和 socket。 UNIX 操作系统的配置文件 MySQL 会按照下列路径来寻找配置文件：\n路径 描述 /etc/my.cnf /etc/mysql/my.cnf SYSCONFDIR/my.cnf $MYSQL_HOME/my.cnf 特定于服务器的选项（仅限服务器） defaults-extra-file 命令行指定的额外配置文件路径 ~/.my.cnf 用户特定选项 ~/.mylogin.cnf 登录路径选项（仅限客户端） SYSCONFDIR 表示在使用 CMake 构建 MySQL 时使用 SYSCONFDIR 选项指定的目录。默认情况下，这是位于编译安装目录下的 etc 目录。 MYSQL_HOME 变量的值是我们自己设置的，也可以不设置。 配置文件的内容 配置文件中的启动选项分为多个组，每个组有一个组名，用中括号 [] 扩起来：\n[server] (具体的启动选项...) [mysqld] (具体的启动选项...) [mysqld_safe] (具体的启动选项...) [client] (具体的启动选项...) [mysql] (具体的启动选项...) [mysqladmin] (具体的启动选项...) 每个组下边可以定义各自的启动选项：\n[server] option1 # option1，该选项不需要选项值 option2 = value2 # option2，该选项需要选项值 ... 配置文件中只能使用长形式的选项。上面的文件转成命令行格式就是 --option1 --option2=value2。\n不同的选项组是给不同的启动命令使用的，如 [mysqld] 和 [mysql] 组分别应用于 mysqld 服务端程序和 mysql 客户端程序。注意：\n[server] 组下边的启动选项将作用于所有的服务器程序。 [client] 组下边的启动选项将作用于所有的客户端程序。 mysqld_safe 和 mysql.server 这两个程序在启动时都会读取 [mysqld] 选项组中的选项。 特定版本的选项组 可以在选项组的名称后加上特定的 MySQL 版本号，比如对于 [mysqld] 选项组来说，我们可以定义一个 [mysqld-5.7] 的选项组，它的含义 和 [mysqld] 一样，只不过只有版本号为 5.7 的 mysqld 程序才能使用这个选项组中的选项。\n配置文件的优先级 如果在多个配置文件中设置了相同的启动选项，那以最后一个配置文件中的为准。文件的顺序按照上面的表格从上到下的顺序。\n同一个配置文件中多个组的优先级 比如 mysqld 可以访问 [mysqld]、[server] 组，如果在同一个配置文件中，比如 ~/.my.cnf，在这些组里出现了同样的配置项，比如这样：\n[server] default-storage-engine=InnoDB [mysqld] default-storage-engine=MyISAM 以最后一组中的启动选项为准，比如上面的文件 default-storage-engine 就以 [mysqld] 组中的配置项为准。\ndefaults-file 如果不想让 MySQL 到默认的路径下搜索配置文件（就是上表中列出的那些），可以在命令行指定 defaults-file 选项 mysqld --defaults-file=/tmp/myconfig.txt。这样，在程序启动的时候将只在 /tmp/myconfig.txt 路径下搜索配置文件。如果文件不存在或无法访问，则会发生错误。\n使用 defaults-extra-file 可以指定额外的配置文件搜索路径。\n命令行和配置文件的优先级 如果同一个启动选项既出现在命令行中，又出现在配置文件中，那么以命令行中的启动选项为准。"},"title":"配置选项"},"/db-learn/docs/mysql/advance/09_character/":{"data":{"":"","mysql-中支持的字符集和排序规则#MySQL 中支持的字符集和排序规则":"字符集的查看 查看当前 MySQL 中支持的字符集 show (character set|charset) [like 匹配的模式];。character set 和 charset 是一个意思。\n比较规则查看 查看 MySQL 中支持的比较规则 show collation [like 匹配的模式];。\n查看 utf8 字符集下的比较规则：\nmysql\u003e show collation like 'utf8\\_%'; +--------------------------+---------+-----+---------+----------+---------+ | Collation | Charset | Id | Default | Compiled | Sortlen | +--------------------------+---------+-----+---------+----------+---------+ | utf8_general_ci | utf8 | 33 | Yes | Yes | 1 | | utf8_bin | utf8 | 83 | | Yes | 1 | | utf8_unicode_ci | utf8 | 192 | | Yes | 8 | | utf8_icelandic_ci | utf8 | 193 | | Yes | 8 | | utf8_latvian_ci | utf8 | 194 | | Yes | 8 | | utf8_romanian_ci | utf8 | 195 | | Yes | 8 | | utf8_slovenian_ci | utf8 | 196 | | Yes | 8 | | utf8_polish_ci | utf8 | 197 | | Yes | 8 | | utf8_estonian_ci | utf8 | 198 | | Yes | 8 | | utf8_spanish_ci | utf8 | 199 | | Yes | 8 | | utf8_swedish_ci | utf8 | 200 | | Yes | 8 | | utf8_turkish_ci | utf8 | 201 | | Yes | 8 | | utf8_czech_ci | utf8 | 202 | | Yes | 8 | | utf8_danish_ci | utf8 | 203 | | Yes | 8 | | utf8_lithuanian_ci | utf8 | 204 | | Yes | 8 | | utf8_slovak_ci | utf8 | 205 | | Yes | 8 | | utf8_spanish2_ci | utf8 | 206 | | Yes | 8 | | utf8_roman_ci | utf8 | 207 | | Yes | 8 | | utf8_persian_ci | utf8 | 208 | | Yes | 8 | | utf8_esperanto_ci | utf8 | 209 | | Yes | 8 | | utf8_hungarian_ci | utf8 | 210 | | Yes | 8 | | utf8_sinhala_ci | utf8 | 211 | | Yes | 8 | | utf8_german2_ci | utf8 | 212 | | Yes | 8 | | utf8_croatian_ci | utf8 | 213 | | Yes | 8 | | utf8_unicode_520_ci | utf8 | 214 | | Yes | 8 | | utf8_vietnamese_ci | utf8 | 215 | | Yes | 8 | | utf8_general_mysql500_ci | utf8 | 223 | | Yes | 1 | +--------------------------+---------+-----+---------+----------+---------+ 27 rows in set (0.00 sec) 比较规则名称都是以 utf8 开头的，后边紧跟着该比较规则主要作用于哪种语言，名称后缀意味着该比较规则是否区分语言中的重音、大小写啥的：\n_ai，accent insensitive，不区分重音 _as，accent sensitive，区分重音 _ci，case insensitive，不区分大小写 _cs，case sensitive，区分大小写 _bin，binary，以二进制方式比较 每种字符集对应多种种比较规则，每种字符集都有一种默认的比较规则（Default 列的值为 YES 的）。","字符集和比较规则的应用#字符集和比较规则的应用":"MySQL 有 4 个级别的字符集和比较规则，分别是：\n服务器级别 数据库级别 表级别 列级别 服务器级别 MySQL 提供了两个系统变量来表示服务器级别的字符集和比较规则：\ncharacter_set_server 服务器级别的字符集。 collation_server 服务器级别的比较规则。 数据库级别 创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下：\ncreate database 数据库名 [[DEFAULT] character set 字符集名称] [[DEFAULT] collate 比较规则名称]; alter database 数据库名 [[DEFAULT] character set 字符集名称] [[DEFAULT] collate 比较规则名称]; DEFAULT 可以省略。\nmysql\u003e CREATE DATABASE charset_demo_db -\u003e CHARACTER SET gb2312 -\u003e COLLATE gb2312_chinese_ci; Query OK, 1 row affected (0.01 sec) 上面语句表示创建一个名叫 charset_demo_db 的数据库，指定它的字符集为 gb2312，比较规则为 gb2312_chinese_ci。\n查看当前数据库使用的字符集和比较规则（先使用 use 语句选择当前数据库）：\nshow variables like 'character_set_database'; character_set_database 当前数据库的字符集。 collation_database 当前数据库的比较规则。 character_set_database 和 collation_database 这两个系统变量是只读的，不能修改。\n如果数据库没有指定字符集和比较规则，那么会使用服务器级别的字符集和比较规则。\n表级别 指定表的字符集和比较规则，语法如下：\ncreate table 表名 (列的信息) [[DEFAULT] character set 字符集名称] [collate 比较规则名称]] alter table 表名 [[DEFAULT] character set 字符集名称] [collate 比较规则名称] 比如创建一个名为 t 的表，并指定这个表的字符集和比较规则：\nmysql\u003e create table t( -\u003e col varchar(10) -\u003e ) character set utf8 collate utf8_general_ci; Query OK, 0 rows affected (0.03 sec) 如果表没有指定字符集和比较规则，那么使用该表所在数据库的字符集和比较规则。\n列级别 同一个表中的不同的列也可以有不同的字符集和比较规则。指定列的字符集和比较规则，语法如下：\ncretae table 表名( 列名 字符串类型 [character set 字符集名称] [collate 比较规则名称], 其他列... ); alter table 表名 modify 列名 字符串类型 [character set 字符集名称] [collate 比较规则名称]; 比如我们修改一下表 t 中列 col 的字符集和比较规则可以这么写：\nmysql\u003e alter table t modify col varchar(10) character set gbk collate gbk_chinese_ci; Query OK, 0 rows affected (0.04 sec) Records: 0 Duplicates: 0 Warnings: 0 如果没有指定该列的字符集和比较规则，那么会使用该列所在表的字符集和比较规则。\n在转换列的字符集时，如果转换前列中存储的数据不能用转换后的字符集进行表示，就会发生错误。如，之前列的字符集是 utf8，并存储了一些汉字，现在把列的字符集转换为 ascii 的话就会出错，因为 ascii 字符集并不能表示汉字字符。\n仅修改字符集或仅修改比较规则 由于字符集和比较规则是互相有联系的，如果只修改了字符集，比较规则也会跟着变化，如果只修改了比较规则，字符集也会跟着变化，具体规则如下：\n只修改字符集，则比较规则将变为修改后的字符集默认的比较规则。 只修改比较规则，则字符集将变为修改后的比较规则对应的字符集。 不论哪个级别的字符集和比较规则，这两条规则都适用。\n客户端和服务器通信中的字符集 字符集转换 字符 '我' 在 utf8 字符集编码下的字节串长这样：0xE68891。\n如果接收 0xE68891 这个字节串的程序按照 utf8 字符集进行解码，然后又把它按照 gbk 字符集进行编码，最后编码后的字节串就是 0xCED2，把这个过程称为字符集的转换，也就是字符串 '我' 从 utf8 字符集转换为 gbk 字符集。\nMySQL 中字符集的转换 从客户端发往服务端的请求本质上就是一个字符串，服务端向客户端返回的结果本质上也是一个字符串，而字符串其实是使用某种字符集编码的二进制数据。这个字符串可不是使用一种字符集的编码方式一条道走到黑的，从发送请求到返回结果这个过程中伴随着多次字符集的转换，在这个过程中会用到 3 个系统变量：\ncharacter_set_client 服务端解码请求时使用的字符集 character_set_connection 服务端处理请求时会把请求字符串从 character_set_client 转为 character_set_connection character_set_results 服务端向客户端返回数据时使用的字符集 这三个系统变量的值可能默认都是 utf8。为了体现出字符集在请求处理过程中的变化，这里特意修改一个系统变量的值：\nmysql\u003e set character_set_connection = gbk; Query OK, 0 rows affected (0.00 sec) 所以现在系统变量 character_set_client 和 character_set_results 的值还是 utf8，而 character_set_connection 的值为 gbk。现在假设客户端发送的请求是下边这个字符串：\nSELECT * FROM t WHERE s = '我'; 分析字符 '我' 在这个过程中字符集的转换。请求从发送到结果返回过程中字符集的变化：\n客户端发送请求所使用的字符集 一般情况下客户端所使用的字符集和当前操作系统一致，不同操作系统使用的字符集可能不一样，如下：\n类 Unix 系统使用的是 utf8 Windows 使用的是 gbk 例如在使用的 linux 操作系统时，客户端使用的就是 utf8 字符集。所以字符 '我' 在发送给服务端的请求中的字节形式就是：0xE68891。\n服务端接收到客户端发送来的请求其实是一串二进制的字节，它会认为这串字节采用的字符集是 character_set_client，然后把这串字节转换为 character_set_connection 字符集编码的字符。由于计算机上 chacharacter_set_client 的值是 utf8，首先会按照 utf8 字符集对字节串 0xE68891 进行解码，得到的字符串就是 '我'，然后按照 character_set_connection 代表的字符集，也就是 gbk 进行编码，得到的结果就是字节串 0xCED2。\n因为表 t 的列 col 采用的是 gbk 字符集，与 character_set_connection 一致，所以直接到列中找字节值为 0xCED2 的记录，最后找到了一条记录。\n如果某个列使用的字符集和 character_set_connection 代表的字符集不一致的话，还需要再进行一次字符集转换。\n上一步骤找到的记录中的 col 列其实是一个字节串 0xCED2，col 列是采用 gbk 进行编码的，所以首先会将这个字节串使用 gbk 进行解码，得到字符串'我'，然后再把这个字符串使用 character_set_results 代表的字符集，也就是 utf8 进行编码，得到了新的字节串：0xE68891，然后发送给客户端。\n由于客户端是用的字符集是 utf8，所以可以顺利的将 0xE68891 解释成字符'我'，从而显示到我们的显示器上。\n几点需要注意的地方：\n假设你的客户端采用的字符集和 character_set_client 不一样的话，这就会出现意想不到的情况。 假设你的客户端采用的字符集和 character_set_results 不一样的话，这就可能会出现客户端无法解码结果集的情况 通常都把 character_set_client、character_set_connection、character_set_results 这三个系统变量设置成和客户端使用的字符集一致的情况，这样减少了很多无谓的字符集转换。\nMySQL 提供了一条非常简便的语句:\nSET NAMES 字符集名; 效果与下面的三条语句的执行效果一样：\nSET character_set_client = 字符集名; SET character_set_connection = 字符集名; SET character_set_results = 字符集名; 也可以写到配置文件里：\n[client] default-character-set=utf8 如果使用的是 Windows 系统，那应该设置成 gbk。\n比较规则的应用 比较规则的作用通常体现在比较字符串大小的表达式以及对某个字符串列进行排序中，所以有时候也称为排序规则。比方说表 t 的列 col 使用的字符集是 gbk，使用的比较规则是 gbk_chinese_ci，我们向里边插入几条记录：\nmysql\u003e INSERT INTO t(col) VALUES('a'), ('b'), ('A'), ('B'); Query OK, 4 rows affected (0.00 sec) Records: 4 Duplicates: 0 Warnings: 0 mysql\u003e 查询的时候按照 col 列排序一下：\nmysql\u003e SELECT * FROM t ORDER BY col; +------+ | col | +------+ | a | | A | | b | | B | | 我 | +------+ 5 rows in set (0.00 sec) 可以看到在默认的比较规则 gbk_chinese_ci 中是不区分大小写的，我们现在把列 col 的比较规则修改为 gbk_bin：\nmysql\u003e ALTER TABLE t MODIFY col VARCHAR(10) COLLATE gbk_bin; Query OK, 5 rows affected (0.02 sec) Records: 5 Duplicates: 0 Warnings: 0 gbk_bin 是直接比较字符的编码，所以是区分大小写的，我们再看一下排序后的查询结果：\nmysql\u003e SELECT * FROM t ORDER BY col; +------+ | s | +------+ | A | | B | | a | | b | | 我 | +------+ 5 rows in set (0.00 sec) mysql\u003e ","字符集和比较规则简介#字符集和比较规则简介":"字符集简介 计算机中只能存储二进制数据，如何存储字符串？当然是建立字符与二进制数据的映射关系，建立这个关系要搞清楚两件事儿：\n要把哪些字符映射成二进制数据？也就是界定清楚字符范围。 怎么映射？将一个字符映射成一个二进制数据的过程也叫做编码，将一个二进制数据映射到一个字符的过程叫做解码。 字符集就是来描述某个字符范围的编码规则。\n比较规则简介 怎么比较两个字符？最容易的就是直接比较这两个字符对应的二进制编码的大小。如，字符 ‘a’ 的编码为 0x01，字符 ‘b’ 的编码为 0x02，所以 ‘a’ 小于 ‘b’。\n二进制比较规则很简单，但有时候并不符合现实需求，比如在有些场景对于英文字符不区分大小写。这时候可以这样指定比较规则：\n将两个大小写不同的字符全都转为大写或者小写。 再比较这两个字符对应的二进制数据。 但是实际生活中的字符不止英文字符一种，比如汉字有几万之多，同一种字符集可以有多种比较规则。\n为什么会有乱码 字符 '我' 在 utf8 字符集编码下的字节串长这样：0xE68891，如果一个程序把这个字节串发送到另一个程序里，另一个程序用不同的字符集去解码这个字节串，假设使用的是 gbk 字符集来解释这串字节，解码过程就是这样的：\n首先看第一个字节 0xE6，它的值大于 0x7F（十进制：127），说明是两字节编码，继续读一字节后是 0xE688，然后从 gbk 编码表中查找字节为 0xE688 对应的字符，发现是字符'鎴' 继续读一个字节 0x91，它的值也大于 0x7F，再往后读一个字节发现木有了，所以这是半个字符。 所以 0xE68891 被 gbk 字符集解释成一个字符 '鎴' 和半个字符。 如果对于同一个字符串编码和解码使用的字符集不一样，就会产生乱码。"},"title":"字符集"},"/db-learn/docs/mysql/advance/10_cost/":{"data":{"":"MySQL 查询优化器决策是否使用某个索引执行查询时的依据是使用该索引的成本是否足够低，而成本很大程度上取决于需要扫描的二级索引记录数量占表中所有记录数量的比例。\n需要扫描的二级索引记录越多，需要执行的回表操作也就越多。如果需要扫描的二级索引记录占全部记录的比例达到某个范围，那优化器就可能选择使用全表扫描的方式执行查询（一个极端的例子就是扫描全部的二级索引记录，那么将对所有的二级索引记录执行回表操作，显然还不如直接全表扫描）。","is-nullis-not-null-到底能不能用索引#IS NULL、IS NOT NULL、!= 到底能不能用索引？":"s1 表的聚簇索引示意图：\nidx_key1 二级索引示意图：\n图中可以看出，值为 NULL 的二级索引记录都被放到了 B+ 树的最左边，InnoDB 的设计：We define the SQL null to be the smallest possible value of a field. ，也就是认为 NULL值是最小的。\nIS NULL 示例：\nSELECT * FROM t WHERE key1 IS NULL; 优化器在真正执行查询前，会首先少量的访问一下索引，调查一下 key1 在 [NULL, NULL] 这个区间的记录有多少条。上面的示意图中可以看出需要扫描的二级索引记录占总记录条数的比例是 3/16。它觉得这个查询使用二级索引来执行比较靠谱，所以会使用这个 idx_key1 索引来执行查询。\nIS NOT NULL SELECT * FROM t WHERE key1 IS NOT NULL; NULL 作为最小值对待，上面的扫描区间就是 (NULL, +∞) 是开区间，也就意味这不包括 NULL 值。需要扫描的二级索引记录占总记录条数的比例是 13/16，跟显然这个比例已经非常大了，所以会使用全表扫描的方式来执行查询。\n现在更新一下数据：\nUPDATE t SET key1 = NULL WHERE key1 \u003c 80; 更新后的 idx_key1 索引示意图：\n再执行查询：\nSELECT * FROM t WHERE key1 IS NOT NULL; 优化器经过调查得知，需要扫描的二级索引记录占总记录条数的比例是 3/16，它觉得这个查询使用二级索引来执行比较靠谱，所以会使用这个 idx_key1 索引来执行查询。\n!= SELECT * FROM t WHERE key1 != 80; 优化器在真正执行查询前，会首先少量的访问一下索引，调查一下 key1 在 (NULL, 80) 和 (80, +∞) 这两个区间内记录有多少条：\n可以看出，需要扫描的二级索引记录占总记录条数的比例是 2/16，它觉得这个查询使用二级索引来执行比较靠谱，所以会使用这个 idx_key1 索引来执行查询。","总结#总结":"成本决定执行计划，跟使用什么查询条件并没有什么关系。优化器会首先针对可能使用到的二级索引划分几个扫描区间，然后分别调查这些区间内有多少条记录，在这些扫描区间内的二级索引记录的总和占总共的记录数量的比例达到某个值时，优化器将放弃使用二级索引执行查询，转而采用全表扫描。","扫描区间和边界条件#扫描区间和边界条件":"示例：\nCREATE TABLE t ( id INT UNSIGNED NOT NULL AUTO_INCREMENT, key1 INT, common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1) ) Engine=InnoDB CHARSET=utf8; -- 插入一些数据 INSERT INTO t VALUES (1, 30, 'b'), (2, 80, 'b'), (3, 23, 'b'), (4, NULL, 'b'), (5, 11, 'b'), (6, 53, 'b'), (7, 63, 'b'), (8, NULL, 'b'), (9, 99, 'b'), (10, 12, 'b'), (11, 66, 'b'), (12, NULL, 'b'), (13, 66, 'b'), (14, 30, 'b'), (15, 11, 'b'), (16, 90, 'b'); SELECT * FROM t WHERE key1 \u003e 20 AND key1 \u003c 50; 如果使用 idx_key1 执行该查询的话，那么就需要扫描 key1 值在 (20, 50) 这个区间中的所有二级索引记录，(20, 50) 就是 idx_key1 执行上述查询时的扫描区间，把 key1 \u003e 20 AND key1 \u003c 50 称作形成该扫描区间的边界条件。\n只要索引列和常数使用 =、\u003c=\u003e、IN、NOT IN、IS NULL、IS NOT NULL、\u003e、\u003c、\u003e=、\u003c=、BETWEEN、!= 或者 LIKE 操作符连接起来，就可以产生所谓的扫描区间。\nIN 操作符的语义和若干个等值匹配操作符 = 之间用 OR 连接起来的语义是一样的，它们都会产生多个单点扫描区间，比如下边这两个语句的语义上的效果是一样的： SELECT * FROM single_table WHERE key1 IN ('a', 'b'); SELECT * FROM single_table WHERE key1 = 'a' OR key1 = 'b'; 优化器会将 IN 子句中的条件看成是 2 个范围区间（虽然这两个区间中都仅仅包含一个值）：\n['a', 'a'] ['b', 'b'] != 产生的扫描区间： SELECT * FROM single_table WHERE key1 != 'a'; 对应的扫描区间就是：(-∞, 'a') 和 ('a', +∞)。\nLIKE 操作符比较特殊，例如 key1 LIKE 'a%' 形成的扫描区间相当于是 ['a', 'b')。 "},"title":"查询成本和扫描区间"},"/db-learn/docs/mysql/advance/11_group_by/":{"data":{"":"准备数据：\nCREATE TABLE student_score ( number INT(11) NOT NULL, name VARCHAR(30) NOT NULL, subject VARCHAR(30) NOT NULL, score TINYINT(4) DEFAULT NULL, PRIMARY KEY (number,subject) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- 插入一些数据后，表中数据如下： mysql\u003e SELECT * FROM student_score; +----------+-----------+-----------------------------+-------+ | number | name | subject | score | +----------+-----------+-----------------------------+-------+ | 20180101 | 杜子腾 | 母猪的产后护理 | 78 | | 20180101 | 杜子腾 | 论萨达姆的战争准备 | 88 | | 20180102 | 杜琦燕 | 母猪的产后护理 | 100 | | 20180102 | 杜琦燕 | 论萨达姆的战争准备 | 98 | | 20180103 | 范统 | 母猪的产后护理 | 59 | | 20180103 | 范统 | 论萨达姆的战争准备 | 61 | | 20180104 | 史珍香 | 母猪的产后护理 | 55 | | 20180104 | 史珍香 | 论萨达姆的战争准备 | 46 | +----------+-----------+-----------------------------+-------+ 8 rows in set (0.00 sec) ","group-by-是在干什么#GROUP BY 是在干什么？":"MySQL 中提供了一系列的聚集函数，例如：\nCOUNT：统计记录数。 MAX：查询某列的最大值。 MIN：查询某列的最小值。 SUM：某列数据的累加总和。 AVG：某列数据的平均数。 如果想查看一下 student_score 表中所有人的平均成绩就可以这么写：\nmysql\u003e SELECT AVG(score) FROM student_score; +------------+ | AVG(score) | +------------+ | 73.1250 | +------------+ 1 row in set (0.00 sec) 如果只想查看 《母猪的产后护理》 这个科目的平均成绩，那加个 WHERE 子句就好了：\nmysql\u003e SELECT AVG(score) FROM student_score WHERE subject = '母猪的产后护理'; +------------+ | AVG(score) | +------------+ | 73.0000 | +------------+ 1 row in set (0.00 sec) 那么如果这个 student_score 表中存储了 20 门科目的成绩信息，那如何得到这 20 门课程的平均成绩呢？单独写 20 个查询语句？\n可以按照某个列将表中的数据进行分组，比方说现在按照 subject 列对表中数据进行分组，那么所有的记录就会被分成 2 组：\n# 母猪的产后护理 组 +----------+-----------+-----------------------------+-------+ | number | name | subject | score | +----------+-----------+-----------------------------+-------+ | 20180101 | 杜子腾 | 母猪的产后护理 | 78 | | 20180102 | 杜琦燕 | 母猪的产后护理 | 100 | | 20180103 | 范统 | 母猪的产后护理 | 59 | | 20180104 | 史珍香 | 母猪的产后护理 | 55 | +----------+-----------+-----------------------------+-------+ # 论萨达姆的战争准备 组 +----------+-----------+-----------------------------+-------+ | number | name | subject | score | +----------+-----------+-----------------------------+-------+ | 20180101 | 杜子腾 | 论萨达姆的战争准备 | 88 | | 20180102 | 杜琦燕 | 论萨达姆的战争准备 | 98 | | 20180103 | 范统 | 论萨达姆的战争准备 | 61 | | 20180104 | 史珍香 | 论萨达姆的战争准备 | 46 | +----------+-----------+-----------------------------+-------+ 分组的子句就是 GROUP BY：\nmysql\u003e SELECT subject, AVG(score) FROM student_score GROUP BY subject; +-----------------------------+------------+ | subject | AVG(score) | +-----------------------------+------------+ | 母猪的产后护理 | 73.0000 | | 论萨达姆的战争准备 | 73.2500 | +-----------------------------+------------+ 2 rows in set (0.00 sec) ","报错#报错":" mysql\u003e SELECT subject, name, AVG(score) FROM student_score GROUP BY subject; ERROR 1055 (42000): Expression #2 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.student_score.name' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by mysql\u003e 为什么会报错？使用 GROUP BY 子句是想把记录分为若干组，然后再对各个组分别调用聚集函数去做一些统计工作。\n本例 SQL 的查询列表中有一个 name 列，既不是分组的列，也不是聚集函数的列。\n那么从各个分组中的记录中取一个记录的 name 列？该取哪条记录为好呢？比方说对于'母猪的产后护理'这个分组中的记录来说，name 列的值应该取 杜子腾，还是杜琦燕，还是范统，还是 史珍香 呢？这个不知道，所以把非分组列放到查询列表中会引起争议，导致结果不确定，所以 MySQL 才会报错。\n如果一定要把非分组列也放到查询列表中，可以修改 sql_mode 的系统变量：\nmysql\u003e SHOW VARIABLES LIKE 'sql_mode'; +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | Variable_name | Value | +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | sql_mode | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION | +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.02 sec) 其中一个称之为 ONLY_FULL_GROUP_BY 的值，把这个值从 sql_mode 系统变量中移除，就不会报错了：\nmysql\u003e set sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; Query OK, 0 rows affected (0.00 sec) "},"title":"Group By"},"/db-learn/docs/mysql/guide/":{"data":{"mysql-的安装目录#MySQL 的安装目录":"Linux 下的安装目录一般为 /usr/local/mysql/。Windows 一般为 C:\\Program Files\\MySQL\\MySQL Server x.x （记住你自己的安装目录）。\nbin 目录 打开 /usr/local/mysql/bin，执行 tree：\n. ├── mysql ├── mysql.server -\u003e ../support-files/mysql.server ├── mysqladmin ├── mysqlbinlog ├── mysqlcheck ├── mysqld # mysql 的服务端程序 ├── mysqld_multi # 运行多个 MySQL 服务器进程 ├── mysqld_safe ├── mysqldump ├── mysqlimport ├── mysqlpump ... (省略其他文件) 0 directories, 40 files 这个目录一般需要配置到环境变量的 PATH 中，Linux 中各个路径以 : 分隔。也可以选择不配：\n/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/mysql/bin mysqld mysqld 是 MySQL 服务端程序。不常用。\nmysqld_safe mysqld_safe 是一个启动脚本，最终也是调用 mysqld，但是还会另外一个监控进程，在服务器进程挂了的时候，可以帮助重启服务器进程。 mysqld_safe 启动服务端程序时，会将服务端程序的出错信息和其他诊断信息重定向到某个文件中，产生出错日志，这样可以方便找出发生错误的原因。\nmysql.server mysql.server 也是一个启动脚本，会间接的调用 mysqld_safe，使用 mysql.server 启动服务端程序：\nmysql.server start mysql.server 文件其实是一个链接文件，它的实际文件是 ../support-files/mysql.server。\nclient bin 目录下的 mysql、mysqladmin、mysqldump、mysqlcheck 都是客户端程序，启动客户端程序连接服务器进程：\nmysql --host=\u003c主机名\u003e --user=\u003c用户名\u003e --password=\u003c密码\u003e 注意：\n最好不要在一行命令中输入密码，可能会导致密码泄露。 如果使用的是类 UNIX 系统，并且省略 -u 参数后，会把你登陆操作系统的用户名当作 MySQL 的用户名去处理。 ","安装-mysql#安装 MySQL":"安装 MySQLMySQL 的大部分安装包都包含了服务器程序和客户端程序\n注意，在 Linux 下使用 RPM 安装，有时需要分别安装服务器 RPM 包和客户端 RPM 包。","连接服务器#连接服务器":" mysql -hlocalhost -uroot -p123456 注意，不推荐使用 -p 输入密码。-p 和密码值之间不能有空白字符（其他参数名之间可以有空白字符）。 如果服务端和客户端安装在同一台机器上，-h 参数可以省略。\n退出：quit，exit，\\q 任意指令都可以关闭连接。\nWindows 下可以使用 mysql shell：\n\\sql \\connect --mysql root@localhost "},"title":"使用指南"},"/db-learn/docs/mysql/guide/01_type/":{"data":{"":"","json-数据类型#JSON 数据类型":"JSON（JavaScript Object Notation）主要用于互联网应用服务之间的数据交换。JSON 类型是从 MySQL 5.7 版本开始支持的功能，而 8.0 版本解决了更新 JSON 的日志性能瓶颈。如果要在生产环境中使用 JSON 数据类型，推荐使用 MySQL 8.0 版本。","二进制类型#二进制类型":"二进制类型存储二进制字符串，与字符集无关。\nBIT(M) BIT(M) 是用来存放位的类型，M 表示该类型最多可以存放的位的个数，取值范围是 1~64。默认值为 1，也就是说 BIT(1) 和 BIT 是一样的。\nMySQL 是以字节为单位存储数据的，一个字节拥有 8 个比特位。如果存储的比特位个数不足整数个字节，那么 MySQL 会将它填充满，例如：\nBIT(1) 类型仅仅需要存储 1 个比特位的数据，但是 MySQL 会为其申请 (1+7)/8 = 1 个字节。 BIT(5) 类型仅仅需要存储 5 个比特位的数据，但是 MySQL 会为其申请 (5+7)/8 = 1 个字节。 BIT(9) 类型仅仅需要存储 9 个比特位的数据，但是 MySQL 会为其申请 (9+7)/8 = 2 个字节。 BINARY(M) 与 VARBINARY(M) BINARY(M) 和 VARBINARY(M) 对应于 CHAR(M) 和 VARCHAR(M)，都是前者是固定长度的类型，后者是可变长度的类型，只不过 BINARY(M) 和 VARBINARY(M) 是用来存放字节的，M 表示最多能存放的字节数。\n其他的二进制类型 TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB 是针对数据量很大的二进制数据提出的，比如图片、音频、压缩文件。很像 TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT，不过各种 BLOB 类型是用来存储字节的，而各种 TEXT 类型是用来存储字符的。\nℹ️ 对于比较大的二进制数据，比方说图片、音频，通常情况下都不直接存储到数据库中，而是将它们保存到文件系统中，然后在数据库中之存放一个文件路径即可。 ","字符串#字符串":" 类型 最大长度 含义 CHAR(M) M 个字符 固定长度的字符串 VARCHAR(M) M 个字符 可变长度的字符串 TINYTEXT 2⁸-1 个字节 非常小的字符串 TEXT 2¹⁶-1 个字节 小型的字符串 MEDIUMTEXT 2²⁴-1 个字节 中等大小的字符串 LONGTEXT 2³²-1 个字节 大型的字符串 其中最常使用的是 CHAR、VARCHAR。\nCHAR(M) CHAR(M) 中的 M 代表该类型最多可以存储的字符数量，注意，表示的是字符的个数，而不是字节的个数。其中 M 的取值范围是 0~255。默认值就是 1。也就是说 CHAR 就是 CHAR(1)。\nCHAR(0) 是一种特别的类型，它只能存储空字符串 '' 或者 NULL 值。\nCHAR(M) 在不同的字符集下需要的存储空间也是不一样的。当列采用的是定长字符集时，该列占用的字节数不会被加到变长字段长度列表，而如果采用变长字符集时，该列占用的字节数也会被加到变长字段长度列表。\n例如：\nascii 字符集的 CHAR(5)，一个字符最多需要 1 个字节，也就是 M=5、W=1，所以该类型占用的存储空间大小就是 5×1 = 5 个字节。 utf8mb3 字符集的 CHAR(5)，一个字符需要 1~3 个字节，所以该类型占用的存储空间大小至少需要就是 5×1 = 5 个字节，最多 5×3 = 15 个字节。即使向该列中存储一个空字符串也会占用 5 个字节。 如果实际存储的字符串在特定字符集编码下占用的字节数不足 M×W，那么剩余的那些存储空间用空格字符（也就是：' '）补齐。\n例如：\nascii 字符集的 CHAR(5) 类型，存入字符串 'abc'，需要 3 个字节存储，而采用 ascii 字符集的 CHAR(5) 类型又需要 5 个字节的存储空间，那么剩下的那两个字节的存储空间就会存储空格字符 ' ' 的编码。 ℹ️ 一旦确定了 CHAR(M) 类型的 M 的值，如果 M 的值很大，而实际存储的字符串占用字节数又很少，会造成存储空间的浪费。 ℹ️ 对于使用变长字符集的 CHAR(M) 类型来说，它的存储机制其实是和 VARCHAR(M) 是一样的，都是真正的字符串内容+字符串内容占用字节数。这个时候直接使用 VARCHAR(M) 类型就可以了。 VARCHAR(M) 低于长短不一的字符串，使用 CHAR(M) 可能会浪费很多存储空间，VARCHAR(M) 正是为了解决这个问题而生的。\nVARCHAR(M) 中的 M 也是代表是字符的个数，而不是字节的个数，理论上的取值范围是 1~65535。但是 MySQL 中还有一个规定，表中某一行包含的所有列中存储的数据大小总共不得超过 65535 个字节（注意是字节），也就是说 VARCHAR(M) 类型实际能够容纳的字符数量肯定是小于 65535 的。\n一个 VARCHAR(M) 类型表示的数据由两部分组成：\n真正的字符串内容，长度是 L 个字节。 字符串内容占用字节数，长度是 1~2 个字节。 假设 VARCHAR(M) 类型采用的字符集编码一个字符最多需要 W 个字节，那么： 当 M×W \u003c 256 时，只需要一个字节来表示占用的字节数。 当 M×W \u003e= 256 且 M×W \u003c 65536 时，需要两个字节来表示占用的字节数。 所以，VARCHAR(M) 类型占用的存储空间大小就是 L+1~2 个字节。\n例如：\nutf8mb3 字符集的 VARCHAR(5)，也就是说 M = 5、W = 3，所以 M × W= 5×3 = 15，而 15 \u003c 256，所以只需要一个字节来表示真实数据占用的字节长度就好了。 utf8mb3 字符集的 VARCHAR(100)，也就是说 M = 100、W = 3，所以 M × W= 100×3 = 300，而 300 \u003e 256，所以需要 2 个字节来表示真实数据占用的字节长度。 如图：\n大字符串的类型 对于很长的字符串，如果 VARCHAR(M) 如果还是不够用，可以使用 TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT 这四种可以存储大型的字符串的类型。\n它们也都是变长类型，这些类型占用的存储空间由实际内容和内容占用的字节长度两部分构成。\nTINYTEXT 最多可以存储 2⁸-1 个字节，所以内容占用的字节长度用 1 个字节（L+1）就可以表示。 TEXT 最多可以存储 2¹⁶-1 个字节，所以内容占用的字节长度用 2 个字节（L+2）就可以表示。 MEDIUMTEXT 最多可以存储 2²⁴-1 个字节，所以内容占用的字节长度用 3 个字节（L+3）就可以表示。 LONGTEXT 最多可以存储 2³²-1 个字节，所以内容占用的字节长度用 4 个字节（L+4）就可以表示。 ENUM 和 SET 类型 ENUM 枚举类型，是一种字符串类型。\n使用：\nENUM('str1', 'str2', 'str3' ⋯) 它表示在给定的字符串列表里选择一个。例如性别一列可以定义成 ENUM('男', '女') 类型。这个的意思就是性别一列只能在 ‘男’ 或者 ‘女’ 这两个字符串之间选择一个。相当于一个单选框。\nENUM 只能在给定的字符串列表里选择一个，并且只能选择一个，不能选择多个。如果要选择多个，可以使用 SET 类型。\nSET 集合类型，也是一种字符串类型。\n使用：\nSET('str1','str2','str3' ⋯) 它表示在给定的字符串列表里选择多个。例如兴趣一列就可以定义成 SET('打球', '画画', '扯犊子', '玩游戏') 类型。这个的意思就是兴趣一列可以在给定的这几个字符串中选择一个或多个，相当于一个多选框。","数值#数值":"整数类型 MySQL 支持的整型类型：\nSQL 标准类型：INT、SMALLINT。 TINYINT、MEDIUMINT、BIGINT。 类型 占用的存储空间（单位：字节） 无符号数取值范围 有符号数取值范围 含义 TINYINT 1 0 ~ 2⁸-1 -2⁷ ~ 2⁷-1 非常小的整数 SMALLINT 2 0 ~ 2¹⁶-1 -2¹⁵ ~ 2¹⁵-1 小的整数 MEDIUMINT 3 0 ~ 2²⁴-1 -2²³ ~ 2²³-1 中等大小的整数 INT（别名：INTEGER） 4 0 ~ 2³²-1 -2³¹ ~ 2³¹-1 标准的整数 BIGINT 8 0 ~ 2⁶⁴-1 -2⁶³ ~ 2⁶³-1 大整数 避免使用整数的显示宽度。也就是说，不要用 INT(10) 类似的方法指定字段显示宽度，直接用 INT。\n显示宽度 创建数据表时整数类型可以指定一个长度，如下：\nCREATE TABLE `user`( `id` TINYINT(2) UNSIGNED ); 这里表示 user 表的 id 字段的类型是 TINYINT，可以存储的最大数值是 255。这里的 TINYINT(2) 中的 2 并不是类型存储的最大长度，而是显示的最大长度。比如，存入 200，虽然超过 2 位，但是没有超出 TINYINT 类型的最大数值 255，所以可以正常保存；如果存入值大于 255，如 500，那么 MySQL 会自动保存为 TINYINT 类型的最大值 255。\n显示宽度的作用：\n当需要在查询结果前填充 0 时，定义类型时加上 ZEROFILL 就可以实现，如：\n`id` TINYINT(2) UNSIGNED ZEROFILL 这样，查询结果如果是 5，那输出就是 05。如果指定 TINYINT(5)，那输出就是 00005，其实实际存储的值还是 5，而且存储的数据不会超过 255，只是 MySQL 输出数据时在前面填充了 0。\n浮点数 Float、Double：这两个类型不是高精度，也不是 SQL 标准的类型，并且在 8.0 之后的版本中将会废弃。所以在真实的生产环境中不推荐使用。 使用：\nFLOAT(M, D) DOUBLE(M, D) M 表示该小数最多需要的十进制有效数字个数。注意这个有效数字个数，例如小数 -2.3 来说有效数字个数就是 2，对于小数 0.9 来说有效数字个数就是 1。 D 表示该小数的小数点后的十进制数字个数。 示例：\n类型 取值范围 FLOAT(4, 1) -999.9 ~ 999.9 FLOAT(5, 1) -9999.9 ~ 9999.9 FLOAT(6, 1) -99999.9 ~ 99999.9 FLOAT(4, 0) -9999 ~ 9999 FLOAT(4, 1) -999.9 ~ 999.9 FLOAT(4, 2) -99.99 ~ 99.99 M 的取值范围是 1~255，D 的取值范围是 0~30，D 不能大于 M。 Float 占用 4 字节，DOUBLE 占用 8 字节。它们占用的存储空间大小并不随着 M 和 D 的值的变动而变动。 为什么用浮点数表示小数可能会有不精确的情况？ 这是由浮点数的二进制存储机制和 IEEE 754 标准的特性决定的。\n浮点数是用来表示小数的，十进制小数也可以被转换成二进制后被计算机存储。例如 9.875 的二进制就是：9.875 = 8 + 1 + 0.5 + 0.25 + 0.125 = 1 × 2³ + 1 × 2⁰ + 1 × 2⁻¹ + 1 × 2⁻² + 1 × 2⁻³ = 1001.111。\n但是更多的小数是无法直接转换成二进制的，比如说 0.3，它转换成的二进制小数就是一个无限小数（二进制表示 0.0100110011001...，其中 1001 是循环），但是现在只能用 4 个字节或者 8 个字节来表示这个小数，所以只能进行一些舍入来近似的表示，所以说计算机的浮点数表示有时是不精确的。\n高精度类型 为了保证小数是精确的，MySQL 提供了高精度类型：\nDecimal 使用：\nDECIMAL(M, D) M, D 的含义和浮点数类型的 M, D 含义一样。 M, D 都是可选的，M 的默认值是 10，D 的默认值是 0。 M 的范围是 1~65，D 的范围是 0~30，D 的值不能超过 M。 DECIMAL 类型占用的存储空间大小就和 M、D 的取值有关。 DECIMAL 的存储方式和浮点数不一样，它是把小数点左右的两个十进制整数给存储起来，然后将它们组合起来。\nDECIMAL 的存储不是简单的分为左右两个部分，而是从小数点位置出发，每 9 个十进制数字划分为 1 组，将每个组中的十进制数字，将其转换为二进制数字进行存储。根据组中包含的十进制数字位数不同，所需的存储空间大小也不同（变长类型）：\n组中包含的十进制位数 占用存储空间大小（单位：字节） 1 或 2 1 3 或 4 2 5 或 6 3 7 或 8 或 9 4 例如：\nDECIMAL(18,9) 表示的整数部分是 9 个，小数部分是 9 个，那么就可以划分为 2 组，每组 9 个占 4 字节，4+4=8 字节。 DECIMAL(20,6) 表示的整数部分是 14 个，小数部分是 6 个，那么就可以划分为 3 组，每组分别是 5、9、6 个数字，分别占用 3、4、3 个字节，3+4+3=10 字节。 无符号数值类型 无符号数就是非负数。就是在原数值类型后加一个 UNSIGNED。","日期和时间#日期和时间":"MySQL 支持的日期和时间类型：\n类型 存储空间要求 取值范围 含义 YEAR 1 字节 1901~2155 年份值 DATE 3 字节 ‘1000-01-01’ ~ ‘9999-12-31’ 日期值 TIME 3 字节 ‘-838:59:59’ ~ ‘838:59:59’ 时间值 DATETIME 8 字节 ‘1000-01-01 00:00:00’～‘9999-12-31 23:59:59’ 日期加时间值 TIMESTAMP 4 字节 ‘1970-01-01 00:00:01’～‘2038-01-19 03:14:07’ 时间戳 最常使用的日期类型为 DATETIME 和 TIMESTAMP，因为大部分业务场景都需要将日期精确到秒。\n5.6.4 版本以后 TIME、DATETIME、TIMESTAMP 这几种类型添加了对毫秒、微秒的支持。由于毫秒、微秒都不到 1 秒，所以也被称为小数秒，MySQL 最多支持 6 位小数秒的精度。\n使用：\n# 小数秒位数可以在 0、1、2、3、4、5、6 中选择 类型(小数秒位数) 例如：\n-- 表示精确到秒 DATETIME(0) -- 表示精确到毫秒 DATETIME(3) -- 表示精确到 10 微秒 DATETIME(5) 保留小数秒，需要额外的存储空间。不同位数的小数秒，占用的存储空间大小是不一样的。\n保留的小数秒位数 额外需要的存储空间 0 0 字节 1 或 2 1 字节 3 或 4 2 字节 5 或 6 3 字节 也就是说如果选择使用 DATETIME(1)，那么需要的存储空间就是在 DATETIME 的空间上再加上小数秒需要的空间，就是 8 + 1 = 9 个字节。"},"title":"数据类型"},"/db-learn/docs/mysql/guide/02_database/":{"data":{"":"","切换数据库#切换数据库":"USE 数据库名称;：\nmysql\u003e USE test; Database changed mysql\u003e ","创建#创建":"CREATE DATABASE 数据库名;：\nmysql\u003e CREATE DATABASE test; Query OK, 1 row affected (0.00 sec) mysql\u003e IF NOT EXISTS 使用 CREATE DATABASE 去创建一个已经存在数据库会产生错误，可以使用 IF NOT EXISTS 来避免错误。\nmysql\u003e CREATE DATABASE IF NOT EXISTS test; Query OK, 0 rows affected (0.00 sec) mysql\u003e 这个命令的意思是如果指定的数据库不存在的话就创建它，否则就什么也不做。","删除#删除":"DROP DATABASE 数据库名;：\nmysql\u003e DROP DATABASE test; Query OK, 0 rows affected (0.00 sec) mysql\u003e IF EXISTS 使用 DROP DATABASE 去删除一个不存在的数据库会产生错误，可以使用 IF EXISTS 来避免错误。\nmysql\u003e DROP DATABASE IF EXISTS test; Query OK, 0 rows affected (0.00 sec) mysql\u003e 这个命令的意思是如果指定的数据库存在的话就删除它，否则就什么也不做。","查看数据库#查看数据库":" mysql\u003e SHOW DATABASES; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.01 sec) mysql\u003e "},"title":"数据库操作"},"/db-learn/docs/mysql/guide/03_table/":{"data":{"":"","修改#修改":"修改表名 ALTER TABLE 旧表名 RENAME TO 新表名; -- 或者 RENAME TABLE 旧表名1 TO 新表名1, 旧表名2 TO 新表名2, ... 旧表名n TO 新表名n; 如果在修改表名的时候指定了数据库名，还可以将该表转移到对应的数据库下：\nALTER TABLE test.first_table RENAME TO demo.first_table; 增加列 基本语法：\nALTER TABLE 表名 ADD COLUMN 列名 数据类型 [列的属性]; 示例：\nmysql\u003e ALTER TABLE first_table ADD COLUMN third_column CHAR(4) ; Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e SHOW CREATE TABLE first_table\\G *************************** 1. row *************************** Table: first_table Create Table: CREATE TABLE `first_table` ( `first_column` int(11) DEFAULT NULL, `second_column` varchar(100) DEFAULT NULL, `third_column` char(4) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='第一个表' 1 row in set (0.01 sec) 增加列到特定位置 默认的情况下列都是加到现有列的最后一列后面，但是也可以在添加列的时候指定它的位置：\n-- 添加到第一列 ALTER TABLE 表名 ADD COLUMN 列名 列的类型 [列的属性] FIRST; -- 添加到指定列的后边 ALTER TABLE 表名 ADD COLUMN 列名 列的类型 [列的属性] AFTER 指定列名; 删除列 ALTER TABLE 表名 DROP COLUMN 列名; 修改列 ALTER TABLE 表名 MODIFY 列名 新数据类型 [新属性]; -- 或者 -- 这种方式可以再修改数据类型和属性的同时，也可以修改列名。 ALTER TABLE 表名 CHANGE 旧列名 新列名 新数据类型 [新属性]; 示例：\nmysql\u003e ALTER TABLE first_table MODIFY second_column VARCHAR(2); Query OK, 0 rows affected (0.04 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e SHOW CREATE TABLE first_table\\G *************************** 1. row *************************** Table: first_table Create Table: CREATE TABLE `first_table` ( `first_column` int(11) DEFAULT NULL, `second_column` varchar(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='第一个表' 1 row in set (0.00 sec) mysql\u003e ALTER TABLE first_table CHANGE second_column second_column1 VARCHAR(2)\\G Query OK, 0 rows affected (0.04 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e SHOW CREATE TABLE first_table\\G *************************** 1. row *************************** Table: first_table Create Table: CREATE TABLE `first_table` ( `first_column` int(11) DEFAULT NULL, `second_column1` varchar(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='第一个表' 1 row in set (0.00 sec) 修改列顺序 -- 将列设为表的第一列 ALTER TABLE 表名 MODIFY 列名 列的类型 列的属性 FIRST; -- 将列放到指定列的后边 ALTER TABLE 表名 MODIFY 列名 列的类型 列的属性 AFTER 指定列名; 同时执行多个操作 ALTER TABLE 表名 操作1, 操作2, ..., 操作n; 示例：\nALTER TABLE first_table DROP COLUMN third_column, DROP COLUMN fourth_column, DROP COLUMN fifth_column; 用一条语句删除了 third_column、fourth_column 和 fifth_column 这三个列。\n修改表的存储引擎 ALTER TABLE 表名 ENGINE=新的存储引擎; 创建和删除索引 添加索引：\nALTER TABLE 表名 ADD [INDEX|KEY] 索引名 (需要被索引的单个列或多个列); 示例：\nALTER TABLE index_demo ADD INDEX idx_c2_c3 (c2, c3); 在创建表的时候就添加索引，例如：\nCREATE TABLE index_demo( c1 INT, c2 INT, c3 CHAR(1), PRIMARY KEY(c1), INDEX idx_c2_c3 (c2, c3) ); 删除索引：\nALTER TABLE 表名 DROP [INDEX|KEY] 索引名; 示例：\nALTER TABLE index_demo DROP INDEX idx_c2_c3; ","创建#创建":"基本语法：\nCREATE TABLE 表名 ( 列名1 数据类型 [列的属性], 列名2 数据类型 [列的属性], ... 列名n 数据类型 [列的属性] ); 列的属性 用中括号 [] 引起来表示是可选的。\n示例：\nCREATE TABLE first_table ( first_column INT, second_column VARCHAR(100) ); 添加表注释 CREATE TABLE 表名 ( 各个列的信息 ... ) COMMENT '表的注释信息'; IF NOT EXISTS 和重复创建数据库一样，创建表时也可以使用 IF NOT EXISTS 关键字，这样如果表不存在，则创建表；如果表已经存在，则不执行创建表的操作。\nCREATE TABLE IF NOT EXISTS first_table ( first_column INT, second_column VARCHAR(100) ); ","删除#删除":"基本语法：\nDROP TABLE 表1, 表2, ..., 表n; 示例：\nDROP TABLE first_table; IF EXISTS 和重复删除数据库一样，删除表时也可以使用 IF EXISTS 关键字，这样如果表存在，则删除表；如果表不存在，则不执行删除表的操作。\nDROP TABLE IF EXISTS first_table; ","操作其他库的表#操作其他库的表":"例如当前所在的库是 test，如果不想使用 use 语句切换到其他库，就必须显式的指定这些表所属的数据库（数据库名.表名）。\nmysql\u003e SHOW TABLES FROM demo; +---------------------+ | Tables_in_demo | +---------------------+ | first_table | | student_info | | student_score | +---------------------+ 3 rows in set (0.00 sec) mysql\u003e SHOW CREATE TABLE test.first_table\\G ","查看表#查看表":" mysql\u003e SHOW TABLES; Empty set (0.00 sec) 查看表结构 下面的语句效果是一样的，可以用来查看定义的表的结构：\nDESCRIBE 表名; DESC 表名; EXPLAIN 表名; SHOW COLUMNS FROM 表名; SHOW FIELDS FROM 表名; 示例：\nmysql\u003e DESC student_info; +-----------------+-------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-----------------+-------------------+------+-----+---------+-------+ | number | int(11) | YES | | NULL | | | name | varchar(5) | YES | | NULL | | | sex | enum('男','女') | YES | | NULL | | | id_number | char(18) | YES | | NULL | | | department | varchar(30) | YES | | NULL | | | major | varchar(30) | YES | | NULL | | | enrollment_time | date | YES | | NULL | | +-----------------+-------------------+------+-----+---------+-------+ 7 rows in set (0.00 sec) 查看表的创建语句 SHOW CREATE TABLE 表名; 示例：\nmysql\u003e SHOW CREATE TABLE student_info; +--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | student_info | CREATE TABLE `student_info` ( `number` int(11) DEFAULT NULL, `name` varchar(5) DEFAULT NULL, `sex` enum('男','女') DEFAULT NULL, `id_number` char(18) DEFAULT NULL, `department` varchar(30) DEFAULT NULL, `major` varchar(30) DEFAULT NULL, `enrollment_time` date DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='学生基本信息表' | +--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) ℹ️ 如果输出数据太长，显示效果不好，可以把标记语句结束的分号 ; 改为 \\G，以垂直的方式展示每一列数据。 "},"title":"表操作"},"/db-learn/docs/mysql/guide/04_column/":{"data":{"":"","auto_increment#AUTO_INCREMENT":"AUTO_INCREMENT 自增属性,如果一个表中的某个列的数据类型是整数类型或者浮点数类型，就可以设置 AUTO_INCREMENT 属性。\n设置了 AUTO_INCREMENT 属性之后，如果在插入新记录的时候不指定该列的值，那么新插入的记录的该列的值就会自动递增。\nmysql\u003e CREATE TABLE first_table ( -\u003e id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY, -\u003e first_column INT, -\u003e second_column VARCHAR(100) DEFAULT 'abc' -\u003e ); Query OK, 0 rows affected (0.01 sec) ","not-null#NOT NULL":"如果列中必须有值，那么可以在创建列时使用 NOT NULL 关键字，这样在插入记录时，如果该列没有值，那么就会报错：\n列名 列的类型 NOT NULL 示例：\nmysql\u003e CREATE TABLE first_table ( -\u003e first_column INT NOT NULL, -\u003e second_column VARCHAR(100) DEFAULT 'abc' -\u003e ); Query OK, 0 rows affected (0.02 sec) ","unique#UNIQUE":"一个表中可以有多个 UNIQUE 约束，UNIQUE 约束的值不能重复，但是可以为 NULL。\n单个列声明 UNIQUE 属性：\nCREATE TABLE student_info ( number INT PRIMARY KEY, name VARCHAR(5), sex ENUM('男', '女'), id_number CHAR(18) UNIQUE, department VARCHAR(30), major VARCHAR(30), enrollment_time DATE ); 也可以把 UNIQUE 属性的声明单独提取出来：\nCREATE TABLE student_info ( number INT PRIMARY KEY, name VARCHAR(5), sex ENUM('男', '女'), id_number CHAR(18), department VARCHAR(30), major VARCHAR(30), enrollment_time DATE, UNIQUE KEY uk_id_number (id_number) ); UNIQUE [约束名称] (列名1, 列名2, ...) 中的 UNIQUE 也可以使用 UNIQUE KEY 关键字来声明。\n对于多个列的组合具有 UNIQUE 属性的情况，必须使用这种单独声明的形式。","zerofill#ZEROFILL":"ZEROFILL 属性配合显示宽度，可以实现数字的左边补 0。","主键#主键":"一个表最多只能有一个主键，主键的值不能重复，并且不能为 NULL，通过主键可以找到唯一的一条记录。如果一个列被指定为主键，那么在插入记录时，如果该列没有值，那么就会报错。\n主键可以是一个列，也可以是多个列的组合。\n如果主键只是单个列的话，可以直接在该列后声明 PRIMARY KEY：\nCREATE TABLE student_info ( number INT PRIMARY KEY, name VARCHAR(5), sex ENUM('男', '女'), id_number CHAR(18), department VARCHAR(30), major VARCHAR(30), enrollment_time DATE ); 也可以把主键的声明单独提取出来：\nCREATE TABLE student_info ( number INT, name VARCHAR(5), sex ENUM('男', '女'), id_number CHAR(18), department VARCHAR(30), major VARCHAR(30), enrollment_time DATE, PRIMARY KEY (number) ); 对于多个列的组合作为主键的情况，必须使用这种单独声明的形式：\nCREATE TABLE student_score ( number INT, subject VARCHAR(30), score TINYINT, PRIMARY KEY (number, subject) ); ","注释#注释":"每一个列末尾可以添加 COMMENT 语句来为列来添加注释：\nCREATE TABLE first_table ( id int UNSIGNED AUTO_INCREMENT PRIMARY KEY COMMENT '自增主键', first_column INT COMMENT '第一列', second_column VARCHAR(100) DEFAULT 'abc' COMMENT '第二列' ) COMMENT '第一个表'; ","默认值#默认值":"一个列如果没有显示的指定默认值，那么在插入记录时，该列的值就是 NULL。如果要指定默认值，那么可以使用 DEFAULT 关键字，语法如下：\n列名 列的类型 DEFAULT 默认值 示例：\nmysql\u003e CREATE TABLE first_table ( -\u003e first_column INT, -\u003e second_column VARCHAR(100) DEFAULT 'abc' -\u003e ); Query OK, 0 rows affected (0.02 sec) "},"title":"列"},"/db-learn/docs/mysql/guide/05_query/":{"data":{"":"","select#\u003ccode\u003eselect\u003c/code\u003e":"SQL 语句要以 ; 结尾。不区分大小写。\nuse 选择数据库 show show databases;，查看数据库列表。 show tables;，查看数据库中的表。 show colums，显示某个表中的列，比如 show colums from users。也可以使用 describe users，效果一样。 show status，用于显示广泛的服务器状态信息。 show create database 和 show create table，分别用来显示创建特定数据库或表的语句。 show grants，用来显示授予用户（所有用户或特定用户）的安全权限。 show erros 和 show warnings，用来显示服务器错误或警告消息。 help show; 查看 show 的用法 select 为了使用 select 查询语句，如 select name from users;，找出 users 表中的所有 name 列。\n查询多个列：\nselect name, age, phone from users; 查询所有列，使用星号 *\ndistinct distinct 去重。\nselect distinct name from users; 上面的示例，会对 name 去重，返回 name 不同的行\ndistinct 关键字，会对其后面的所有列去重。如 select distinct vend_id, prod_price from products，vend_id 和 prod_price 列。\nlimit 使用 limit 子句，限制返回的行数。\nselect name from users limit 5; 上面的语句最多返回 5 行。\nselect name from users limit 5,5; 上面的语句的意思是从第 5 行开始，最多返回 5 行。\nMySQL 5 支持 limit 的另一种替代语法。limit 4 offset 3 表示从第 3 行开始取 4 行，和 limit 3, 4 一样。\n限定的表名和列名 select users.name from demo.users; 上面的语句和 select name from users 没什么区别。但是有一些情形需要限定名。比如 join 多个表时，都包含同样的列名，就可以使用限定表名。\norder by 使用 order by 子句对输出进行排序。比如 select name from users order by age，按照 age 排序。\n按多个列排序 select name from users order by age, weight; 会先按照 age 排序，如果有多行有相同的 age，再按照 weight 排序。\n排序方向 查询默认是升序排序（asc），如果要进行降序排序，使用 desc 关键字。\nselect name, age from users order by age desc; 结果会是\n+--------+------+ | name | age | +--------+------+ | ming | 18 | | qiang | 17 | | liang | 16 | | long | 16 | +--------+------+ 多个列降序排序：\nselect name from users order by age desc, weight; 上面的示例，对 age 列降序排序，weight 还是默认的升序排序。\ndesc 关键字只会作用到其前面的列。\n如果想在多个列上进行降序排序，必须对每个列指定 desc 关键字。\nwhere 查询时使用 where 子句中指定过滤条件。\nselect name from users where age = 18; select name from users where name = 'ming'; 注意 order by 必须位于 where 之后。\nwhere 条件操作符 操作符 描述 = 等于 \u003c\u003e, != 不等于 \u003c 小于 \u003c= 小于等于 \u003e 大于 \u003e= 大于等于 between 在指定的两个值之间 between 操作符使用：\nselect name from users where age between 15 and 18; 检索年龄在 15 和 18 之间的用户。\nand 关键字前后分别是开始值和结束值。查询到的数据包括指定的开始值和结束值。\n空值检查 创建表时，列的值可以为空值 NULL。\nIS NULL 子句可用来检查具有 NULL 值的列。\nselect name from users where phone IS NULL; and and 添加过滤条件。\nselect name from users where age = 18 and weight = 60; or 和 and 差不多，只不过是匹配任一满足的条件。\n操作符优先级 select name from users where age = 18 or agr = 19 and weight \u003e= 60; 上面的语句是什么结果？\n是找出年龄是 18 或者 19，体重在 60 以上的行？并不是。\nSQL 在处理 or 操作符前，优先处理 and 操作符。当 SQL 看到上述 where 子句时，它理解为由年龄为 19，并且体重在 60 以上的用户，或者 年龄为 18，不管体重多少的用户，相当于 age = 18 or (agr = 19 and weight \u003e= 60)。\n正确的语法：\nselect name from users where (age = 18 or agr = 19) and weight \u003e= 60; SQL 会首先过滤圆括号内的条件。\nin in 操作符用来指定条件范围，匹配圆括号中的值。\nselect name from users where age in (18, 19); in 操作符与 or 有相同的功能。\nnot not 操作否定条件。\nselect name from users where age not in (18, 19); like 通配符，使用 like 操作符。\n% % 表示任何字符出现任意次数。例如找出所有以词 m 开头的用户：\nselect name from users where name like 'm%'; 使用两个通配符，表示匹配任何位置包含文本 in 的值：\nselect name from users where name like '%in%'; _ _ 匹配单个字符，% 匹配多个字符。\n使用正则表达式 select name from users where name regexp 'ing'; regexp 后面跟 正则表达式。\n正则表达式并没有什么优势，但是有些场景下可以考虑使用:\nselect name from users where weight regexp '.6'; . 是正则表达式语言中一个特殊的字符。它表示匹配任意一个字符，因此，56 和 66 都匹配且返回。\nor 匹配 select name from users where weight regexp '46|56'; | 为正则表达式的 or 操作符。\n匹配几个字符之一 想匹配特定的字符可以指定一组用 [ 和 ] 括起来的字符来完成。\nselect name from users where weight REGEX '[456]6'; [456] 表示匹配 4，5，6。[] 是另一种形式的 or 语句。可以使用一些正则的语法例如 [^123]，匹配除了 1，2，3 之外的字符， [1-9] 匹配 1 到 9 范围的字符。匹配特殊字符钱加 \\\\ 比如 . 要用 \\\\. 来查找。","select-子句顺序#select 子句顺序":" 子句 是否必须使用 select 是 from 仅在从表选择数据时使用 where 否 group by 仅在按组计算聚集时使用 having 否 order by 否 limit 否 ","show#\u003ccode\u003eshow\u003c/code\u003e":"","use-选择数据库#\u003ccode\u003euse\u003c/code\u003e 选择数据库":"","分组#分组":"group by 创建分组。\nselect vend_id, count(*) as prod_num from products group by vend_id; 上面的语句按 vend_id 排序并分组数据。\n注意：\ngroup by 句必须出现在 where 子句之后，order by 子句之前。 如果分组列中具有 NULL 值，则 NULL 将作为一个分组返回。 group by 子句中列出的每个列都必须是检索列或有效的表达式（但不能是聚集函数）。如果在 select 中使用表达式，则必须在 group by 子句中指定相同的表达式。不能使用别名。 除了聚集计算语句，select 语句中的每个列都必须在 group by 子句中给出。 过滤分组 having 子句过滤分组。having 类似于 where，不过 where 过滤的是行。它们的句法是相同的。\nwhere 在数据分组前进行过滤，having 在数据分组后进行过滤。\nselect vend_id, count(*) as prod_num from products group by vend_id having count(*) \u003e= 2; 它过滤 count(*) \u003e=2 的那些分组。","数据处理函数#数据处理函数":"文本函数 upper() 文本转大写，upper(name) as newName lower() 文本转小写 left() 返回串左边的字符 right() 返回串右边的字符 length() 返回串的长度 locate() 找出串的一个子串 trim()，删除两边空格。 ltrim() 去掉串左边的空格 rtrim() 去掉串右边的空格 soundex() 返回串的 SOUNDEX 值 substring() 返回子串的字符 SOUNDEX 是一个将任何文本串转换为描述其语音表示的字母数字模式的算法。\n日期和时间处理函数 adddate() 增加一个日期（天、周等） addtime() 增加一个时间（时、分等） curdate() 返回当前日期 curtime() 返回当前时间 date() 返回日期时间的日期部分 datediff() 计算两个日期之差 date_add() 高度灵活的日期运算函数 date_format() 返回一个格式化的日期或时间串 day() 返回一个日期的天数部分 dayofweek() 对于一个日期，返回对应的星期几 hour() 返回一个时间的小时部分 minute() 返回一个时间的分钟部分 month() 返回一个日期的月份部分 now() 返回当前日期和时间 second() 返回一个时间的秒部分 time() 返回一个日期时间的时间部分 year() 返回一个日期的年份部分 数值函数 abs() 返回一个数的绝对值 cos() 返回一个角度的余弦 exp() 返回一个数的指数值 mod() 返回除操作的余数 pi() 返回圆周率 rand() 返回一个随机数 sin() 返回一个角度的正弦 sqrt() 返回一个数的平方根 tan() 返回一个角度的正切 ","聚合函数#聚合函数":"avg avg 函数可用来返回所有列的平均值，也可以用来返回特定列或行的平均值。\nselect avg(price) as avg_price from orders; 返回订单的平均价格。\navg() 函数忽略列值为 NULL 的行。\ncount count(*) 对行数进行计数。count(column) 对特定列中具有值的行进行计数，忽略 NULL 值。\nmax 返回指定列中的最大值。忽略列值为 NULL 的行。例如 select max(price) as max_price from products; 返回 products 表中 最贵的物品的价格。\nmin 与 max() 功能相反。\nsum sum() 函数返回指定列值的和。忽略列值为 NULL 的行。例如 select sum(item_price*quantity) as total_price from products;\n聚合不同值 上面的几个函数都可以使用 distinct，比如 avg(distinct price) as avg_total_price\ndistinct 只能用于 count()。distinct 不能用于 count(*)。","计算字段#计算字段":"存储在数据库表中的数据一般不是应用程序所需要的格式。\n拼接字段 concat() 拼接串，多个串之间用 , 分隔。\nmysql\u003e select concat(c1, '(', c2, ')') from record_format_demo; +--------------------------+ | Concat(c1, '(', c2, ')') | +--------------------------+ | aaaa(bbb) | | eeee(fff) | +--------------------------+ 2 rows in set (0.01 sec) 别名 别名（alias） as 关键字：\nmysql\u003e select concat(c1, '(', c2, ')') as c5 from record_format_demo; +-----------+ | c5 | +-----------+ | aaaa(bbb) | | eeee(fff) | +-----------+ 2 rows in set (0.01 sec) 算术计算 select price, name, quantity, quantity*price as total_price from orders where order_num = 2005; price 是物品的价格，quantity 是数量， total_price （quantity*price）就是总价。\n基本算术操作符 +，-，*，/。() 用来区分优先顺序。"},"title":"简单查询"},"/db-learn/docs/mysql/guide/06_advanced_query/":{"data":{"":"","子查询#子查询":"SQL 还允许创建子查询（subquery），即嵌套在其他查询中的查询。\n利用子查询过滤 订单存储在两个表中。对于包含订单号、客户 ID、订单日期的每个订单，orders 表存储一行。各订单的物品存储在相关的 orderitems 表中。orders 表不存储客户信息。它只存储客户的 ID。实际的客户信息存储在 customers 表中。\n假如需要列出订购物品 TNT2 的所有客户，需要下面几步：\n检索包含物品 TNT2 的所有订单的编号。 检索具有前一步骤列出的订单编号的所有客户的 ID。 检索前一步骤返回的所有客户ID的客户信息。 # 检索包含物品 TNT2 的所有订单的编号 mysql\u003e select order_num from orderitems where pod_id= 'TNT2'; +-------------+ | order_num | +-------------+ | 2005 | | 2007 | +-------------+ # 查询具有订单 20005 和 20007 的客户 ID mysql\u003e select cust_id from orders where order_num in (2005,2007); +-------------+ | cust_id | +-------------+ | 1001 | | 1004 | +-------------+ 把第一个查询（返回订单号的那一个）变为子查询组合两个查询:\nmysql\u003e select cust_id from orders where order_num in (select order_num from orderitems where pod_id= 'TNT2'); +-------------+ | cust_id | +-------------+ | 1001 | | 1004 | +-------------+ 子查询总是从内向外处理。\n进一步根据客户 id 查出客户信息：\nselect cust_name from customers where cust_id in ( select cust_id from orders where order_num in (select order_num from orderitems where pod_id= 'TNT2')); 对于能嵌套的子查询的数目没有限制，但是不要嵌套太多的子查询，会影响性能。 在 where 子句中使用子查询，应该保证 select 语句具有与 where 子句中的列必须匹配。\n作为计算字段使用子查询 子查询的另一方法是创建计算字段。\n假如需要显示 customers 表中每个客户的订单总数。订单与相应的客户 ID 存储在 orders 表中。\nselect cust_name, (select COUNT(*) from orders where orders.cust_id = customers.cust_id) as orders from customers order by cust_name; orders 是一个计算字段，它是由圆括号中的子查询建立的。该子查询对检索出的每个客户执行一次。子查询中的 where 子句使 用了完全限定列名。","组合查询#组合查询":"MySQL 允许执行多个查询（多条 select 语句），并将结果作为单个查询结果集返回。这些组合查询通常称为并（union）或复合查询（compound query）。\nselect vend_id, prod_id, prod_price from products where prod_price \u003e= 5 union select vend_id, prod_id, prod_price from products where vend_id in (1001,1002); 转成多条 where 子句的写法：\nselect vend_id, prod_id, prod_price from products where prod_price \u003e= 5 or vend_id in (1001,1002); 实际中应该试一下这两种方式，以确定对特定的查询哪一种性能更好。\nunion 规则 union 必须由两条或两条以上的 select 语句组成，语句之间用关键字 union 分隔。 union 中的每个查询必须包含相同的列、表达式或聚集函数（列的次序无所谓）。 列数据类型必须兼容：类型不必完全相同，但必须是 DBMS 可以隐含地转换的类型（例如，不同的数值类型或不同的日期类型）。 包含或取消重复的行 union 从查询结果集中自动去除了重复的行。如果想返回所有匹配行，使用 union all 而不是 union。\n组合查询结果排序 用 union 组合查询时，只能使用一条 order by 子句，它必须出现在最后一条 select 语句之后。对于结果集，不存在用一种方式排序一部分，而又用另一种方式排序另一部分的情况，因此不允许使用多条 order by 子句。","联结表#联结表":"join 表。\n关系表 相同数据出现多次决不是一件好事，此因素是关系数据库设计的基础。关系表的设计就是要保证把信息分解成多个表，一类数据一个表。各表通过某些常用的值（即关系设计中的关系（relational））互相关联。\n创建联结 select vend_name, prod_name, prod_price from vendors, products where vendors.vend_id = products.vend_id order by vend_name, prod_name; 列 prod_name 和 prod_price 在一个表中，而列 vend_name 在另一个表中。from 子句列出了两个表，分别是 vendors 和 products。这两个表用 where 子句联结。\n笛卡儿积（cartesian product）由没有联结条件的表关系返回的结果为笛卡儿积。检索出的行的数目将是第一个表中的行数乘以第二个表中的行数。\n内部联结 目前为止所用的联结称为等值联结（equijoin）（也叫内部联结），使用 join 来表示：\nselect vend_name, prod_name, prod_price from vendors inner join products on vendors.vend_id = products.vend_id; 此语句中的 from 子句与上面的不同。两个表之间的关系是 from 子句的组成部分，以 INNER JOIN 指定。在使用这种语法时，联结条件用特定的 on 子句 而不是 where 子句给出。传递给 on 的实际条件与传递给 where 的相同。\nSQL 规范首选 INNER JOIN 语法。此外，尽管使用 where 子句定义联结的确比较简单，但是使用明确的联结语法能够确保不会忘记联结条件，有时候这样做也能影响性能。不要联结不必要的表。联结的表越多，性能下降越厉害。\n当两张表的数据量比较大，又需要连接查询时，应该使用 FROM table1 JOIN table2 ON xxx 的语法，避免使用 FROM table1,table2 WHERE xxx 的语法，因为后者会在内存中先生成一张数据量比较大的笛卡尔积表，增加了内存的开销。\n联结多个表 SQL 对一条 SELECT 语句中可以联结的表的数目没有限制。\n使用前面子查询的例子：\nselect cust_name from customers where cust_id in ( select cust_id from orders where order_num in (select order_num from orderitems where pod_id = 'TNT2')); 可以改造为：\nselect cust_name from customers, orders, orderitems where customers.cust_id = orders.cust_id and orderitems.order_num = orders.order_num and pod_id = 'TNT2'; 高级联结 表别名 使用表别名：\n缩短 SQL 语句 允许在单条 select 语句中多次使用相同的表 使用表别名的主要原因之一是能在单条 select 语句中不止一次引用相同的表。\nselect cust_name from customers as c, orders as o, orderitems as oi where c.cust_id = o.cust_id and oi.order_num = o.order_num and pod_id = 'TNT2'; 外部联结 select customers.cust_id, orders.order_num from customers inner join orders on customers.cust_id = orders.cust_id; 上面的语句很简单，就是检索所有客户及其订单。\n那么如果想要检索所有客户，包括没有订单的客户，如下：\nselect customers.cust_id, orders.order_num from customers left outer join orders on customers.cust_id = orders.cust_id; 这条语句使用了关键字 OUTER JOIN 来指定联结的类型。外部联结还包括没有关联行的行。在使用 OUTER JOIN 语法时，必须使用 RIGHT或 LEFT 关键字指定包括其所有行的表（RIGHT 指的是 OUTER JOIN 右边的表，而 LEFT 指的是 OUTER JOIN 左边的表）。\n上面的例子使用 LEFT OUTER JOIN 从 from 子句的左边表（customers 表）中选择所有行。"},"title":"复杂查询"},"/db-learn/docs/mysql/guide/07_write_operation/":{"data":{"":"","删除数据#删除数据":" delete from customers where cust_id = 1005; delete 不需要列名或通配符。\ndelete 一定要注意添加过滤条件，避免删除所有行。 删除表中所有行使用 truncate table，它的速度比使用 delete 快很多（truncate 实际是删除原来的表并重新创建一个表，而不是逐行删除表中的数据）。\n对 update 或 delete 语句应该先用 select 进行测试 where 过滤条件，保证它过滤的是正确的记录。","插入数据#插入数据":"插入完整的行 insert into customers values('xiaoming', 'shanghai', 18); insert into customers(cust_name, cust_address, cust_age) values('xiaoming', 'shanghai', 18); 不建议使用第一种语法，因为各个列必须以它们在表定义中出现的次序填充。高度依赖于表中列的定义次序。\n如果表的定义允许，则可以在 insert 操作中省略某些列。省略的列必须满足以下某个条件。该列定义为允许 NULL 值（无值或空值）。在表定义中给出默认值。这表示如果不给出值，将使用默认值。 如果不提供列名，values 则必须给每个表列提供一个值。如果提供列名，values 则必须对每个列出的列给出一个值。\n插入多行 insert into customers(cust_name, cust_address, cust_age) values('xiaoming', 'shanghai', 18), values('xiaoliang', 'shanghai', 18); 其中单条 insert 语句有多组值，每组值用一对圆括号括起来，用 , 分隔。\nMySQL 用单条 insert 语句处理多个插入比使用多条 insert 语句快。\n插入检索出的数据 INSERT 还存在另一种形式，可以利用它将一条 select 语句的结果插入表中。\ninsert into customers(cust_name, cust_address, cust_age) select cust_name, cust_address, cust_age from custnew; select 语句从 custnew 检索出要插入的值。\ninsert 和 select 语句中使用了相同的列名。但是，不一定要求列名匹配。事实上，MySQL 使用的是列的位置，因此 select 中的第一列（不管其列名）将用来填充表列中指定的第一个列","更新数据#更新数据":"使用 update 语句更新（修改）表中的数据。\nupdate 语句由 3 部分组成，分别是：\n要更新的表 列名和它们的新值 确定要更新行的过滤条件 update customers set cust_email = '111111@demo.com' where cust_id = 1005; update 一定要注意添加过滤条件，避免更新所有行。 udpate 语句更新多行，并且在更新这些行中的一行或多行时出一个现错误，则整个 UPDATE 操作被取消。为即使是发生错误，也继续进行更新，可使用 ignore 关键字，如：update ignore customers。"},"title":"写操作"},"/db-learn/docs/mysql/guide/08_other/":{"data":{"":"","存储过程#存储过程":"存储过程简单来说，就是为以后的使用而保存的一条或多条 MySQL 语句的集合。\n为什么要使用存储过程 简化复杂的操作 由于不要求反复建立一系列处理步骤，这保证了数据的完整性。所有开发人员和应用程序都使用同一（试验和测试）存储过程，则所使用的代码都是相同的。这一点的延伸就是防止错误。防止错误保证了数据的一致性。 简化对变动的管理。如果表名、列名或业务逻辑（或别的内容）有变化，只需要更改存储过程的代码。使用它的人员甚至不需要知道这些变化。 提高性能。存储过程比使用单独的 SQL 语句要快。 总结就是，简单、安全、高性能。\n但是存储过程的编写比基本 SQL 语句复杂。\n使用存储过程会增加数据库服务器系统的负荷，所以在使用时需要综合业务考虑。通常复杂的业务场景都在应用层面开发，可以更好的管理维护和优化。\n使用 MySQL 称存储过程的执行为调用，因此执行存储过程的语句为 CALL。CALL 接受存储过程的名字以及需要传递给它的任意参数。\ncall productpricing(@pricelow, @pricehigh, @priceacerage); 执行名为 productpricing 的存储过程，它计算并返回产品的最低、最高和平均价格。\n存储过程实际上是一种函数。\n创建 create procedure productpricing() begin select Avg(prod_price) as priceacerage from products; end; 此存储过程名为 productpricing，用 create procedure productpricing() 语句定义。begin 和 end 语句用来限定存储过程体，过程体本身仅是一个简单的 select 语句。\n删除 drop procedure productpricing; 使用参数 create procedure productpricing( out pl DECIMAL(8,2), out ph DECIMAL(8,2), out pa DECIMAL(8,2) ) begin select Min(prod_price) into pl from products; select Max(prod_price) into ph from products; select Avg(prod_price) into pa from products; end; 此存储过程接受 3 个参数：pl 存储产品最低价格，ph 存储产品最高价格，pa 存储产品平均价格。每个参数必须具有指定的类型，这里使用十进制值。关键字 OUT 指出相应的参数用来从存储过程传出一个值（返回给调用者）。\nMySQL 支持三种类型参数：\nIN 传递给存储过程 OUT 从存储过程传出 INOUT 对存储过程传入和传出 存储过程的代码位于 begin 和 end 语句内，它们是一系列 select 语句，用来检索值，然后保存到相应的变量（通过指定 into 关键字）。\n调用此修改过的存储过程，必须指定 3 个变量名，如下所示：\ncall productpricing(@pricelow, @pricehigh, @priceacerage); 此存储过程要求 3 个参数，因此必须正好传递 3 个参数。存储过程将保存结果到这 3 个变量。\n所有 MySQL 变量都必须以 @ 开始。\n调用这条语句并不显示任何数据。为了显示检索出的产品平均价格，可使用下面的语句：\nselect @priceacerage; select @pricelow, @pricehigh, @priceacerage; 使用 IN 和 OUT create procedure ordertotal( in onumber int, inout ototal DECIMAL(8,2) ) begin select Sum(item_price*quantity) from orderitems where order_num = onumber into otital; end; onumber 定义为 IN，因为需要传订单号给存储过程。ototal 定义为 OUT，因为要从存储过程返回合计。select 语句使用这两个参数，where 子句使用 onumber 选择正确的行，INTO 使用 ototal 存储计算出来的合计。\ncall ordertotal(2005, @total); 必须给 ordertotal 传递两个参数；第一个参数为订单号，第二个参数为包含计算出来的合计的变量名。\n显示此合计：\nselect @total; 智能存储过程 存储过程只有在包含业务规则和智能处理时，才真正显现出来他的作用。\n例如，需要获得一份订单合计，但需要对合计增加营业税，不过只针对某些顾客。\n获得合计（与以前一样） 把营业税有条件地添加到合计 返回合计（带或不带税） -- Name: ordertotal -- Parameters: onumber = order number -- taxable = 0 if not taxable, 1 if taxable -- ototal = order total variable create procedure odertotal( in onumber int, in taxable boolean, out ototal decimal(8, 2) ) comment 'Obtain order total, optionally adding tax' begin -- Declare variable for total declare total decimal(8, 2); -- Declare tax percentage declare taxrate int default 6; -- Get the order total select sum(item_price*quantity) from orderitems where order_num = onumber into total; -- Is this taxable? IF taxable THEN -- Yes, so add taxrate to the total select total+(total/100*taxrate) into total; END IF; -- And finally, save to out variable select total into ototal; end; -- 表示注释。参数 taxable，它是一个布尔值，表示是否增加税。 DECLARE 语句定义了两个局部变量。DECLARE 要求指定变量名和数据类型，它也支持可选的默认值（这里的 taxrate 的默认被设置为 6%）\nIF 语句检查 taxable 是否为真，如果为真，则用另一 select 语句增加营业税到局部变量 total。 最后，用另一 select 语句将 total 保存到 ototal。\n检查存储过程 SHOW CREATE PROCEDURE name 和 SHOW PROCEDURE STATUS name。","游标#游标":"MySQL 检索操作返回一组称为结果集的行。这组返回的行都是与 SQL 语句相匹配的行（零行或多行）。\n有时，需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因。游标（cursor）是一个存储在 MySQL 服务器上的数据库查询，它不是一条 select 语句，而是被该语句检索出来的结果集。在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据。\nMySQL 游标只能用于存储过程（和函数）。\n使用游标 在能够使用游标前，必须声明（定义）它。这个过程实际上没有检索数据，它只是定义要使用的 select 语句。 一旦声明后，必须打开游标以供使用。这个过程用前面定义的 select 语句把数据实际检索出来。 对于填有数据的游标，根据需要取出（检索）各行。 在结束游标使用时，必须关闭游标。 创建游标 DECLARE 语句创建游标。DECLARE 命名游标，并定义相应的 select 语句，根据需要带 WHERE 和其他子句。\ncreate procedure processorders() begin declare ordernumbers cursor for select order_num from orders; end; 存储过程处理完成后，游标就消失。\n打开关闭 打开使用：OPEN ordernumbers; 关闭使用：CLOSE ordernumbers;\n使用声明过的游标不需要再次声明，用 OPEN 语句打开它就可以了。 如果你不明确关闭游标，MySQL 将会在到达 END 语句时自动关闭它。\n使用游标数据 在一个游标被打开后，可以使用 FETCH 语句分别访问它的每一行。FETCH 指定检索什么数据（所需的列），检索出来的数据存储在什么地方。\ncreate procedure processorders() begin -- Declare local variables declare o int; -- Delare the cursor declare ordernumbers cursor for select order_num from orders; -- open the cursor open ordernumbers; -- get order number fetch ordernumbers into o; -- close the cursor close ordernumbers; end; ","用户管理#用户管理":"在实际开发中，决不能使用 root。应该创建一系列的账号，有的用于管理，有的供用户使用，有的供开发人员使用，等等。\nMySQL 用户账号和信息存储在名为 mysql 的库中。一般不需要直接访问 mysql 数据库和表，但有时需要直接访问。需要直接访问它的时机之一是在需要获得所有用户账号列表时。\nmysql 库有一个名为 user 的表，它包含所有用户账号。user 表有一个名为 user 的列，它存储用户登录名。","视图#视图":" select cust_name from customers, orders, orderitems where customers.cust_id = orders.cust_id and orderitems.order_num = orders.order_num and pod_id = 'TNT2'; 上面的语句，涉及到三个表，用来检索订购了某个特定产品的客户。任何需要这个数据的人都必须理解相关表的结构，并且知道如何创建查询和对表进行联结。为了检索其他产品（或多个产品）的相同数据，必须修改最后的 where 子句。\n假如可以把整个查询包装成一个名为 productcustomers 的虚拟表，则可以如下轻松地检索出相同的数据：\nselect cust_name from productcustomers where pod_id = 'TNT2'; 这就是视图的作用，是一个虚拟的表。productcustomers 就是一个视图，作为视图，它不包含表中应该有的任何列或数据，它包含的是一个 SQL 查询。\n为什么使用视图 重用 SQL 语句。 简化复杂的 SQL 操作。 使用表的组成部分而不是整个表。 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限。 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。 视图创建之后，可以用与表基本相同的方式利用它们。可以对视图执行 select 操作，过滤和排序数据，将视图联结到其他视图或表，甚至能添加和更新数据。\n视图本身不包含数据，因此它们返回的数据是从其他表中检索出来的。在添加或更改这些表中的数据时，视图将返回改变过的数据。\n性能问题 因为视图不包含数据，所以每次使用视图时，都必须处理查询执行时所需的任一个检索。如果你用多个联结和过滤创建了复杂的视图或者嵌套了视图，可能会发现性能下降得很厉害。在部署使用了大量视图的应用前，应该进行测试。\n规则和限制 与表一样，视图必须唯一命名（不能给视图取与别的视图或表相同的名字）。 创建的视图数目没有限制。 为了创建视图，必须具有足够的访问权限。 视图可以嵌套，即可以利用从其他视图中检索数据的查询来构造一个视图。 order by 可以用在视图中，但如果从该视图检索数据 select 中也含有 order by，那么该视图中的 order by 将被覆盖。 视图不能索引，使用视图的时候可能会引发很多查询性能问题。也不能有关联的触发器或默认值。 视图可以和表一起使用。例如，编写一条联结表和视图的 select 语句。 使用视图 create view 语句创建视图。 show create view viewname 查看创建视图的语句。 drop 删除视图，其语法为 drop view viewname。 更新视图时，可以先用 drop 再用 create，也可以直接用 create or replace view。 select Concat(RTrim(vend_name)), '(' , RTrim(vend_country), ')') as vend_title from vendors order by vend_name; 上面的语句，格式化了结果，如果要经常用，可以创建一个视图：\ncreate view vendorlocations as select Concat(RTrim(vend_name)), '(' , RTrim(vend_country), ')') as vend_title from vendors order by vend_name; 然后可以直接使用：\nselect * from vendorlocations; 视图也可以用来过滤数据，或者计算字段。\n更新视图 并非所有视图都是可更新的。如果视图定义中有以下操作，则不能进行视图的更新：\n分组（使用 group by 和 having）； 联结； 子查询； 并； 聚集函数（min()、count()、sum() 等）； distinct； 导出（计算）列。 在指定条件允许的情况下，可以通过在视图上操作更新，删除，甚至写入数据，进而更新视图所涉及的相关表。\nupdate vendorlocations set vend_name='smile' where id='1'; 这里就通过对视图执行更新操作，进而更新 vendors 表数据。","触发器#触发器":"如果你想要某条语句（或某些语句）在事件发生时自动执行，怎么办呢？使用触发器。\n触发器是 MySQL 响应以下任意语句而自动执行的一条 MySQL 语句（或位于 begin 和 end 语句之间的一组语句）：\ndelete insert update 创建触发器时，需要给出 4 条信息：\n唯一的触发器名； 触发器关联的表； 触发器应该响应的活动（delete、insert 或 update）； 触发器何时执行（处理之前或之后） 创建 用 create trigger 语句创建。\ncreate trigger newproduct after insert on products for each row select 'Procduct added'; 创建名为 newproduct 的新触发器。触发器可在一个操作发生之前或之后执行，这里给出了 after insert，所以此触发器将在 insert 语句成功执行后执行。这个触发器还指定 FOR EACH ROW，因此代码对每个插入行执行。在这个例子中，文本 Product added 将对每个插入的行显示一次。使用 insert 语句添加一行或多行到 products 中，你将看到对每个成功的插入，显示 Product added 消息。\n每个表每个事件每次只允许一个触发器。因此，每个表最多支持 6 个触发器（每条 insert、update 和 delete 的之前和之后）。 如果 BEFORE 触发器失败，则 MySQL 将不执行请求的操作。此外，如果 BEFORE 触发器或语句本身失败，MySQL 将不执行 AFTER 触发器（如果有的话）。 MySQL 触发器中不支持 CALL 语句。也就是不能从触发器内调用存储过程。\n删除 删除触发器使用：drop trigger newproduct;。为了修改一个触发器，必须先删除它，然后再重新创建。"},"title":"其他"},"/db-learn/docs/mysql/practice/01_table_design/":{"data":{"":"在选择数据类型时，一般应该遵循下面两步：\n确定合适的大类型：数字、字符串、时间、二进制； 确定具体的类型：有无符号、取值范围、变长定长等。 在 MySQL 数据类型设置方面，尽量用更小的数据类型，它们通常有更好的性能，花费更少的硬件资源。","json-数据类型#JSON 数据类型":"用户登录设计 JSON 类型比较适合存储一些修改较少、相对静态的数据，比如用户登录信息的存储：\nDROP TABLE IF EXISTS UserLogin; CREATE TABLE UserLogin ( userId BIGINT NOT NULL, loginInfo JSON, PRIMARY KEY(userId) ); 由于现在业务的登录方式越来越多样化，如同一账户支持手机、微信、QQ 账号登录，所以这里可以用 JSON 类型存储登录的信息。\n插入下面的数据：\nSET @a = ' { \"cellphone\" : \"13918888888\", \"wxchat\" : \"破产码农\", \"QQ\" : \"82946772\" } '; INSERT INTO UserLogin VALUES (1,@a); SET @b = ' { \"cellphone\" : \"15026888888\" } '; INSERT INTO UserLogin VALUES (2,@b); 上面的例子中，用户 1 登录有三种方式：手机验证码登录、微信登录、QQ 登录，而用户 2 只有手机验证码登录。\n而如果不采用 JSON 数据类型，就要用下面的方式建表：\nCREATE TABLE UserLogin ( userId\tBIGINT NOT NULL, cellphone\tVARCHAR(255), wechat\tVARCHAR(255) QQ\tVARCHAR(255), PRIMARY KEY(userId) ); 上面传统的方式存在两个问题：\n有些列可能是比较稀疏的，一些列可能大部分都是 NULL 值； 如果要新增一种登录类型，如微博登录，则需要添加新列，而 JSON 类型无此烦恼。 使用 JSON 类型，再配合 JSON 字段处理函数，实现更加简便。其中，最常见的 JSON 字段处理函数就是 JSON_EXTRACT，它用来从 JSON 数据中提取所需要的字段内容。\n例如查询用户的手机和微信信息：\nSELECT userId, JSON_UNQUOTE(JSON_EXTRACT(loginInfo,\"$.cellphone\")) cellphone, JSON_UNQUOTE(JSON_EXTRACT(loginInfo,\"$.wxchat\")) wxchat FROM UserLogin; +--------+-------------+--------------+ | userId | cellphone | wxchat | +--------+-------------+--------------+ | 1 | 13918888888 | 破产码农 | | 2 | 15026888888 | NULL | +--------+-------------+--------------+ 2 rows in set (0.01 sec) MySQL 还提供了 -\u003e\u003e 表达式，和 JSON_EXTRACT、JSON_UNQUOTE 的效果完全一样：\nSELECT userId, loginInfo-\u003e\u003e\"$.cellphone\" cellphone, loginInfo-\u003e\u003e\"$.wxchat\" wxchat FROM UserLogin; 当 JSON 数据量非常大，用户希望对 JSON 数据进行有效检索时，可以利用 MySQL 的函数索引功能对 JSON 中的某个字段进行索引。\n比如在上面的用户登录示例中，假设用户必须绑定唯一手机号，且希望未来能用手机号码进行用户检索时，可以创建下面的索引：\nALTER TABLE UserLogin ADD COLUMN cellphone VARCHAR(255) AS (loginInfo-\u003e\u003e\"$.cellphone\"); ALTER TABLE UserLogin ADD UNIQUE INDEX idx_cellphone(cellphone); 首先创建了一个虚拟列 cellphone，这个列是由函数 loginInfo-\u003e\u003e\"$.cellphone\" 计算得到的。 在这个虚拟列上创建一个唯一索引 idx_cellphone。 通过虚拟列 cellphone 进行查询，就可以看到优化器会使用到新创建的 idx_cellphone 索引：\nEXPLAIN SELECT * FROM UserLogin WHERE cellphone = '13918888888'\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: UserLogin partitions: NULL type: const possible_keys: idx_cellphone key: idx_cellphone key_len: 1023 ref: const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) 用户画像设计 用户画像（也就是对用户打标签）比如：\n在电商行业中，根据用户的穿搭喜好，推荐相应的商品； 在音乐行业中，根据用户喜欢的音乐风格和常听的歌手，推荐相应的歌曲； 在金融行业，根据用户的风险喜好和投资经验，推荐相应的理财产品。 就可以使用 JSON 类型在数据库中存储用户画像信息，并结合 JSON 数组类型和多值索引的特点进行高效查询。假设有张画像定义表：\nCREATE TABLE Tags ( tagId bigint auto_increment, tagName varchar(255) NOT NULL, primary key(tagId) ); SELECT * FROM Tags; +-------+--------------+ | tagId | tagName | +-------+--------------+ | 1 | 70后 | | 2 | 80后 | | 3 | 90后 | | 4 | 00后 | | 5 | 爱运动 | | 6 | 高学历 | | 7 | 小资 | | 8 | 有房 | | 9 | 有车 | | 10 | 常看电影 | | 11 | 爱网购 | | 12 | 爱外卖 | +-------+--------------+ 表 Tags 是一张画像定义表，用于描述当前定义有多少个标签，接着给每个用户打标签，比如用户 David，他的标签是 80 后、高学历、小资、有房、常看电影；用户 Tom，90 后、常看电影、爱外卖。\n若不用 JSON 数据类型进行标签存储，通常会将用户标签通过字符串，加上分割符的方式，在一个字段中存取用户所有的标签：\n+-------+---------------------------------------+ |用户 |标签 | +-------+---------------------------------------+ |David |80后 ； 高学历 ； 小资 ； 有房 ；常看电影 | |Tom |90后 ；常看电影 ； 爱外卖 | +-------+---------------------------------------+ 缺点：不好搜索特定画像的用户，另外分隔符也是一种自我约定，在数据库中其实可以任意存储其他数据，最终产生脏数据。\nJSON 数据类型就能很好解决这个问题：\nDROP TABLE IF EXISTS UserTag; CREATE TABLE UserTag ( userId bigint NOT NULL, userTags JSON, PRIMARY KEY (userId) ); INSERT INTO UserTag VALUES (1,'[2,6,8,10]'); INSERT INTO UserTag VALUES (2,'[3,10,12]'); MySQL 8.0.17 版本开始支持 Multi-Valued Indexes，用于在 JSON 数组上创建索引，并通过函数 member of、json_contains、json_overlaps 来快速检索索引数据。\nALTER TABLE UserTag ADD INDEX idx_user_tags ((cast((userTags-\u003e\"$\") as unsigned array))); 如果想要查询用户画像为常看电影的用户，可以使用函数 MEMBER OF：\nEXPLAIN SELECT * FROM UserTag WHERE 10 MEMBER OF(userTags-\u003e\"$\")\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: UserTag partitions: NULL type: ref possible_keys: idx_user_tags key: idx_user_tags key_len: 9 ref: const rows: 1 filtered: 100.00 Extra: Using where 1 row in set, 1 warning (0.00 sec) SELECT * FROM UserTag WHERE 10 MEMBER OF(userTags-\u003e\"$\"); +--------+---------------+ | userId | userTags | +--------+---------------+ | 1 | [2, 6, 8, 10] | | 2 | [3, 10, 12] | +--------+---------------+ 2 rows in set (0.00 sec) 如果想要查询画像为 80 后，且常看电影的用户，可以使用函数 JSON_CONTAINS：\nEXPLAIN SELECT * FROM UserTag WHERE JSON_CONTAINS(userTags-\u003e\"$\", '[2,10]')\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: UserTag partitions: NULL type: range possible_keys: idx_user_tags key: idx_user_tags key_len: 9 ref: NULL rows: 3 filtered: 100.00 Extra: Using where 1 row in set, 1 warning (0.00 sec) SELECT * FROM UserTag WHERE JSON_CONTAINS(userTags-\u003e\"$\", '[2,10]'); +--------+---------------+ | userId | userTags | +--------+---------------+ | 1 | [2, 6, 8, 10] | +--------+---------------+ 1 row in set (0.00 sec) 如果想要查询画像为 80 后或 90 后，且常看电影的用户，则可以使用函数 JSON_OVERLAP：\nEXPLAIN SELECT * FROM UserTag WHERE JSON_OVERLAPS(userTags-\u003e\"$\", '[2,3,10]')\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: UserTag partitions: NULL type: range possible_keys: idx_user_tags key: idx_user_tags key_len: 9 ref: NULL rows: 4 filtered: 100.00 Extra: Using where 1 row in set, 1 warning (0.00 sec) SELECT * FROM UserTag WHERE JSON_OVERLAPS(userTags-\u003e\"$\", '[2,3,10]'); +--------+---------------+ | userId | userTags | +--------+---------------+ | 1 | [2, 6, 8, 10] | | 2 | [3, 10, 12] | +--------+---------------+ 2 rows in set (0.01 sec) SET @a = '{ \"cellphone\": \"188888888888\", \"wxchat\": \"this.\", \"QQ\": \"166666666\" }'; INSERT INTO UserLogin VALUES (1, @a); SELECT user_id, JSON_UNQUOTE(JSON_EXTRACT(loginInfo,\"$.cellphone\")) cellphone, JSON_UNQUOTE(JSON_EXTRACT(loginInfo,\"$.wxchat\")) wxchat FROM UserLogin; 使用 -\u003e\u003e 表达式，效果一样：\nSELECT userId, loginInfo-\u003e\u003e\"$.cellphone\" cellphone, loginInfo-\u003e\u003e\"$.wxchat\" wxchat FROM UserLogin; ","大类型#大类型":"尽量少用 BLOB 和 TEXT 等大类型，如果实在要用可以考虑将 BLOB 和 TEXT 字段单独存一张表，用 id 关联。","字符串#字符串":"大部分场景中，对于字符串使用类型 VARCHAR 就足够了。\n字符串的长度相差较大用 VARCHAR；字符串短，且所有值都接近一个长度用 CHAR。 尽量少用 BLOB 和 TEXT，如果实在要用可以考虑将 BLOB 和 TEXT 字段单独存一张表，用 id 关联。 utf8 utf8 字符集：是 Unicode 字符集的一种编码方案，收录地球上能想到的所有字符，而且还在不断扩充。这种字符集兼容 ASCII 字符集，采用变长编码方式，编码一个字符需要使用 1~4 个字节。\nutf8 字符集表示一个字符需要使用 1~4 个字节，但是我们常用的一些字符使用 1~3 个字节就可以表示了。而在一个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以MySQL 定义了两个概念：\nutf8mb3：阉割过的 utf8 字符集，只使用 1~3 个字节表示字符。无法存储需要 4 字节编码的字符，如表情符号、其他补充字符等。 utf8mb4：正宗的 utf8 字符集，使用 1~4 个字节表示字符。 MySQL 8.0 和以上版本：\n字符集默认为 utf8mb4。 utf8 默认指向的也是 utf8mb4。 8.0 之前的版本：\n字符集默认为 latin1。 utf8 默认指向的也是 utf8mb3。 如果主要涉及英文和少量特殊符号，并且不打算使用表情符号或任何特殊 Unicode 字符，那么使用 utf8mb3 就足够了。\n如果需要支持多种语言（国际化），包括那些使用大量特殊字符（如表情符号）的语言，那么使用 utf8mb4。\n字符集排序规则 MySQL 支持多种字符集，每个字符集都会有默认的排序规则。可以用命令 SHOW CHARSET （CHARACTER SET 和 CHARSET 是同义词）来查看：\nmysql\u003e SHOW CHARSET LIKE 'utf8%'; +---------+---------------+--------------------+--------+ | Charset | Description | Default collation | Maxlen | +---------+---------------+--------------------+--------+ | utf8 | UTF-8 Unicode | utf8_general_ci | 3 | | utf8mb4 | UTF-8 Unicode | utf8mb4_0900_ai_ci | 4 | +---------+---------------+--------------------+--------+ 2 rows in set (0.01 sec) mysql\u003e SHOW COLLATION LIKE 'utf8mb4%'; +----------------------------+---------+-----+---------+----------+---------+---------------+ | Collation | Charset | Id | Default | Compiled | Sortlen | Pad_attribute | +----------------------------+---------+-----+---------+----------+---------+---------------+ | utf8mb4_0900_ai_ci | utf8mb4 | 255 | Yes | Yes | 0 | NO PAD | | utf8mb4_0900_as_ci | utf8mb4 | 305 | | Yes | 0 | NO PAD | | utf8mb4_0900_as_cs | utf8mb4 | 278 | | Yes | 0 | NO PAD | | utf8mb4_0900_bin | utf8mb4 | 309 | | Yes | 1 | NO PAD | | utf8mb4_bin | utf8mb4 | 46 | | Yes | 1 | PAD SPACE | ...... 其中 Default collation 列表示这种字符集中一种默认的比较规则。排序规则\n以 _ci 结尾，表示不区分大小写（Case Insentive） _cs 表示大小写敏感 _bin 表示通过存储字符的二进制进行比较。 绝大部分业务的表结构设计无须设置排序规则为大小写敏感。\n正确修改字符集 ALTER TABLE emoji_test CHARSET utf8mb4; 上面的 SQL 将表的字符集修改为 utf8mb4，下次新增列时，若不显式地指定字符集，新列的字符集会变更为 utf8mb4，但对于已经存在的列，其默认字符集并不做修改。\nmysql\u003e SHOW CREATE TABLE emoji_test\\G *************************** 1. row *************************** Table: emoji_test Create Table: CREATE TABLE `emoji_test` ( `a` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, PRIMARY KEY (`a`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci 1 row in set (0.00 sec) 表的列 a 的字符集依然是 utf8。\n所以，正确修改列字符集的命令应该使用 ALTER TABLE ... CONVERT TO：\nmysql\u003e ALTER TABLE emoji_test CONVERT TO CHARSET utf8mb4; Query OK, 0 rows affected (0.94 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e SHOW CREATE TABLE emoji_test\\G *************************** 1. row *************************** Table: emoji_test Create Table: CREATE TABLE `emoji_test` ( `a` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL, PRIMARY KEY (`a`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci 1 row in set (0.00 sec) 性别或状态等枚举类型设计 设计表结构时，你会遇到一些固定选项值的字段。例如，性别字段（Sex），只有男或女；又或者状态字段（State），有效的值为运行、停止、重启等有限状态。\n不推荐使用 ENUM 类型，因为 ENUM 类型并非 SQL 标准的数据类型，而是 MySQL 所独有的一种字符串类型。抛出的错误提示也并不直观。\n通常情况下，枚举类型，可以使用 unsigned tinyint 类型替代，占用 1 个字节，取值范围是 0~255。\n例如，性别字段，可以使用 unsigned tinyint 类型替代，其中 0 表示男，1 表示女。\nCHECK 约束功能 MySQL 8.0.16 版本开始，数据库原生提供了 CHECK 约束功能，用来检查字段值是否符合指定的条件。通过 CHECK 约束，可以实现枚举类型的功能。\nmysql\u003e SHOW CREATE TABLE User\\G *************************** 1. row *************************** Table: User Create Table: CREATE TABLE `User` ( `id` bigint NOT NULL AUTO_INCREMENT, `sex` char(1) COLLATE utf8mb4_general_ci DEFAULT NULL, PRIMARY KEY (`id`), CONSTRAINT `user_chk_1` CHECK (((`sex` = _utf8mb4'M') or (`sex` = _utf8mb4'F'))) ) ENGINE=InnoDB 1 row in set (0.00 sec) mysql\u003e INSERT INTO User VALUES (NULL,'M'); Query OK, 1 row affected (0.07 sec) mysql\u003e INSERT INTO User VALUES (NULL,'Z'); ERROR 3819 (HY000): Check constraint 'user_chk_1' is violated. 示例中的约束定义 user_chk_1 表示列 sex 的取值范围，只能是 M 或者 F。插入非法数据 Z 时，看到 MySQL 显式地抛出了违法约束的提示。\n通过 CHECK 约束，实现枚举类型可以避免 tinyint 实现的两个问题：\n表达不清：在具体存储时，0 表示女，还是 1 表示女呢？每个业务可能有不同的潜规则； 脏数据：因为是 tinyint，因此除了 0 和 1，用户完全可以插入 2、3、4 这样的数值，最终表中可能存在无效数据，如果后期再进行清理，代价就非常大了。 账户密码存储设计 密码不能明文存储，通常的做法是对密码进行加密存储。\n不要直接使用 MD5 算法，虽然 MD5 算法并不可逆，但是可以通过暴力破解的方式，计算出所有可能的字符串对应的 MD5 值。\n所以，在设计密码存储使用，还需要加盐（salt），每个公司的盐值都是不同的，因此计算出的值也是不同的。若盐值为 psalt，则密码 12345678 在数据库中的值为：\npassword = MD5（‘psalt12345678’） 这是一种固定盐值的加密算法，其中存在三个主要问题：\n若 salt 值被（离职）员工泄漏，则外部黑客依然存在暴利破解的可能性； 对于相同密码，其密码存储值相同，一旦一个用户密码泄漏，其他相同密码的用户的密码也将被泄漏； 固定使用 MD5 加密算法，一旦 MD5 算法被破解，则影响很大。 所以一个真正好的密码存储设计，应该是：动态盐 + 非固定加密算法。例如密码的存储格式为：\n$salt$cryption_algorithm$value $salt：表示动态盐，每次用户注册时业务产生不同的盐值，并存储在数据库中。若做得再精细一点，可以动态盐值 + 用户注册日期合并为一个更为动态的盐值。 $cryption_algorithm：表示加密的算法，如 v1 表示 MD5 加密算法，v2 表示 AES256 加密算法，v3 表示 AES512 加密算法等。 $value：表示加密后的字符串。 CREATE TABLE User ( id BIGINT NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, sex CHAR(1) NOT NULL, password VARCHAR(1024) NOT NULL, CHECK (sex = 'M' OR sex = 'F'), PRIMARY KEY(id) ); SELECT * FROM User\\G *************************** 1. row *************************** id: 1 name: David sex: M password: $fgfaef$v1$2198687f6db06c9d1b31a030ba1ef074 *************************** 2. row *************************** id: 2 name: Amy sex: F password: $zpelf$v2$0x860E4E3B2AA4005D8EE9B7653409C4B133AF77AEF53B815D31426EC6EF78D882 上面的例子中，用户 David 和 Amy 密码都是 12345678，然而由于使用了动态盐和动态加密算法，两者存储的内容完全不同。","数值#数值":"signed 和 unsigned 在整型类型中，有 signed 和 unsigned 属性，默认为 signed。\n表结构设计中时：\n如果整形数据确定没有负数，如主键 ID，建议指定为 unsigned 无符号类型，容量可以扩大一倍。 其他情况不建议刻意去用 unsigned 属性，因为在做一些数据分析时，unsigned 会导致一些问题。 MySQL 要求 unsigned 数值相减之后依然为 unsigned，否则就会报错。在一些统计计算的 SQL 中，可能会出现 unsigned 数值相减之后为负数的情况，这时就会报错。\n为了避免这个错误，需要对数据库参数 sql_mode 设置为 NO_UNSIGNED_SUBTRACTION，允许相减的结果为 signed：\nSET sql_mode='NO_UNSIGNED_SUBTRACTION'; 整型类型与自增设计 整型类型最常见的就是在业务中用来表示某件物品的数量。例如销售数量、库存数量、购买次数等。 另一个重要的使用用法是作为表的主键。 整型结合属性 auto_increment，可以实现自增功能，但在表结构设计时用自增做主键，要注意以下两点：\n用 BIGINT 做主键，而不是 INT，不要为了节省 4 个字节使用 INT，当达到上限时，再进行表结构的变更，要付出巨大的代价。当达到 INT 上限后，再次进行自增插入时，会报重复错误，MySQL 数据库并不会自动将其重置为 1。 MySQL 8.0 版本前自增值是不持久化的，可能会有回溯现象。 回溯现象 mysql\u003e SELECT * FROM t; +---+ | a | +---+ | 1 | | 2 | | 3 | +---+ 3 rows in set (0.01 sec) mysql\u003e DELETE FROM t WHERE a = 3; Query OK, 1 row affected (0.02 sec) mysql\u003e SHOW CREATE TABLE t\\G *************************** 1. row *************************** Table: t Create Table: CREATE TABLE `t` ( `a` int NOT NULL AUTO_INCREMENT, PRIMARY KEY (`a`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci 1 row in set (0.00 sec) 可以看到，在删除自增为 3 的这条记录后，下一个自增值依然为 4（AUTO_INCREMENT=4），这里并没有错误。但若这时数据库发生重启，那数据库启动后，表 t 的自增起始值将再次变为 3，即自增值发生回溯。\n若要彻底解决这个问题，有 2 种方法：\n升级 MySQL 版本到 8.0 版本，每张表的自增值会持久化； 为了之后更好的分布式架构扩展性，不建议使用自增整型类型做主键，更为推荐的是字符串类型，例如 UUID。 资金字段设计 通常在表结构设计中，用户的工资、账户的余额等精确到小数点后 2 位的业务（精确到分），可以使用 DECIMAL 类型（例如 DECIMAL(8,2)）。\n在海量互联网业务的设计标准中，并不推荐用 DECIMAL 类型，更推荐将 DECIMAL 转化为整型类型。也就是说，资金类型更推荐使用用分单位存储，而不是用元单位存储。如 1 元在数据库中用整型类型 100 存储。\n金额字段为什么不使用 DECIMAL 类型？ 金额字段的取值范围如果用 DECIMAL 表示的，如何定义长度？因为类型 DECIMAL 是个变长字段，如果定义为 DECIMAL(8,2) ，那么存储最大值为 999999.99，百万级的资金存储，这是远远不够的。\n用户的金额至少要存储百亿的字段，而统计局的 GDP 金额字段则可能达到数十万亿级别。用类型 DECIMAL 定义，不好统一。而如果使用 BIGINT 来存储金额，用分来做单位（小数点中的数据呢，可以交由前端进行处理并展示），可以轻松存储千兆级别的金额。\n存储高效：所有金额相关字段都是定长字段，占用 8 个字节。 扩展性强：轻松支持百亿级甚至万亿级金额存储。 计算高效：整型运算比 DECIMAL 的二进制转换计算更快。 在数据库设计中，非常强调定长存储，因为定长存储的性能更好。\nIP 地址字段设计 IP 也是可以使用整型来存储的，因为 IP 地址本身是一个变长字段，如果使用 INT 存储，就是占用固定的 4 个字节（IP 地址最大为 255.255.255.255，二进制形式 11111111 11111111 11111111 11111111）。这种方法比使用字符串更节省空间且查询效率更高。\n存储 IP 地址的表结构：\nCREATE TABLE `network_logs` ( id INT AUTO_INCREMENT PRIMARY KEY, ip_address INT UNSIGNED, -- 存储转换后的 IP 整数值 -- 其他字段... INDEX (ip_address) -- 为提高查询效率添加索引 ); 一定要使用 INT UNSIGNED 类型，使用 INT UNSIGNED 可以存储 0 到 4294967295 的值，正好对应 IPv4 的 32 位地址空间。所以这种方法只适用于 IPv4 地址，IPv6 地址需要其他存储方式。\n-- 将IP字符串转为整型 SELECT INET_ATON('192.168.1.1'); -- 返回: 3232235777 -- 将整型转为 IP 字符串 SELECT INET_NTOA(3232235777); -- 返回: '192.168.1.1' -- 插入记录 INSERT INTO network_logs (ip_address) VALUES (INET_ATON('192.168.1.1')); -- 查询记录 -- 查询特定IP SELECT * FROM network_logs WHERE ip_address = INET_ATON('192.168.1.1'); -- 查询时显示IP字符串格式 SELECT id, INET_NTOA(ip_address) AS ip FROM network_logs; 也可以在应用程序中先转换再存储，减少数据库的计算负担。\n性能优势 存储空间：整型(4 字节) vs 字符串(7-15字节) 查询效率：整型比较比字符串比较快得多 索引效率：整型索引更小更高效 小数类型 浮点类型 Float 和 Double，不是高精度，也不是 SQL 标准的类型，不推荐使用。\n小数类型使用 DECIMAL 类型。如果存储的数值范围超过 DECIMAL 的范围，可以将数值拆成整数和小数并分开存储。","日期和时间#日期和时间":"在做表结构设计时，对日期字段的存储，开发人员通常会有 3 种选择：DATETIME、TIMESTAMP、INT。\n建议使用类型 DATETIME。\n为什么不使用 INT 类型？\nINT 类型就是直接存储 ‘1970-01-01 00:00:00’ 到现在的毫秒数，本质和 TIMESTAMP 一样。而且 INT 存储日期可读性，运维性太差了。\n有些同学会认为 INT 比 TIMESTAMP 性能更好。但是，当前每个 CPU 每秒可执行上亿次的计算，所以无须为这种转换的性能担心。\n为什么不使用 TIMESTAMP 类型？\n虽然 TIMESTAMP 占用 4 个字节，比 DATETIME 小一半的存储空间。但是 TIMESTAMP 的最大值 ‘2038-01-19 03:14:07’ 已经很接近了，要慎重考虑。\nTIMESTAMP 虽然有时区属性的优势，对于 DATETIME 的时区问题，可以由前端或者服务端做一次转化，不一定非要在数据库中解决。\nTIMESTAMP 的性能问题，\n虽然从毫秒数转换到类型 TIMESTAMP 本身需要的 CPU 指令并不多，这并不会带来直接的性能问题。但是如果使用默认的操作系统时区，则每次通过时区计算时间时，要调用操作系统底层系统函数 __tz_convert()，而这个函数需要额外的加锁操作，以确保这时操作系统时区没有修改。所以，当大规模并发访问时，由于热点资源竞争，会产生两个问题。\n性能不如 DATETIME： DATETIME 不存在时区转化问题。 性能抖动： 海量并发时，存在性能抖动问题。 可以通过设置显式的时区，来优化 TIMESTAMP。比如在配置文件中显示地设置时区，而不要使用系统时区：\n[mysqld] time_zone = \"+08:00\" 日期字段推荐使用 DATETIME，没有时区转化。即便使用 TIMESTAMP，也需要在数据库中显式地配置时区，而不是用系统时区。"},"title":"表设计"},"/db-learn/docs/mysql/practice/02_index_design/":{"data":{"":"索引设计的核心思想就是尽量利用一两个复杂的多字段联合索引，抗下 80% 以上的查询，然后用一两个辅助索引尽量抗下剩余的一些非典型查询，保证这种大数据量表的查询尽可能多的都能充分利用索引，这样就能保证查询速度和性能了。","不要在小基数字段上建立索引#不要在小基数字段上建立索引":"索引基数是指这个字段在表里总共有多少个不同的值，比如一张表总共 100 万行记录，其中有个性别字段，其值不是‘男’就是‘女’，那么该字段的基数就是 2。对这种小基数字段建立索引的话，还不如全表扫描。因为索引树里就包含‘男’和‘女’两种值，根本没法进行快速的二分查找，那用索引就没有太大的意义了。","主键插入顺序#主键插入顺序":"据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果插入的记录的主键值是依次增大的话，那每插满一个数据页就换到下一个数据页继续插，而如果插入的主键值忽大忽小的话，可能需要页面分裂和记录移位。意味着：性能损耗。\n最好让插入的记录的主键值依次递增，这样就不会发生这样的性能损耗了。所以建议：让主键具有 AUTO_INCREMENT，让存储引擎自己为表生成主键，而不是手动插入。","代码先行索引后上#代码先行，索引后上":"一般应该等到主体业务功能开发完毕，把涉及到该表相关 SQL 都要拿出来分析之后再建立索引。","优先-where#优先 where":"在 where 和 order by 出现索引设计冲突时，到底是针对 where 去设计索引，还是针对 order by 设计索引？\n一般是让 where 条件去使用索引来快速筛选出来一部分指定的数据，接着再进行排序。因为大多数情况基于索引进行 where 筛选往往可以最快速度筛选出你要的少部分数据，然后做排序的成本可能会小很多。","只为用于搜索排序或分组的列创建索引#只为用于搜索、排序或分组的列创建索引":"","基于慢-sql-查询做优化#基于慢 SQL 查询做优化":"可以根据监控后台的一些慢 SQL，针对这些慢 SQL 查询做特定的索引优化。参考 SQL 慢查询。","索引列的类型尽量小#索引列的类型尽量小":"尽量对字段类型较小的列设计索引，因为字段类型较小的话，占用磁盘空间也会比较小。\n以整数类型为例，有 TINYINT、MEDIUMINT、INT、BIGINT 这么几种，它们占用的存储空间依次递增，我们这里所说的类型大小指的就是该类型表示的数据范围的大小。\n在表示的整数范围允许的情况下，尽量让索引列使用较小的类型：\n数据类型越小，在查询时进行的比较操作越快。 数据类型越小，索引占用的存储空间就越少，在一个数据页内就可以放下更多的记录，从而减少磁盘 I/O 带来的性能损耗，也就意味着可以把更多的数据页缓存在内存中，从而加快读写效率。 ","联合索引尽量覆盖条件#联合索引尽量覆盖条件":"可以设计一个或者两三个联合索引(尽量少建单值索引)，让每一个联合索引都尽量去包含 SQL 语句里的 where、order by、group by 的字段，还要确保这些联合索引的字段顺序尽量满足 SQL 查询的最左前缀原则。\n联合索引中的某个字段如果是范围查找，最好把这个字段放在联合索引的最后面。因为一般情况下，范围查找之后的字段就无法走索引了。\n示例：\n-- 联合索引 (province,city,sex) SELECT * FROM users WHERE province = xx AND city = xx AND age \u003c= xx AND age \u003e= xx; 上面的语句 age 是一个范围查找字段，所以最好把它放在联合索引的最后面，即 (province,city,sex,age)。但是由于上面的 SQL 并没有用到 sex 字段，会导致 age 无法走索引，所以可以优化为：\n-- 加上 sex in ('female','male') 的条件 SELECT * FROM users WHERE province = xx AND city = xx AND sex in ('female','male') AND age \u003c= xx AND age \u003e= xx; 假设还有一个筛选条件，要筛选最近一周登录过的用户，对应后台 SQL 可能是这样：\nwhere province=xx and city=xx and sex in ('female','male') and age\u003e=xx and age\u003c=xx and latest_login_time\u003e= xx latest_login_time 也是一个范围查找字段，如果把它放在联合索引里，如 (province,city,sex,hobby,age,latest_login_time)，age 和 latest_login_time 两个范围查找，显然是不行的。可以换一种思路，设计一个字段 is_login_in_latest_7_days，用户如果一周内有登录值就为 1，否则为 0，那么就可以把索引设计成 (province,city,sex,hobby,is_login_in_latest_7_days,age) 来满足上面那种场景。","长字符串可以采用前缀索引#长字符串可以采用前缀索引":"假设字符串很长，那存储一个字符串就需要占用很大的存储空间。\n例如，varchar(100) 这种大字段建立索引，可以稍微优化下，比如针对这个字段的前 20 个字符建立索引，就是说，对这个字段里的每个值的前 20 个字符放在索引树里。\n这样在根据 name 字段来搜索记录时虽然不能精确的定位到记录的位置，但是能定位到相应前缀所在的位置，然后根据前缀相同的记录的主键值回表查询完整的字符串值，再对比就好了。\nCREATE TABLE person_info( name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, KEY idx_name_birthday_phone_number (name(10), birthday, phone_number) ); 但是对于 order by name，那么此时 name 因为在索引树里仅仅包含了前 20 个字符，无法对后边的字符不同的记录进行排序，group by 也是同理。"},"title":"索引设计"},"/db-learn/docs/mysql/practice/03_index_optimization/":{"data":{"":"","explain-分析#Explain 分析":"Explain 关键字可以模拟优化器执行 SQL 语句，分析查询语句或是结构的性能瓶颈。\n在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返回执行计划的信息，而不是执行这条 SQL。\n注意：如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中。\n创建示例表：\nDROP TABLE IF EXISTS `actor`; CREATE TABLE `actor` ( `id` int(11) NOT NULL, `name` varchar(45) DEFAULT NULL, `update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,'a','2017-12-22 15:27:18'), (2,'b','2017-12-22 15:27:18'), (3,'c','2017-12-22 15:27:18'); DROP TABLE IF EXISTS `film`; CREATE TABLE `film` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film` (`id`, `name`) VALUES (3,'film0'),(1,'film1'),(2,'film2'); DROP TABLE IF EXISTS `film_actor`; CREATE TABLE `film_actor` ( `id` int(11) NOT NULL, `film_id` int(11) NOT NULL, `actor_id` int(11) NOT NULL, `remark` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_film_actor_id` (`film_id`,`actor_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1); Explain 列 id select 查询的序列号，有几个 select 就有几个 id，并且 id 的顺序是按 select 出现的顺序增长的。id 列越大执行优先级越高，id 相同则从上往下执行，id 为 NULL 最后执行。\nselect_type 表示对应行是简单还是复杂的查询。\nsimple：简单的 select 查询，查询中不包含子查询或者 UNION。 explain select * from film where id = 2; primary：查询中若包含任何复杂的子部分，最外层查询则被标记为 primary。 subquery：包含在 select 中的子查询（不在 from 子句中） derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表。 -- 关闭衍生表的合并优化 set session optimizer_switch='derived_merge=off'; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der; -- 还原默认配置 set session optimizer_switch='derived_merge=on'; 可以看出来：\n(select 1 from actor where id = 1) 就是 id 为 2， 类型为 subquery 的 select。 (select * from film where id = 1) 就是 id 为 3，类型为 derived 的 select。将查询语句的结果集放到一个临时表中。 最外层的 select 就是 id 为 1，类型为 primary。这个语句的 table 的值为 \u003cdirived3\u003e，表示这个查询是从衍生表中查询的。\u003cdirived3\u003e 中的 3 表示 id 为 3 的 select。 union：在 union 中的第二个和随后的 select type 这一列表示关联类型或访问类型，即 MySQL 决定如何查找表中的行，查找数据行记录的大概范围。依次从最优到最差分别为：system \u003e const \u003e eq_ref \u003e ref \u003e range \u003e index \u003e ALL。一般来说，要保证查询达到 range 级别，最好达到 ref。\nNULL：MySQL 优化器优化查询语句时，判断不需要再扫描表或着索引树。例如：在索引列中选取最小值，select min(id) from film;，直接从索引树中获取最小值，不需要扫描表。 system，const：主键索引或唯一二级索引的等值查询。MySQL 能对查询的某部分进行优化并将其转化成一个常量，就是说像查询常量一样快。因为表最多有一个匹配行，读取 1 次，所以速度很快。system 是 const 类型的特列，表示要查询的表或者结果集中只有一条数据。 EXPLAIN SELECT * FROM (SELECT * FROM film WHERE id = 1) tmp; SHOW WARNINGS; SHOW WARNINGS 的 message 可以看到表示查询被优化成了 select 1 AS 'id','film1' AS 'name' from dual。\neq_ref：主键索引或唯一二级索引的所有列都被被连接使用，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种类型。 EXPLAIN SELECT * FROM film_actor LEFT JOIN film ON film_actor.film_id = film.id; 可以看到，两个 select 的 id 都是 1，说明关联的两张表没有明确的前后顺序，会一起去查询，不过真正执行的时候，会先执行上面的 select。\nfilm 表对应的 type 是 eq_ref，因为关联的条件使用的是 film 表的主键 id，所以会使用主键索引来查询数据。使用主键来查询，只会返回一条记录，速度还是很快的。 ref：相比 eq_ref，不使用唯一索引，而是使用二级索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。 -- name 是普通索引 EXPLAIN SELECT * FROM film WHERE name = 'film1'; range：利用索引进行范围匹配。范围扫描通常出现在 in(), between ,\u003e ,\u003c, \u003e= 等操作中。 EXPLAIN SELECT * FROM actor WHERE id \u003e 1; index：全索引扫描，一般是扫描某个二级索引，直接对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这种通常比 ALL 快一些。 EXPLAIN SELECT * FROM film; 这里扫描的是二级索引 idx_name，没有去扫描聚簇索引。MySQL 在优化时，如果查询的字段在二级索引中全部都有，会优先使用二级索引，而不会去扫描聚簇索引。因为一般情况下，聚簇索引的叶子节点存储的是完整的行数据，所要扫描的数据量会比较大，而二级索引的叶子节点存储的是主键值，所以扫描的行数会比较少。\nALL：Full Table Scan，即全表扫描，直接扫描聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了。 EXPLAIN SELECT * FROM actor; possible_keys 查询时，可能使用的索引。如果 possible_keys 有列，而 key 为 NULL，这种情况是因为表中数据不多，MySQL 认为索引对此查询帮助不大，选择了全表查询。\n如果 possible_keys 为 NULL，则没有相关的索引。在这种情况下，可以考虑创造一个适当的索引来提高查询性能。\nkey 实际使用的索引。如果为 NULL，则没有使用索引。如果想强制 MySQL 使用或忽视 possible_keys 列中的索引，可以在查询中使用 force index、ignore index。\nkey_len 显示 MySQL 在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。例如，film_actor 的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个 int 列组成，并且每个 int 是 4 字节。通过结果中的 key_len=4 可推断出查询使用了第一个列：film_id 列来执行索引查找。\nkey_len 计算规则如下：\n字符串，char(n) 和 varchar(n)，n 均代表字符数，而不是字节数，如果是 utf-8，一个数字或字母占 1 个字节，一个汉字占 3 个字节 char(n)：如果存汉字长度就是 3n 字节 varchar(n)：如果存汉字则长度是 3n + 2 字节，加的 2 字节用来存储字符串长度，因为 varchar 是变长字符串 数值类型 tinyint：1 字节 smallint：2 字节 int：4 字节 bigint：8 字节　时间类型　date：3 字节 timestamp：4 字节 datetime：8 字节 如果字段允许为 NULL，需要 1 字节记录是否为 NULL 索引最大长度是 768 字节，当字符串过长时，MySQL 会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引 ref 显示索引的哪些列或常量被使用了。\nrows 表示 MySQL 估计要读取并检测的行数，注意这个不是结果集里的行数。\nfiltered 该列是一个百分比的值，rows*filtered/100 可以估算出将要和 explain 中前一个表进行连接的行数。\nextra 展示的是额外信息：\nUsing index：使用覆盖索引，避免访问了表的数据行，效率不错。 Using where：表示使用了 where 过滤，并且查询的列未被索引覆盖。 Usering index condition：表示使用了索引下推优化。 Using temporary：要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化。例如 EXPLAIN SELECT DISTINCT name FROM actor; actor.name 没有索引，此时创建了张临时表来 distinct。可以为 name 列创建索引，然后再去重，MySQL 在扫描索引树的过程中就可以直接去重。因为索引是有序的，相同的记录是在一起的，相同的记录直接扔掉就可以了。 Using filesort：将用外部排序而不是索引排序，数据较小时在内存排序，否则需要在磁盘完成排序。这种情况下一般也是要考虑使用索引来优化。索引本身就是排好序的。 Using join buffer：使用连接缓存。 Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段。 table 表示 explain 的一行正在访问哪个表。\npartitions 如果查询是基于分区表的话，partitions 字段会显示查询将访问的分区。\nUsing filesort 文件排序原理详解 文件排序方式：\n单路排序：是一次性取出（聚簇索引）满足条件行的所有字段，然后在 sort buffer 中进行排序；trace 工具可以看到 sort_mode 信息里显示 \u003csort_key, additional_fields\u003e 或者 \u003csort_key,packed_additional_fields\u003e，sort_key 就表示排序的 key，additional_fields 表示表中的其他字段。 双路排序（又叫回表排序模式）：是首先根据相应的条件取出（聚簇索引）相应的排序字段和可以直接定位行数据的主键 ID，然后在 sort buffer 中进行排序，排序完后需要回表去取回其它需要的字段；trace 工具可以看到 sort_mode 信息里显示 \u003csort_key, rowid\u003e，sort_key 就表示排序的 key，rowid 表示主键 ID。占用内存空间小，但是需要多回表一次。 判断使用哪种排序模式：\n如果字段的总长度（表中所有的列）小于 max_length_for_sort_data ，那么使用单路排序模式； 如果字段的总长度大于 max_length_for_sort_data ，那么使用双路排序模式。 如果 MySQL 排序内存 sort_buffer 配置的比较小并且没有条件继续增加了，可以适当把 max_length_for_sort_data 配置小点，让优化器选择使用双路排序算法，可以在 sort_buffer 中一次排序更多的行，只是需要再根据主键回到原表取数据。\n如果 MySQL 排序内存有条件可以配置比较大，可以适当增大 max_length_for_sort_data 的值，让优化器优先选择全字段排序(单路排序)，把需要的字段放到 sort_buffer 中，这样排序后就会直接从内存里返回查询结果了。\n所以，MySQL 通过 max_length_for_sort_data 这个参数来控制排序，在不同场景使用不同的排序模式，从而提升排序效率。\n注意，如果全部使用 sort_buffer 内存排序一般情况下效率会高于磁盘文件排序，但不能因为这个就随便增大 sort_buffer(默认 1M)，MySQL 很多参数设置都是做过优化的，不要轻易调整。\nℹ️ 磁盘排序，最终还是要加载到内存中进行排序的，只不过由于数据量太大，需要先创建临时文件，然后在一块更大的内存中再加载临时文件进行排序，不会在 sort_buffer 中进行排序了。 ","查询优化#查询优化":"常见的分页场景优化技巧 select * from employees limit 10000,10; 从表 employees 中取出从 10001 行开始的 10 行记录。看似只查询了 10 条记录，实际这条 SQL 是先读取 10010 条记录，然后抛弃前 10000 条记录，然后返回后面 10 条想要的数据。因此要查询一张大表比较靠后的数据，执行效率是非常低的。\nℹ️ limit 的执行过程，Server 层向 InnoDB 要第 1 条记录，InnoDB 得到完整的聚簇索引记录，然后返回给 Server 层。Server 将其发送给客户端之前，发现判断 limit 10000,10 的要求，意味着符合条件的记录中的第 10001 条才可以真正发送给客户端，所以在这里先做个统计。假设 Server 层维护了一个称作 limit_count 的变量用于统计已经跳过了多少条记录，此时就应该将 limit_count 设置为 1。\nServer 层再向 InnoDB 要下一条记录，limit_count 变为了 2。重估上面的操作。直到 limit_count 等于 10010 的时候，Server 层才会真正的将 InnoDB 返回的完整聚簇索引记录发送给客户端。\n自增且连续的主键排序的分页查询 select * from employees limit 90000,5; -- 优化为 select * from employees where id \u003e 90000 limit 5; 但是，这条改写的 SQL 在很多场景并不实用，因为表中可能某些记录被删后，主键空缺，导致结果不一致。\n根据非主键字段排序的分页查询 -- 联合索引 (name,age,position) -- 该 sql 没有使用 name 字段的索引，因为查找联合索引的结果集太大，并回表的成本比扫描全表的成本更高，所以优化器放弃使用索引。 select * from employees ORDER BY name limit 90000,5; -- 关键是让排序时返回的字段尽可能少，所以可以让排序和分页操作先查出主键，然后根据主键查到对应的记录，SQL 改写如下 -- 优化为 select * from employees e inner join (select id from employees order by name limit 90000,5) ed on e.id = ed.id; 优化后的语句全部都走了索引，其中 (select id from employees order by name limit 90000,5) 使用了覆盖索引来优化，查询的字段只有 id 字段，而且排好了序。(select id from employees order by name limit 90000,5) ed 产生的临时表只有 5 条记录，然后再根据主键 id 去 employees 表中查询对应的记录。\nJOIN 关联查询优化 测试数据：\n-- 示例表： CREATE TABLE `t1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_a` (`a`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; create table t2 like t1; -- 插入一些示例数据 -- 往t1表插入1万行记录 DROP PROCEDURE IF EXISTS insert_t1; DELIMITER ;; CREATE PROCEDURE insert_t1() BEGIN DECLARE i INT; SET i = 1; WHILE i \u003c= 10000 DO INSERT INTO t1(a, b) VALUES(i, i); SET i = i + 1; END WHILE; END;; DELIMITER ; CALL insert_t1(); -- 往t2表插入100行记录 DROP PROCEDURE IF EXISTS insert_t2; DELIMITER ;; CREATE PROCEDURE insert_t2() BEGIN DECLARE i INT; SET i = 1; WHILE i \u003c= 100 DO INSERT INTO t2(a, b) VALUES(i, i); SET i = i + 1; END WHILE; END;; DELIMITER ; CALL insert_t2(); MySQL 的表关联常见有两种算法：\nNested-Loop Join 算法 Block Nested-Loop Join 算法 1. Nested-Loop Join 算法 一次一行循环地从第一张表（称为驱动表）中读取行，在这行数据中取到关联字段，根据关联字段在另一张表（被驱动表）里取出满足条件的行，然后取出两张表的结果合集。\nEXPLAIN select * from t1 inner join t2 on t1.a= t2.a; 从执行计划中可以看到：\n驱动表是 t2，被驱动表是 t1。先执行的就是驱动表（执行计划结果的 id 如果一样则按从上到下顺序执行 sql）； 如果执行计划 Extra 中未出现 Using join buffer 则表示使用的 join 算法是 NLJ。 ℹ️ 优化器一般会优先选择小表做驱动表。所以使用 inner join 时，排在前面的表并不一定就是驱动表 SQL 的大致流程如下：\n从表 t2 中读取一行数据（如果 t2 表有查询过滤条件的，会从过滤结果里取出一行数据）； 从第 1 步的数据中，取出关联字段 a，到表 t1 中查找； 取出表 t1 中满足条件的行，跟 t2 中获取到的结果合并，作为结果返回给客户端； 重复上面 3 步。 整个过程会读取 t2 表的所有数据(扫描 100 行)，然后遍历这每行数据中字段 a 的值，根据 t2 表中 a 的值索引扫描 t1 表中的对应行(扫描 100 次 t1 表的索引，1 次扫描可以认为最终只扫描 t1 表一行完整数据，也就是总共 t1 表也扫描了 100 行)。因此整个过程扫描了 200 行。\n如果被驱动表的关联字段没索引，那就需要去聚簇索引扫描全表，所以使用 NLJ 算法性能会比较低，MySQL 会选择 Block Nested-Loop Join 算法。\n2. Block Nested-Loop Join 算法 把驱动表的数据读入到 join_buffer 中，然后扫描被驱动表，把被驱动表每一行取出来跟 join_buffer 中的所有数据一起做对比。\nEXPLAIN select * from t1 inner join t2 on t1.b = t2.b; Extra 中 的 Using join buffer (Block Nested Loop) 说明该关联查询使用的是 BNL 算法。\nSQL 的大致流程如下：\n把 t2 的所有数据放入到 join_buffer 中 把表 t1 中每一行取出来，跟 join_buffer 中的所有数据做对比 返回满足 join 条件的数据。 整个过程对表 t1 和 t2 都做了一次全表扫描，因此扫描的总行数为 10000 (表 t1 的数据总量) + 100 (表 t2 的数据总量) = 10100。并且 join_buffer 里的数据是无序的，因此对表 t1 中的每一行，都要做 100 次判断，所以内存中的判断次数是 100 * 10000= 100 万次。\n被驱动表的关联字段没索引为什么要选择使用 BNL？ 如果上面第二条 SQL 使用 Nested-Loop Join，由于没有 t1.b 是没有索引的，意味这要进行全表扫描，10000 行扫描 100 次就是 100 * 10000 = 100 万行。很显然，用 BNL 磁盘扫描次数少很多，相比于磁盘扫描，BNL 的内存计算会快得多。\n对于关联 SQL 的优化 关联字段加索引，让 MySQL 做 join 操作时尽量选择 NLJ 算法 小表驱动大表，写多表连接 SQL 时如果明确知道哪张表是小表可以用 straight_join 写法固定连接驱动方式，省去 mysql 优化器自己判断的时间。 straight_join 功能同 join 类似，但能让左边的表来驱动右边的表，能改表优化器对于联表查询的执行顺序。比如：select * from t2 straight_join t1 on t2.a = t1.a; 代表指定 MySQL 选择 t2 表作为驱动表。straight_join只适用于 inner join，并不适用于 left join，right join。（因为 left join，right join 已经指定了表的执行顺序）。尽可能让优化器去判断，因为大部分情况下 MySQL 优化器是比人要聪明的。使用 straight_join 一定要慎重，因为部分情况下人为指定的执行顺序并不一定会比优化引擎要靠谱。\n对于小表定义的明确：\n在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。\njoin buffer：\n当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录只会和驱动表结果集的一条记录做匹配，之后就会被从内存中清除掉。然后再从驱动表结果集中拿出另一条记录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从磁盘上加载到内存中多少次。所以可以在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载被驱动表的代价了。\njoin buffer 就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个 join buffer 中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和 join buffer 中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的 I/O 代价。\njoin_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t2 的所有数据话，策略很简单，就是分段放。\n比如 t2 表有 1000 行记录， join_buffer 一次只能放 800 行数据，那么执行过程就是先往 join_buffer 里放 800 行记录，然后从 t1 表里取数据跟 join_buffer 中数据对比得到部分结果，然后清空 join_buffer，再放入 t2 表剩余 200 行记录，再次从 t1 表里取数据跟 join_buffer 中数据对比。所以就多扫了一次 t1 表。\nℹ️ 注意，驱动表的记录并不是所有列都会被放到 join_buffer 中，只有查询列表中的列和过滤条件中的列才会被放到 join_buffer 中，所以，最好不要把 * 作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在 join buffer 中放置更多的记录。 Hash Join 原理（仅支持等值连接） MySQL 中的 Hash Join 是一种高效的等值连接算法，尤其适合没有索引、表不太大或临时表操作的场景。\nSELECT * FROM t1 JOIN t2 ON t1.id = t2.id; Hash Join 分两个阶段进行：\nBuild Phase（构建哈希表） 优化器选择较小的一张表（如 t2）作为构建表（build input）。 把这张表的连接列（如 t2.id）作为 key，构建哈希表，存入内存。 Probe Phase（探测匹配项） 遍历另一张较大的表 t1。 以连接键 t1.id 去刚刚构建的哈希表中查找匹配项。 -- 探测并输出结果： for each row in t1: if hash_table contains t1.id: output (t1, hash_table[t1.id]) 相对于传统的 Nested Loop Join（嵌套循环），Hash Join 将连接时间复杂度从 O(n*m) 降低到接近 O(n + m)，适合无索引的中小表等值连接。\n限制：\n只支持等值连接，例如 ON a.id = b.id，不能用范围条件如 \u003e、\u003c。 大表内存不够会溢出，如果构建的哈希表过大，会使用磁盘上的临时表，性能降低 不在索引列上做任何操作（计算、函数、（自动 or 手动）类型转换），会导致索引失效而转向全表扫描 SELECT * FROM person_info WHERE left(name,3) = 'LiLei'; 索引列上做了函数操作，得到的结果在索引树上是无法匹配的，所以索引失效了。left(name,3) 有点类似 LiL% 的效果，但是 MySQL 并没有对这种情况做优化，所以索引失效了。\n范围查询优化 select * from employees where id \u003e=1 and id \u003c=20000; mysql 内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引。比如这个例子，可能是由于单次数据量查询过大导致优化器最终选择不走索引。\n优化方法：可以将大的范围拆分成多个小范围：\nselect * from employees where id \u003e=1 and id \u003c=5000; select * from employees where id \u003e=5001 and id \u003c=10000; select * from employees where id \u003e=10001 and id \u003c=15000; select * from employees where id \u003e=15001 and id \u003c=20000; 让索引列在比较表达式中单独出现 假设为整数列 my_col 建立索引：\nWHERE my_col * 2 \u003c 4 是以 my_col * 2 这样的表达式的形式出现的，存储引擎会依次遍历所有的记录，计算这个表达式的值是不是小于 4。\nWHERE my_col \u003c 4/2 my_col 列是以单独列的形式出现的，这样的情况可以直接使用 B+ 树索引。\n如果索引列在比较表达式中不是以单独列的形式出现，而是以某个表达式，或者函数调用形式出现的话，是用不到索引的。\nin 和 exsits 优化 原则：小表驱动大表。\nin：当 B 表的数据集小于 A 表的数据集时，in 优于 exists。 例如：select * from A where id in (select id from B)：\n# 等价于： for(select id from B){ select * from A where A.id = B.id } exists：当 B 表的数据集大于 A 表的数据集时，exists 优于 in。 例如：select * from A where exists (select 1 from B where B.id = A.id)。\nEXISTS (subquery) 只返回 TRUE 或 FALSE，因此子查询中的 SELECT * 也可以用 SELECT 1 替换，官方说法是实际执行时会忽略 SELECT 清单，因此没有区别。\n#等价于： for(select * from A){ select * from B where B.id = A.id } count(*) 查询优化 EXPLAIN select count(1) from employees; EXPLAIN select count(id) from employees; EXPLAIN select count(name) from employees; EXPLAIN select count(*) from employees; 四个 SQL 的执行计划是一样的，也就说明这四个 SQL 执行效率应该差不多。\ncount(*) MySQL 是专门做了优化，并不会把全部字段取出来，不取值，按行累加，效率很高。\ncount(1) 跟 count(字段) 执行过程类似，不过 count(1) 是用常量 1 做统计，count(字段) 还需要取出字段，所以理论上 count(1) 比 count(字段) 会快一点。\ncount(*)、count(1) 或者任意的 count(常数)：对于 count(*)、count(1) 或者任意的 count(常数) 来说，读取哪个索引的记录其实并不重要，因为 Server 层只关心存储引擎是否读到了记录，而并不需要从记录中提取指定的字段来判断是否为 NULL。所以优化器会使用占用存储空间最小的那个索引（主键索引包含完整的记录，占用的存储空间是最大的）来执行查询。 count(主键 id)：对于 count(主键 id) 来说，由于 id 是主键，不论是聚簇索引记录，还是任意一个二级索引记录中都会包含主键字段，所以其实读取任意一个索引中的记录都可以获取到 id 字段，此时优化器也会选择占用存储空间最小的那个索引来执行查询。 count(字段)：如果字段有索引，指定的字段可能并不会包含在每一个索引中。优化器只能选择包含指定字段的索引去执行查询，这就可能导致优化器选择的索引并不是最小的那个。如果字段没有索引，就无法使用索引优化查询了，只能选择全表扫描。 所以查询效率：count(*)≈count(1)\u003ecount(主键 id)\u003e=count(字段)，count(字段) 和 count(主键 id) 还需要取出字段判断是否为 NULL（虽然主键不会为 NULL，但优化器仍会检查），所以效率会比 count(*) 和 count(1) 低。如果 count(字段) 有索引，并且是占用存储空间最小的那个索引，那么效率会和 count(主键 id) 差不多。\n为什么对于 count(id)，MySQL 最终选择辅助索引而不是主键聚集索引？ 因为二级索引相对主键索引存储数据更少，检索性能应该更高。\n不带 WHERE 条件的常见优化方法 对于 MyISAM 存储引擎的表做不带 where 条件的 count 查询性能是很高的，因为 MyISAM 存储引擎的表的总行数会被 MySQL 存储在磁盘上，查询不需要计算。 show table status 可以看到表的行数，但是这个行数是不准确的。性能很高。例如 show table status like 'employees'。 将总数维护到 Redis 里，插入或删除表数据行的时候同时维护 Redis 里的表总行数 key 的计数值(用 incr 或 decr 命令)，但是这种方式可能不准，很难保证表操作和 Redis 操作的事务一致性。 增加数据库计数表，插入或删除表数据行的时候同时维护计数表，让他们在同一个事务里操作。 索引选择异常和处理 一种方法是，采用 force index 强行选择一个索引。\nset long_query_time=0; select * from t where a between 10000 and 20000; /*Q1*/ select * from t force index(a) where a between 10000 and 20000;/*Q2*/ 第二种方法就是，可以考虑修改语句，引导 MySQL 使用我们期望的索引。 第三种方法是，在有些场景下，可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。"},"title":"Explain 和查询优化"},"/db-learn/docs/mysql/practice/04_config_8/":{"data":{"":"","80-新特性#8.0 新特性":"支持降序索引 # ====MySQL 5.7演示==== mysql\u003e create table t1(c1 int,c2 int,index idx_c1_c2(c1,c2 desc)); Query OK, 0 rows affected (0.04 sec) mysql\u003e insert into t1 (c1,c2) values(1, 10),(2,50),(3,50),(4,100),(5,80); Query OK, 5 rows affected (0.02 sec) mysql\u003e show create table t1\\G *************************** 1. row *************************** Table: t1 Create Table: CREATE TABLE `t1` ( `c1` int(11) DEFAULT NULL, `c2` int(11) DEFAULT NULL, KEY `idx_c1_c2` (`c1`,`c2`) --注意这里，c2字段是升序 ) ENGINE=InnoDB DEFAULT CHARSET=latin1 1 row in set (0.00 sec) mysql\u003e explain select * from t1 order by c1,c2 desc; --5.7也会使用索引，但是Extra字段里有filesort文件排序 +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ | 1 | SIMPLE | t1 | NULL | index | NULL | idx_c1_c2 | 10 | NULL | 1 | 100.00 | Using index; Using filesort | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ 1 row in set, 1 warning (0.01 sec) # ====MySQL 8.0演示==== mysql\u003e create table t1(c1 int,c2 int,index idx_c1_c2(c1,c2 desc)); Query OK, 0 rows affected (0.02 sec) mysql\u003e insert into t1 (c1,c2) values(1, 10),(2,50),(3,50),(4,100),(5,80); Query OK, 5 rows affected (0.02 sec) mysql\u003e show create table t1\\G *************************** 1. row *************************** Table: t1 Create Table: CREATE TABLE `t1` ( `c1` int DEFAULT NULL, `c2` int DEFAULT NULL, KEY `idx_c1_c2` (`c1`,`c2` DESC) --注意这里的区别，降序索引生效了 ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci 1 row in set (0.00 sec) mysql\u003e explain select * from t1 order by c1,c2 desc; --Extra字段里没有filesort文件排序，充分利用了降序索引 +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+ | 1 | SIMPLE | t1 | NULL | index | NULL | idx_c1_c2 | 10 | NULL | 1 | 100.00 | Using index | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+ 1 row in set, 1 warning (0.00 sec) mysql\u003e explain select * from t1 order by c1 desc,c2; --Extra字段里有Backward index scan，意思是反向扫描索引; +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+----------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+----------------------------------+ | 1 | SIMPLE | t1 | NULL | index | NULL | idx_c1_c2 | 10 | NULL | 1 | 100.00 | Backward index scan; Using index | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+----------------------------------+ 1 row in set, 1 warning (0.00 sec) mysql\u003e explain select * from t1 order by c1 desc,c2 desc; --Extra字段里有filesort文件排序，排序必须按照每个字段定义的排序或按相反顺序才能充分利用索引 +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ | 1 | SIMPLE | t1 | NULL | index | NULL | idx_c1_c2 | 10 | NULL | 1 | 100.00 | Using index; Using filesort | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ 1 row in set, 1 warning (0.00 sec) mysql\u003e explain select * from t1 order by c1,c2; --Extra字段里有filesort文件排序，排序必须按照每个字段定义的排序或按相反顺序才能充分利用索引 +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ | 1 | SIMPLE | t1 | NULL | index | NULL | idx_c1_c2 | 10 | NULL | 1 | 100.00 | Using index; Using filesort | +----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-----------------------------+ 1 row in set, 1 warning (0.00 sec) group by 不再隐式排序 对于 group by 字段不再隐式排序，如需要排序，必须显式加上 order by 子句。\n# ====MySQL 5.7演示==== mysql\u003e select count(*),c2 from t1 group by c2; +----------+------+ | count(*) | c2 | +----------+------+ | 1 | 10 | | 2 | 50 | | 1 | 80 | | 1 | 100 | +----------+------+ 4 rows in set (0.00 sec) # ====MySQL 8.0演示==== mysql\u003e select count(*),c2 from t1 group by c2; --8.0 版本 group by 不再默认排序 +----------+------+ | count(*) | c2 | +----------+------+ | 1 | 10 | | 2 | 50 | | 1 | 100 | | 1 | 80 | +----------+------+ 4 rows in set (0.00 sec) mysql\u003e select count(*),c2 from t1 group by c2 order by c2; --8.0 版本 group by 不再默认排序，需要自己加 order by +----------+------+ | count(*) | c2 | +----------+------+ | 1 | 10 | | 2 | 50 | | 1 | 80 | | 1 | 100 | +----------+------+ 4 rows in set (0.00 sec) 支持隐藏索引 使用 invisible 关键字在创建表或者进行表变更中设置索引为隐藏索引。索引隐藏只是不可见，但是数据库后台还是会维护隐藏索引的，在查询时优化器不使用该索引，即使使用 force index，优化器也不会使用该索引，同时优化器也不会报索引不存在的错误，因为索引仍然真实存在，必要时，也可以把隐藏索引快速恢复成可见。注意，主键不能设置为 invisible。\n比如我们觉得某个索引没用了，删除后发现这个索引在某些时候还是有用的，于是又得把这个索引加回来，如果表数据量很大的话，这种操作耗费时间是很多的，成本很高，这时，可以将索引先设置为隐藏索引，等到真的确认索引没用了再删除。\n# 创建 t2 表，里面的 c2 字段为隐藏索引 mysql\u003e create table t2(c1 int, c2 int, index idx_c1(c1), index idx_c2(c2) invisible); Query OK, 0 rows affected (0.02 sec) mysql\u003e show index from t2\\G *************************** 1. row *************************** Table: t2 Non_unique: 1 Key_name: idx_c1 Seq_in_index: 1 Column_name: c1 Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: Visible: YES Expression: NULL *************************** 2. row *************************** Table: t2 Non_unique: 1 Key_name: idx_c2 Seq_in_index: 1 Column_name: c2 Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: Visible: NO --隐藏索引不可见 Expression: NULL 2 rows in set (0.00 sec) mysql\u003e explain select * from t2 where c1=1; +----+-------------+-------+------------+------+---------------+--------+---------+-------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+--------+---------+-------+------+----------+-------+ | 1 | SIMPLE | t2 | NULL | ref | idx_c1 | idx_c1 | 5 | const | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+---------------+--------+---------+-------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) mysql\u003e explain select * from t2 where c2=1; --隐藏索引c2不会被使用 +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | t2 | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ 1 row in set, 1 warning (0.00 sec) mysql\u003e select @@optimizer_switch\\G --查看各种参数 *************************** 1. row *************************** @@optimizer_switch: index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on,engine_condition_pushdown=on,index_condition_pushdown=on,mrr=on,mrr_cost_based=on,block_nested_loop=on,batched_key_access=off,materialization=on,semijoin=on,loosescan=on,firstmatch=on,duplicateweedout=on,subquery_materialization_cost_based=on,use_index_extensions=on,condition_fanout_filter=on,derived_merge=on,use_invisible_indexes=off,skip_scan=on,hash_join=on 1 row in set (0.00 sec) mysql\u003e set session optimizer_switch=\"use_invisible_indexes=on\"; ----在会话级别设置查询优化器可以看到隐藏索引 Query OK, 0 rows affected (0.00 sec) mysql\u003e select @@optimizer_switch\\G *************************** 1. row *************************** @@optimizer_switch: index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on,engine_condition_pushdown=on,index_condition_pushdown=on,mrr=on,mrr_cost_based=on,block_nested_loop=on,batched_key_access=off,materialization=on,semijoin=on,loosescan=on,firstmatch=on,duplicateweedout=on,subquery_materialization_cost_based=on,use_index_extensions=on,condition_fanout_filter=on,derived_merge=on,use_invisible_indexes=on,skip_scan=on,hash_join=on 1 row in set (0.00 sec) mysql\u003e explain select * from t2 where c2=1; +----+-------------+-------+------------+------+---------------+--------+---------+-------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+--------+---------+-------+------+----------+-------+ | 1 | SIMPLE | t2 | NULL | ref | idx_c2 | idx_c2 | 5 | const | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+---------------+--------+---------+-------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) mysql\u003e alter table t2 alter index idx_c2 visible; Query OK, 0 rows affected (0.02 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e alter table t2 alter index idx_c2 invisible; Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 支持函数索引 我们知道，如果在查询中加入了函数，索引不生效，所以 MySQL 8 引入了函数索引，MySQL 8.0.13 开始支持在索引中使用函数(表达式)的值。\n函数索引基于虚拟列功能实现，在 MySQL 中相当于新增了一个列，这个列会根据你的函数来进行计算结果，然后使用函数索引的时候就会用这个计算后的列作为索引。\nmysql\u003e create table t3(c1 varchar(10),c2 varchar(10)); Query OK, 0 rows affected (0.02 sec) mysql\u003e create index idx_c1 on t3(c1); --创建普通索引 Query OK, 0 rows affected (0.03 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e create index func_idx on t3((UPPER(c2))); --创建一个大写的函数索引 Query OK, 0 rows affected (0.03 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e show index from t3\\G *************************** 1. row *************************** Table: t3 Non_unique: 1 Key_name: idx_c1 Seq_in_index: 1 Column_name: c1 Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: Visible: YES Expression: NULL *************************** 2. row *************************** Table: t3 Non_unique: 1 Key_name: func_idx Seq_in_index: 1 Column_name: NULL Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: Visible: YES Expression: upper(`c2`) --函数表达式 2 rows in set (0.00 sec) mysql\u003e explain select * from t3 where upper(c1)='ZHUGE'; +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | t3 | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ 1 row in set, 1 warning (0.00 sec) mysql\u003e explain select * from t3 where upper(c2)='ZHUGE'; --使用了函数索引 +----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+ | 1 | SIMPLE | t3 | NULL | ref | func_idx | func_idx | 43 | const | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) 跳过锁等待 对于 select ... for share (8.0 新增加查询共享锁的语法) 或 select ... for update， 在语句后面添加 NOWAIT、SKIP LOCKED 语法可以跳过锁等待，或者跳过锁定。\n在 5.7 及之前的版本，select...for update，如果获取不到锁，会一直等待，直到 innodb_lock_wait_timeout 超时。\n在 8.0 版本，通过添加 nowait，skip locked 语法，能够立即返回。如果查询的行已经加锁，那么 nowait 会立即报错返回，而 skip locked 也会立即返回，只是返回的结果中不包含被锁定的行。应用场景比如查询余票记录，如果某些记录已经被锁定，用 skip locked 可以跳过被锁定的记录，只返回没有锁定的记录，提高系统性能。\n# 先打开一个session1: mysql\u003e select * from t1; +------+------+ | c1 | c2 | +------+------+ | 1 | 10 | | 2 | 50 | | 3 | 50 | | 4 | 100 | | 5 | 80 | +------+------+ 5 rows in set (0.00 sec) mysql\u003e begin; Query OK, 0 rows affected (0.00 sec) mysql\u003e update t1 set c2 = 60 where c1 = 2; --锁定第二条记录 Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 # 另外一个 session2: mysql\u003e select * from t1 where c1 = 2 for update; --等待超时 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction mysql\u003e select * from t1 where c1 = 2 for update nowait; --查询立即返回 ERROR 3572 (HY000): Statement aborted because lock(s) could not be acquired immediately and NOWAIT is set. mysql\u003e select * from t1 for update skip locked; --查询立即返回，过滤掉了第二行记录 +------+------+ | c1 | c2 | +------+------+ | 1 | 10 | | 3 | 50 | | 4 | 100 | | 5 | 80 | +------+------+ 4 rows in set (0.00 sec) innodb_dedicated_server 自适应参数 能够让 InnoDB 根据服务器上检测到的内存大小自动配置 innodb_buffer_pool_size，innodb_log_file_size 等参数，会尽可能多的占用系统可占用资源提升性能。前提是服务器是专用来给 MySQL 数据库的，如果还有其他软件或者资源或者多实例 MySQL 使用，不建议开启该参数，不然会影响其它程序。\nmysql\u003e show variables like '%innodb_dedicated_server%'; --默认是 OFF 关闭，修改为 ON 打开 +-------------------------+-------+ | Variable_name | Value | +-------------------------+-------+ | innodb_dedicated_server | OFF | +-------------------------+-------+ 1 row in set (0.02 sec) 窗口函数 从 MySQL 8.0 开始，新增了一个叫窗口函数的概念，它可以用来实现若干新的查询方式。窗口函数与 SUM()、COUNT() 这种分组聚合函数类似，在聚合函数后面加上 over() 就变成窗口函数了，在括号里可以加上 partition by 等分组关键字指定如何分组，窗口函数即便分组也不会将多行查询结果合并为一行，而是将结果放回多行当中，即窗口函数不需要再使用 GROUP BY。\n# 创建一张账户余额表 CREATE TABLE `account_channel` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '姓名', `channel` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '账户渠道', `balance` int DEFAULT NULL COMMENT '余额', PRIMARY KEY (`id`) ) ENGINE=InnoDB # 插入一些示例数据 INSERT INTO `test`.`account_channel` (`id`, `name`, `channel`, `balance`) VALUES ('1', 'zhuge', 'wx', '100'); INSERT INTO `test`.`account_channel` (`id`, `name`, `channel`, `balance`) VALUES ('2', 'zhuge', 'alipay', '200'); INSERT INTO `test`.`account_channel` (`id`, `name`, `channel`, `balance`) VALUES ('3', 'zhuge', 'yinhang', '300'); INSERT INTO `test`.`account_channel` (`id`, `name`, `channel`, `balance`) VALUES ('4', 'lilei', 'wx', '200'); INSERT INTO `test`.`account_channel` (`id`, `name`, `channel`, `balance`) VALUES ('5', 'lilei', 'alipay', '100'); INSERT INTO `test`.`account_channel` (`id`, `name`, `channel`, `balance`) VALUES ('6', 'hanmeimei', 'wx', '500'); mysql\u003e select * from account_channel; +----+-----------+---------+---------+ | id | name | channel | balance | +----+-----------+---------+---------+ | 1 | zhuge | wx | 100 | | 2 | zhuge | alipay | 200 | | 3 | zhuge | yinhang | 300 | | 4 | lilei | wx | 200 | | 5 | lilei | alipay | 100 | | 6 | hanmeimei | wx | 500 | +----+-----------+---------+---------+ 6 rows in set (0.00 sec) mysql\u003e select name,sum(balance) from account_channel group by name; +-----------+--------------+ | name | sum(balance) | +-----------+--------------+ | zhuge | 600 | | lilei | 300 | | hanmeimei | 500 | +-----------+--------------+ 3 rows in set (0.00 sec) # 在聚合函数后面加上 over() 就变成窗口函数了，后面可以不用再加 group by 制定分组，因为在 over 里已经用 partition 关键字指明了如何分组计算，这种可以保留原有表数据的结构，不会像分组聚合函数那样每组只返回一条数据 mysql\u003e select name,channel,balance,sum(balance) over(partition by name) as sum_balance from account_channel; +-----------+---------+---------+-------------+ | name | channel | balance | sum_balance | +-----------+---------+---------+-------------+ | hanmeimei | wx | 500 | 500 | | lilei | wx | 200 | 300 | | lilei | alipay | 100 | 300 | | zhuge | wx | 100 | 600 | | zhuge | alipay | 200 | 600 | | zhuge | yinhang | 300 | 600 | +-----------+---------+---------+-------------+ 6 rows in set (0.00 sec) mysql\u003e select name,channel,balance,sum(balance) over(partition by name order by balance) as sum_balance from account_channel; +-----------+---------+---------+-------------+ | name | channel | balance | sum_balance | +-----------+---------+---------+-------------+ | hanmeimei | wx | 500 | 500 | | lilei | alipay | 100 | 100 | | lilei | wx | 200 | 300 | | zhuge | wx | 100 | 100 | | zhuge | alipay | 200 | 300 | | zhuge | yinhang | 300 | 600 | +-----------+---------+---------+-------------+ 6 rows in set (0.00 sec) # over() 里如果不加条件，则默认使用整个表的数据做运算 mysql\u003e select name,channel,balance,sum(balance) over() as sum_balance from account_channel; +-----------+---------+---------+-------------+ | name | channel | balance | sum_balance | +-----------+---------+---------+-------------+ | zhuge | wx | 100 | 1400 | | zhuge | alipay | 200 | 1400 | | zhuge | yinhang | 300 | 1400 | | lilei | wx | 200 | 1400 | | lilei | alipay | 100 | 1400 | | hanmeimei | wx | 500 | 1400 | +-----------+---------+---------+-------------+ 6 rows in set (0.00 sec) mysql\u003e select name,channel,balance,avg(balance) over(partition by name) as avg_balance from account_channel; +-----------+---------+---------+-------------+ | name | channel | balance | avg_balance | +-----------+---------+---------+-------------+ | hanmeimei | wx | 500 | 500.0000 | | lilei | wx | 200 | 150.0000 | | lilei | alipay | 100 | 150.0000 | | zhuge | wx | 100 | 200.0000 | | zhuge | alipay | 200 | 200.0000 | | zhuge | yinhang | 300 | 200.0000 | +-----------+---------+---------+-------------+ 6 rows in set (0.00 sec) binlog 日志过期时间精确到秒 在 8.0 版本之前，binlog 日志过期时间设置都是设置 expire_logs_days 参数，单位是天，而在 8.0 版本以后，MySQL 默认使用 binlog_expire_logs_seconds 参数，单位是秒。\n默认字符集由 latin1 变为 utf8mb4 在 8.0 版本之前，默认字符集为 latin1，utf8 指向的是 utf8mb3，8.0 版本默认字符集为 utf8mb4，utf8 默认指向的也是 utf8mb4。\n元数据存储变动 MySQL 8.0 删除了之前版本的元数据文件，例如表结构 .frm 等文件，全部集中放入 mysql.ibd 文件里。\nAUTO_INCREMENT 自增变量持久化 8.0 版本对 AUTO_INCREMENT 值进行了持久化，MySQL 重启后，该值不会改变。\nDDL 原子化 MySQL 8.0 开始支持原子 DDL 操作。\n参数修改持久化 MySQL 8.0 开始支持参数修改持久化，即修改参数后，重启 MySQL 后，参数值不会改变。通过加上 PERSIST 关键字，可以将修改的参数持久化到新的配置文件（mysqld-auto.cnf）中。","常用服务端配置#常用服务端配置":"假设服务器配置为：\nCPU：32 核 内存：64 G DISK：2T SSD 常用的服务端参数在 [mysqld] 标签下。\nmax_connections max_connections=3000 连接的创建和销毁都需要系统资源，比如内存、文件句柄，业务说的支持多少并发，指的是每秒请求数，也就是 QPS。\n一个连接最少占用内存是 256K，最大是 64M，如果一个连接的请求数据超过 64MB（比如排序），就会申请临时空间，放到硬盘上。\n如果 3000 个用户同时连上 MySQL，最小需要内存 3000*256KB=750M，最大需要内存3000*64MB=192G。\n如果 innodb_buffer_pool_size 是 40GB，给操作系统分配 4G，给连接使用的最大内存不到 20G，如果连接过多，使用的内存超过 20G，将会产生磁盘 SWAP，此时将会影响性能。连接数过高，不一定带来吞吐量的提高，而且可能占用更多的系统资源。\nmax_user_connections max_user_connections=2980 允许用户连接的最大数量，剩余连接数用作 DBA 管理\nback_log back_log=300 MySQL 能够暂存的连接数量。如果 MySQL 的连接数达到 max_connections 时，新的请求将会被存在堆栈中，等待某一连接释放资源，该堆栈数量即 back_log，如果等待连接的数量超过 back_log，将被拒绝。\nwait_timeout wait_timeout=300 指的是 app 应用通过 jdbc 连接 MySQL 进行操作完毕后，空闲 300 秒后断开，默认是 28800，单位秒，即 8 个小时。\ninteractive_timeout interactive_timeout=300 指的是 MySQL Client 进行操作完毕后，在 300 秒内没有操作，断开连接，默认是 28800，单位秒，即 8 个小时。\ninnodb_thread_concurrency innodb_thread_concurrency=64 此参数用来设置 InnoDB 线程的并发数，默认值为 0 表示不被限制，若要设置则与服务器的 CPU 核心数相同或是 CPU 的核心数的 2 倍，如果超过配置并发数，则需要排队，这个值不宜太大，不然可能会导致线程之间锁争用严重，影响性能。\ninnodb_buffer_pool_size innodb_buffer_pool_size=40GB InnoDB Buffer Pool 缓存大小，一般为物理内存的 60%-70%，需要留一部分内存给服务器上其他可能会运行的进程。\ninnodb_lock_wait_timeout innodb_lock_wait_timeout=10 InnoDB 锁等待超时时间，默认是 50s。一般来说 50s 太长了，根据公司业务定，没有标准值。\nsync_binlog 和 innodb_flush_log_at_trx_commit 参考 MySQL 日志机制。一般对数据比较敏感的业务，比如金融、电商等，这两个值都会设置为 1。\nsort_buffer_size sort_buffer_size=4M 每个需要排序的线程分配该大小的一个缓冲区。增加该值可以加速 ORDER BY 或 GROUP BY 操作。\nsort_buffer_size 是一个 connection 级的参数，在每个 connection（session）第一次需要使用这个 buffer 的时候，一次性分配设置的内存。\nsort_buffer_size 并不是越大越好，由于是connection级的参数，过大的设置+高并发可能会耗尽系统的内存资源。例如：500 个连接将会消耗 500*sort_buffer_size(4M)=2G。\njoin_buffer_size join_buffer_size=4M 于表关联缓存的大小，和 sort_buffer_size 一样，该参数对应的分配内存也是每个连接独享。"},"title":"常用配置和 8.0 特性"},"/db-learn/docs/mysql/practice/05_cluster/":{"data":{"":"","主从复制延迟优化#主从复制延迟优化":"MySQL 数据库中，大事务除了会导致提交速度变慢，还会导致主从复制延迟。为什么说会导致主从复制延迟？假设一个大事务运行了十分钟，那么在从服务器上也需要运行十分钟回放这个大事务。也就是说 从服务器可能需要十分钟才能追上主服务器。主从服务器之间的数据就产生了延迟。\n优化：\n把大事务拆分成小事务，可以避免主从复制延迟， 设置复制回放相关的配置参数。 要彻底避免 MySQL 主从复制延迟，数据库版本至少要升级到 5.7，因为之前的 MySQL 版本从机回放二进制都是单线程的（5.6 是基于库级别的单线程）。从 MySQL 5.7 版本开始，MySQL 支持了从机多线程回放二进制日志的方式，通常把它叫作并行复制，官方文档中称为 “Multi-Threaded Slave（MTS）”。\nMySQL 的从机并行复制有两种模式。\nCOMMIT ORDER：主机怎么并行，从机就怎么并行。从机完全根据主服务的并行度进行回放。理论上来说，主从延迟极小。但如果主服务器上并行度非常小，事务并不小，比如单线程每次插入 1000 条记录，则从机单线程回放，也会存在一些复制延迟的情况。 WRITESET：基于每个事务，只要事务更新的记录不冲突，就可以并行。以 “单线程每次插入 1000 条记录” 为例，如果插入的记录没有冲突，比如唯一索引冲突，那么虽然主机是单线程，但从机可以是多线程并行回放！！！ 启用 WRITESET 复制模式：\nbinlog_transaction_dependency_tracking = WRITESET transaction_write_set_extraction = XXHASH64 slave-parallel-type = LOGICAL_CLOCK slave-parallel-workers = 16 ","主从集群#主从集群":"搭建 MySQL 服务 准备两台服务器，用来搭建一个 MySQL 的服务集群。两台服务器均安装 CentOS7 操作系统。MySQL 版本采用 mysql-8.0.20 版本。\n两台服务器的 IP 分别为 192.168.232.128 和 192.168.232.129。其中 128 服务器规划为 MySQL 主节点，129 服务器规划为 MySQL 的从节点。\n在两台服务器上分别安装 MySQL 服务。\ngroupadd mysql useradd -r -g mysql -s /bin/false mysql # 这里是创建一个 mysql 用户用于承载 mysql 服务，但是不需要登录权限。 tar -zxvf mysql-8.0.20-el7-x86_64.tar.gz #解压 ln -s mysql-8.0.20-el7-x86_64 mysql #建立软链接 cd mysql mkdir mysql-files chown mysql:mysql mysql-files chmod 750 mysql-files bin/mysqld --initialize --user=mysql # 初始化 mysql 数据文件 注意点1 bin/mysql_ssl_rsa_setup bin/mysqld_safe --user=mysql cp support-files/mysql.server /etc/init.d/mysql.server 注意点：\n初始化过程中会初始化一些 MySQL 的数据文件，经常会出现一些文件或者文件夹权限不足的问题。如果有文件权限不足的问题，需要根据他的报错信息，创建对应的文件或者文件夹，并配置对应的文件权限。\n初始化过程如果正常完成，日志中会打印出一个 root 用户的默认密码。这个密码需要记录下来。\n2020-12-10T06:05:28.948043Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: P6kigsT6Lg\u003e=\n启动 MySQL 服务：\nbin/mysqld --user=mysql \u0026 连接 MySQL：\n默认是只能从本机登录，远程是无法访问的。所以需要用 root 用户登录下，配置远程访问的权限。\ncd /root/mysql bin/mysql -uroot -p # 然后用之前记录的默认密码登录 如果遇到 ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' 报错，可以参照下面的配置，修改下 /etc/my.cnf 配置文件，来配置下 sock 连接文件的地址。主要是下面 client 部分：\n[mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock user=mysql # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 # Settings user and group are ignored when systemd is used. # If you need to run mysqld under a different user or group, # customize your systemd unit file for mariadb according to the # instructions in http://fedoraproject.org/wiki/Systemd [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid # # include all files from the config directory # !includedir /etc/my.cnf.d [client] port=3306 socket=/var/lib/mysql/mysql.sock ​登录进去后，需要配置远程登录权限：\nalter user 'root'@'localhost' identified by '123456'; #修改 root 用户的密码 use mysql; update user set host='%' where user='root'; flush privileges; 搭建完成，可以使用 navicat 等连接工具远程访问 MySQL 服务了。\n搭建主从集群的多个服务，有两个必要的条件：\nMySQL 版本必须一致。 集群中各个服务器的时间需要同步。 配置主从集群 主从同步原理 数据库的主从同步，就是为了要保证多个数据库之间的数据保持一致。最简单的方式就是使用数据库的导入导出工具，定时将主库的数据导出，再导入到从库当中。这是一种很常见，也很简单易行的数据库集群方式。也有很多的工具帮助我们来做这些事情。但是这种方式进行数据同步的实时性比较差。\n要保证数据能够实时同步，对于 MySQL，通常就要用到他自身提供的一套通过 Binlog 日志在多个 MySQL 服务之间进行同步的集群方案。基于这种集群方案，一方面可以提高数据的安全性，另外也可以以此为基础，提供读写分离、故障转移等其他高级的功能。\n在主库上打开 Binlog 日志，记录对数据的每一步操作。然后在从库上打开 RelayLog 日志，用来记录跟主库一样的 Binlog 日志，并将 RelayLog 中的操作日志在自己数据库中进行重放。这样就能够更加实时的保证主库与从库的数据一致。\n在从库上启动一系列 IO 线程，负责与主库建立 TCP 连接，请求主库在写入 Binlog 日志时，也往从库传输一份。 主库上会有一个 IO Dump 线程，负责将 Binlog 日志通过这些 TCP 连接传输给从库的 IO 线程。 从库为了保证日志接收的稳定性，并不会立即重演 Binlog 数据操作，而是先将接收到的 Binlog 日志写入到自己的 RelayLog 日志当中。然后再异步的重演 RelayLog 中的数据操作。 ​MySQL 的 BinLog 日志能够比较实时的记录主库上的所有操作，因此他也被很多其他工具用来实时监控 MySQL 的数据变化。例如 Canal 框架，可以模拟一个 slave 节点，同步 MySQL 的 Binlog，然后将具体的数据操作按照定制的逻辑进行转发。例如转发到 Redis 实现缓存一致，转发到 Kafka 实现数据实时流转等。甚至像 ClickHouse，还支持将自己模拟成一个 MySQL 的从节点，接收 MySQL 的 Binlog 日志，实时同步 MySQL 的数据。\n​接下来就在这两个 MySQL 服务的基础上，搭建一个主从集群。\n配置 Master 服务 配置主节点的 MySQL 配置文件： /etc/my.cnf (没有的话就手动创建一个)\n主要是需要打开 binlog 日志，以及指定 server-id。打开 MySQL 主服务的 my.cnf 文件，在文件中一行 server-id 以及一个关闭域名解析的配置。然后重启服务。\n[mysqld] server-id=47 # 开启binlog log_bin=master-bin log_bin-index=master-bin.index skip-name-resolve # 设置连接端口 port=3306 # 设置 MySQL 的安装目录 basedir=/usr/local/mysql # 设置 MySQL 数据库的数据的存放目录 datadir=/usr/local/mysql/mysql-files # 允许最大连接数 max_connections=200 # 允许连接失败的次数。 max_connect_errors=10 # 服务端使用的字符集默认为 UTF8 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB # 默认使用 “mysql_native_password” 插件认证 #mysql_native_password default_authentication_plugin=mysql_native_password server-id：服务节点的唯一标识。需要给集群中的每个服务分配一个单独的 ID。 log_bin：打开 Binlog 日志记录，并指定文件名。 log_bin-index：Binlog 日志文件 重启 MySQL 服务： service mysqld restart。\n给 root 用户分配一个 replication slave 的权限：\n# 登录主数据库 mysql -u root -p GRANT REPLICATION SLAVE ON *.* TO 'root'@'%'; flush privileges; # 查看主节点同步状态 show master status; # 输出 +------------------+-----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+-----------+--------------+------------------+ | master-bin.000004 | 156 | | | +------------------+-----------+--------------+------------------+ 通常不会直接使用 root 用户，而会创建一个拥有全部权限的用户来负责主从同步。\n配置 Slave 服务 修改配置文件：\n[mysqld] # 主库和从库需要不一致 server-id=48 # 打开 MySQL 中继日志 relay-log-index=slave-relay-bin.index relay-log=slave-relay-bin # 打开从服务二进制日志 log-bin=mysql-bin # 使得更新的数据写进二进制日志中 log-slave-updates=1 # 设置 3306 端口 port=3306 # 设置 MySQL 的安装目录 basedir=/usr/local/mysql # 设置 MySQL 数据库的数据的存放目录 datadir=/usr/local/mysql/mysql-files # 允许最大连接数 max_connections=200 # 允许连接失败的次数。 max_connect_errors=10 # 服务端使用的字符集默认为 UTF8 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB # 默认使用 “mysql_native_password” 插件认证 # mysql_native_password default_authentication_plugin=mysql_native_password 启动 MySQL 服务，并设置他的主节点同步状态：\n# 登录从服务 mysql -u root -p; # 设置同步主节点 CHANGE MASTER TO MASTER_HOST='192.168.232.128', MASTER_PORT=3306, MASTER_USER='root', MASTER_PASSWORD='root', MASTER_LOG_FILE='master-bin.000004', MASTER_LOG_POS=156, GET_MASTER_PUBLIC_KEY=1; # 开启 slave start slave; # 查看主从同步状态 show slave status; # 或者用 show slave status \\G; 这样查看比较简洁 注意，CHANGE MASTER 指令中需要指定的 MASTER_LOG_FILE 和 MASTER_LOG_POS 必须与主服务中命令 show master status; 查到的保持一致。\n并且后续如果要检查主从架构是否成功，也可以通过检查主服务与从服务之间的 File 和 Position 这两个属性是否一致来确定。\nmysql\u003e show slave status \\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.232.128 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-bin.000004 # 主节点的 binlog 文件名 Read_Master_Log_Pos: 156 # 主节点的 binlog 位置 Relay_Log_File: slave-relay-bin.000006 Relay_Log_Pos: 373 Relay_Master_Log_File: master-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: # 从节点需要同步的数据库 Replicate_Ignore_DB: # 从节点不需要同步的数据库 Replicate_Do_Table: # 从节点需要同步的数据库中的表 Replicate_Ignore_Table: # 从节点不需要同步的数据库中的表 Replicate_Wild_Do_Table: # 从节点需要同步的数据库中的通配符表，例如 test.% Replicate_Wild_Ignore_Table: # 从节点不需要同步的数据库中的通配符表，例如 test.% ℹ️ Replicate_ 开头这些属性指定了两个服务之间要同步哪些数据库、哪些表的配置。只是在这个示例中全都没有进行配置，就标识是全库进行同步。 主从集群测试 先用 showdatabases，查看下两个 MySQL 服务中的数据库情况：\n# master mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | masterdemo | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.00 sec) # slave mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | masterdemo | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.01 sec) ​ 然后在主服务器上创建一个数据库：\nmysql\u003e create database syncdemo; Query OK, 1 row affected (0.00 sec) 再用 show databases，来看下这个 syncdemo 的数据库是不是已经同步到了从服务：\n# slave mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | masterdemo | | mysql | | performance_schema | | syncdemo | | sys | +--------------------+ 6 rows in set (0.00 sec) 继续在 syncdemo 这个数据库中创建一个表，并插入一条数据：\nmysql\u003e use syncdemo; Database changed mysql\u003e create table demoTable(id int not null); Query OK, 0 rows affected (0.02 sec) mysql\u003e insert into demoTable value(1); Query OK, 1 row affected (0.01 sec) 查一下这个 demoTable 是否同步到了从服务：\n# slave mysql\u003e use syncdemo; Database changed mysql\u003e show tables; +--------------------+ | Tables_in_syncdemo | +--------------------+ | demoTable | +--------------------+ 1 row in set (0.00 sec) # 查看 demoTable 表中的数据 mysql\u003e select * from demoTable; +----+ | id | +----+ | 1 | +----+ 1 row in set (0.00 sec) 这样，一个主从集群就搭建完成了。\n另外，这个主从架构是有可能失败的，如果在 slave 从服务上查看 slave 状态，发现Slave_SQL_Running=no，就表示主从同步失败了。这有可能是因为在从数据库上进行了写操作，与同步过来的 SQL 操作冲突了，也有可能是 slave 从服务重启后有事务回滚了。\n如果是因为 slave 从服务事务回滚的原因，可以按照以下方式重启主从同步：\nmysql\u003e stop slave ; mysql\u003e set GLOBAL SQL_SLAVE_SKIP_COUNTER=1; mysql\u003e start slave ; 另一种解决方式就是重新记录主节点的 binlog 文件消息：\nmysql\u003e stop slave ; mysql\u003e change master to ..... mysql\u003e start slave ; 这种方式要注意 binlog 的文件和位置，如果修改后和之前的同步接不上，那就会丢失部分数据。所以不太常用。\n全库同步与部分同步 目前配置的主从同步是针对全库配置的，而实际环境中，一般并不需要针对全库做备份，而只需要对一些特别重要的库或者表来进行同步。那如何针对库和表做同步配置？\n在 Master 端：在 my.cnf 中，可以通过以下这些属性指定需要针对哪些库或者哪些表记录 binlog：\n# 需要同步的二进制数据库名 binlog-do-db=masterdemo # 只保留 7 天的二进制日志，以防磁盘被日志占满(可选) expire-logs-days = 7 # 不备份的数据库 binlog-ignore-db=information_schema binlog-ignore-db=performation_schema binlog-ignore-db=sys 在 Slave 端：在 my.cnf 中，需要配置备份库与主服务的库的对应关系：\n# 如果 salve 库名称与 master 库名相同，使用本配置 replicate-do-db = masterdemo # 如果 master 库名 [mastdemo] 与 salve 库名 [mastdemo01] 不同，使用以下配置 [需要做映射] replicate-rewrite-db = masterdemo -\u003e masterdemo01 # 如果不是要全部同步 [默认全部同步]，则指定需要同步的表 replicate-wild-do-table=masterdemo01.t_dict replicate-wild-do-table=masterdemo01.t_num 配置完成了之后，在 show master status 指令中，就可以看到 Binlog_Do_DB 和 Binlog_Ignore_DB 两个参数的作用了。\nGTID 同步集群 上面搭建的集群方式，是基于 Binlog 日志记录点的方式来搭建的，这也是最为传统的 MySQL 集群搭建方式。\n另外一种搭建主从同步的方式，即 GTID 搭建方式。这种模式是从 MySQL 5.6 版本引入的。\nGTID 的本质也是基于 Binlog 来实现主从同步，只是他会基于一个全局的事务 ID 来标识同步进度。GTID 即全局事务 ID，全局唯一并且趋势递增，他可以保证为每一个在主节点上提交的事务在复制集群中可以生成一个唯一的 ID。\n在基于 GTID 的复制中，首先从服务器会告诉主服务器已经在从服务器执行完了哪些事务的 GTID 值，然后主库会有把所有没有在从库上执行的事务，发送到从库上进行执行，并且使用 GTID 的复制可以保证同一个事务只在指定的从库上执行一次，这样可以避免由于偏移量的问题造成数据不一致。\n搭建方式跟主从架构整体搭建方式差不多。只是需要在 my.cnf 中修改一些配置：\n主节点：\ngtid_mode=on enforce_gtid_consistency=on log_bin=on server_id=单独设置一个 binlog_format=row 从节点：\ngtid_mode=on enforce_gtid_consistency=on log_slave_updates=1 server_id=单独设置一个 然后分别重启主服务和从服务，就可以开启 GTID 同步复制方式。","多源复制#多源复制":"无论是异步复制还是半同步复制，都是 1 个 Master 对应 N 个 Slave。其实 MySQL 也支持 N 个 Master 对应 1 个 Slave，这种架构就称之为多源复制。\n多源复制允许在不同 MySQL 实例上的数据同步到 1 台 MySQL 实例上，方便在 1 台 Slave 服务器上进行一些统计查询，如常见的 OLAP 业务查询。\n上图显示了订单库、库存库、供应商库，通过多源复制同步到了一台 MySQL 实例上，接着就可以通过 MySQL 8.0 提供的复杂 SQL 能力，对业务进行深度的数据分析和挖掘。","应用层提供管理多个数据源的能力#应用层提供管理多个数据源的能力":"有了集群化的后端数据库之后，接下来在应用层面，就需要能够随意访问多个数据库的能力。\n多数据源访问的实现方式有很多，例如基 Spring 提供的 AbstractRoutingDataSource 组件，就可以快速切换后端访问的实际数据库。\n可以配置两个不同的目标数据库，然后通过 DynamicDataSource 组件中的一个 ThreadLocal 变量实现快速切换目标数据库，从而让写接口与读接口分别操作两个不同的数据库。\n由于主库与从库之间可以同步数据，虽然写接口与读接口是访问的不同的数据库，但是由于两个数据库之间可以通过主从集群进行数据同步，所以看起来，课程管理的两个接口就像是访问同一个数据库一样。这其实就是对于数据库非常常见的一种分布式的优化方案读写分离。\n读写分离 数据库读写分离是一种常见的数据库优化方案，其基础思想是将对数据的读请求和写请求分别分配到不同的数据库服务器上，以提高系统的性能和可扩展性。\n一般情况下，数据库的读操作比写操作更为频繁，而且读操作并不会对数据进行修改，因此可以将读请求分配到多个从数据库服务器上进行处理。这样，即使一个从数据库服务器故障或者过载，仍然可以使用其他从数据库服务器来处理读请求，保证系统的稳定性和可用性。同时，将写操作分配到主数据库服务器上，可以保证数据的一致性和可靠性。主数据库服务器负责所有的写操作，而从数据库服务器只需要从主数据库服务器同步数据即可。由于主数据库服务器是唯一的写入点，可以保证数据的正确性和一致性。\n将多个数据源抽象成一个统一的数据源 DynamicDataSource 的实现方式其实对开发方式的侵入是挺大的，每次进行数据库操作之前，都需要先选择要操作那个数据库。有没有更为自然的多数据源管理方式？就是让业务真正像操作单数据源一样访问多个数据。MyBatis-Plus 框架的开发者就开发了这样的一个框架 DynamicDataSource，可以简化多数据源访问的过程。\n这个开源的 DynamicDataSource 小框架会自行在 Spring 容器当中注册一个具备多数据源切换能力的 DataSource 数据源，这样，在应用层面，只需要按照 DynamicDataSource 框架的要求修改配置接口，其他地方几乎感知不到与传统操作单数据源有什么区别。\n应用只需要像访问单个数据源一样，访问 DynamicDataSource 框架提供的一个逻辑数据库。而这个逻辑数据库会帮我们将实际的 SQL 语句转发到后面的真实数据库当中去执行。\n其实也是 ShardingSphere 需要做的事情。只不过，ShardingSphere 提供的逻辑库功能要强大很多，也复杂很多。","延迟复制#延迟复制":"前面介绍的复制架构，Slave 在接收二进制日志后会尽可能快地回放日志，这样是为了避免主从之间出现延迟。而延迟复制却允许 Slave 延迟回放接收到的二进制日志，为了避免主服务器上的误操作，马上又同步到了从服务器，导致数据完全丢失。\n可以通过以下命令设置延迟复制：\n# 设置了 Slave 落后 Master 服务器 1 个小时 CHANGE MASTER TO master_delay = 3600 延迟复制在数据库的备份架构设计中非常常见，比如可以设置一个延迟一天的延迟备机，这样本质上说，用户可以有 1 份 24 小时前的快照。\n那么当线上发生误操作，如 DROP TABLE、DROP DATABASE 这样灾难性的命令时，用户有一个 24 小时前的快照，数据可以快速恢复。","搭建半同步复制#搭建半同步复制":"半同步复制的原理 主从复制数据丢失问题 MySQL 的主从集群，是有丢失数据的风险的。\nMySQL 主从集群默认采用的是一种异步复制的机制。主服务在执行用户提交的事务后，写入 binlog 日志，然后就给客户端返回一个成功的响应了。而 binlog 会由一个 dump 线程异步发送给 Slave 从服务。\n由于这个发送 binlog 的过程是异步的。主服务在向客户端反馈执行结果时，是不知道 binlog 是否同步成功了的。这时候如果主服务宕机了，而从服务还没有备份到新执行的 binlog，那就有可能会丢数据。\n怎么解决这个问题，这就要靠 MySQL 的半同步复制机制来保证数据安全。\n​半同步复制机制是一种介于异步复制和全同步复制之前的机制。主库在执行完客户端提交的事务后，并不是立即返回客户端响应，而是等待至少 N 个从库接收并写到 relay log 中，才会返回给客户端。MySQL 在等待确认时，默认会等 10 秒，如果超过 10 秒没有收到 ack，就会降级成为异步复制（也就是说超时了以后 master 就不等了，直接返回客户端去了，也不会当做失败来处理）。\n这种半同步复制相比异步复制，能够有效的提高数据的安全性。但是这种安全性也不是绝对的，他只保证事务提交后的 binlog 至少传输到了一个从库，并且并不保证从库应用这个事务的 binlog 是成功的。另一方面，半同步复制机制也会造成一定程度的延迟，这个延迟时间最少是一个 TCP/IP 请求往返的时间。整个服务的性能是会有所下降的。而当从服务出现问题时，主服务需要等待的时间就会更长，要等到从服务的服务恢复或者请求超时才能给用户响应。\n无损半同步 MySQL 5.7 版本前的半同步复制机制是有损的，这种半同步复制在 Master 发生宕机时，Slave 会丢失最后一批提交的数据\n有损半同步是在 Master 事务提交后，即步骤 4 后，等待 Slave 返回 ACK，表示至少有 Slave 接收到了二进制日志，如果这时二进制日志还未发送到 Slave，Master 就发生宕机，则此时 Slave 就会丢失 Master 已经提交的数据。\nMySQL 5.7 版本之后，引入了无损半同步复制的机制。\n无损半同步复制 WAIT ACK 发生在事务提交之前，这样即便 Slave 没有收到二进制日志，但是 Master 宕机了，*由于最后一个事务还没有提交，所以本身这个数据对外也不可见，不存在丢失的问题。\n搭建半同步复制集群 半同步复制需要基于特定的扩展模块来实现。而 MySQL 从 5.5 版本开始，往上的版本都默认自带了这个模块。这个模块包含在 MySQL 安装目录下的 lib/plugin 目录下的 semisync_master.so 和 semisync_slave.so 两个文件中。需要在主服务上安装 semisync_master 模块，在从服务上安装 semisync_slave 模块。\n登陆主节点，安装 semisync_master 模块：\n# 安装半同步复制模块，指定扩展库的文件名 mysql\u003e install plugin rpl_semi_sync_master soname 'semisync_master.so'; Query OK, 0 rows affected (0.01 sec) # 查看系统全局参数，rpl_semi_sync_master_timeout 就是半同步复制时等待应答的最长等待时间，默认是 10 秒，可以根据情况自行调整。 mysql\u003e show global variables like 'rpl_semi%'; +-------------------------------------------+------------+ | Variable_name | Value | +-------------------------------------------+------------+ | rpl_semi_sync_master_enabled | OFF | | rpl_semi_sync_master_timeout | 10000 | | rpl_semi_sync_master_trace_level | 32 | | rpl_semi_sync_master_wait_for_slave_count | 1 | | rpl_semi_sync_master_wait_no_slave | ON | | rpl_semi_sync_master_wait_point | AFTER_SYNC | +-------------------------------------------+------------+ 6 rows in set, 1 warning (0.02 sec) # 打开半同步复制的开关 mysql\u003e set global rpl_semi_sync_master_enabled=ON; Query OK, 0 rows affected (0.00 sec) 最后的一个参数 rpl_semi_sync_master_wait_point 其实表示一种半同步复制的方式。\n半同步复制有两种方式，一种是我们现在看到的这种默认的 AFTER_SYNC 方式。这种方式下，主库把日志写入 binlog，并且复制给从库，然后开始等待从库的响应。从库返回成功后，主库再提交事务，接着给客户端返回一个成功响应。\n而另一种方式是叫做 AFTER_COMMIT 方式。他不是默认的。这种方式，在主库写入 binlog 后，等待 binlog 复制到从库，主库就提交自己的本地事务，再等待从库返回给自己一个成功响应，然后主库再给客户端返回响应。\n登陆从节点，安装 smeisync_slave 模块：\nmysql\u003e install plugin rpl_semi_sync_slave soname 'semisync_slave.so'; Query OK, 0 rows affected (0.01 sec) mysql\u003e show global variables like 'rpl_semi%'; +---------------------------------+-------+ | Variable_name | Value | +---------------------------------+-------+ | rpl_semi_sync_slave_enabled | OFF | | rpl_semi_sync_slave_trace_level | 32 | +---------------------------------+-------+ 2 rows in set, 1 warning (0.01 sec) mysql\u003e set global rpl_semi_sync_slave_enabled = on; Query OK, 0 rows affected (0.00 sec) mysql\u003e show global variables like 'rpl_semi%'; +---------------------------------+-------+ | Variable_name | Value | +---------------------------------+-------+ | rpl_semi_sync_slave_enabled | ON | | rpl_semi_sync_slave_trace_level | 32 | +---------------------------------+-------+ 2 rows in set, 1 warning (0.00 sec) mysql\u003e stop slave; Query OK, 0 rows affected (0.01 sec) mysql\u003e start slave; Query OK, 0 rows affected (0.01 sec) ","读写分离#读写分离":"只能从主节点同步到从节点，而从节点的数据表更是无法同步到主节点的。为了保证数据一致，通常会需要保证数据只在主节点上写，而从节点只进行数据读取。这就是读写分离。\nMySQL 主从本身是无法提供读写分离的服务的，需要由业务自己来实现。\n一种常见的业务读写分离的架构设计：\n引入了 Load Balance 负载均衡的组件，这样 Server 对于数据库的请求不用关心后面有多少个从机，对于业务来说也就是透明的，只需访问 Load Balance 服务器的 IP 或域名就可以。\n要限制用户写数据，我们可以在从服务中将 read_only 参数的值设为 1 ( set global read_only=1;)。这样就可以限制用户写入数据。但是这个属性有两个需要注意的地方：\nread_only=1 设置的只读模式，不会影响 slave 同步复制的功能。 所以在 MySQL slave 库中设定了 read_only=1 后，通过 show slave status\\G 命令查看 salve 状态，可以看到 salve 仍然会读取 master 上的日志，并且在 slave 库中应用日志，保证主从数据库同步一致； read_only=1 设置的只读模式，限定的是普通用户进行数据修改的操作，但不会限定具有 super 权限的用户的数据修改操作。在 MySQL 中设置 read_only=1 后，普通的应用用户进行 insert、update、delete 等会产生数据变化的 DML 操作时，都会报出数据库处于只读模式不能发生数据变化的错误，但具有 super 权限的用户，例如在本地或远程通过 root 用户登录到数据库，还是可以进行数据变化的 DML 操作； 如果需要限定 super 权限的用户写数据，可以设置 super_read_only=0。另外 如果要想连 super 权限用户的写操作也禁止，就使用 flush tables with read lock;，这样设置也会阻止主从同步复制！ ℹ️ 读写分离设计的前提是从机不能落后主机很多，最好是能准实时数据同步，务必一定要开始并行复制，并确保线上已经将大事务拆成小事务。 ℹ️ 在 Load Balance 服务器，可以配置较小比例的读取请求访问主机，如主服务器权重为的 1% 的读请求，其余三台从服务器各自承担 33% 的读取请求。\n如果发生严重的主从复制延迟情况，可以设置下面从机权重为 0，将主机权重设置为 100%，这样就不会因为数据延迟，导致对于业务的影响了。","集群扩容与-mysql-数据迁移#集群扩容与 MySQL 数据迁移":"如果要扩展到一主多从的集群架构，其实就比较简单了，只需要增加一个 binlog 复制就行了。\n但是如果我们的集群是已经运行过一段时间，这时候如果要扩展新的从节点就有一个问题，之前的数据没办法从 binlog 来恢复了。这时候在扩展新的 slave 节点时，就需要增加一个数据复制的操作。\n​MySQL 的数据备份恢复操作相对比较简单，可以通过 SQL 语句直接来完成。具体操作可以使用 MySQL 的 bin 目录下的 mysqldump 工具：\nmysqldump -u root -p --all-databases \u003e backup.sql #输入密码 通过这个指令，就可以将整个数据库的所有数据导出成 backup.sql，然后把这个 backup.sql 分发到新的 MySQL 服务器上，并执行下面的指令将数据全部导入到新的 MySQL 服务中：\nmysql -u root -p \u003c backup.sql #输入密码 这样新的 MySQL 服务就已经有了所有的历史数据，然后就可以再按照上面的步骤，配置 Slave 从服务的数据同步了。","高可用方案#高可用方案":"MySQL 主从复制是高可用的技术基础，高可用套件是 MySQL 高可用实现的解决方案，负责 Failover 操作。\n为了不让业务感知到数据库的宕机切换，还要要用到 VIP（Virtual IP）技术。VIP 不是真实的物理 IP，而是可以随意绑定在任何一台服务器上。\n业务访问数据库，不是服务器上与网卡绑定的物理 IP，而是这台服务器上的 VIP。当数据库服务器发生宕机时，高可用套件会把 VIP 漂移到新的服务器上。数据库 Failover后，业务依旧访问的还是 VIP，所以使用 VIP 可以做到对业务透明。\n但是 VIP 也是有局限性的，仅限于同机房同网段的 IP 设定。如果是跨机房容灾架构，VIP 就不可用了。这时就要用 DNS（Domain Name Service）服务。\n上层业务通过域名进行访问。当发生宕机，进行机房级切换后，高可用套件会把域名指向为新的 MySQL 主服务器，这样也实现了对于上层服务的透明性。\nMHA MHA（Master High Availability） 是一款开源的 MySQL 高可用程序，它为 MySQL 数据库主从复制架构提供了 automating master failover 的功能。它由两大组件所组成，MHA Manger 和 MHA Node。\nMHA Manager 可以单独部署在一台独立的机器上管理多个 master-slave 集群，也可以部署在一台 slave 节点上。MHA Manager会定时探测集群中的 master 节点，当 master 出现故障时，它可以自动将最新数据的 slave 提升为新的 master，然后将所有其他的 slave 重新指向新的 master。整个故障转移过程对应用程序完全透明。\n而 MHA Node 部署在每台 MySQL 服务器上，MHA Manager 通过执行 Node 节点的脚本完成 failover 切换操作。\nMHA Manager 和 MHA Node 的通信是采用 ssh 的方式，也就是需要在生产环境中打通 MHA Manager 到所有 MySQL 节点的 ssh 策略，那么这里就存在潜在的安全风险。\n另外，ssh 通信，效率也不是特别高。所以，MHA 比较适合用于规模不是特别大的公司，所有 MySQL 数据库的服务器数量不超过 20 台。\nMGR MGR：MySQL Group Replication。 是 MySQL 官方在 5.7.17 版本正式推出的一种组复制机制。主要是解决传统异步复制和半同步复制的数据一致性问题。\n不要简单认为它是一种新的数据同步技术，而是应该把它理解为高可用解决方案，而且特别适合应用于对于数据一致性要求极高的金融级业务场景。\n首先，MGR 之间的数据同步并没有采用复制技术，而是采用 GCS（Group Communication System）协议的日志同步技术。\nGSC 本身是一种类似 Paxos 算法的协议，要求组中的大部分节点都接收到日志，事务才能提交。所以，MRG 是严格要求数据一致的，特别适合用于金融级的环境。由于是类 Paxos 算法，集群的节点要求数量是奇数个，这样才能满足大多数的要求。\n之前介绍的无损半同步也能保证数据强一致的要求吗？\n是的，虽然通过无损半同步复制也能保证主从数据的一致性，但通过 GCS 进行数据同步有着更好的性能：当启用 MGR 插件时，MySQL 会新开启一个端口用于数据的同步，而不是如复制一样使用 MySQL 服务端口，这样会大大提升复制的效率。\nMGR 有两种模式：\n单主（Single Primary）模式； 多主（Multi Primary）模式。 单主模式只有 1 个节点可以写入，多主模式能让每个节点都可以写入。而多个节点之间写入，如果存在变更同一行的冲突，MySQL 会自动回滚其中一个事务，自动保证数据在多个节点之间的完整性和一致性。\n最后，在单主模式下，MGR 可以自动进行 Failover 切换，不用依赖外部的各种高可用套件，所有的事情都由数据库自己完成，比如最复杂的选主（Primary Election）逻辑，都是由 MGR 自己完成，用户不用部署额外的 Agent 等组件。\nMGR 的限制 仅支持 InnoDB 表，并且每张表一定要有一个主键； 目前一个 MGR 集群，最多只支持 9 个节点； 有一个节点网络出现抖动或不稳定，会影响集群的性能。 第 1、2 点问题不大，因为目前用 MySQL 主流的就是使用 InnoDB 存储引擎，9 个节点也足够用了。\n而第 3 点要注意，和复制不一样的是，由于 MGR 使用的是 Paxos 协议，对于网络极其敏感，如果其中一个节点网络变慢，则会影响整个集群性能。而半同步复制，比如 ACK 为1，则 1 个节点网络出现问题，不影响整个集群的性能。所以，在决定使用 MGR 后，切记一定要严格保障网络的质量。\nℹ️ 依赖网络通信 Paxos 算法的核心是通过节点之间的消息传递来实现一致性。它需要多个阶段的通信，包括提案阶段（Prepare 阶段）、接受阶段（Accept 阶段）和学习阶段（Learn 阶段）。每个阶段都需要节点之间发送和接收消息。\nPrepare 阶段：提议者（Proposer）向其他节点发送提案请求（Prepare 消息），询问是否可以接受新的提案。 Accept 阶段：提议者在收到足够多的响应后，向其他节点发送接受请求（Accept 消息），要求其他节点接受该提案。 Learn 阶段：接受者（Acceptor）将接受的提案告知其他节点，最终达成一致。 如果网络延迟过高或消息丢失，这些阶段的通信可能会被阻塞或失败，导致整个协议无法正常推进。\n对网络延迟敏感 Paxos 算法的性能高度依赖网络延迟。在每个阶段，节点都需要等待其他节点的响应。例如： 在 Prepare 阶段，提议者需要等待大多数节点的响应才能进入下一个阶段。 在 Accept 阶段，提议者需要等待大多数节点接受提案后才能认为提案成功。\n如果网络延迟过高，节点等待响应的时间就会增加，导致整个协议的执行时间变长。在极端情况下，如果延迟过高，可能会导致协议超时，从而需要重新发起一轮新的协商过程。"},"title":"集群架构"},"/db-learn/docs/mysql/practice/06_shards/":{"data":{"":"分库分表，能不分就不分。","shardingsphere#ShardingSphere":"​ShardingSphere 是一款起源于当当网内部的应用框架。2015年在当当网内部诞生，最初就叫 ShardingJDBC。2016 年的时候，由其中一个主要的开发人员张亮，带入到京东数科，组件团队继续开发。在国内历经了当当网、电信翼支付、京东数科等多家大型互联网企业的考验，在 2017 年开始开源。并逐渐由原本只关注于关系型数据库增强工具的 ShardingJDBC 升级成为一整套以数据分片为基础的数据生态圈，更名为 ShardingSphere。到 2020 年 4 月，已经成为了 Apache 软件基金会的顶级项目。发展至今，已经成为了业界分库分表最成熟的产品。\nShardingSphere 这个词可以分为两个部分，其中 Sharding 就是我们之前介绍过的数据分片。从官网介绍上就能看到，他的核心功能就是可以将任意数据库组合，转换成为一个分布式的数据库，提供整体的数据库集群服务。只是给自己的定位并不是 Database，而是Database plus。其实这个意思是他自己并不做数据存储，而是对其他数据库产品进行整合。整个 ShardingSphere 其实就是围绕数据分片这个核心功能发展起来的。后面的 Sphere 是生态的意思。这意味着 ShardingSphere 不是一个单独的框架或者产品，而是一个由多个框架以及产品构成的一个完整的技术生态。目前 ShardingSphere 中比较成型的产品主要包含核心的 ShardingJDBC 以及 ShardingProxy 两个产品，以及一个用于数据迁移的子项目 ElasticJob，另外还包含围绕云原生设计的一系列未太成型的产品。\n​ShardingSphere 经过这么多年的发展，已经不仅仅只是用来做分库分表，而是形成了一个围绕分库分表核心的技术生态。他的核心功能已经包括了数据分片、分布式事务、读写分离、高可用、数据迁移、联邦查询、数据加密、影子库、DistSQL 庞大的技术体系。\n客户端分库分表与服务端分库分表 ShardingSphere 最为核心的产品有两个：一个是 ShardingJDBC，这是一个进行客户端分库分表的框架。另一个是 ShardingProxy，这是一个进行服务端分库分表的产品。他们代表了两种不同的分库分表的实现思路。\nShardingJDBC 客户端分库分表 ShardingSphere-JDBC 定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。\n适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC； 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, HikariCP 等； 支持任意实现 JDBC 规范的数据库，目前支持 MySQL，PostgreSQL，Oracle，SQLServer 以及任何可使用 JDBC 访问的数据库。 ShardingProxy 服务端分库分表 ShardingSphere-Proxy 定位为透明化的数据库代理端，通过实现数据库二进制协议，对异构语言提供支持。 目前提供 MySQL 和 PostgreSQL 协议，透明化数据库操作，对 DBA 更加友好。\n向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用； 兼容 MariaDB 等基于 MySQL 协议的数据库，以及 openGauss 等基于 PostgreSQL 协议的数据库； 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端，如：MySQL Command Client, MySQL Workbench, Navicat 等。 ShardingSphere 混合部署架构 ShardingJDBC 跟客户端在一起，使用更加灵活。而 ShardingProxy 是一个独立部署的服务，所以他的功能相对就比较固定。他们的整体区别如下：\nShardingSphere-JDBC ShardingSphere-Proxy 数据库 任意 MySQL/PostgreSQL 连接消耗数 高 低 异构语言 仅 Java 任意 性能 损耗低 损耗略高 无中心化 是 否 静态入口 无 有 在产品图中，Governance Center 也是其中重要的部分。他的作用有点类似于微服务架构中的配置中心，可以使用第三方服务统一管理分库分表的配置信息，当前建议使用的第三方服务是 Zookeeper，同时也支持 Nacos，Etcd 等其他第三方产品。\n​由于 ShardingJDBC 和 ShardingProxy 都支持通过 Governance Center，将配置信息交个第三方服务管理，因此，也就自然支持了通过 Governance Center 进行整合的混合部署架构。\nShardingSphere 的核心概念 垂直分片与水平分片 这是设计分库分表方案时经常会提到的概念。其中垂直分片表示按照业务的纬度，将不同的表拆分到不同的库当中。这样可以减少每个数据库的数据量以及客户端的连接数，提高查询效率。而水平分表表示按照数据的纬度，将原本存在同一张表中的数据，拆分到多张子表当中。每个子表只存储一份的数据。这样可以将数据量分散到多张表当中，减少每一张表的数据量，提升查询效率。\nShardingSphere 实现分库分表的核心概念 虚拟库：ShardingSphere 的核心就是提供一个具备分库分表功能的虚拟库，他是一个 ShardingSphereDatasource 实例。应用程序只需要像操作单数据源一样访问这个 ShardingSphereDatasource 即可。 真实库：实际保存数据的数据库。这些数据库都被包含在 ShardingSphereDatasource 实例当中，由 ShardingSphere 决定未来需要使用哪个真实库。 逻辑表：应用程序直接操作的逻辑表。 真实表：实际保存数据的表。这些真实表与逻辑表表名不需要一致，但是需要有相同的表结构，可以分布在不同的真实库中。应用可以维护一个逻辑表与真实表的对应关系，所有的真实表默认也会映射成为 ShardingSphere 的虚拟表。 分布式主键生成算法：给逻辑表生成唯一主键。由于逻辑表的数据是分布在多个真实表当中的，所有，单表的索引就无法保证逻辑表的 ID 唯一性。ShardingSphere 集成了几种常见的基于单机生成的分布式主键生成器。比如 SNOWFLAKE，COSID_SNOWFLAKE 雪花算法可以生成单调递增的 long 类型的数字主键，还有 UUID，NANOID 可以生成字符串类型的主键。当然，ShardingSphere 也支持应用自行扩展主键生成算法。比如基于 Redis，Zookeeper 等第三方服务，自行生成主键。 分片策略：表示逻辑表要如何分配到真实库和真实表当中，分为分库策略和分表策略两个部分。分片策略由分片键和分片算法组成。分片键是进行数据水平拆分的关键字段。如果没有分片键，ShardingSphere 将只能进行全路由，SQL 执行的性能会非常差。分片算法则表示根据分片键如何寻找对应的真实库和真实表。简单的分片策略可以使用 Groovy 表达式直接配置，当然，ShardingSphere 也支持自行扩展更为复杂的分片算法。 ShardingJDBC 客户端分库分表机制 ShardingProxy 服务端分库分表机制 ​ShardingSphere-Proxy，早前版本就叫做 ShardingProxy。定位为一个透明化的数据库代理，目前提供 MySQL 和 PostgreSQL 协议，透明化数据库操作。简单理解就是，他会部署成一个 MySQL 或者 PostgreSQL 的数据库服务，应用程序只需要像操作单个数据库一样去访问 ShardingSphere-proxy，由 ShardingProxy 去完成分库分表功能。\nShardinSphere 中的分布式事务机制 由于 ShardingSphere 是需要操作分布式的数据库集群，所以数据库内部的本地事务机制是无法保证 ShardingProxy 中的事务安全的，这就需要引入分布式事务管理机制，保证 ShardingProxy 中的 SQL 语句执行的原子性。也就是说，在ShardingProxy 中打开分布式事务机制后，你就不需要考虑 SQL 语句执行时的分布式事务问题了。\nXA 事务 XA 是由 X/Open Group 组织定义的，处理分布式事务的标准。主流的关系型数据库产品都实现了 XA 协议。例如，MySQL 从 5.0.3 版本开始，就已经可以直接支持 XA 事务了。但是要注意，只有 InnoDB 引擎才提供支持：","为什么要分库分表#为什么要分库分表":"随着现在互联网应用越来越大，数据库会频繁的成为整个应用的性能瓶颈。我们经常使用的 MySQL 数据库，也就不断面临数据量太大、数据访问太频繁、数据读写速度太快等一系列的问题。而传统的这些调优方式，在真正面对海量数据冲击时，往往就会显得很无力。因此，现在互联网对于数据库的使用也越来越小心谨慎。例如添加Redis缓存、增加MQ进行流量削峰等。但是，数据库本身如果性能得不到提升，这就相当于是水桶理论中的最短板。\n要提升数据库的性能，最直接的思路，当然是对数据库本身进行优化。例如对 MySQL 进行调优，优化 SQL 逻辑，优化索引结构，甚至像阿里等互联网大厂一样，直接优化 MySQL 的源码。但是这种思路在面对互联网环境时，会有很多非常明显的弊端。\n数据量和业务量快速增长，会带来性能瓶颈、服务宕机等很多问题。 单点部署的数据库无法保证服务高可用。 单点部署的数据库无法进行水平扩展，难以应对业务爆发式的增长。 ​ 这些问题背后的核心还是数据。数据库不同于上层的一些服务，它所管理的数据甚至比服务本身更重要。即要保证数据能够持续稳定的写入，又不能因为服务故障造成数据丢失。现在互联网上的大型应用，动辄几千万上亿的数据量，就算做好数据的压缩，随随便便也可以超过任何服务器的存储能力。并且，服务器单点部署，也无法真正解决把鸡蛋放在一个篮子里的问题。将数据放在同一个服务器上，如果服务器出现崩溃，就很难保证数据的安全性。这些都不是光靠优化 MySQL 产品，优化服务器配置能够解决的问题。 ","分库分表的优势#分库分表的优势":"​ 那么自然就需要换另外一种思路了。我们可以像微服务架构一样，来维护数据库的服务。把数据库从单体服务升级到数据库集群，这样才能真正全方位解放数据库的性能瓶颈，并且能够通过水平扩展的方式，灵活提升数据库的存储能力。这也就是我们常说的分库分表。通过分库分表可以给数据库带来很大的好处：\n提高系统性能：分库分表可以将大型数据库分成多个小型数据库，每个小型数据库只需要处理部分数据，因此可以提高数据库的并发处理能力和查询性能。 提高系统可用性：分库分表可以将数据复制到多个数据库中，以提高数据的可用性和可靠性。如果一个数据库崩溃了，其他数据库可以接管其工作，以保持系统的正常运行。 提高系统可扩展性：分库分表可以使系统更容易扩展。当数据量增加时，只需要增加更多的数据库和表，而不是替换整个数据库，因此系统的可扩展性更高。 提高系统灵活性：分库分表可以根据数据的使用情况，对不同的数据库和表进行不同的优化。例如，可以将经常使用的数据存储在性能更好的数据库中，或者将特定类型的数据存储在特定的表中，以提高系统的灵活性。 降低系统成本：分库分表可以使系统更加高效，因此可以降低系统的运营成本。此外，分库分表可以使用更便宜的硬件和存储设备，因为每个小型数据库和表需要的资源更少。 ","分库分表的挑战#分库分表的挑战":"​ 可能你会说，分库分表嘛， 也不是很难。一个库存不下，那就把数据拆分到多个库。一张表数据太多了，就把同一张表的数据拆分到多张。至于怎么做，也不难啊。要操作多个数据库，那么建立多个 JDBC 连接就行了。要写到多张表，修改下 SQL 语句就行了。\n​如果你也这么觉得，那么就大错特错了。分库分表也并不是字面意义上的将数据分到多个库或者多个表这么简单，他需要的是一系列的分布式解决方案。\n​因为数据的特殊性，造成数据库服务其实是几乎没有试错的成本的。在微服务阶段，从单机架构升级到微服务架构是很灵活的，中间很多细节步骤都可以随时做调整。比如对于微服务的接口限流功能，你并不需要一上来就用 Sentinel 这样复杂的流控工具。一开始不考虑性能，自己进行限流是很容易的事情。然后你可以慢慢尝试用 Guava 等框架提供的一些简单的流控工具进行零散的接口限流。直到整个应用的负载真正上来了之后，流控的要求更高更复杂了，再开始引入 Sentinel，进行统一流控，这都没有问题。这种试错的过程其实是你能够真正用好一项技术的基础。\n​但是对于数据库就不一样了。当应用中用来存储数据的数据库，从一个单机的数据库服务升级到多个数据库组成的集群服务时，需要考虑的，除了分布式的各种让人摸不着边际的复杂问题外，还要考虑到一个更重要的因素，数据。数据的安全性甚至比数据库服务本身更重要！因此，如果你在一开始做分库分表时的方案不太成熟，对数据的规划不是很合理，那么这些问题大概率会随着数据永远沉淀下去，成为日后对分库分表方案进行调整时最大的拦路虎。\n​所以在决定进行分库分表之前，一定需要提前对于所需要面对的各种问题进行考量。如果你没有考虑清楚数据要如何存储、计算、使用，或者你对于分库分表的各种问题都还没有进行过思考，那么千万不要在真实项目中贸然的进行分库分表。\n分库分表，也称为 Sharding。其实我觉得，Sharding应该比中文的分库分表更为贴切，他表示将数据拆分到不同的数据片中。由于数据往往是一个应用的基础，随着数据从单体服务拆分到多个数据分片，应用层面也需要面临很多新的问题。比如：\n主键避重问题 在分库分表环境中，由于表中数据同时存在不同数据库中，某个分区数据库生成的 ID 就无法保证全局唯一。因此需要单独设计全局主键，以避免跨库主键重复问题。\n数据备份问题 ​随着数据库由单机变为集群，整体服务的稳定性也会随之降低。如何保证集群在各个服务不稳定的情况下，依然保持整体服务稳定就是数据库集群需要面对的重要问题。而对于数据库，还需要对数据安全性做更多的考量。\n数据迁移问题 ​当数据库集群需要进行扩缩容时，集群中的数据也需要随着服务进行迁移。如何在不影响业务稳定性的情况下进行数据迁移也是数据库集群化后需要考虑的问题。\n分布式事务问题 ​原本单机数据库有很好的事务机制能够帮我们保证数据一致性。但是库分表后，由于数据分布在不同库甚至不同服务器，不可避免会带来分布式事务问题。\nSQL 路由问题 ​数据被拆分到多个分散的数据库服务当中，每个数据库服务只能保存一部分的数据。这时，在执行 SQL 语句检索数据时，如何快速定位到目标数据所在的数据库服务，并将 SQL 语句转到对应的数据库服务中执行，也是提升检索效率必须要考虑的问题。\n跨节点查询，归并问题 ​跨节点进行查询时，每个分散的数据库中只能查出一部分的数据，这时要对整体结果进行归并时，就会变得非常复杂。比如常见的 limit、order by 等操作。\n​在实际项目中，遇到的问题还会更多。从这里可以看出，Sharding 其实是一个很复杂的问题，往往很难通过项目定制的方式整体解决。因此，大部分情况下，都是通过第三方的服务来解决 Sharding 的问题。比如像 TiDB、ClickHouse、Hadoop 这一类的 NewSQL 产品，大部分情况下是将数据问题整体封装到一起，从而提供 Sharding 方案。但是这些产品毕竟太重了。更灵活的方式还是使用传统数据库，通过软件层面来解决多个数据库之间的数据问题。这也诞生了很多的产品，比如早前的 MyCat，还有后面我们要学习的ShardingSphere 等。\n​另外，关于何时要开始考虑分库分表呢？当然是数据太大了，数据库服务器压力太大了就要进行分库分表。但是这其实是没有一个具体的标准的，需要根据项目情况进行灵活设计。业界目前唯一比较值得参考的详细标准，是阿里公开的开发手册中提到的，建议预估三年内，单表数据超过 500W，或者单表数据大小超过 2G，就需要考虑分库分表。"},"title":"分库分表"},"/db-learn/docs/redis/":{"data":{"":"Redis 是由 C 语言编写的，使用 ANSI C 作为标准，支持网络、磁盘持久化、主从复制、哨兵、集群等功能。是一个内存数据库，支持多种数据结构，如字符串、哈希、列表、集合、有序集合等。"},"title":"Redis"},"/db-learn/docs/redis/advance/01_sds/":{"data":{"":"Redis 没有直接使用 C 语言传统的字符串表示（以空字符结尾的字符数组，以下简称 C 字符串），而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型， 并将 SDS 用作 Redis 的默认字符串表示。内部结构实现上类似于 Java 的 ArrayList。\nSDS 的结构是一个带长度信息的字节数组：\nstruct SDS\u003cT\u003e { int free; // buf 剩余可用长度 int len; // buf 已占用长度 char buf[]; // 实际保存字符串数据的地方 } ℹ️ 对 buf 分配一些额外的空间，并使用 free 记录未使用空间的大小。Redis 字符串是可以修改的，它支持 append 操作。如果数组没有冗余空间，那么追加操作必然涉及到分配新数组，然后将旧内容复制过来，再 append 新内容。如果字符串的长度非常长，这样的内存分配和复制开销就会非常大。这是利用空间换时间的策略。 使用 SDS 的原因：\n二进制安全。C 字符串是以 \\0 字符为结尾的，如果客户端传过来的字符串包含 \\0 字符，那么服务器在处理这个字符串时，会将 \\0 后的字符串舍弃掉。适配不同语言的客户端。 兼容部分 C 字符串函数。SDS 结构体中的 content 中的字符串是以字节 \\0 结尾的字符串，之所以多出这样一个字节，是为了便于直接使用 glibc 的字符串处理函数。 ","redis-的-key#Redis 的 key":"Redis 是一个键值对数据库，key 是字符串，value 是字符串、列表、哈希、集合、有序集合。\n虽然 Redis 可以使用非字符串类型作为 key，例如：\nredis\u003e set 100 100 OK redis\u003e get 100 \"100\" redis\u003e set 0.1 200 OK redis\u003e get 0.1 \"200\" 虽然看起来是数字或者浮点数，但是 Redis 会将其作为字符串来处理。","扩容策略#扩容策略":"字符串在长度小于 1M 之前，扩容空间采用加倍策略，也就是保留 100% 的冗余空间。当长度超过 1M 之后，为了避免加倍后的冗余空间过大而导致浪费，每次扩容只会多分配 1M 大小的冗余空间。"},"title":"简单动态字符串"},"/db-learn/docs/redis/advance/02_hash/":{"data":{"":"Redis 中除了 hash 结构的数据会用到字典外，整个 Redis 数据库的所有 key 和 value 也组成了一个全局字典，还有带过期时间的 key 集合也是一个字典。zset 集合中存储 value 和 score 值的映射关系也是通过字典实现的。\nset 的结构底层实现也是字典，只不过所有的 value 都是 NULL，其它的特性和字典一模一样。\nstruct RedisDb { dict* dict; // all keys key=\u003evalue dict* expires; // all expired keys key=\u003elong(timestamp) ... } struct zset { dict *dict; // all values value=\u003escore zskiplist *zsl; } ","字典的结构#字典的结构":" struct dict { ... dictht ht[2]; } struct dictEntry { void* key; void* val; dictEntry* next; // 链接下一个 entry，用来解决 hash 冲突 } struct dictht { dictEntry** table; // 二维 long size; // 第一维数组的长度 long used; // hash 表中的元素个数 ... } dict 结构内部包含两个 hashtable，通常情况下只有一个 hashtable 是有值的。但是在 dict 扩缩容时，需要分配新的 hashtable，然后进行渐进式搬迁，这时候两个 hashtable 存储的分别是旧的 hashtable 和新的 hashtable。待搬迁结束后，旧的 hashtable 被删除，新的 hashtable 取而代之。","扩容条件#扩容条件":"正常情况下，当 hash 表中元素的个数等于数组的长度时，就会开始扩容，扩容的新数组是原数组大小的 2 倍。不过如果 Redis 正在做 bgsave，为了减少内存页的过多分离 (Copy On Write)，Redis 尽量不去扩容，但是如果 hash 表已经非常满了，元素的个数已经达到了数组长度的 5 倍，说明 hash 表已经过于拥挤了，这个时候就会强制扩容。","渐进式-rehash#渐进式 rehash":"rehash 动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成的。\n原因在于，Redis 是单线程的，如果哈希表里保存的键值对数量非常庞大，一次性 rehash 庞大的计算量会导致服务器一段时间内停止服务。\n渐进式 rehash 的详细步骤：\n为 ht[1] 分配空间，让字典同时持有 ht[0] 和 ht[1] 两个数组。 在字典中维持一个索引计数器变量 rehashidx ，并将它的值设置为 0， 表示 rehash 工作正式开始。 在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将 ht[0] 在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ，当 rehash 工作完成之后，程序将 rehashidx 属性的值增 1。 随着字典操作的不断执行，最终在某个时间点上，ht[0] 的所有键值对都会被 rehash 至 ht[1]，然后将 ht[0] 指向新的数组，ht[1] 指向 NULL，同时程序将 rehashidx 属性的值设为 -1，表示 rehash 操作已完成。 渐进式 rehash 的过程中，字典会同时使用 ht[0] 和 ht[1] 两个数组，所以在渐进式 rehash 进行期间，字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行。\nℹ️ rehash 进行期间，查找某个 key 的操作，Redis 会先去 ht[0] 数组上进行查找操作，如果 ht[0] 不存在，就会去 ht[1] 数组上进行查找操作。如果 ht[0] 存在 key，就会把对应的 key 搬到 ht[1] 数组上。而且会把 key 所在的桶的整个链表全部迁移到 ht[1] 数组上。删除和更新都依赖于查找，先必须把元素找到，才可以进行数据结构的修改操作。\nRedis 还有一个循环定时器去不断的去执行 rehash 操作，即使没有命令执行，也会不断的执行 rehash 操作。\nrehash 由主线程控制，不会有并发安全的问题。","缩容条件#缩容条件":"当 hash 表因为元素的逐渐删除变得越来越稀疏时，Redis 会对 hash 表进行缩容来减少 hash 表的数组空间占用。缩容的条件是元素个数低于数组长度的 10%。缩容不会考虑 Redis 是否正在做 bgsave。"},"title":"字典"},"/db-learn/docs/redis/advance/03_redis_obj/":{"data":{"":" Redis 在存储 value 时，会把 value 包装成一个 RedisObject 数据结构，RedisObject 是 Redis 中所有 key 和 value 的基础数据结构。比如简单动态字符串（SDS）、双端链表、字典、压缩列表、整数集合，等等。\n每个对象都由一个 redisObject 结构表示:\ntypedef struct redisObject { // 类型 unsigned type:4; // 编码 unsigned encoding:4; // 指向底层实现数据结构的指针 void *ptr; unsigned lru:LRU_BITS; // LRU 时间或 LFU 数据（24位） int refcount; // 引用计数 } robj; lru (24 bits): 用于实现内存回收策略。 在 LRU 模式下：记录对象最后一次被访问的时间 在 LFU 模式下： 16 bits: 最近访问时间（分钟级） 8 bits: 访问频率计数器（logarithmic counter） refcount: 引用计数，用于内存管理。当 refcount 为 0 时，对象会被释放。 redis\u003e set str guanyu OK redis\u003e set 100 1000 OK redis\u003e set long aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa OK redis\u003e type str string redis\u003e type 100 string redis\u003e type long string redis\u003e object encoding str \"embstr\" redis\u003e object encoding 100 \"int\" redis\u003e object encoding long \"raw\" type 命令可以看到 value 的类型都是 string，object encoding 查看编码，可以看到虽然都是字符串，但是编码方式不同。字符串就只有 3 种编码方式：\nint embstr raw ","string-encoding#String Encoding":"raw raw 就是 SDS。\nRedis 的字符串有两种存储方式，在长度特别短时，使用 emb 形式存储 (embeded)，当长度超过 44 时，使用 raw 形式存储。\nint redisObject 中的 ptr 指针是真正存储数据的地方，但是对于 int 编码来说，一个 int 类型的整数最多是 64 位，也就是 8 个字节，而 ptr 指针所占用的存储空间也是 8 个字节。\n那么能不能把 int 类型的整数直接存储在 ptr 指针中呢？\n答案是可以的，但是为了避免内存的浪费，Redis 在存储 value 时，会判断 value 的长度，如果 value 的长度小于 20 个字符并且可以转为整形（2^64 能表示的最大的数字是 20 位），那么就会把 value 直接存储在 ptr 指针中。\n这么做的好处有两个：\n节省了内存，不需要再为 value 分配内存。 可以直接使用 ptr 指针中的值，而不需要再根据 ptr 指针地址去取值，省去了一次内存访问的开销。 embstr 字符串长度小于 44 时，使用 embstr 形式存储。\n为什么是小于 44 呢？ 先看 Redis 对象头结构体：\nstruct redisObject { unsigned type:4; // 4 bits unsigned encoding:4; // 4 bits unsigned lru:LRU_BITS; // 24 bits int refcount; // 4 bytes void *ptr; // 8 bytes，64-bit system } robj; 不同的对象具有不同的类型 type，同一个类型的 type 会有不同的编码形式 encoding，为了记录对象的 LRU 信息，使用了 24 个 bit 来记录 LRU 信息。 每个对象都有个引用计数，当引用计数为零时，对象就会被销毁，内存被回收。 ptr 指针将指向对象内容 (body) 的具体存储位置。这样一个 redisObject 需要占据 16 字节的存储空间。 CPU 缓冲行 cache line 一般一次性读取 64 字节的数据。\nRedis 读取数据时，先从 dictEntry 中读取到 value 的指针，拿到 redisObject 后，再通过 redisObject 中的 ptr 去读取 value 的数据。上面已经知道 redisObject 占据 16 字节，而 CPU cache line 一般一次性读取 64 字节的数据，还有 48 字节的空间没有被使用。\n那这 48 字节的空间可以用来存储什么数据呢？\n再来看 SDS 结构体：\nstruct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; // 1 byte uint8_t alloc; // 1 byte unsigned char flags; // 1 byte char buf[]; }; len，alloc，flags 分别占据 1 字节，而且由于 sds 要兼容 C 语言的函数库，所以 buf 后面还要添加一个字符 \\0，也占据 1 字节。所以这个 sds 对象要占据 4 字节。\n48 减去 4 字节，还剩下 44 字节，刚好可以存储一个 44 字节的字符串。这样就可以一次性把 redisObject 和 sds 一起读取到 CPU cache line 中，减少了内存访问的次数，提高了读取效率。"},"title":"Redis Object"},"/db-learn/docs/redis/advance/04_bitmap/":{"data":{"":"位图不是特殊的数据结构，它的内容其实就是普通的字符串，也就是 byte 数组。\n在日常开发中，可能会有一些 bool 型数据需要存取，如果使用普通的 key/value 方式存储，会浪费很多存储空间,比如签到记录，签了是 true，没签是 false，记录 365 天。如果每个用户存储 365 条记录，当用户量很庞大的时候，需要的存储空间是惊人的。\n对于这种操作，Redis 提供了位操作，这样每天的签到记录只占据一个位，用 1 表示已签到，0 表示没签，那么 365 天就是 365 个 bit，46 个字节就可以完全容纳下，大大节约了存储空间。\n11001101010010 当我们要统计月活的时候，因为需要去重，需要使用 set 来记录所有活跃用户的 id，这非常浪费内存。这时就可以考虑使用位图来标记用户的活跃状态。每个用户会都在这个位图的一个确定位置上，0 表示不活跃，1 表示活跃。然后到月底遍历一次位图就可以得到月度活跃用户数。不过这个方法也是有条件的，那就是 userid 是整数连续的（用户 id 作为 offset），并且活跃占比较高，否则可能得不偿失。","基本使用#基本使用":"Redis 的位数组是自动扩展的，如果设置了某个偏移位置超出了现有的内容范围，就会自动将位数组进行零扩充。\nsetbit 设置指定偏移量上的 bit 的值：\nsetbit key offset 0|1 当 key 不存在时，自动创建一个新的 key。 offset 参数的取值范围为大于等于 0，小于 2^32(bit 映射限制在 512MB 以内)。 # 日活场景，设置 11 月 11 日用户 100 的登录状态为 1 redis\u003e setbit login_11_11 100 1 (integer) 0 redis\u003e getbit login_11_11 100 (integer) 1 redis\u003e getbit login_11_11 101 (integer) 0 # bit 默认被初始化为 0 redis\u003e strlen login_11_11 # 查看长度 (integer) 13 # 13 个字节 redis\u003e type login_11_11 # 查看类型 string # 字符串类型 redis\u003e get login_11_11 # 查看值 \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\" # 13 个字节的字符串 ℹ️ 上面的 length 之所以是 13 个字节，是因为 100 表示的是偏移量为 100 的 bit，而一个字节有 8 个 bit，所以需要 13 个字节才能表示偏移量为 100 的 bit。 getbit 获取指定偏移量上的 bit 的值：\nredis\u003e exists testbit2 (integer) 0 redis\u003e getbit testbit2 100 (integer) 0 redis\u003e setbit testbit2 100 1 (integer) 0 redis\u003e getbit testbit2 100 (integer) 1 offset 比字符串值的长度大，或者 key 不存在时，返回 0。\n统计 统计指令 bitcount 用来统计指定位置范围内 1 的个数。 bitop 指令用来对多个 bit 数组进行位运算。 比如可以通过 bitcount 统计用户一共签到了多少天，如果指定了范围参数 [start, end]，就可以统计在某个时间范围内用户签到了多少天。\nstart 和 end 参数是字节索引，也就是说指定的位范围必须是 8 的倍数，而不能任意指定。\nredis\u003e setbit login_11_11 100 1 # 模拟用户 100 签到 (integer) 0 redis\u003e setbit login_11_11 101 1 # 模拟用户 101 签到 (integer) 0 redis\u003e setbit login_11_11 102 1 # 模拟用户 102 签到 (integer) 0 redis\u003e setbit login_11_11 103 1 # 模拟用户 103 签到 (integer) 0 redis\u003e bitcount login_11_11 # 统计 bit 为 1 的数量，没有指定范围，默认是整个字符串 (integer) 4 redis\u003e strlen login_11_11 # 查看长度 (integer) 13 # 13 个字节 redis\u003e bitcount login_11_11 0 12 # 0 是第一个字节，12 是最后一个字节，统计 0-12 字节范围内 bit 为 1 的数量 (integer) 4 假设要统计用户连续登录的情况，比如用户 100 连续登录了 5 天，用户 101 连续登录了 3 天。\n可以将几天的数据进行位运算，然后再统计结果中 1 的个数。\nlogin_11_07: 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 login_11_08: 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 login_11_09: 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 login_11_10: 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 login_11_11: 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 ---------------------------------------------- 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 # 按位与运算，连续登录的人数 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 # 按位或运算，只要有一天登录，就为 1，可以用来统计周活，月活等 bitop 示例：\nredis\u003e setbit login_11_10 100 1 # 模拟用户 100 签到 (integer) 0 redis\u003e setbit login_11_10 101 1 # 模拟用户 101 签到 (integer) 0 redis\u003e setbit login_11_10 102 1 # 模拟用户 102 签到 (integer) 0 redis\u003e bitop and login_11_10-11 login_11_10 login_11_11 # 按位与运算，连续登录的人数，and 表示按位与运算，结果保存在 login_11_10-11 中 (integer) 13 redis\u003e bitcount login_11_10-11 # 统计连续登录的人数 (integer) 3 redis\u003e bitop or login_11_10-11-active login_11_10 login_11_11 # 按位或运算，只要有一天登录，就为 1，or 表示按位或运算，结果保存在 login_11_10-11-active 中 (integer) 13 redis\u003e bitcount login_11_10-11-active (integer) 4 "},"title":"位图"},"/db-learn/docs/redis/advance/05_ziplist/":{"data":{"":"Redis 为了节约内存空间使用，zset 和 hash 容器对象在元素个数较少的时候，采用压缩列表 (ziplist) 进行存储。压缩列表是一块连续的内存空间，元素之间紧挨着存储，没有任何冗余空隙。\n例如：\nredis\u003e hset testhash name pooky address shanghai f1 v1 f2 v2 f3 v3 (integer) 5 redis\u003e hgetall testhash 1) \"name\" 2) \"pooky\" 3) \"address\" 4) \"shanghai\" 5) \"f1\" 6) \"v1\" 7) \"f2\" 8) \"v2\" 9) \"f3\" 10) \"v3\" redis\u003e object encoding testhash \"ziplist\" 可以看出，hash 中的元素是顺序存放的。这时 hash 底层的存储结构时 ziplist。\nredis\u003e hset testhash f4 aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (integer) 1 redis\u003e hgetall testhash 1) \"name\" 2) \"pooky\" 3) \"f4\" 4) \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" 5) \"f2\" 6) \"v3\" 7) \"f3\" 8) \"v3\" 9) \"f1\" 10) \"v1\" 11) \"address\" 12) \"shanghai\" redis\u003e object encoding testhash \"hashtable\" 可以看出，hash 中的元素时乱序的，这是因为这个时候 hash 底层的存储结构已经从 ziplist 变成了 hashtable。f4 这个元素的长度超过了 hash-max-ziplist-value 的 64 字节。","intset#intset":"当 set 集合容纳的元素都是整数并且元素个数较少时，会使用 intset 来存储结合元素。intset 是紧凑的数组结构，同时支持 16 位、32 位和 64 位整数。\n如果向 set 里存储非整数值时，那么 sadd 立即转变为 hashtable 结构。\nredis\u003e sadd testset 1 2 3 5 10 9 4 4 4 (integer) 7 redis\u003e smembers testset 1) \"1\" 2) \"2\" 3) \"3\" 4) \"4\" 5) \"5\" 6) \"9\" 7) \"10\" redis\u003e type testset \"set\" redis\u003e object encoding testset \"intset\" 可以看到上面的 set 中的元素是有序的，为什么不是无序的？因为 set 底层的存储结构是 intset，intset 是一个紧凑的数组结构。有序的数组查询的时间复杂度是 O(logn)，因为可以使用二分查找。\nredis\u003e sadd testset a (integer) 1 redis\u003e smembers testset 1) \"1\" 2) \"4\" 3) \"9\" 4) \"3\" 5) \"2\" 6) \"10\" 7) \"a\" 8) \"5\" redis\u003e type testset \"set\" redis\u003e object encoding testset \"hashtable\" 可以看到上面的 set 中的元素是无序的，因为 set 底层的存储结构已经变成了 hashtable。\n配置选项 intset 能存储的元素个数可以通过下面的配置项来调整：\nset-max-intset-entries 512 # set 的整数元素个数超过 512，使用 hashtable 存储 数据结构 typedef struct intset { uint32_t encoding; // 编码类型，决定整数位是 16 位、32 位还是 64 位 uint32_t length; // 元素个数 int8_t contents[]; // 元素数组，可以是 16 位、32 位和 64 位 } intset; ","增加元素#增加元素":"因为 ziplist 是紧凑存储，没有冗余空间 (对比一下 Redis 的字符串结构)。意味着插入一个新的元素就需要调用 realloc 扩展内存。取决于内存分配器算法和当前的 ziplist 内存大小，realloc 可能会重新分配新的内存空间，并将之前的内容一次性拷贝到新的地址，也可能在原有的地址上进行扩展，这时就不需要进行旧内容的内存拷贝。\n如果 ziplist 占据内存太大，重新分配内存和拷贝内存就会有很大的消耗。所以 ziplist 不适合存储大型字符串，存储的元素也不宜过多。","数据结构#数据结构":"Redis 的 ziplist 是一个紧凑的 byte 数组结构，如下图，每个元素之间都是紧挨着的。\nzlbytes：整个压缩列表占用字节数。 zltail_offset 最后一个元素距离压缩列表起始位置的偏移量，用于快速定位到最后一个节点。 zllength：元素个数。 entries：元素内容列表，挨个挨个紧凑存储。 zlend：标志压缩列表的结束。 ℹ️ ziplist 为了支持双向遍历，所以才会有 ztail_offset 这个字段，用来快速定位到最后一个元素，然后倒着遍历。 entry 块可以容纳不同的元素类型，也会有不一样的结构：\nprerawlen：表示前一个 entry 的字节长度，当压缩列表倒着遍历时，需要通过这个字段来快速定位到下一个元素的位置。它是一个变长的整数，当字符串长度小于 254 时，使用一个字节表示；如果达到或超出 254 那就使用 5 个字节来表示（第一个字节是 254，剩余四个字节表示字符串长度）。 len：除了表示当前元素的字节长度，还有别的含义。len 的第一个字节分为 9 种情况： 00xxxxxx：前两个 bit 是 00，len 占 1 个字节。剩余的 6 个 bit 表示字符串长度，即最大的长度是 2^6 - 1。 01xxxxxx xxxxxxxx：前两个 bit 是 01，len 占 2 个字节。剩余的 14 个 bit 表示字符串长度，即最大的长度是 2^14 - 1。 10xxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx：前两个 bit 是 10，len 占 5 个字节。剩余的 32 个 bit 表示字符串长度，即最大的长度是 2^32 - 1。 11000000：表示 int16，len 占 1 个字节。后面的 data 占 2 个字节。 11010000：表示 int32，len 占 1 个字节。后面的 data 占 4 个字节。 11100000：表示 int64，len 占 1 个字节。后面的 data 占 8 个字节。 11110000 表示 int24，len 占 1 个字节。后面的 data 占 3 个字节。 11111110 表示 int8，len 占 1 个字节。后面的 data 占 1 个字节。 1111xxxx 表示极小整数，xxxx 的范围只能是 0001~1101, 也就是 1~13，因为 0000、1110、1111（11111111 表示 ziplist 的结束，也就是 zlend）都被其他情况占用了。读取到的 value 需要将 xxxx 减 1，也就是整数 0~12 就是最终的 value。 data：元素的内容。 存储 hash 结构 如果 ziplist 存储的是 hash 结构，那么 key 和 value 会作为两个 entry 相邻存在一起。\n配置选项 当数据量比较少，或者单个元素比较小的时候，Redis 会使用 ziplist 来存储。数据大小和元素数量的阈值可以通过以下配置项来调整：\nhash-max-ziplist-entries 512 # ziplist 的元素个数超过 512，就使用 hashtable 存储 hash-max-ziplist-value 64 # 单个元素大小超过 64 字节，就使用 hashtable 存储 存储 zset 结构 zset 使用 ziplist 来存储元素时，value 和 score 会作为两个 entry 相邻存在一起。\nredis\u003e zadd testzset 100 a 200 b 150 c (integer) 3 redis\u003e zrange testzset 0 -1 withscores 1) \"a\" 2) \"100\" 3) \"c\" 4) \"150\" 5) \"b\" 6) \"200\" redis\u003e type testzset \"zset\" redis\u003e object encoding testzset \"ziplist\" 配置选项 zset-max-ziplist-entries 128 # zset 的元素个数超过 128，使用 skiplist 存储 zset-max-ziplist-value 64 # zset 的任意元素大小超过 64 字节，使用 skiplist 存储 "},"title":"ziplist"},"/db-learn/docs/redis/advance/06_quicklist/":{"data":{"":"Redis 早期版本存储 list 列表数据结构使用的是压缩列表 ziplist 和普通的双向链表 linkedlist，也就是元素少时用 ziplist，元素多时用 linkedlist。\n但是由于 linkedlist 结构，要存储 pre 和 next 指针，一个指针的大小为 8 字节，两个指针的大小为 16 字节（如果节点本身存储的 value 很小，比如 1 个字节，但是还是需要 16 个字节来存储指针，非常浪费空间）。当链表中的节点非常多时，指针占用的空间就会非常大，而且链表的内存是不连续的，会产生内存碎片，影响内存的利用率。\nRedis 使用 quicklist 代替了 ziplist 和 linkedlist。\nquicklist 是 ziplist 和 linkedlist 的混合体，它将 linkedlist 按段切分，每一段使用 ziplist 来紧凑存储，每个节点之间用双向指针串接起来，组成一个双向链表。\nquicklist 虽然还有 pre 和 next 指针，但是节点少了很多。","压缩深度#压缩深度":"当某一个 list 列表中，存储了一部分热点数据，如果头尾节点数据是热点数据经常被访问，中间的数据访问不是那么频繁，就可以设置压缩深度，将中间的数据压缩，减少内存的占用。","增加元素#增加元素":"增加元素时，只需要修改其中一个 ziplist 节点即可。当 ziplist 节点的大小超过了 list-max-ziplist-size 时，就会分裂出新的 quicklistNode，将数据存储在新的 ziplist 节点中。","配置选项#配置选项":"可以设置每个 ziplist 的最大容量，quicklist 的数据压缩范围，提升数据存取效率。\nlist-max-ziplist-size -2 # -2 表示每个 ziplist 最多存储 8kb 大小，超过则会分裂，将数据存储在新的 ziplist 节点中 list-compress-depth 0 # 压缩深度。0 表示不压缩，1 表示头部的一个，尾部的一个，一共两个 ziplist 节点不压缩，中间的节点全部压缩。依次类推，2 表示头部的两个，尾部的两个，一共四个节点不压缩。 list-max-ziplist-size 不建议设置太大，因为 ziplist 增加元素时会重新分配新的内存空间，并将之前的内容一次性拷贝到新的地址。如果 ziplist 占据内存太大，重新分配内存和拷贝内存就会有很大的消耗。\n所以 ziplist 不适合存储大型字符串，存储的元素也不宜过多。\nquicklist 默认的压缩深度是 0。"},"title":"quicklist"},"/db-learn/docs/redis/advance/07_skiplist/":{"data":{"":"zset 的内部实现是一个 hashtable 加一个跳跃列表 (skiplist)。\nstruct zset { dict *dict; // all values value=\u003escore zskiplist *zsl; } dict 是一个 hashtable 结构用来存储 value 和 score 值的映射关系。用来查找数据到分数的对应关系。 zsl 是一个 skiplist 结构，用来根据分数查询数据，支持范围查询。 ","go-实现一个简单的跳表#Go 实现一个简单的跳表":" const ( MaxLevel = 16 // 最大层级 Probability = 0.5 // 每一层晋升的概率 ) type Skiplist struct { head *Node level int // 当前跳表的最大层级 } type Node struct { // nexts 指针是一个 slice 的形式，其长度对应为当前节点的高度 nexts []*Node key, val int } // NewNode 创建一个新节点 func NewNode(key, value int, level int) *node { return \u0026node{ key: key, value: value, next: make([]*node, level), } } // NewSkipList 初始化跳表 func NewSkipList() *SkipList { rand.Seed(time.Now().UnixNano()) head := NewNode(math.MinInt32, nil, MaxLevel) // 头节点使用最小值 return \u0026SkipList{ head: head, level: 1, } } // randomLevel 决定新节点的层级（几率下降） func randomLevel() int { level := 1 for rand.Float64() \u003c Probability \u0026\u0026 level \u003c MaxLevel { level++ } return level } // 根据 key 读取 val，第二个 bool flag 反映 key 在 skiplist 中是否存在 func (s *Skiplist) Get(key int) (int, bool) { // 根据 key 尝试检索对应的 node，如果 node 存在，则返回对应的 val if _node := s.search(key); _node != nil { return _node.val, true } return -1, false } // 从跳表中检索 key 对应的 Node func (s *Skiplist) search(key int) *Node { // 每次检索从头部出发 move := s.head // 每次检索从最大高度出发，直到来到首层 for level := s.level - 1; level \u003e= 0; level-- { // 在每一层中持续向右遍历，直到下一个节点不存在或者 key 值大于等于 key for move.nexts[level] != nil \u0026\u0026 move.nexts[level].key \u003c key { move = move.nexts[level] } // 如果 key 值相等，则找到了目标直接返回 if move.nexts[level] != nil \u0026\u0026 move.nexts[level].key == key { return move.nexts[level] } // 当前层没找到目标，则层数减 1，继续向下 } // 遍历完所有层数，都没有找到目标，返回 nil return nil } // 将 key-val 对加入 skiplist func (s *Skiplist) Put(key, val int) { // 假如 kv对已存在，则直接对值进行更新并返回 if _node := s.search(key); _node != nil { _node.val = val return } // 新节点的高度 level := randomLevel() // 创建出新的节点 newNode := NewNode(key,val,level) // 从头节点的最高层出发 move := s.head for level := s.level - 1; level \u003e= 0; level-- { // 向右遍历，直到右侧节点不存在或者 key 值大于 key for move.nexts[level] != nil \u0026\u0026 move.nexts[level].key \u003c key { move = move.nexts[level] } // 调整指针关系，完成新节点的插入 newNode.nexts[level] = move.nexts[level] move.nexts[level] = \u0026newNode } } // 根据 key 从跳表中删除对应的节点 func (s *Skiplist) Del(key int) { // 如果 kv 对不存在，则无需删除直接返回 if _node := s.search(key); _node == nil { return } // 从头节点的最高层出发 move := s.head for level := s.level - 1; level \u003e= 0; level-- { // 向右遍历，直到右侧节点不存在或者 key 值大于等于 key for move.nexts[level] != nil \u0026\u0026 move.nexts[level].key \u003c key { move = move.nexts[level] } // 右侧节点不存在或者 key 值大于 target，则直接跳过 if move.nexts[level] == nil || move.nexts[level].key \u003e key{ continue } // 走到此处意味着右侧节点的 key 值必然等于 key，则调整指针引用 move.nexts[level] = move.nexts[level].nexts[level] } // 更新跳表层级 for s.level \u003e 1 \u0026\u0026 s.head.nexts[s.level-1] == nil { s.level-- } } ","zskiplist#zskiplist":"\n上图中只有三个元素，a、b、c，对应的 score 值分别是 100、120、200。\nheader 指向跳表的头节点。头节点不存储任何数据，只用来作为跳表的起始点。 tail 指向跳表的尾节点，尾节点不存储任何数据，只用来作为跳表的结束点。 level 表示当前跳表的最高层数，初始化为 1。这是用来遍历跳表时，直接找到最高的一层开始遍历。图中虽然画的是 31 层，但是实际使用时，只使用了 3 层。 length 表示跳表中元素的个数，初始化为 0。 zskiplistNode 跳表的节点结构：\nstruct zslnode { string value; double score; zslnode*[] forwards; // 多层连接指针 zslnode* backward; } backward 指针用来从后往前遍历跳表。 随机层数 不停地往跳表中插入数据时，如果不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。\n所以对于每一个新插入的节点，都需要调用一个随机算法给它分配一个合理的层数。生成高层的索引节点的概率要小于生成低层的索引节点的概率。\n/* Returns a random level for the new skiplist node we are going to create. * The return value of this function is between 1 and ZSKIPLIST_MAXLEVEL * (both inclusive), with a powerlaw-alike distribution where higher * levels are less likely to be returned. */ int zslRandomLevel(void) { int level = 1; while ((random()\u00260xFFFF) \u003c (ZSKIPLIST_P * 0xFFFF)) level += 1; return (level\u003cZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL; } ","跳表的原理#跳表的原理":"假设使用链表来存储数据，查找一个元素的时间复杂度是 O(n)，因为要遍历整个链表。\n可以加一个索引层，索引层的元素指向原始链表中的元素，这样查找元素 79，就可以从索引层开始找。只要找到第一个比 79 大的元素，图中就是 84，然后就通过前一个元素 78 指向的指针，找到原始链表中的 79。\n上面的索引层是一层，相对于原始链表来说，查找的元素个数大概减少了一半。\n如果再加一层索引层，那么查找的元素个数就会减少到原来的 1/4。\n从第一层索引层开始查找，最后一个元素是 78，比 79 小，那就往下一层索引层找。\n如果再加一层索引层，就可以明显的看出来，一次减少一半，类似二分查找：\n时间复杂度：\n如果有 N 个元素：\n第一层索引层的元素个数是 N/2。 第二层索引层的元素个数是 N/4。 第三层索引层的元素个数是 N/8。 第 k 层索引层的元素个数是 N/(2^k)。 假设 k 层索引层的元素个数是 2，那么 N/(2^k)=2 =\u003e 2^k=N/2 =\u003e k=log2(N/2)，索引的访问，每层大概访问两个元素，是常数级的。忽略掉常数，即 k=logN。"},"title":"跳表"},"/db-learn/docs/redis/advance/08_geohash/":{"data":{"":"","geo-指令#Geo 指令":"geoadd geoadd 指令携带集合名称以及多个经纬度名称三元组，注意这里可以加入多个三元组：\n127.0.0.1:6379\u003e geoadd company 116.48105 39.996794 juejin (integer) 1 127.0.0.1:6379\u003e geoadd company 116.514203 39.905409 ireader (integer) 1 127.0.0.1:6379\u003e geoadd company 116.489033 40.007669 meituan (integer) 1 127.0.0.1:6379\u003e geoadd company 116.562108 39.787602 jd 116.334255 40.027400 xiaomi (integer) 2 geo 存储结构上使用的是 zset，意味着可以使用 zset 相关的指令来操作 geo 数据，所以删除指令可以直接使用 zrem 指令。\ngeodist geodist 指令可以用来计算两个元素之间的距离，携带集合名称、2 个名称和距离单位。\n127.0.0.1:6379\u003e geodist company juejin ireader km \"10.5501\" 127.0.0.1:6379\u003e geodist company juejin meituan km \"1.3878\" 127.0.0.1:6379\u003e geodist company juejin jd km \"24.2739\" 127.0.0.1:6379\u003e geodist company juejin xiaomi km \"12.9606\" 127.0.0.1:6379\u003e geodist company juejin juejin km \"0.0000\" 掘金离美团最近，因为它们都在望京。距离单位可以是 m、km、ml、ft，分别代表米、千米、英里和尺。\ngeopos geopos 指令可以获取集合中任意元素的经纬度坐标，可以一次获取多个。\n127.0.0.1:6379\u003e geopos company juejin 1) 1) \"116.48104995489120483\" 2) \"39.99679348858259686\" 127.0.0.1:6379\u003e geopos company ireader 1) 1) \"116.5142020583152771\" 2) \"39.90540918662494363\" 127.0.0.1:6379\u003e geopos company juejin ireader 1) 1) \"116.48104995489120483\" 2) \"39.99679348858259686\" 2) 1) \"116.5142020583152771\" 2) \"39.90540918662494363\" 获取的经纬度坐标和 geoadd 进去的坐标有轻微的误差，原因是 geohash 对二维坐标进行的一维映射是有损的，通过映射再还原回来的值会出现较小的差别。\ngeohash geohash 可以获取元素的经纬度编码字符串，它是 base32 编码。\n127.0.0.1:6379\u003e geohash company ireader 1) \"wx4g52e1ce0\" 127.0.0.1:6379\u003e geohash company juejin 1) \"wx4gd94yjn0\" 可以使用 geohash 来解编码。格式 http://geohash.org/wx4g52e1ce0。\ngeoradiusbymember georadiusbymember 指令是最为关键的指令，它可以用来查询指定元素附近的其它元素。\n# 范围 20 公里以内最多 3 个元素按距离正排，它不会排除自身 127.0.0.1:6379\u003e georadiusbymember company ireader 20 km count 3 asc 1) \"ireader\" 2) \"juejin\" 3) \"meituan\" # 范围 20 公里以内最多 3 个元素按距离倒排 127.0.0.1:6379\u003e georadiusbymember company ireader 20 km count 3 desc 1) \"jd\" 2) \"meituan\" 3) \"juejin\" # 三个可选参数 withcoord withdist withhash 用来携带附加参数 # withdist 很有用，它可以用来显示距离 127.0.0.1:6379\u003e georadiusbymember company ireader 20 km withcoord withdist withhash count 3 asc 1) 1) \"ireader\" 2) \"0.0000\" 3) (integer) 4069886008361398 4) 1) \"116.5142020583152771\" 2) \"39.90540918662494363\" 2) 1) \"juejin\" 2) \"10.5501\" 3) (integer) 4069887154388167 4) 1) \"116.48104995489120483\" 2) \"39.99679348858259686\" 3) 1) \"meituan\" 2) \"11.5748\" 3) (integer) 4069887179083478 4) 1) \"116.48903220891952515\" 2) \"40.00766997707732031\" georadius georadius 可以用来查询指定坐标附近的其它元素。参数和 georadiusbymember 基本一致。\n127.0.0.1:6379\u003e georadius company 116.514202 39.905409 20 km withdist count 3 asc 1) 1) \"ireader\" 2) \"0.0000\" 2) 1) \"juejin\" 2) \"10.5501\" 3) 1) \"meituan\" 2) \"11.5748\" ","geohash-算法#GeoHash 算法":"GeoHash 是一种地理位置编码的方法。GeoHash 算法将二维的经纬度数据映射到一维的整数，这样所有的元素都将在挂载到一条线上，距离靠近的二维坐标映射到一维后，两点之间距离也会很接近。当我们想要计算附近的人时，首先将目标位置映射到这条线上，然后在这个一维的线上获取附近的点就行了。\n地图元素的位置数据使用二维的经纬度表示，经度范围 (-180, 180]，纬度范围 (-90, 90]，纬度正负以赤道为界，北正南负，经度正负以本初子午线 (英国格林尼治天文台) 为界，东正西负。\n如果纬度范围 [-90, 0) 用二进制 0 表示，(0, 90] 用二进制 1 表示。经度范围 [-180, 0) 用二进制 0 表示，(0, 180] 用二进制 1 表示。那么地球可以分为 4 个区域：\n00 第一个 0 表示纬度 [-90, 0)，第二个 0 表示经度 [-180, 0)。 10 0 表示纬度 [-90, 0)，1 表示经度 (0, 180]。 01 1 表示纬度 (0, 90]，0 表示经度 [-180, 0)。 11 第一个 1 表示纬度 (0, 90]，第二个 1 表示经度 (0, 180]。 分成 4 个区域之后，大概可以知道在地球的哪个方位了。如果想要更精确的定位，就可以继续切分，比如把 00 继续分成 4 个区域。\n00 分成了 4 个区域，这四个区域前面的 00 就是父区域的编码。子区域的四个编码还是 00、01、10、11，再加上父区域的编码作为前缀，就得到了 0000、0001、0010、0011。 有共同前缀的区域，可以理解为距离也是比价近的。 继续切下去，正方形就会越来越小，二进制整数也会越来越长，精确度就会越来越高。\n通过上述的过程，最终会得到一串二进制的编码。这串二进制编码通常很长，不便于阅读和传输。为了使得 Geohash 更加紧凑和易于使用，通常会将这串二进制编码转换成 Base32 编码。\n在 Redis 里面，将这个编码值放了 zset 的 score 中。\n在使用 Redis 进行 Geo 查询时，通过 zset 的 score 排序就可以得到坐标附近的其它元素，通过将 score 还原成坐标值就可以得到元素的原始坐标。","注意#注意":"地图应用中，车的数据、餐馆的数据可能会有百万千万条，如果使用 Redis 的 Geo 数据结构，它们将全部放在一个 zset 集合中。在 Redis 的集群环境中，集合可能会从一个节点迁移到另一个节点，如果单个 key 的数据过大，会对集群的迁移工作造成较大的影响，在集群环境中单个 key 对应的数据量不宜超过 1M，否则会导致集群迁移出现卡顿现象，影响线上服务的正常运行。\n建议 Geo 的数据使用单独的 Redis 实例部署，不使用集群环境。\n如果数据量过亿甚至更大，就需要对 Geo 数据进行拆分，按国家拆分、按省拆分，按市拆分，在人口特大城市甚至可以按区拆分。这样就可以显著降低单个 zset 集合的大小。"},"title":"GeoHash"},"/db-learn/docs/redis/advance/09_hyperloglog/":{"data":{"":"如果你负责开发维护一个大型的网站，有一天老板找产品经理要网站每个网页每天的 UV（用户访问量，一个用户一天不论访问一次还是多次，都只算一次）数据，然后让你来开发这个统计模块，如何实现？\n如果统计 PV（页面访问量，只要页面被访问一次，就算一次，不管用户访问了多少次）那非常好办，给每个网页一个独立的 Redis 计数器就可以了，这个计数器的 key 后缀加上当天的日期。这样来一个请求，incrby 一次，最终就可以统计出所有的 PV 数据。\n但是 UV 不一样，它要去重，同一个用户一天之内的多次访问请求只能计数一次。这就要求每一个网页请求都需要带上用户的 ID，无论是登陆用户还是未登陆用户都需要一个唯一 ID 来标识。\n一个非常简单的方案：\n为每一个页面一个独立的 set 集合来存储所有当天访问过此页面的用户 ID。当一个请求过来时，使用 sadd 将用户 ID 塞进去就可以了。通过 scard 可以取出这个集合的大小，这个数字就是这个页面的 UV 数据。\n但是，如果页面访问量非常大，比如一个爆款页面几千万的 UV，就需要一个很大的 set 集合来统计，这就非常浪费空间。如果这样的页面很多，还需要为每个页面都去创建一个 set 集合，那所需要的存储空间是惊人的。\nRedis 提供了 HyperLogLog 数据结构就是用来解决这种统计问题的。HyperLogLog 提供不精确的去重计数方案，虽然不精确但是也不是非常不精确，可以满足上面的 UV 统计需求了。","hyperloglog-原理#HyperLogLog 原理":"基本原理 HyperLogLog 基于概率论中伯努利试验并结合了极大似然估算方法，并做了分桶优化。\n实际上目前还没有发现更好的在大数据场景中准确计算基数的高效算法，因此在不追求绝对准确的情况下，使用概率算法算是一个不错的解决方案。概率算法不直接存储数据集合本身，通过一定的概率统计方法预估值，这种方法可以大大节省内存，同时保证误差控制在一定范围内。目前用于基数计数的概率算法包括:\nLinear Counting(LC)：早期的基数估计算法，LC 在空间复杂度方面并不算优秀。 LogLog Counting(LLC)：LogLog Counting相比于 LC 更加节省内存，空间复杂度更低。 HyperLogLog Counting(HLL)：HyperLogLog Counting 是基于 LLC 的优化和改进，在同样空间复杂度情况下，能够比 LLC 的基数估计误差更小。 举个例子来理解 HyperLogLog 算法，有一天 Fox 老师和 Mark 老师玩抛硬币的游戏，规则是 Mark 老师负责抛硬币，每次抛的硬币可能正面，可能反面，每当抛到正面为一回合，Mark 老师可以自己决定进行几个回合。最后需要告诉 Fox 老师最长的那个回合抛了多少次以后出现了正面，再由 Fox 老师来猜 Mark 老师一共进行了几个回合。\n进行了 n 个回合试验，比如上图：\n第一次: 抛了 3 次才出现正面，此时 `k=3，n=1`。 第二次试验: 抛了 2 次才出现正面，此时 `k=2，n=2`。 第三次试验: 抛了 4 次才出现正面，此时 `k=4，n=3`。 ………… 第 n 次试验：抛了 7 次才出现正面，此时我们估算，`k=7，n=n`。 k 是每回合抛到 1（硬币的正面）所用的次数，我们已知的是最大的 k 值，也就是 Mark 老师告诉 Fox 老师的数，可以用 k_max 表示。由于每次抛硬币的结果只有 0 和 1 两种情况，因此，能够推测出 k_max 在任意回合出现的概率 ，并由 kmax 结合极大似然估算的方法推测出 n = 2^(k_max)。概率学把这种问题叫做伯努利实验。\n现在 Mark 老师已经完成了 n 个回合，并且告诉 Fox 老师最长的一次抛了 4 次，Fox 老师此时也胸有成竹，马上说出他的答案 16，最后的结果是：Mark 老师只抛了 3 回合，这三个回合中 k_max=4，放到公式中，Fox 老师算出 n=2^4，于是推测 Mark 老师抛了 16 个回合，但是 Fox 老师输了。\n所以这种预估方法存在较大误差，为了改善误差情况，HLL 中引入分桶平均的概念。\n同样举抛硬币的例子，如果只有一组抛硬币实验，显然根据公式推导得到的实验次数的估计误差较大；如果 100 个组同时进行抛硬币实验，样本数变大，受运气影响的概率就很低了，每组分别进行多次抛硬币实验，并上报各自实验过程中抛到正面的抛掷次数的最大值，就能根据 100 组的平均值预估整体的实验次数了。\n分桶平均的基本原理是将统计数据划分为 m 个桶，每个桶分别统计各自的 k_max,​ 并能得到各自的基数预估值，最终对这些基数预估值求平均得到整体的基数估计值。LLC 中使用几何平均数预估整体的基数值，但是当统计数据量较小时误差较大；HLL 在 LLC 基础上做了改进，采用调和平均数过滤掉不健康的统计值。\n调和平均数 举个例子：\n求平均工资：A 的是 1000/月，B 的 30000/月。采用平均数的方式就是：(1000 + 30000) / 2 = 15500\n采用调和平均数的方式就是：2/(1/1000 + 1/30000) ≈ 1935.484。\n可见调和平均数比平均数的好处就是不容易受到大的数值的影响，比平均数的效果是要更好的。\n结合实例 下面我们结合实例来理解 HyperLogLog 算法。\n统计网页每天的 UV 数据。\n转为比特串 通过 hash 函数，将用户 ID 转为二进制数，例如用户 ID 为 5，便转为 101。\n为什么要这样转化呢？\n是因为要和抛硬币对应上，二进制数中，0 代表了反面，1 代表了正面，如果一个数据最终被转化了 10010000，那么从右往左，从低位往高位看，我们可以认为，首次出现 1 的时候，就是正面。\n那么基于上面的估算结论，我们可以通过多次抛硬币实验的最大抛到正面的次数来预估总共进行了多少次实验，同样也就可以根据存入数据中，转化后的出现了 1 的最大的位置 k_max 来估算存入了多少数据。\n分桶 分桶就是分多少轮。就是一个长度为 L 的 bitmap S，将 S 平均分为 m 组，这个 m 组，就是对应多少轮，然后每组所占有的比特个数是平均的，设为 P。容易得出下面的关系：\nL = S.length L = m * p 以 KB 为单位，S 占用的内存 = L / 8 / 1024。\n对应 假设访问用户 ID 为：idn, n-\u003e0,1,2,3....\n在这个统计问题中，不同的用户 ID 标识了一个用户，那么我们可以把用户的 ID 作为被 hash 的输入。即：hash(id) = 二进制数。\n不同的用户 ID，拥有不同的二进制数。每一个二进制数，也必然会至少出现一次 1 的位置。我们类比每一个二进制数为一次伯努利试验。\n现在要分轮，也就是分桶。所以我们可以设定，每个二进制数的前多少位转为 10 进制后，其值就对应于所在桶的标号。假设二进制数的低两位用来计算桶下标志，总共有 4 个桶，此时有一个用户的 ID 的二进制数是：1001011000011。它的所在桶下标为：1*2^1 + 1*2^0 = 3，处于第 3 个桶，即第 3 轮中。\n上面例子中，计算出桶号后，剩下的二进制数是：10010110000，从右往左，从低位到高位看，第一次出现 1 的位置是 5 。也就是说，此时第 3 个桶中，k_max = 5。5 对应的二进制是：101，将 101 存入第 3 个桶。\n模仿上面的流程，多个不同的用户 ID，就被分散到不同的桶中去了，且每个桶有其 k_max。然后当要统计出某个页面有多少用户点击量的时候，就是一次估算。最终结合所有桶中的 k_max，代入估算公式，便能得出估算值。\nRedis 中的 HyperLogLog 实现 Redis 的实现中，HyperLogLog 占据 12KB 的大小，共设有 16384 个桶，即：2^14 = 16384，每个桶有 6 位 (占用内存=16834 * 6 / 8 / 1024 = 12K)，每个桶可以表达的最大数字是：111111=63。\n对于命令：pfadd key value，它的工作原理是：\n在存入时，value 会被 hash 成 64 位的二进制数，前 14 位用来分桶，剩下 50 位用来记录第一个 1 出现的位置。之所以选 14 位来表达桶编号是因为分了 16384 个桶，而 2^14 = 16384，刚好地，最大的时候可以把桶利用完，不造成浪费。假设一个字符串的前 14 位是：00000000000010 (从右往左看) ，其十进制值为 2。那么 value 对应转化后的值放到编号为 2 的桶。\nindex 的转化规则：\n首先因为完整的 value 比特字符串是 64 位形式，减去 14 后，剩下 50 位，假设极端情况，出现 1 的位置，是在第 50 位，即位置是 50。此时 index = 50。此时先将 index 转为 2 进制，它是：50=110010，所以 6 bit 的桶足够表示。\n因为 16384 个桶中，每个桶是 6 bit 组成的。于是 110010 就被设置到了第 2 号桶中去了。请注意，50 已经是最坏的情况，且它都被容纳进去了。那么其他的不用想也肯定能被容纳进去。\n因为 pfadd 的 key 可以设置多个 value。例如下面的例子：\npfadd lgh golang pfadd lgh python pfadd lgh java 根据上面的做法，不同的 value，会被设置到不同桶中去，如果出现了在同一个桶的，即前 14 位值是一样的，但是后面出现 1 的位置不一样。那么比较原来的 index 是否比新 index 大。是，则替换。否，则不变。\n最终，一个 key 所对应的 16384 个桶都设置了很多的 value 了，每个桶有一个 k_max。此时调用 pfcount 时，按照调和平均数进行估算，同时加以偏差修正，便可以计算出 key 的设置了多少次 value，也就是统计值。\nvalue 被转为 64 位的比特串，最终被按照上面的做法记录到每个桶中去。64 位转为十进制就是：2^64，HyperLogLog 仅用了：16384 * 6 /8 / 1024 =12K 存储空间就能统计多达 2^64 个数。","pfmerge#pfmerge":"pfmerge 用于将多个 pf 计数值累加在一起形成一个新的 pf 值。\n比如在网站中我们有两个内容差不多的页面，运营说需要这两个页面的数据进行合并。其中页面的 UV 访问量也需要合并，那这个时候 pfmerge 就可以派上用场了。","使用#使用":"HyperLogLog 提供了两个指令 pfadd 和 pfcount，根据字面意义很好理解，一个是增加计数，一个是获取计数。\n127.0.0.1:6379\u003e pfadd 08-15:u:id user1 (integer) 1 127.0.0.1:6379\u003e pfcount 08-15:u:id (integer) 1 127.0.0.1:6379\u003e pfadd 08-15:u:id user2 (integer) 1 127.0.0.1:6379\u003e pfcount 08-15:u:id (integer) 2 127.0.0.1:6379\u003e pfadd 08-15:u:id user3 (integer) 1 127.0.0.1:6379\u003e pfcount 08-15:u:id (integer) 3 127.0.0.1:6379\u003e pfadd 08-15:u:id user4 (integer) 1 127.0.0.1:6379\u003e pfcount 08-15:u:id (integer) 4 127.0.0.1:6379\u003e pfadd 08-15:u:id user5 (integer) 1 127.0.0.1:6379\u003e pfcount 08-15:u:id (integer) 5 127.0.0.1:6379\u003e pfadd 08-15:u:id user6 (integer) 1 127.0.0.1:6379\u003e pfcount 08-15:u:id (integer) 6 127.0.0.1:6379\u003e pfadd 08-15:u:id user7 user8 user9 user10 (integer) 1 127.0.0.1:6379\u003e pfcount 08-15:u:id (integer) 10 但是在数据量比较大的时候，pfcount 的结果就会出现误差。\n以使用集合类型和 HperLogLog 统计百万级用户访问次数的占用空间对比：\n数据类型 1天 1个月 1年 集合类型 80M 2.4G 28G HyperLogLog 15k 450k 5M 可以看到，HyperLogLog 内存占用量小得惊人，但是用如此小空间来估算如此巨大的数据，必然不是 100% 的正确，其中一定存在误差率。前面说过，Redis 官方给出的数字是 0.81% 的失误率。"},"title":"HyperLogLog"},"/db-learn/docs/redis/advance/10_redis_expire_strategy/":{"data":{"":"Redis 是怎么删除过期的 key 的？而且 Redis 是单线程的，删除 key 过于频繁会不会造成阻塞？\nRedis 有三种清除策略\n懒惰删除就是在客户端访问这个 key 的时候，redis 对 key 的过期时间进行检查，如果过期了就立即删除。 定时删除是集中处理，惰性删除是零散处理。 当前已用内存超过 maxmemory 限定时，触发主动清理策略 ","内存淘汰机制#内存淘汰机制":"当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)。交换会让 Redis 的性能急剧下降，对于 Redis 来说，这样龟速的存取效率基本上等于不可用。\nRedis 为了限制最大使用内存，提供了配置参数 maxmemory，可以在 redis.conf 中配置。当内存超出 maxmemory，Redis 提供了几种策略（maxmemory-policy）让用户选择：\nnoeviction：当内存超出 maxmemory，写入请求会报错，但是删除和读请求可以继续。（这个可是默认的策略）。 allkeys-lru：当内存超出 maxmemory，在所有的 key 中，移除最少使用的 key。 allkeys-random：当内存超出 maxmemory，在所有的 key 中，随机移除某个 key。（应该没人用吧） volatile-lru：当内存超出 maxmemory，在设置了过期时间 key 的字典中，移除最少使用的 key。 volatile-random：当内存超出 maxmemory，在设置了过期时间 key 的字典中，随机移除某个key。 volatile-ttl：当内存超出 maxmemory，在设置了过期时间 key 的字典中，优先移除 ttl 小的。 volatile 和 allkeys 的区别 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰。 allkeys-xxx 策略会对所有的 key 进行淘汰。 如果只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。如果还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。\nLFU Redis 4.0 里引入了一个新的淘汰策略 —— LFU（Least Frequently Used） 模式。\nLFU 表示按最近的访问频率进行淘汰，它比 LRU 更加精准地表示了一个 key 被访问的热度。\n如果一个 key 长时间不被访问，只是刚刚偶然被用户访问了一下，那么在使用 LRU 算法下它是不容易被淘汰的，因为 LRU 算法认为当前这个 key 是很热的。而 LFU 是需要追踪最近一段时间的访问频率，如果某个 key 只是偶然被访问一次是不足以变得很热的，它需要在近期一段时间内被访问很多次才有机会被认为很热。\n启用 LFU Redis 4.0 给淘汰策略配置参数 maxmemory-policy 增加了 2 个选项，\nvolatile-lfu：对带过期时间的 key 执行 lfu 淘汰算法 allkeys-lfu：对所有的 key 执行 lfu 淘汰算法 使用 object freq 指令获取对象的 lfu 计数值：\n\u003e config set maxmemory-policy allkeys-lfu OK \u003e set codehole yeahyeahyeah OK // 获取计数值，初始化为 LFU_INIT_VAL=5 \u003e object freq codehole (integer) 5 // 访问一次 \u003e get codehole \"yeahyeahyeah\" // 计数值增加了 \u003e object freq codehole (integer) 6 ","定期删除策略#定期删除策略":"Redis 会将每个设置了过期时间的 key 放入到一个独立的字典中，默认每 100ms 进行一次过期扫描：\n随机抽取 20 个 key。 删除这 20 个 key 中过期的 key。 如果过期的 key 比例超过 1/4，就重复步骤 1，继续删除。 之所以不扫描所有的 key，是因为 Redis 是单线程，全部扫描会导致线程卡死。\n而且为了防止每次扫描过期的 key 比例都超过 1/4，导致不停循环卡死线程，Redis 为每次扫描添加了上限时间，默认是 25ms。\n如果一个大型的 Redis 实例中所有的 key 在同一时间过期了，会出现怎样的结果 大量的 key 在同一时间过期，那么 Redis 会持续扫描过期字典 (循环多次)，直到过期字典中过期的 key 变得稀疏，才会停止 (循环次数明显下降)。这会导致线上读写请求出现明显的卡顿现象。导致这种卡顿的另外一种原因是内存管理器需要频繁回收内存页，这也会产生一定的 CPU 消耗。\n而且，如果客户端将请求超时时间设置的比较短，比如 10ms，但是请求以为过期扫描导致至少等待 25ms 后才会进行处理，那么就会出现大量的请求因为超时而关闭，业务端就会出现很多异常。这时你还无法从 Redis 的 slowlog 中看到慢查询记录，因为慢查询指的是逻辑处理过程慢，不包含等待时间。\n所以要避免大批量的 key 同时过期，可以给过期时间设置一个随机范围，分散过期处理的压力。","懒惰删除#懒惰删除":"Redis 为什么要懒惰删除(lazy free) 删除指令 del 会直接释放对象的内存，大部分情况下，这个指令非常快，没有明显延迟。不过如果删除的 key 是一个非常大的对象，比如一个包含了千万元素的 hash，又或者在使用 FLUSHDB 和 FLUSHALL 删除包含大量键的数据库时，那么删除操作就会导致线程卡顿。\nredis 4.0 引入了 lazyfree 的机制，它可以将删除键或数据库的操作放在后台线程里执行， 从而尽可能地避免服务器阻塞。\nunlink unlink 指令，它能对删除操作进行懒处理，丢给后台线程来异步回收内存。\n\u003e unlink key OK flush flushdb 和 flushall 指令，用来清空数据库，这也是极其缓慢的操作。Redis 4.0 同样给这两个指令也带来了异步化，在指令后面增加 async 参数就可以扔给后台线程慢慢处理。\n\u003e flushall async OK "},"title":"Redis 的过期策略和内存淘汰机制"},"/db-learn/docs/redis/advance/11_pipeline/":{"data":{"":"Redis 管道 (Pipeline) 这个技术本质上是由客户端提供的，跟服务器没有什么直接的关系。","redis-的消息交互#Redis 的消息交互":"当使用客户端对 Redis 进行一次操作时，客户端将请求传送给服务器，服务器处理完毕后，再将响应回复给客户端。这要花费一个网络数据包来回的时间。\n如果连续执行多条指令，那就会花费多个网络数据包来回的时间。\n管道操作的本质，服务器根本没有任何区别对待，还是收到一条消息，执行一条消息，回复一条消息的正常的流程。客户端通过对管道中的指令列表改变读写顺序，合并 write 和 read 操作，就可以大幅节省 IO 时间。\n打包的命令越多，缓存消耗内存也越多。所以并不是打包的命令越多越好。\npipeline 中发送的每个 command 都会被 server立即执行，如果执行失败，将会在此后的响应中得到信息；也就是 pipeline 并不是表达“所有 command 都一起成功”的语义，管道中前面命令失败，后面命令不会有影响，继续执行。也就是说管道不具备原子性。","管道压力测试#管道压力测试":"Redis 自带了一个压力测试工具 redis-benchmark，使用这个工具就可以进行管道测试。\n首先我们对一个普通的 set 指令进行压测，QPS 大约 5w/s。\n\u003e redis-benchmark -t set -q SET: 51975.05 requests per second 加入管道选项 -P 参数，它表示单个管道内并行的请求数量，看下面 P=2，QPS 达到了 9w/s。\n\u003e redis-benchmark -t set -P 2 -q SET: 91240.88 requests per second 再看看 P=3，QPS 达到了 10w/s。\n\u003e redis-benchmark -t set -P 3 -q SET: 102354.15 requests per second 但如果再继续提升 P 参数，发现 QPS 已经上不去了。因为这里 CPU 处理能力已经达到了瓶颈，无法再继续提升了。"},"title":"管道"},"/db-learn/docs/redis/advance/12_io/":{"data":{"":"Redis 为什么快？\n单线程 基于内存的操作 高效的 IO 模型，非阻塞 IO RESP 协议简单高效 ","reacrtor-模型#Reacrtor 模型":"Reactor 反应器模型是一种事件驱动的编程模型，用于处理网络事件。\n具体事件处理程序不调用反应器，而向反应器注册一个事件处理器，表示自己对某些事件感兴趣，有使件来了，具体事件处理程序通过事件处理器对某个指定的事件发生做出反应。\n例如，路人甲去做男士 SPA，前台的接待小姐接待了路人甲，路人甲现在只对 10000 技师感兴趣，但是路人甲去的比较早，就告诉接待小姐，等 10000 技师上班了或者是空闲了，通知我。等路人甲接到接待小姐通知，做出了反应，把 10000 技师占住了。然后，路人甲想起上一次的那个 10000 号房间不错，设备舒适，灯光暧昧，又告诉前台的接待小姐，我对 10000 号房间很感兴趣，房间空出来了就告诉我，我现在先和 10000 这个小姐聊下人生，10000 号房间空出来了，路人甲再次接到接待小姐通知，路人甲再次做出了反应。路人甲就是具体事件处理程序，前台的接待小姐就是所谓的反应器，“10000技师上班了”和“10000号房间空闲了”就是事件，路人甲只对这两个事件感兴趣，其他，比如 10001 号技师或者 10002 号房间空闲了也是事件，但是路人甲不感兴趣。\n前台的接待小姐不仅仅服务路人甲 1 人，他还可以同时服务路人乙、丙……..，每个人所感兴趣的事件是不一样的，前台的接待小姐会根据每个人感兴趣的事件通知对应的每个人。\n单线程 Reactor 模式 服务器端的 Reactor 是一个线程对象，该线程会启动事件循环，并使用 Acceptor 事件处理器关注 ACCEPT 事件，这样 Reactor 会监听客户端向服务器端发起的连接请求事件 (ACCEPT 事件)。\n客户端向服务器端发起一个连接请求，Reactor 监听到了该 ACCEPT 事件的发生并将该 ACCEPT 事件派发给相应的 Acceptor 处理器来进行处理。 建立连接后关注 READ 事件，这样一来 Reactor 就会监听该连接的 READ 事件了。 当 Reactor 监听到有读 READ 事件发生时，将相关的事件派发给对应的处理器进行处理。比如，读处理器会通过读取数据，此时 read() 操作可以直接读取到数据，而不会堵塞与等待可读的数据到来。 在目前的单线程 Reactor 模式中，不仅 I/O 操作在该 Reactor 线程上，连非 I/O 的业务操作也在该线程上进行处理了，这可能会大大延迟 I/O 请求的响应。\n单线程 Reactor，工作者线程池 与单线程 Reactor 模式不同的是，添加了一个工作者线程池，并将非 I/O 操作从 Reactor 线程中移出转交给工作者线程池来执行。这样能够提高 Reactor 线程的 I/O 响应，不至于因为一些耗时的业务逻辑而延迟对后面 I/O 请求的处理。\n对于一些小容量应用场景，可以使用单线程模型，对于高负载、大并发或大数据量的应用场景却不合适，主要原因如下：\n一个 NIO 线程同时处理成百上千的链路，性能上无法支撑，即便 NIO 线程的 CPU 负荷达到 100%，也无法满足海量消息的读取和发送； 当 NIO 线程负载过重之后，处理速度将变慢，这会导致大量客户端连接超时，超时之后往往会进行重发，这更加重了 NIO 线程的负载，最终会导致大量消息积压和处理超时，成为系统的性能瓶颈； 多线程 Reactor 模式 Reactor 线程池中的每一个 Reactor 线程都会有自己的 Selector、线程和分发的事件循环逻辑。\nMainReactor 可以只有一个，但 SubReactor 一般会有多个。Main Reactor 线程主要负责接收客户端的连接请求，然后将接收到的 SocketChannel 传递给 SubReactor，由 SubReactor 来完成和客户端的通信。\n多 Reactor 线程模式将“接受客户端的连接请求”和“与该客户端的通信”分在了两个 Reactor 线程来完成。MainReactor 完成接收客户端连接请求的操作，它不负责与客户端的通信，而是将建立好的连接转交给 SubReactor 线程来完成与客户端的通信，这样一来就不会因为 read() 数据量太大而导致后面的客户端连接请求得不到即时处理的情况。并且多 Reactor 线程模式在海量的客户端并发请求的情况下，还可以通过实现 SubReactor 线程池来将海量的连接分发给多个 SubReactor 线程，在多核的操作系统中这能大大提升应用的负载和吞吐量。","redis-6-中的多线程#Redis 6 中的多线程":"Redis 6 之前的版本真的是单线程么？ Redis 在处理客户端的请求时，所有操作（网络 I/O、命令解析、执行、响应）均由单个主线程顺序处理，这就是所谓的 “单线程”。通过 epoll/kqueue 等系统调用实现非阻塞 I/O，监听多个客户端连接。\n但如果严格来讲从 Redis 4 之后并不是单线程，除了主线程外，它也有后台线程在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 key 的删除等等。\nRedis 单线程为什么还能这么快？ 因为它所有的数据都在内存中，所有的运算都是内存级别的运算，而且单线程避免了多线程的切换性能损耗问题。正因为 Redis 是单线程，所以要小心使用 Redis 指令，对于那些耗时的指令(比如 keys)，一定要谨慎使用，一不小心就可能会导致 Redis 卡顿。\nRedis 6 之前为什么一直不使用多线程？ 官方曾做过类似问题的回复：使用 Redis 时，几乎不存在 CPU 成为瓶颈的情况，Redis 主要受限于内存和网络。例如在一个普通的 Linux 系统上，Redis 通过使用 pipeline 每秒可以处理 100 万个请求，所以如果应用程序主要使用 O(N) 或 O(log(N)) 的命令，它几乎不会占用太多 CPU。\n多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程上下文切换、甚至加锁解锁、死锁造成的性能损耗。Redis 通过 AE 事件模型以及 IO 多路复用等技术，处理性能非常高，因此没有必要使用多线程。\n单线程机制使得 Redis 内部实现的复杂度大大降低，Hash 的惰性 Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行。\nRedis 6 为什么要引入多线程？ Redis 将所有数据放在内存中，内存的响应时长大约为 100 纳秒，对于小数据包，Redis 服务器可以处理 80,000 到 100,000 QPS，这也是 Redis 处理的极限了，对于 80% 的公司来说，单线程的 Redis 已经足够使用了。\n但随着越来越复杂的业务场景，有些公司动不动就上亿的交易量，因此需要更大的 QPS。常见的解决方案是在分布式架构中对数据进行分区并采用多个服务器，但该方案有非常大的缺点，例如：\n要管理的 Redis 服务器太多，维护代价大； 某些适用于单个 Redis 服务器的命令（mset）不适用于数据分区； 数据分区无法解决热点读/写问题，例如：虽然使用了集群，但是只是将不同的数据分散在不同的节点上，但是如果某个 key 被频繁访问，这个 key 还是会被分配到同一个节点上，这样访问压力还是会集中在一个节点上。 数据偏斜，重新分配和放大/缩小变得更加复杂等等。Redis 集群虽然尽量平均分配槽位，但是每个 key 对应的 value 的数据大小不同。例如：一个 key 的值是 10 KB，而另一个 key 的值是 1MB，这就会导致数据倾斜，导致部分节点的内存占用率很高，而其他节点的内存占用率很低。 从 Redis 自身角度来说，因为读写网络的 read/write 系统调用占用了 Redis 执行期间大部分 CPU 时间，瓶颈主要在于网络的 IO 消耗, 优化主要有两个方向:\n提高网络 IO 性能，典型的实现比如使用 DPDK（跳过操作系统的网络栈，直接处理网卡的内容）来替代内核网络栈的方式。 使用多线程充分利用多核，典型的实现比如 Memcached。 DPDK 协议栈优化的这种方式相当复杂，Redis 没有选择，很明显支持多线程是一种最有效最便捷的操作方式。所以总结起来，Redis 支持多线程主要就是两个原因：\n可以充分利用服务器 CPU 资源，目前主线程只能利用一个核。 多线程任务可以分摊 Redis 同步 IO 读写负荷。 Redis 6 是否默认开启了多线程？ 没有，默认是单线程，但是可以通过配置文件开启多线程。\n# 开启多线程 io-threads-do-reads yes # 线程数量，必须设置线程数，否则是不生效的。 io-threads 4 关于线程数的设置，官方有一个建议：4 核的机器建议设置为 2 或 3 个线程，8 核的建议设置为 6 个线程，线程数一定要小于机器核数。还需要注意的是，线程数并不是越大越好，官方认为超过了 8 个基本就没什么意义。\nRedis 6 多线程的实现机制 多线程网络 I/O：主线程负责接收连接，将就绪的 socket 分发给多个 I/O 线程（配置项 io-threads 控制线程数）。 单线程命令执行：命令解析、执行、响应构建仍由主线程处理，保持原子性。 流程简述如下：\n主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列。 主线程处理完读事件之后，通过 RR (Round Robin) 将这些连接分配给这些 IO 线程。 主线程阻塞等待 IO 线程读取 socket 完毕。 主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行回写 socket。 主线程阻塞等待 IO 线程将数据回写 socket 完毕。 解除绑定，清空等待队列。 该设计有如下特点：\nIO 线程要么同时在读 socket，要么同时在写，不会同时读或写。 IO 线程只负责读写 socket 解析命令，不负责命令处理。 可以看出 Redis 6 的多线程类似于多线程 Reactor 模式，不过它的 Sub Reactor 只负责读写 socket，不负责命令处理。\n开启多线程后，是否会存在线程并发安全问题？ 从上面的实现机制可以看出，Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行。所以不需要去考虑控制 key、lua、事务，LPUSH/LPOP 等等的并发及线程安全问题。\nRedis 6 的多线程和 Memcached 多线程模型进行对比 Memcached 主线程负责接收建立连接请求，然后 socket 的读写，命令的执行，都交给子线程去处理。\nRedis 主线程负责接收建立连接请求，然后 socket 的读写，交给子线程去处理。但是命令的执行还是由主线程来执行的。","redis-的线程和-io-概述#Redis 的线程和 IO 概述":"Redis 基于 Reactor 模式开发了自己的网络事件处理器 - 文件事件处理器（file event handler，后文简称为 FEH），而该处理器又是单线程的，所以 Redis 设计为单线程模型。\n采用 I/O 多路复用同时监听多个 socket，根据 socket 当前执行的事件来为 socket 选择对应的事件处理器。\n当被监听的 socket 准备好执行 accept、read、write、close 等操作时，和操作对应的文件事件就会产生，这时 FEH 就会调用 socket 之前关联好的事件处理器来处理对应事件。\n所以虽然 FEH 是单线程运行，但通过 I/O 多路复用监听多个 socket，不仅实现高性能的网络通信模型，又能和 Redis 服务器中其它同样单线程运行的模块交互，保证了 Redis 内部单线程模型的简洁设计。"},"title":"Redis 线程模型"},"/db-learn/docs/redis/advance/13_transaction/":{"data":{"":"","pipeline-和事务的区别#Pipeline 和事务的区别":"","redis-事务#Redis 事务":"","优化#优化":"事务表示一组动作，要么全部执行，要么全部不执行。\nRedis 事务 Redis 提供了简单的事务功能，将一组需要一起执行的命令放到 multi 和 exec 两个命令之间。\nmulti 命令代表事务开始，exec 命令代表事务结束，如果要停止事务的执行，可以使用 discard 命令代替 exec 命令即可。它们之间的命令是原子顺序执行的，例如：\nredis\u003e multi OK redis\u003e sadd u:a:follow ub QUEUED redis\u003e sadd u:b:fans ua QUEUED 命令返回的是 QUEUED，代表命令并没有真正执行，而是暂时保存在 Redis 中的一个缓存队列（所以 discard 也只是丢弃这个缓存队列中的未执行命令，并不会回滚已经操作过的数据，这一点要和关系型数据库的 rollback 操作区分开）。\n如果此时另一个客户端执行 sismember u:a:follow ub 返回结果应该为 0，因为上面的命令还没有执行。\nredis\u003e sismember u:a:follow ub (integer) 0 只有当执行 exec 命令后，会将缓存队列中的命令按照顺序执行，并返回执行结果。\nredis\u003e exec 1) (integer) 1 2) (integer) 1 另一个客户端：\nredis\u003e sismember u:a:follow ub (integer) 1 如果事务中的命令出现错误,Redis 的处理机制也不尽相同。如果是 MySQL 数据库，事务中的错误会导致整个事务的执行失败，并且会回滚到事务开始之前的状态。\n而 Redis 中，事务中的错误分为两种情况：\n命令错误 例如下面操作错将 set 写成了 sett，属于语法错误，会造成整个事务无法执行，key 和counter 的值未发生变化：\nredis\u003e set txkey hello OK redis\u003e set txcount 100 OK redis\u003e mget txkey txcount 1) \"hello\" 2) \"100\" redis\u003e multi OK redis\u003e set k v QUEUED redis\u003e sett txkey world (error) ERR unknown command `sett`, with args beginning with: `txkey`, `world`, redis\u003e incr txcount QUEUED redis\u003e exec (error) EXECABORT Transaction discarded because of previous errors. redis\u003e mget txkey txcount 1) \"hello\" 2) \"100\" 可以看出，对于命令错误，Redis 会将整个事务的放弃，不执行任何命令，并且返回错误信息。\n运行时错误 例如用户 B 在添加粉丝列表时，误把 sadd 命令 (针对集合) 写成了 zadd 命令 (针对有序集合)，这种就是运行时命令，因为语法是正确的：\nredis\u003e multi OK redis\u003e sadd u:a:follow ub QUEUED redis\u003e zadd u:b:fans 1 uc QUEUED redis\u003e exec 1) (integer) 1 2) (error) WRONGTYPE Operation against a key holding the wrong kind of value redis\u003e sismember u:c:follow ub (integer) 1 u:b:fans 在前面已经是一个集合了，但是 zadd 是操作有序集合的命令，虽然命令没有错，但是运行时会出现错误。\n可以看出，命令没有错，在运行时才出现的错误，Redis 会将其他命令正常执行，并没有全部回滚。如果碰到这种问题，需要开发人员根据具体情况进行处理。\nwatch 命令 有些应用场景需要在事务之前，确保事务中的 key 没有被其他客户端修改过，才执行事务，否则不执行 (类似乐观锁)。\n可以使用 watch 命令来实现，例如：\n客户端 1：\nredis\u003e set testwatch java OK redis\u003e watch testwatch OK redis\u003e multi OK redis\u003e 客户端 2：\nredis\u003e append testwatch python (integer) 10 客户端 1：\nredis\u003e append testwatch jedis QUEUED redis\u003e exec (nil) redis\u003e get testwatch \"javapython\" 可以看到“客户端-1”在执行 multi 之前执行了 watch 命令，“客户端-2”在“客户端-1”执行 exec 之前修改了 key 值，造成“客户端-1”事务没有执行 ( exec 结果为 nil，就是因为 watch 命令观察到 key 值被修改了，导致事务没有执行)。\nℹ️ Redis 禁止在 multi 和 exec 之间执行 watch 指令，而必须在 multi 之前做好盯住关键变量，否则会出错。 Pipeline 和事务的区别 pipeline 是客户端的行为，对于服务器来说无法区分客户端发送来的查询命令是以普通命令的形式还是以 pipeline 的形式发送到服务器的。 事务则是实现在服务器端的行为，用户执行 MULTI 命令时，服务器会将对应这个用户的客户端对象设置为一个特殊的状态，在这个状态下后续用户执行的查询命令不会被真的执行，而是被服务器缓存起来，直到用户执行 EXEC 命令为止，服务器会将这个用户对应的客户端对象中缓存的命令按照提交的顺序依次执行。 应用 pipeline 可以提服务器的吞吐能力，并提高 Redis 处理查询请求的能力。但是无法保证原子性。 优化 可以将事务和 pipeline 结合起来使用，减少事务的命令在网络上的传输时间，将多次网络 IO 缩减为一次网络 IO。\npipe = redis.pipeline(transaction=true) pipe.multi() pipe.incr(\"books\") pipe.incr(\"books\") values = pipe.execute() ","总结#总结":"Redis 的事务过于简单，可以使用 Lua 脚本实现复杂的事务。"},"title":"事务"},"/db-learn/docs/redis/advance/14_stream/":{"data":{"":"Redis 5.0 最大的新特性就是多出了一个数据结构 Stream，它是一个新的强大的支持多播的可持久化的消息队列（MQ），借鉴了 Kafka 的设计。\nRedis Stream 的结构如上图所示：\n每一个 Stream 都有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容。消息是持久化的，Redis 重启后，内容还在。 每个 Stream 都有唯一的名称，它就是 Redis 的 key，首次使用 xadd 指令追加消息时自动创建。 每个 Stream 都可以挂多个消费组，每个消费组会有个游标 last_delivered_id 在 Stream 数组之上往前移动，表示当前消费组已经消费到哪条消息了。每个消费组都有一个 Stream 内唯一的名称，消费组不会自动创建，它需要单独的指令 xgroup create 进行创建，需要指定从 Stream 的某个消息 ID 开始消费，这个 ID 用来初始化 last_delivered_id 变量。 每个消费组 (Consumer Group) 的状态都是独立的，相互不受影响。也就是说同一份 Stream 内部的消息，可以被不同消费组消费。 同一个消费组 (Consumer Group) 可以挂接多个消费者 (Consumer)，这些消费者之间是竞争关系，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动。每个消费者有一个组内唯一名称。也就是说一份消息只会被组内的一个消费者消费。 消费者 (Consumer) 内部会有个状态变量 pending_ids，它记录了当前已经被客户端读取,但是还没有 ack 的消息。如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一旦某个消息被 ack，它就开始减少。这个 pending_ids 变量在 Redis 官方被称之为 PEL（Pending Entries List），这是一个很核心的数据结构，它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理。 消息 ID 的形式是 timestampInMillis-sequence，例如 1527846880572-5，它表示当前的消息在毫米时间戳 1527846880572 时产生，并且是该毫秒内产生的第 5 条消息。消息 ID 可以由服务器自动生成，也可以由客户端自己指定，但是形式必须是 整数-整数，而且必须是后面加入的消息的 ID 要大于前面的消息 ID。 消息内容就是键值对，形如 hash 结构的键值对，这没什么特别之处。 ","redis-队列的几种实现#Redis 队列的几种实现":"基于 List 的 LPUSH+BRPOP 的实现 优点：\n足够简单，消费消息延迟几乎为零。\n缺点：\n不支持广播模式，不能重复消费，一旦消费就会被删除。 不支持分组消费。 如果线程一直阻塞在那里，Redis 客户端的连接就成了闲置连接，闲置过久，服务器一般会主动断开连接，减少闲置资源占用，这个时候 blpop 和 brpop 或抛出异常，所以在编写客户端消费者的时候要小心，如果 捕获到异常需要重试。 做消费者确认 ACK 麻烦，不能保证消费者消费消息后是否成功处理的问题（宕机或处理异常等），通常需要维护一个 Pending 列表，保证消息处理确认。 基于 ZSet 的实现 多用来实现延迟队列，当然也可以实现有序的普通的消息队列，但是消费者无法阻塞的获取消息，只能轮询，不允许重复消息。\n实现延迟队列时将消息序列化成一个字符串作为 zset 的 value，这个消息的到期处理时间作为 score。按照时间来排序，到期的消息会排在前面。消费消息时，只需要轮询 zset，获取到到期的消息进行处理即可。\n订阅/发布模式 优点：\n典型的广播模式，一个消息可以发布到多个消费者。 多信道订阅，消费者可以同时订阅多个信道，从而接收多类消息。 消息即时发送，消息不用等待消费者读取，消费者会自动接收到信道发布的消息。 缺点：\n消息一旦发布，发布时若客户端不在线，则消息丢失。 不能保证每个消费者接收的时间是一致的。 若消费者客户端出现消息积压，到一定程度，会被强制断开，导致消息意外丢失。通常发生在消息的生产远大于消费速度时。 可见，Pub/Sub 模式不适合做消息存储，消息积压类的业务，而是擅长处理广播，即时通讯，即时反馈的业务。\n基于 Stream 类型的实现 基本上已经有了一个消息中间件的雏形，可以考虑在生产过程中使用，当然真正要在生产中应用，要做的事情还很多，比如消息队列的管理和监控就需要花大力气去实现，而专业消息队列都已经自带或者存在着很好的第三方方案和插件。","stream-的问题#Stream 的问题":"Stream 已经具备了一个消息队列的基本要素，生产者 API、消费者 API，消息 Broker，消息的确认机制等等，所以在使用消息中间件中产生的问题，这里一样也会遇到。\n消息太多怎么办? 吐过消息积累太多，Stream 的链表很长，内存会不会爆掉? 而且 xdel 指令又不会删除消息，它只是给消息做了个标志位。\nRedis 它提供了一个定长 Stream 功能。在 xadd 的指令提供一个定长长度 maxlen，就可以将老的消息干掉，确保最多不超过指定长度。\n消息如果忘记 ACK 会怎样? Stream 在每个消费者结构中保存了正在处理中的消息 ID 列表 PEL，如果消费者收到了消息处理完了但是没有回复 ack，就会导致 PEL 列表不断增长，如果有很多消费组的话，那么这个 PEL 占用的内存越来越大。所以消息要尽可能的快速消费并确认。\nPEL 是已被消费者组获取但未被确认 (ACK) 的消息 ID 集合，这些消息处于\"处理中\"状态，具有以下特征：\n已被某个消费者 (Consumer) 通过 xreadgroup 获取 尚未被 xack 确认处理完成 仍在消费者的\"责任范围\"内 会被包含在 xpenging 命令的返回结果中 PEL 如何避免消息丢失? 在消费者读取 Stream 消息时，Redis 服务器将消息回复给客户端的过程中，客户端突然断开了连接，消息就丢失了。但是 PEL 里已经保存了发出去的消息 ID。待客户端重新连上之后，可以再次收到 PEL 中的消息 ID 列表。不过此时 xreadgroup 的起始消息 ID 不能为参数 \u003e，而必须是任意有效的消息 ID，一般将参数设为 0-0，表示读取所有的 PEL 消息以及自 last_delivered_id 之后的新消息。\n死信问题 如果某个消息，不能被消费者处理，也就是不能被 XACK，这是要长时间处于 Pending 列表中，即使被反复的转移给各个消费者也是如此。此时该消息的 delivery counter（通过 XPENDING 可以查询到）就会累加，当累加到某个预设的临界值时，就认为是坏消息（也叫死信，DeadLetter，无法投递的消息），由于有了判定条件，我们将坏消息处理掉即可，删除即可。删除一个消息，使用 XDEL 语法，注意，这个命令并没有删除 Pending 中的消息，因此查看 Pending，消息还会在，可以在执行执行 XDEL 之后，XACK 这个消息标识其处理完毕。\n专业的 MQ 中间件，例如 RabbitMQ，有死信队列的概念，当消息在一定时间内没有被消费，就会被投递到死信队列中，有专门的死信队列消费者来处理。但是 Stream 没有提供死信队列的概念，需要我们自己处理。\nStream 的高可用 Stream 的高可用是建立主从复制基础上的，它和其它数据结构的复制机制没有区别，也就是说在 Sentinel 和 Cluster 集群环境下 Stream 是可以支持高可用的。不过由于 Redis 的指令复制是异步的，在 failover 发生时，Redis 可能会丢失极小部分数据，这点 Redis 的其它数据结构也是一样的。\n分区 Partition Redis 的服务器没有原生支持分区能力，如果想要使用分区，那就需要分配多个 Stream，然后在客户端使用一定的策略来生产消息到不同的 Stream。","什么时候选择-redis-stream#什么时候选择 Redis Stream？":"Stream 的消费模型借鉴了 Kafka 的消费分组的概念，它弥补了 Redis Pub/Sub 不能持久化消息的缺陷。但是它又不同于 kafka，Kafka 的消息可以分 partition，而 Stream 不行。如果非要分 parition 的话，得在客户端做，提供不同的 Stream 名称，对消息进行 hash 取模来选择往哪个 Stream 里塞。\n如果是中小项目和企业，在工作中已经使用了 Redis，在业务量不是很大，而又需要消息中间件功能的情况下，可以考虑使用 Redis 的 Stream 功能。但是如果并发量很高，资源足够支持下，还是以专业的消息中间件，比如 RocketMQ、Kafka 等来支持业务更好。","常用命令#常用命令":"生产端 xadd：追加消息。 xdel：删除消息，这里的删除仅仅是设置了标志位，不会实际删除消息。 xrange：获取消息列表，会自动过滤已经删除的消息。 xlen：消息长度。 del：删除 Stream。 示例：\n127.0.0.1:6880\u003e xadd streamtest * name mark age 18 \"1626705954593-0\" streamtest 表示当前这个队列的名字，也就是 Redis 中的 key。 * 号表示服务器自动生成 ID， name mark age 18，是存入当前 streamtest 这个队列的消息，采用的也是 key/value 的存储形式。 返回值 1626705954593-0 则是生成的消息 ID，由两部分组成：时间戳-序号。时间戳时毫秒级单位，是生成消息的 Redis 服务器时间，它是个 64 位整型。序号是在这个毫秒时间点内的消息序号。它也是个 64 位整型。 ℹ️ 为了保证消息是有序的，Redis 生成的 ID 是单调递增有序的。由于 ID 中包含时间戳部分，为了避免服务器时间错误而带来的问题（例如服务器时间延后了），Redis 的每个 Stream 类型数据都维护一个 latest_generated_id 属性，用于记录最后一个消息的 ID。若发现当前时间戳退后（小于 latest_generated_id 所记录的），则采用时间戳不变而序号递增的方案来作为新消息 ID（这也是序号为什么使用 int64 的原因，保证有足够多的的序号），从而保证 ID 的单调递增性质。 如果不是非常特别的需求，强烈建议使用 Redis 的方案生成消息 ID，因为这种 时间戳+序号 的单调递增的 ID 方案，几乎可以满足全部的需求，但 ID 是支持自定义的。\n127.0.0.1:6880\u003e xadd streamtest * name james age 20 \"1626706380924-0\" 127.0.0.1:6880\u003e xadd streamtest * name pooky age 33 \"1626706393957-0\" 127.0.0.1:6880\u003e xlen streamtest (integer) 3 127.0.0.1:6880\u003e xrange streamtest - + 1) 1) \"1626705954593-0\" 2) 1) \"name\" 2) \"mark\" 3) \"age\" 4) \"18\" 2) 1) \"1626706380924-0\" 2) 1) \"name\" 2) \"james\" 3) \"age\" 4) \"20\" 3) 1) \"1626706393957-0\" 2) 1) \"name\" 2) \"pooky\" 3) \"age\" 4) \"33\" xrange streamtest - + 中的 - 表示从消息 ID 最小的开始，+ 表示到消息 ID 最大的结束。\n也可以指定消息 ID 范围：\n127.0.0.1:6880\u003e xrange streamtest 1626706380924-0 1626706393957-0 1) 1) \"1626706380924-0\" 2) 1) \"name\" 2) \"james\" 3) \"age\" 4) \"20\" 2) 1) \"1626706393957-0\" 2) 1) \"name\" 2) \"pooky\" 3) \"age\" 4) \"33\" 127.0.0.1:6880\u003e xrange streamtest - 1626706380924-0 1) 1) \"1626705954593-0\" 2) 1) \"name\" 2) \"mark\" 3) \"age\" 4) \"18\" 2) 1) \"1626706380924-0\" 2) 1) \"name\" 2) \"james\" 3) \"age\" 4) \"20\" 127.0.0.1:6880\u003e xrange streamtest 1626706380924-0 + 1) 1) \"1626706380924-0\" 2) 1) \"name\" 2) \"james\" 3) \"age\" 4) \"20\" 2) 1) \"1626706393957-0\" 2) 1) \"name\" 2) \"pooky\" 3) \"age\" 4) \"33\" 删除指定的消息：\n127.0.0.1:6880\u003e xdel streamtest 1626706380924-0 (integer) 1 消费端 单消费者 虽然 Stream 中有消费者组的概念，但是可以在不定义消费组的情况下进行 Stream 消息的独立消费，当 Stream 没有新消息时，甚至可以阻塞等待。Redis 设计了一个单独的消费指令xread，可以将 Stream 当成普通的消息队列 (list) 来使用。使用 xread 时，我们可以完全忽略消费组 (Consumer Group) 的存在，就好比 Stream 就是一个普通的列表 (list)。\n127.0.0.1:6880\u003e xread count 1 streams stream2 0-0 1) 1) \"stream2\" 2) 1) 1) \"1626706393957-0\" 2) 1) \"name\" 2) \"pooky\" 3) \"age\" 4) \"33\" count 1 表示获取一条消息 streams Redis 关键字 stream2 0-0 表示从 stream2 这个 Stream 中，从消息 ID 为 0-0 的消息开始读取。0-0 表示从头开始读取。 从指定的消息 ID 开始读取(不包括命令中的消息 ID)：\n127.0.0.1:6880\u003e xread count 2 streams stream2 1626705954593-0 1) 1) \"stream2\" 2) 1) 1) \"1626706380924-0\" 2) 1) \"name\" 2) \"james\" 3) \"age\" 4) \"20\" 2) 1) \"stream2\" 2) 1) 1) \"1626706393957-0\" 2) 1) \"name\" 2) \"pooky\" 3) \"age\" 4) \"33\" 从尾部读取最新的一条消息，$ 代表从尾部读取，此时默认不返回任何消息：\n127.0.0.1:6880\u003e xread count 1 streams stream2 $ (nil) 所以最好以阻塞的方式读取尾部最新的一条消息，直到新的消息的到来：\n127.0.0.1:6880\u003e xread block 0 count 1 streams stream2 $ block 后面的数字代表阻塞时间，单位毫秒。0 表示一直阻塞，直到有新的消息的到来。 新开一个客户端，往 stream2 中写入一条消息：\n$ ./redis-cli -p 127.0.0.1:6880\u003e xadd stream2 * name xiaoqiang age 18 \"1626706489131-0\" 再回到原来的客户端，就可以看到新的消息：\n127.0.0.1:6880\u003e xread block 0 count 1 streams stream2 $ 1) 1) \"stream2\" 2) 1) 1) \"1626706489131-0\" 2) 1) \"name\" 2) \"xiaoqiang\" 3) \"age\" 4) \"18\" (127.87s) 可以看到阻塞解除了，返回了新的消息内容，而且还显示了一个等待时间，这里等待了 127.87s\n客户端如果想要使用 xread 进行顺序消费，一定要记住当前消费到哪里了，也就是返回的消息 ID（Redis 是不会记住消费者消费的位置的）。下次继续调用 xread 时，将上次返回的最后一个消息 ID 作为参数传递进去，就可以继续消费后续的消息。\n消费组 创建消费组，需要传递起始消息 ID 参数用来初始化 last_delivered_id 变量：\n127.0.0.1:6880\u003e xgroup create stream2 cg1 0-0 stream2 指定要读取的队列 cg1 消费组的名称 0-0 表示从头开始消费 127.0.0.1:6880\u003e xgroup create stream2 cg2 $ $ 表示从尾部开始消费，**只接受新消息*8，当前 Stream 消息会全部忽略。 xinfo 命令查看队列的情况：\n127.0.0.1:6880\u003e xinfo stream stream2 1) \"length\" 2) (integer) 3 # 消息长度为 3 3) \"radix-tree-keys\" 4) (integer) 1 # 基数树键为 1 5) \"radix-tree-nodes\" 6) (integer) 2 # 基数树节点为 2 7) \"last-generated-id\" 8) \"1626706489131-0\" # 最后一个消息 ID 为 1626706489131-0 9) \"groups\" 10) (integer) 2 # 2 个消费组 11) \"first-entry\" 12) 1) 1) \"1626705954593-0\" # 第一个消息 ID 为 1626705954593-0 2) 1) \"name\" 2) \"mark\" 3) \"age\" 4) \"18\" 13) \"last-entry\" 14) 1) 1) \"1626706489131-0\" # 最后一个消息 ID 为 1626706489131-0 2) 1) \"name\" 2) \"xiaoqiang\" 3) \"age\" 4) \"18\" 查看队列的消费组信息：\nS127.0.0.1:6880\u003e xinfo groups stream2 1) 1) \"name\" 2) \"cg1\" 3) \"consumers\" 4) (integer) 1 5) \"pending\" 6) (integer) 0 7) \"last-delivered-id\" 8) \"0-0\" 2) 1) \"name\" 2) \"cg2\" 3) \"consumers\" 4) (integer) 1 5) \"pending\" 6) (integer) 0 7) \"last-delivered-id\" 8) \"1626706489131-0\" 消费消息 有了消费组，自然还需要消费者，Stream 提供了 xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消费者名称和起始消息 ID。\n它同 xread 一样，也可以阻塞等待新消息。读到新消息后，对应的消息 ID 就会进入消费者的 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知服务器，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除。\n127.0.0.1:6880\u003e xreadgroup GROUP cg1 c1 count 1 streams stream2 \u003e 1) 1) \"stream2\" 2) 1) 1) \"1626706489131-0\" 2) 1) \"name\" 2) \"xiaoqiang\" 3) \"age\" 4) \"18\" GROUP 关键字 cg1 消费组名称 c1 消费者名称 \u003e 表示从当前消费组的 last_delivered_id 后面开始读，每当消费者读取一条消息，last_delivered_id 变量就会前进。创建消费者组的时候设置了 last_delivered_id。 count 1 表示获取一条消息。 设置阻塞等待：\n127.0.0.1:6880\u003e xreadgroup GROUP cg1 c1 block 0 count 1 streams stream2 \u003e 和 xread 一样，block 后面的数字代表阻塞时间，单位毫秒。0 表示一直阻塞，直到有新的消息的到来。\n如果同一个消费组有多个消费者，我们还可以通过 xinfo consumers 指令观察每个消费者的状态：\n127.0.0.1:6880\u003e xinfo consumers stream2 cg1 1) 1) \"name\" 2) \"c1\" 3) \"pending\" 4) (integer) 5 5) \"idle\" 6) (integer) 15440 pending 表示当前消费者的 PEL 里面有多少条消息，也就是说有 5 条消息在等待 ACK 确认。 idle 表示这个消费者已经空闲了多少秒了。上面的例子是空闲了 15440 秒。 确认消息：\n127.0.0.1:6880\u003e xack stream2 cg1 1626706489131-0 (integer) 1 再次查看确认消息变成了 4 条：\n127.0.0.1:6880\u003e xinfo consumers stream2 cg1 1) 1) \"name\" 2) \"c1\" 3) \"pending\" 4) (integer) 4 5) \"idle\" 6) (integer) 91528 xack 允许带多个消息 ID：\n127.0.0.1:6880\u003e xack stream2 cg1 1626706489131-0 1626706489131-1 (integer) 2 更多的 Redis 的 Stream 命令参考 Redis 官方文档：\nhttps://redis.io/topics/streams-intro https://redis.io/commands "},"title":"Stream"},"/db-learn/docs/redis/guide/":{"data":{"":"","redis-安装#Redis 安装":" # 安装 gcc yum install gcc # 下载并解压到 /usr/local wget http://download.redis.io/releases/redis-5.0.3.tar.gz tar -zxvf redis-5.0.3.tar.gz cd redis-5.0.3 # 编译 make # 运行服务器，daemonize 表示在后台运行 ./src/redis-server --daemonize yes # 或者修改配置文件后运行 # 修改配置 daemonize yes # 后台启动 protected-mode no # 关闭保护模式，开启的话，只有本机才可以访问 Redis # 需要注释掉bind # bind 127.0.0.1（bind 绑定的是自己机器网卡的 ip，如果有多块网卡可以配多个 ip，代表允许客户端通过机器的哪些网卡 ip 去访问，内网一般可以不配置 bind，注释掉即可） # 运行服务器并指定配置文件 ./src/redis-server redis.conf # 验证启动是否成功 ps -ef | grep redis # 进入 Redis 客户端 src/redis-cli # 退出客户端 quit # 退出 Redis 服务： （1）pkill redis-server （2）kill 进程号 （3）src/redis-cli shutdown ","配置密码认证登录#配置密码认证登录":"Redis 默认配置是不需要密码认证，需要手动配置启用 Redis 的认证密码，增加 Redis 的安全性。\n修改配置 打开 redis.conf 配置文件，找到下面的内容：\n################################## SECURITY ################################### # Require clients to issue AUTH \u003cPASSWORD\u003e before processing any other # commands. This might be useful in environments in which you do not trust # others with access to the host running redis-server. # # This should stay commented out for backward compatibility and because most # people do not need auth (e.g. they run their own servers). # # Warning: since Redis is pretty fast an outside user can try up to # 150k passwords per second against a good box. This means that you should # use a very strong password otherwise it will be very easy to break. # #requirepass foobared 去掉注释，设置密码：\nrequirepass {your password} 修改后重启 Redis。\n登录验证 重启后登录时需要使用 -a 参数输入密码，否则登录后没有任何操作权限。如下：\n./src/redis-cli -h 127.0.0.1 -p 6379 127.0.0.1:6379\u003e set testkey (error) NOAUTH Authentication required. 使用密码认证登录：\n./redis-cli -h 127.0.0.1 -p 6379 -a myPassword 127.0.0.1:6379\u003e set testkey hello OK 或者在连接后进行验证：\n./redis-cli -h 127.0.0.1 -p 6379 127.0.0.1:6379\u003e auth yourpassword OK 127.0.0.1:6379\u003e set testkey hello OK 客户端配置密码 127.0.0.1:6379\u003e config set requirepass yourpassword OK 127.0.0.1:6379\u003e config get requirepass 1) \"requirepass\" 2) \"yourpassword\" 注意：使用客户端配置密码，重启 Redis 后仍然会使用 redis.conf 配置文件中的密码。\n在集群中配置认证密码 如果 Redis 使用了集群。除了在 master 中配置密码外，slave 中也需要配置。在 slave 的配置文件中找到如下行，去掉注释并修改为与 master 相同的密码：\n# masterauth your-master-password "},"title":"使用指南"},"/db-learn/docs/redis/guide/01_getting_started/":{"data":{"":"","基础数据类型#基础数据类型":"字符串 (string) 字符串 string 是 Redis 最简单的数据结构。Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。不同类型的数据结构的差异就在于 value 的结构不一样。\nstring 类型是二进制安全的。也就是说 string 可以包含任何数据。比如 jpg 图片或者 序列化的对象 。一个键最大能存储 512MB。\nredis\u003e set testkey hello OK redis\u003e get testkey \"hello\" 批量操作 redis\u003e set name1 codehole OK redis\u003e set name2 holycoder OK redis\u003e mget name1 name2 name3 # 返回一个列表 1) \"codehole\" 2) \"holycoder\" 3) (nil) redis\u003e mset name1 boy name2 girl name3 unknown redis\u003e mget name1 name2 name3 1) \"boy\" 2) \"girl\" 3) \"unknown\" 过期时间 可以对 key 设置过期时间，到点自动删除，这个功能常用来控制缓存的失效时间。\nredis\u003e set name codehole redis\u003e get name \"codehole\" redis\u003e expire name 5 # 5s 后过期 ... # wait for 5s redis\u003e get name (nil) redis\u003e setex name 5 codehole # 5s 后过期，等价于 set+expire redis\u003e get name \"codehole\" ... # wait for 5s redis\u003e get name (nil) redis\u003e setnx name codehole # 如果 name 不存在就执行 set 创建 (integer) 1 redis\u003e get name \"codehole\" redis\u003e setnx name holycoder (integer) 0 # 因为 name 已经存在，所以 set 创建不成功 redis\u003e get name \"codehole\" # 没有改变 计数 如果 value 值是一个整数，还可以对它进行自增操作。自增是有范围的，它的范围是 signed long 的最大最小值，超过了这个值，Redis 会报错。\nredis\u003e set age 30 OK redis\u003e incr age (integer) 31 redis\u003e incrby age 5 (integer) 36 redis\u003e incrby age -5 (integer) 31 redis\u003e set codehole 9223372036854775807 # Long.Max OK redis\u003e incr codehole (error) ERR increment or decrement would overflow 应用场景 单值缓存 set key value get ket 对象缓存 set user:1 value (json string) # 这种方式适合修改比较频繁的场景，可以单独修改某个字段 # 上面的方式是整个对象一起修改，再转成字符串存储 mset user:1:name guanzhu user:1:balance 6666 mget user:1:name user:1:balance 分布式锁 setnx product:10000 true # 返回 1 代表获取锁成功 setnx product:10000 true # 返回 0 代表获取锁失败 # 释放锁 del product:10000 set product:10000 true ex 10 ns # 加上过期时间，避免锁未释放导致死锁 计数器 incr aticle:readcount:{articleId} get aticle:readcount:{articleId} session 共享 spring session + redis 实现 session 共享\n分布式全局 ID incrby orderId 1000 // 相当于批量生成了 1000 个 ID 服务端在内存中将 ID 去加 1，例如服务端从 Redis 取 1000 个，Redis 执行 incrby orderId 1000，然后服务端在内存中执行一千次 + 1 之后再去取新的一批 ID。这种方式有两个问题：\n分布式的环境下 ID 不是连续的。 如果 ID 没有用完服务挂了，比如 ID 还有 500 个的时候服务挂了，那就会浪费 500 个 ID。 这种要根据业务场景来使用。\n列表（list） Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)，这点让人非常意外。\n当列表弹出了最后一个元素之后，该数据结构自动被删除，内存被回收。\nRedis 的列表结构常用来做异步队列使用。将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。\n底层使用 quicklist + ziplist 存储。\n队列 右边进左边出（先进先出）：\nredis\u003e rpush books python java golang (integer) 3 redis\u003e llen books (integer) 3 redis\u003e lpop books \"python\" redis\u003e lpop books \"java\" redis\u003e lpop books \"golang\" redis\u003e lpop books (nil) 还可以使用 lpush 和 rpop 来实现队列，效果是一样的。\n栈 右边进右边出（先进后出）：\nredis\u003e rpush books python java golang (integer) 3 redis\u003e rpop books \"golang\" redis\u003e rpop books \"java\" redis\u003e rpop books \"python\" redis\u003e rpop books (nil) ltrim lindex 相当于 Java 链表的 get(int index) 方法，它需要对链表进行遍历，性能随着参数 index 增大而变差。\nltrim 和字面上的含义不太一样，叫它 lretain (保留) 可能更合适一些，因为 ltrim 跟的两个参数 start_index 和 end_index 定义了一个区间，在这个区间内的值，ltrim 要保留，区间之外统统砍掉。可以通过 ltrim 来实现一个定长的链表，这一点非常有用。\nindex 可以为负数，index=-1 表示倒数第一个元素，同样 index=-2 表示倒数第二个元素。\nredis\u003e rpush books python java golang (integer) 3 redis\u003e lindex books 1 # O(n) 慎用 \"java\" redis\u003e lrange books 0 -1 # 获取所有元素，O(n) 慎用 1) \"python\" 2) \"java\" 3) \"golang\" redis\u003e ltrim books 1 -1 # O(n) 慎用 OK redis\u003e lrange books 0 -1 1) \"java\" 2) \"golang\" redis\u003e ltrim books 1 0 # 这其实是清空了整个列表，因为区间范围长度为负 OK redis\u003e llen books (integer) 0 应用场景 微博和微信公众号消息流 刘备关注了 MacTalk，备胎说车等大 V\n刘备如何拿到关注的大 V 发的消息。例如，MacTalk 发微博，消息 ID 为 10018，可以创建一个 list 来存储刘备关注的大 V 发的消息。 LPUSH msg:{刘备-ID} 10018 备胎说车发微博，消息 ID 为 10086 LPUSH msg:{刘备-ID} 10086 查看最新微博消息 LRANGE msg:{刘备-ID} 0 4 如果一个大 V 有几千万的粉丝，那如果大 V 发一个消息，要给几千万的用户都去推送消息？\n活跃用户使用 push 的方式，不活跃的用户使用 pull 的方式。\npush 的方式比较简单，用户上线可以直接使用 lrange 命令来获取自己的消息流。\npull 的方式，是从每个关注的大 V 的消息列表中，取出最新的消息来进行拉取。然后进行一个排序。\n哈希（hash） Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。内部实现结构上同 Java 的 HashMap 也是一致的，同样的 数组 + 链表 二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。\n不同的是，Redis 的字典的值只能是字符串，另外它们 rehash 的方式不一样，因为 Java 的 HashMap 在字典很大时，rehash 是个耗时的操作，需要一次性全部 rehash。Redis 为了高性能，不能堵塞服务，所以采用了渐进式 rehash 策略。\n当 hash 移除了最后一个元素之后，该数据结构自动被删除，内存被回收。\nHash 结构也可以用来存储 JSON 数据，不同于字符串一次性需要全部序列化整个对象，Hash 可以对 JSON 数据中的每个字段单独存取。而以整个字符串的形式去保存 JSON 数据的话就只能一次性存取，这样就会比较浪费网络流量。\n底层使用 ziplist 或者 hashtable 存储。在数据量比较小，或者单个元素比较小的时候，会使用 ziplist 来存储。\nredis\u003e hset books java \"think in java\" # 命令行的字符串如果包含空格，要用引号括起来 (integer) 1 redis\u003e hset books golang \"concurrency in go\" (integer) 1 redis\u003e hset books python \"python cookbook\" (integer) 1 redis\u003e hgetall books # entries()，key 和 value 间隔出现 1) \"java\" 2) \"think in java\" 3) \"golang\" 4) \"concurrency in go\" 5) \"python\" 6) \"python cookbook\" redis\u003e hlen books (integer) 3 redis\u003e hget books java \"think in java\" redis\u003e hset books golang \"learning go programming\" # 因为是更新操作，所以返回 0 (integer) 0 redis\u003e hget books golang \"learning go programming\" redis\u003e hmset books java \"effective java\" python \"learning python\" golang \"modern golang programming\" # 批量 set OK hincrby Hash 结构中的单个子 key 也可以进行计数，它对应的指令是 hincrby，和 incr 使用基本一样。\n\u003e hincrby user-xiaoqiang age 1 (integer) 30 应用场景 对象缓存 HMSET user {userId}:name guanyu {userId}:balance 6666 HMSET user 1:name guanyu 1:balance 6666 HMGET user 1:name 1:balance 要避免大 key。\n购物车 以用户 ID 作为 key，商品 ID 作为 field，商品数量作为 value。 商品数量可以使用 hincrby 来增减。 可以使用 hdel 来删除商品。 可以使用 hgetall 来获取购物车中的所有商品。 # 添加商品 hset cart:1001 10001 1 # 1001 是用户 ID，10001 商品 ID，1 数量 # 增加商品数量 hincrby cart:1001 10001 1 # 减少商品数量 hincrby cart:1001 10001 -1 # 获得购物车商品总数 hlen cart:1001 # 删除商品 hdel cart:1001 10001 # 获取购物车中的所有商品 hgetall cart:1001 业务中这些数据最终还是要存储到数据库中。\n集合（set） Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值 NULL。\n当集合中最后一个元素移除之后，数据结构自动删除，内存被回收。\n底层使用 intset 或者 hashtable 存储。元素都是整数并且元素个数较小的时候，会使用 intset 来存储。\nredis\u003e sadd books python (integer) 1 redis\u003e sadd books python # 重复 (integer) 0 redis\u003e sadd books java golang (integer) 2 redis\u003e smembers books # 注意顺序，和插入的并不一致，因为 set 是无序的 1) \"java\" 2) \"python\" 3) \"golang\" redis\u003e sismember books java # 查询某个 value 是否存在，相当于 contains(o) (integer) 1 redis\u003e sismember books rust (integer) 0 redis\u003e scard books # 获取长度相当于 count() (integer) 3 redis\u003e spop books # 弹出一个 \"java\" 假设现在有两个集合，一个是 books1，里面有 python、java、golang，另一个是 books2，里面有 java、golang、rust。\n交集：\nsinter books1 books2 # 求两个集合的交集 # 输出 1) \"java\" 2) \"golang\" 并集：\nsunion books1 books2 # 求两个集合的并集 # 输出 1) \"java\" 2) \"python\" 3) \"golang\" 4) \"rust\" 差集：\nsdiff books1 books2 # 求两个集合的差集 # 输出 1) \"python\" 2) \"rust\" 应用场景 抽奖小程序 点击参与抽奖则加入到集合中。 sadd key {userid} 查看参与抽奖所有用户 smembers key 抽取 count 名中奖用户 srandmember key count # 从集合中随机抽取 count 个元素，并且将抽取的元素从集合中移除 # 针对那种中奖之后不能参与其他抽奖的场景 spop key count 微信微博点赞、收藏 点赞 sadd like:{消息 ID} {用户 ID} 取消点赞 srem like:{消息 ID} {用户 ID} 检查用户是否点过赞，是否点亮点赞的 button sismember like:{消息 ID} {用户 ID} 获取点赞用户列表 smembers like:{消息 ID} 获取点赞用户数 scard like:{消息 ID} 集合操作实现微博关注模型 刘备关注的人：liubeiSet -\u003e {guojia, xushu} 杨过关注的人: yangguoSet--\u003e {liubei, baiqi, guojia, xushu} 郭嘉关注的人: guojiaSet-\u003e {liubei, yangguo, baiqi, xushu, xunyu} 刘备和杨过共同关注: SINTER liubeiSet yangguoSet--\u003e {guojia, xushu} 刘备关注的人（郭嘉、徐庶）也关注了他（杨过） SISMEMBER guojiaSet yangguo SISMEMBER xushuSet yangguo 刘备可能认识的人: SDIFF yangguoSet liubeiSet -\u003e {liubei, baiqi} 集合操作实现电商商品筛选 sadd brand:huawei p40 sadd brand:xiaomi mi-10 sadd brand:iphone iphone12 sadd os:android p40 mi-10 sadd cpu:brand:intel p40 mi-10 sadd ram:8G p40 mi-10 iphone12 筛选出安卓手机、intel CPU、8G 内存的商品: sinter os:android cpu:brand:intel ram:8G {p40，mi-10}\n有序集合（zset） zset 可能是 Redis 提供的最为特色的数据结构，它也是在面试中面试官最爱问的数据结构。它类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。它的内部实现用的是一种叫做跳跃列表的数据结构。\nzset 中最后一个 value 被移除后，数据结构自动删除，内存被回收。\nzset 可以用来存粉丝列表，value 值是粉丝的用户 ID，score 是关注时间。我们可以对粉丝列表按关注时间进行排序。\nzset 还可以用来存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次。\n底层使用 ziplist 或者 skiplist + hashtable 存储。当元素个数比较少的时候，会使用 ziplist 来存储。\nredis\u003e zadd books 9.0 \"think in java\" (integer) 1 redis\u003e zadd books 8.9 \"java concurrency\" (integer) 1 redis\u003e zadd books 8.6 \"java cookbook\" (integer) 1 redis\u003e zrange books 0 -1 # 按 score 排序列出，参数区间为排名范围 1) \"java cookbook\" 2) \"java concurrency\" 3) \"think in java\" redis\u003e zrevrange books 0 -1 # 按 score 逆序列出，参数区间为排名范围 1) \"think in java\" 2) \"java concurrency\" 3) \"java cookbook\" redis\u003e zcard books # 相当于 count() (integer) 3 redis\u003e zscore books \"java concurrency\" # 获取指定 value 的 score \"8.9000000000000004\" # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题 redis\u003e zrank books \"java concurrency\" # 排名 (integer) 1 redis\u003e zrangebyscore books 0 8.91 # 根据分值区间遍历 zset 1) \"java cookbook\" 2) \"java concurrency\" redis\u003e zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。 1) \"java cookbook\" 2) \"8.5999999999999996\" 3) \"java concurrency\" 4) \"8.9000000000000004\" redis\u003e zrem books \"java concurrency\" # 删除 value (integer) 1 redis\u003e zrange books 0 -1 1) \"java cookbook\" 2) \"think in java\" 应用场景 新闻排行榜 点击新闻：zincrby hotNews:20190819 1 守护香港 展示当日排行前十：zrevrange hotNews:20190819 0 9 WITHSCORES 七日搜索榜单计算：zunionstore hotNews:20190813-20190819 7 hotNews:20190813 hotNews:20190814... hotNews:20190819 展示七日排行前十: zrevrange hotNews:20190813-20190819 0 9 WITHSCORES 容器型数据结构 list/set/hash/zset 这四种都属于容器型数据结构，他们有两条通用规则：\n如果容器不存在，那就创建一个，再进行操作。比如 RPUSH，如果列表不存在，Redis 就会自动创建一个，然后再执行 RPUSH。 如果容器里元素没有了，那么立即删除 key，释放内存。比如 LPOP 操作到最后一个元素，列表 key 就会自动删除。 过期时间 Redis 所有的数据结构都可以设置过期时间，时间到了，Redis 会自动删除相应的对象。需要注意的是过期是以对象为单位，比如一个 hash 结构的过期是整个 hash 对象的过期，而不是其中的某个子 key。\n还有一个需要特别注意的地方是如果一个字符串已经设置了过期时间，然后调用了 set 方法修改了它，它的过期时间会消失。\n127.0.0.1:6379\u003e set codehole yoyo OK 127.0.0.1:6379\u003e expire codehole 600 (integer) 1 127.0.0.1:6379\u003e ttl codehole (integer) 597 127.0.0.1:6379\u003e set codehole yoyo OK 127.0.0.1:6379\u003e ttl codehole (integer) -1 "},"title":"Redis 入门"},"/db-learn/docs/redis/practice/06_persistence/":{"data":{"":"Redis 的数据全部在内存里，如果突然宕机，数据就会全部丢失，Redis 的持久化机制就是用来来保证 Redis 的数据不会因为故障而丢失。\nRedis 的持久化机制有两种：\n快照，快照是一次全量备份。 AOF 日志，AOF 日志是连续的增量备份。 ","aof-日志#AOF 日志":"RDB 快照功能并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化，将修改的每一条指令记录进文件 appendonly.aof 中 (先写入 os page cache，每隔一段时间 fsync 到磁盘)。\n通过修改配置文件来打开 AOF 功能：\n# appendonly yes 每当 Redis 执行一个改变数据集的命令时（比如 SET）， 这个命令就会被追加到 AOF 文件的末尾。这样的话，当 Redis 重新启动时，程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。\nAOF 日志存储的是 Redis 服务器的顺序指令序列，AOF 日志只记录对内存进行修改的指令记录。\nfsync AOF 日志是以文件的形式存在的，当程序对 AOF 日志文件进行写操作时，实际上是将内容写到了内核为文件描述符分配的一个内存缓存中，然后内核会异步将脏数据刷到磁盘的。\n这就意味着如果机器突然宕机，AOF 日志内容可能还没有来得及完全刷到磁盘中，这个时候会丢失部分数据。\n为了避免这种情况，Redis 提供了三种 fsync 策略：\nappendfsync always：每次有新命令追加到 AOF 文件时就执行一次 fsync ，非常慢，也非常安全。 appendfsync everysec：每秒 fsync 一次，足够快，并且在故障时只会丢失 1 秒钟的数据。 appendfsync no：从不 fsync，将数据交给操作系统来处理。更快，也更不安全的选择。 推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。\nAOF 重写 AOF 文件里可能有太多没用指令，所以 AOF 会定期根据内存的最新数据生成 aof 文件。例如，执行了如下几条命令：\n1 127.0.0.1:6379\u003e incr readcount 2 (integer) 1 3 127.0.0.1:6379\u003e incr readcount 4 (integer) 2 5 127.0.0.1:6379\u003e incr readcount 6 (integer) 3 7 127.0.0.1:6379\u003e incr readcount 8 (integer) 4 9 127.0.0.1:6379\u003e incr readcount 10 (integer) 5 重写后 AOF 文件里变成：\n... 5 readcount 6 $1 7 5 Redis 启动时如果既有 rdb 文件又有 aof 文件则优先选择 aof 文件恢复数据，因为一般来说 aof 中的数据更新一点。\n自动重写 下面两个配置可以控制 AOF 自动重写频率：\n# auto‐aof‐rewrite‐min‐size 64mb // aof 文件至少要达到 64M 才会自动重写，文件太小恢复速度本来就很快，重写的意义不大 # auto‐aof‐rewrite‐percentage 100 // aof 文件自上一次重写后文件大小增长了 100% 则再次触发重写 手动重写 进入 Redis 客户端执行命令 bgrewriteaof 可以手动重写 AOF 日志。其原理就是开辟一个子进程对内存进行遍历转换成一系列 Redis 的操作指令（比如添加一个 key，但是这时已经失效了，就不需要再添加到 AOF 日志中区），序列化到一个新的 AOF 日志文件中。序列化完毕后再将操作期间发生的增量 AOF 日志追加到这个新的 AOF 日志文件中，追加完毕后就立即替代旧的 AOF 日志文件了，瘦身工作就完成了。","rdb#RDB":"快照（RDB）是内存数据的二进制序列化形式，在存储上非常紧凑。\n自动快照 在默认情况下， Redis 将内存数据库快照保存在名字为 dump.rdb 的二进制文件中。\n可以对 Redis 进行设置， 让它在 “N 秒内数据集至少有 M 个改动” 这一条件被满足时，自动保存一次数据集。\n比如说，以下设置会让 Redis 在满足 “60 秒内有至少有 1000 个键被改动” 这一条件时，自动保存一次数据集：\n# save 60 1000 // 关闭 RDB 只需要将所有的 save 保存策略注释掉即可 手动快照 RDB 快照还可以手动执行命令生成，进入 Redis 客户端执行命令 save 或 bgsave 可以生成 dump.rdb 文件，每次命令执行都会将所有 Redis 内存快照到一个新的 rdb 文件里，并覆盖原有 rdb 快照文件。\nsave 与 bgsave 对比：\n命令 save bgsave IO 类型 同步 异步 是否阻塞 Redis 其它命令 是 否 (在生成子进程执行调用 fork 函数时会有短暂阻塞) 复杂度 O(n) O(n) 优点 不会消耗额外内存 不阻塞客户端命令 缺点 阻塞客户端命令 需要 fork 子进程，消耗内存 bgsave 的写时复制 (COW) 机制 Redis 借助操作系统提供的写时复制技术（Copy-On-Write, COW），在生成快照的同时，依然可以正常处理其他写命令。简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。\nbgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。此时，如果主线程对这些数据也都是读操作，那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据，那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。\n写时复制 fork 时全量内存拷贝是难以接受的，假设需要在一个进程中通过 fork 创建一个新的进程执行一段逻辑，fork 拷贝的大量内存空间对于子进程来说可能完全没有任何作用的，但是却引入了巨大的额外开销。\n写时拷贝（Copy-on-Write）的出现就是为了解决这一问题，写时拷贝的主要作用就是将拷贝推迟到写操作真正发生时，这也就避免了大量无意义的拷贝操作。在一些早期的 unix 系统上，系统调用 fork 确实会立刻对父进程的内存空间进行复制，但是在今天的多数系统中，fork 并不会立刻触发这一过程。\n在 fork 函数调用时，父进程和子进程会被 Kernel 分配到不同的虚拟内存空间中，所以在两个进程看来它们访问的是不同的内存。\n在真正访问虚拟内存空间时，Kernel 会将虚拟内存映射到同一块物理内存上，所以父子进程共享了物理上的内存空间。\n当父进程或者子进程对共享的内存进行修改时，共享的内存才会以页为单位进行拷贝，父进程会保留原有的物理空间，而子进程会使用拷贝后的新物理空间。","redis-数据备份策略#Redis 数据备份策略":" 写 crontab 定时调度脚本，每小时都 copy 一份 rdb 或 aof 的备份到一个目录中去，仅仅保留最近 48 小时的备份。 每天都保留一份当日的数据备份到一个目录中去，可以保留最近 1 个月的备份。 每次 copy 备份的时候，都把太旧的备份给删了。 每天晚上将当前机器上的备份复制一份到其他机器上，以防机器损坏。 通常 Redis 的主节点是不会进行持久化操作，持久化操作主要在从节点进行，从节点是备份节点，没有来自客户端请求的压力。但是如果出现网络分区，从节点长期连不上主节点，就会出现数据不一致的问题，所以在生产环境要做好实时监控工作，保证网络畅通或者能快速修复。 ","混合持久化#混合持久化":"重启 Redis 时，一般很少使用 rdb 来恢复内存状态，因为会丢失大量数据。通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 rdb 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。\nRedis 4.0 为了解决这个问题，带来了一个新的持久化选项——混合持久化。将 rdb 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，而是自 rdb 持久化开始到持久化结束的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小。\n于是在 Redis 重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。\n开启混合持久化 (必须先开启 aof)：\n# aof‐use‐rdb‐preamble yes 如果开启了混合持久化，AOF 在重写时，不再是单纯将内存数据转换为 RESP 命令写入 AOF 文件，而是将重写这一刻之前的内存做 RDB 快照处理，并且将 RDB 快照内容和增量的 AOF（在重写时，和重写后新的命令执行）修改内存数据的命令存在一起，都写入新的 AOF 文件，新的文件一开始不叫 appendonly.aof，等到重写完新的 AOF 文件才会进行改名，覆盖原有的 AOF 文件，完成新旧两个 AOF 文件的替换。\n于是在 Redis 重启的时候，可以先加载 RDB 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，因此重启效率大幅得到提升。"},"title":"持久化"},"/db-learn/docs/redis/practice/07_cluster/":{"data":{"":"","cap-原理#CAP 原理":"","redis-70#Redis 7.0":"Redis 7.0 对主从复制进行了优化，性能有了很大的提升。\nRedis 7.0 之前的主从复制的问题 多从库时主库内存占用过多 Redis 的主从复制主要分为两步：\n全量同步，主库通过 fork 子进程产生内存快照，然后将数据序列化为 RDB 格式同步到从库，使从库的数据与主库某一时刻的数据一致。 命令传播：全量同步期间，master 会继续接收客户端的请求，它会把这些可能修改数据集的请求缓存在内存中。当从库与主库完成全量同步后，进入命令传播阶段，主库将变更数据的命令发送到从库，从库将执行相应命令，使从库与主库数据持续保持一致。 复制积压区，可以理解为是一个备份，因为主从复制的过程中，如果从库的连接突然断开了，那么从库对应的从库复制缓冲区会被释放掉，包括其他的网络资源。等到从库重新连接时，重新开始复制，就刻意从复制积压区找到断开连接时数据复制的位置，从这个断开的位置开始继续复制。\n如上图所示，对于 Redis 主库，当用户的写请求到达时，主库会将变更命令分别写入所有从库复制缓冲区（OutputBuffer)，以及复制积压区 (ReplicationBacklog)。\n该实现一个明显的问题是内存占用过多，所有从库的连接在主库上是独立的，也就是说每个从库 OutputBuffer 占用的内存空间也是独立的，那么主从复制消耗的内存就是所有从库缓冲区内存大小之和。如果我们设定从库的 client-output-buffer-limit 为 1GB，如果有三个从库，则在主库上可能会消耗 3GB 的内存用于主从复制。另外，真实环境中从库的数量不是确定的，这也导致 Redis 实例的内存消耗不可控。\nℹ️ 当全量复制的时间过长或者 client-output-buffer-limit 设置的 buffer 过小，会导致增量的指令在 buffer 中被覆盖，导致全量复制后无法进行增量复制，然后会再次发起快照同步，如此极有可能会陷入快照同步的死循环。 OutputBuffer 拷贝和释放的堵塞问题 Redis 为了提升多从库全量复制的效率和减少 fork 产生 RDB 的次数，会尽可能的让多个从库共用一个 RDB，从代码 (replication.c) 上看：\n当已经有一个从库触发 RDB BGSAVE 时，后续需要全量同步的从库会共享这次 BGSAVE 的 RDB，为了从库复制数据的完整性，会将第一个触发 RDB BGSAVE 从库的 OutputBuffer 拷贝到后续请求全量同步从库的 OutputBuffer 中。\n代码中的 copyClientOutputBuffer 可能存在堵塞问题，因为 OutputBuffer 链表上的数据可达数百 MB 甚至数 GB 之多，对其拷贝的耗时可能达到百毫秒甚至秒级的时间，而且该堵塞问题没法通过日志或者 latency 观察到，但对 Redis 性能影响却很大，甚至造成 Redis 阻塞。\n同样地，当 OutputBuffer 大小触发 limit 限制时，Redis 就是关闭该从库链接，而在释放 OutputBuffer 时，也需要释放数百 MB 甚至数 GB 的数据，其耗时对 Redis 而言也很长。\n而且如果重新设置 ReplicationBacklog 大小时，Redis 会重新申请一块内存，然后将 ReplicationBacklog 中的内容拷贝过去，这也是非常耗时的操作。\nReplicationBacklog 的限制 复制积压缓冲区 ReplicationBacklog 是 Redis 实现部分重同步的基础，如果从库可以进行增量同步，则主库会从 ReplicationBacklog 中拷贝从库缺失的数据到其 OutputBuffer。拷贝的数据量最大当然是 ReplicationBacklog 的大小，为了避免拷贝数据过多的问题，通常不会让该值过大，一般百兆左右。但在大容量实例中，为了避免由于主从网络中断导致的全量同步，又希望该值大一些，这就存在矛盾了。\nRedis 7.0 主从复制的优化 每个从库都有自己的 OutputBuffer，但其存储的内容却是一样的，一个最直观的想法就是主库在命令传播时，将这些命令放在一个全局的复制数据缓冲区中，多个从库共享这份数据。复制积压缓冲区（ReplicationBacklog）中的内容与从库 OutputBuffer 中的数据也是一样的，所以该方案中，ReplicationBacklog 和从库一样共享一份复制缓冲区的数据，也避免了 ReplicationBacklog 的内存开销。\n共享复制缓存区方案中复制缓冲区 (ReplicationBuffer) 的表示采用链表的表示方法，将 ReplicationBuffer 数据切割为多个 16KB 的数据块 (replBufBlock)，然后使用链表来维护起来。为了维护不同从库的对 ReplicationBuffer 的使用信息，在 replBufBlock 中存在字段：\nrefcount：block 被引用的次数。 id：block 的 id。 repl_offset：block 中数据的偏移量。 ReplicationBuffer 由多个 replBufBlock 组成链表，当复制积压区或从库对某个 block 使用时，便对正在使用的 replBufBlock 增加引用计数，上图中可以看到，复制积压区正在使用的 replBufBlock refcount 是 1，从库 A 和 B 正在使用的 replBufBlock 的 refcount 是 2。当从库使用完当前的 replBufBlock（已经将数据发送给从库）时，就会对其 refcount 减 1 而且移动到下一个 replBufBlock，并对其 refcount 加 1。\n堵塞问题和限制问题的解决 多从库消耗内存过多的问题通过共享复制缓存区方案得到了解决，对于 OutputBuffer 拷贝和释放的堵塞问题和 ReplicationBacklog 的限制问题是否解决了？\n首先来看 OutputBuffer 拷贝和释放的堵塞问题问题，这个问题很好解决，因为 ReplicationBuffer 是个链表实现，当前从库的 OutputBuffer 只需要维护共享 ReplicationBuffer 的引用信息即可。所以无需进行数据深拷贝，只需要更新引用信息，即对正在使用的 replBufBlock 的 refcount 加 1，这仅仅是一条简单的赋值操作，非常轻量。\nOutputBuffer 释放问题呢？在当前的方案中释放从库 OutputBuffer 就变成了对其正在使用的 replBufBlock 的 refcount 减 1，也是一条赋值操作，不会有任何阻塞。\n对于 ReplicationBacklog 的限制问题也很容易解决了，因为 ReplicatonBacklog 也只是记录了对 ReplicationBuffer 的引用信息，对 ReplicatonBacklog 的拷贝也仅仅成了找到正确的 replBufBlock，然后对其 refcount 加 1。这样的话就不用担心 ReplicatonBacklog 过大导致的拷贝堵塞问题。而且对 ReplicatonBacklog 大小的变更也仅仅是配置的变更，不会清掉数据。\nReplicationBuffer 的裁剪和释放 ReplicationBuffer 不可能无限增长，Redis 有相应的逻辑对其进行裁剪，简单来说，Redis 会从头访问 replBufBlock 链表，如果发现 replBufBlock 的 refcount 为 0，则会释放它，直到迭代到第一个 replBufBlock 的 refcount 不为 0 才停止。所以想要释放 ReplicationBuffer，只需要减少相应 replBufBlock 的 refcount，会减少 refcount 的主要情况有：\n当从库使用完当前的 replBufBlock 会对其 refcount 减 1； 当从库断开链接时会对正在引用的 replBufBlock 的 refcount 减 1，无论是因为超过 client-output-buffer-limit 导致的断开还是网络原因导致的断开； 3、当 ReplicationBacklog 引用的 replBufBlock 数据量超过设置的该值大小时，会对正在引用的 replBufBlock 的 refcount 减 1，以尝试释放内存； 不过当一个从库引用的 replBufBlock 过多，它断开时释放的 replBufBlock 可能很多，也可能造成堵塞问题，所以 Redis7 里会限制一次释放的个数，未及时释放的内存在系统的定时任务中渐进式释放。\n数据结构的选择 当从库尝试与主库进行增量重同步时，会发送自己的 repl_offset，主库在每个 replBufBlock 中记录了该其第一个字节对应的 repl_offset，但如何高效地从数万个 replBufBlock 的链表中找到特定的那个？\n链表只能直接从头到位遍历链表查找对应的 replBufBlock，这个操作必然会耗费较多时间而堵塞服务。\nRedis 7 使用 rax 树实现了对 replBufBlock 固定区间间隔的索引，每 64 个记录一个索引点。一方面，rax 索引占用的内存较少；另一方面，查询效率也是非常高，理论上查找比较次数不会超过 100，耗时在 1 毫秒以内。\nRAX 树 Redis 中还有其他地方使用了 Rax 树，比如 streams 这个类型里面的 consumer group (消费者组) 的名称还有和 Redis 集群名称存储。\nRAX 叫做基数树（前缀压缩树），就是有相同前缀的字符串，其前缀可以作为一个公共的父节点，什么又叫前缀树？\nTrie 树\n即字典树，也有的称为前缀树，是一种树形结构。广泛应用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是最大限度地减少无谓的字符串比较，查询效率比较高。\nTrie 的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。\n先看一下几个场景问题：\n我们输入 n 个单词，每次查询一个单词，需要回答出这个单词是否在之前输入的 n 单词中出现过。 答：当然是用 map 来实现。\n我们输入 n 个单词，每次查询一个单词的前缀，需要回答出这个前缀是之前输入的 n 单词中多少个单词的前缀？ 答：还是可以用 map 做，把输入 n 个单词中的每一个单词的前缀分别存入 map 中，然后计数，这样的话复杂度会非常的高。若有 n 个单词，平均每个单词的长度为 c，那么复杂度就会达到 n*c。\n因此我们需要更加高效的数据结构，这时候就是 Trie 树的用武之地了。现在我们通过例子来理解什么是 Trie 树。现在我们对 cat、cash、apple、aply、ok 这几个单词建立一颗Trie 树。\n从图中可以看出：\n每一个节点代表一个字符 有相同前缀的单词在树中就有公共的前缀节点。 整棵树的根节点是空的。 每个节点结束的时候用一个特殊的标记来表示，这里用 -1 来表示结束，从根节点到 -1 所经过的所有的节点对应一个英文单词。 查询和插入的时间复杂度为 O(k)，k 为字符串长度，当然如果大量字符串没有共同前缀时还是很耗内存的。 所以，总的来说，Trie 树把很多的公共前缀独立出来共享了。这样避免了很多重复的存储。想想字典集的方式，一个个的key被单独的存储，即使他们都有公共的前缀也要单独存储。相比字典集的方式，Trie 树显然节省更多的空间。\nTrie 树其实依然比较浪费空间，比如前面所说的“如果大量字符串没有共同前缀时”。比如这个字符串列表：“deck”, “did”, “doe”, “dog”, “doge” , “dogs”。“deck” 这一个分支，有没有必要一直往下来拆分吗？还有 “did”，存在着一样的问题。像这样的不可分叉的单支分支，其实完全可以合并，也就是压缩。\nRadix 树：压缩后的 Trie 树\n所以 Radix 树就是压缩后的 Trie 树，因此也叫压缩 Trie 树。比如上面的字符串列表完全可以这样存储：\n同时在具体存储上，Radix 树的处理是以 bit（或二进制数字）来读取的。一次被对比 r 个 bit。\n比如 “dog”, “doge” , “dogs”，按照人类可读的形式，dog 是 dogs 和 doge 的子串。但是如果按照计算机的二进制比对：\ndog: 01100100 01101111 01100111\ndoge: 01100100 01101111 01100111 01100101\ndogs: 01100100 01101111 01100111 01110011\n可以发现 dog 和 doge 是在第二十五位的时候不一样的。dogs 和 doge 是在第二十八位不一样的。也就是说，从二进制的角度还可以进一步进行压缩。把第二十八位前面相同的 011 进一步压缩。","redis-cluster#Redis Cluster":"Redis 3.0 以前的版本要实现集群一般是借助哨兵来监控 master 节点的状态，如果 master 节点异常，则会做主从切换，将某一台 slave 作为 master，哨兵的配置略微复杂，并且性能和高可用性等各方面表现一般，特别是在主从切换的瞬间存在访问瞬断的情况，而且哨兵模式只有一个主节点对外提供服务，没法支持很高的并发，且单个主节点内存也不宜设置得过大，否则会导致持久化文件过大，影响数据恢复或主从同步的效率（有可能会陷入快照同步的死循环），一般推荐小于 10G。\nRedis Cluster 架构 Redis Cluster 是 Redis 官方提供的分布式集群方案。\nRedis 集群是一个由多个主从节点组成的分布式服务器群，它具有复制、高可用和分片特性。Redis 集群不需要哨兵也能完成节点移除和故障转移的功能。需要将每个节点设置成集群模式，这种集群模式没有中心节点，可水平扩展，据官方文档称可以线性扩展到上万个节点 (官方推荐不超过 1000 个节点)。Redis 集群的性能和高可用性均优于之前版本的哨兵模式，且集群配置非常简单。\nRedis 集群搭建 Redis 集群需要至少三个 master 节点，这里搭建三个 master 节点，并且给每个 master 再搭建一个 slave 节点，总共 6 个 Redis 节点，这里用三台机器部署 6 个 Redis 实例，每台机器一主一从，搭建集群的步骤如下：\n第一步：在第一台机器的 /usr/local 下创建文件夹 redis-cluster，然后在其下面分别创建 2 个文件夾如下 （1）mkdir -p /usr/local/redis-cluster （2）mkdir 8001 8004 第一步：把之前的 redis.conf 配置文件 copy 到 8001 下，修改如下内容： （1）daemonize yes （2）port 8001（分别对每个机器的端口号进行设置） （3）pidfile /var/run/redis_8001.pid # 把 pid 进程号写入 pidfile 配置的文件 （4）dir /usr/local/redis-cluster/8001/（指定数据文件存放位置，必须要指定不同的目录位置，不然会丢失数据） （5）cluster-enabled yes（启动集群模式） （6）cluster-config-file nodes-8001.conf（集群节点信息文件，这里 800x 最好和 port 对应上） （7）cluster-node-timeout 10000 (8)# bind 127.0.0.1（bind 绑定的是自己机器网卡的 ip，如果有多块网卡可以配多个 ip，代表允许客户端通过机器的哪些网卡 ip 去访问，内网一般可以不配置 bind，注释掉即可） (9) protected-mode no （关闭保护模式） (10) appendonly yes 如果要设置密码需要增加如下配置： (11) requirepass zhuge (设置 Redis 访问密码) (12) masterauth zhuge (设置集群节点间访问密码，跟上面一致) 第三步：把修改后的配置文件，copy 到 8004，修改第 2、3、4、6 项里的端口号，可以用批量替换： :%s/源字符串/目的字符串/g 第四步：另外两台机器也需要做上面几步操作，第二台机器用 8002 和 8005，第三台机器用 8003 和 8006 第五步：分别启动 6 个 Redis 实例，然后检查是否启动成功 （1）/usr/local/redis-5.0.3/src/redis-server /usr/local/redis-cluster/800*/redis.conf （2）ps -ef | grep redis # 查看是否启动成功 第六步：用 redis-cli 创建整个 Redis 集群( Redis 5 以前的版本集群是依靠 ruby 脚本 redis-trib.rb 实现) # 执行这条命令需要确认三台机器之间的 Redis 实例要能相互访问，可以先简单把所有机器防火墙关掉，如果不关闭防火墙则需要打开 Redis 服务端口和集群节点 gossip 通信端口 16379 (默认是在 Redis 端口号上加 10000) # 关闭防火墙 # systemctl stop firewalld # 临时关闭防火墙 # systemctl disable firewalld # 禁止开机启动 # 注意：下面这条创建集群的命令大家不要直接复制，里面的空格编码可能有问题导致创建集群不成功 （1）/usr/local/redis-5.0.3/src/redis-cli -a zhuge --cluster create --cluster-replicas 1 192.168.0.61:8001 192.168.0.62:8002 192.168.0.63:8003 192.168.0.61:8004 192.168.0.62:8005 192.168.0.63:8006 # --cluster-replicas 1 表示为每个创建的主服务器节点创建一个从服务器节点，对于这里 6 个节点来说，就是 3 主 3 从 第七步：验证集群： （1）连接任意一个客户端即可：./redis-cli -c -h -p (-a 访问服务端密码，-c 表示集群模式，指定 ip 地址和端口号） 如：/usr/local/redis-5.0.3/src/redis-cli -a zhuge -c -h 192.168.0.61 -p 800* （2）进行验证： cluster info（查看集群信息）、cluster nodes（查看节点列表） （3）进行数据操作验证 （4）关闭集群则需要逐个进行关闭，使用命令： /usr/local/redis-5.0.3/src/redis-cli -a zhuge -c -h 192.168.0.60 -p 800* shutdown 其中 slots 就是分配给每个节点的槽位，只有主节点才会分配槽位。\n前面创建集群时，8001 和 8004 是在一个节点上的，8002 和 8005 是在一个节点上的，8003 和 8006 是在一个节点上的，但是上面的节点列表中，8001 是主节点，它的从节点却是 8005，8002 的从节点是 8006，8003 的从节点是 8004，这是为什么？\n因为更加安全，避免一个节点挂了导致小的主从集群不可用。\ncluster-config-file nodes-8001.conf 集群创建好以后，整个集群节点的信息会被保存到这个配置文件中。\n为什么要保存到这个文件中？\n因为如果整个集群如果关掉了，再次启动的时候是不能再使用 --cluster create 命令的，只需要把每个节点的 Redis 重新启动即可。Redis 启动的时候会读取这个配置文件中的节点信息，然后再重新组件集群。\n增加节点 在 /usr/local/redis-cluster 下创建 8007 和 8008 文件夹，并拷贝 8001 文件夹下的 redis.conf 文件到 8007 和 8008 这两个文件夹下。\nmkdir 8007 8008 cd 8001 cp redis.conf /usr/local/redis-cluster/8007/ cp redis.conf /usr/local/redis-cluster/8008/ # 修改 8007 文件夹下的 redis.conf 配置文件 vim /usr/local/redis-cluster/8007/redis.conf # 修改如下内容 port 8007 dir /usr/local/redis-cluster/8007/ cluster-config-file nodes-8007.conf # 修改 8008 文件夹下的 redis.conf 配置文件 vim /usr/local/redis-cluster/8008/redis.conf # 修改内容如下 port 8008 dir /usr/local/redis-cluster/8008/ cluster-config-file nodes-8008.conf # 启动 8007 和 8008 俩个服务并查看服务状态 /usr/local/redis-5.0.3/src/redis-server /usr/local/redis-cluster/8007/redis.conf /usr/local/redis-5.0.3/src/redis-server /usr/local/redis-cluster/8008/redis.conf ps -el | grep redis 配置 8007 为集群主节点：\n/usr/local/redis-5.0.3/src/redis-cli -a zhuge --cluster add-node 192.168.0.61:8007 192.168.0.61:8001 使用 add-node 命令新增一个主节点 8007 (master)，前面的 ip:port 为新增节点，后面的 ip:port 为已知存在节点，看到日志最后有 “[OK] New node added correctly” 提示代表新节点加入成功。\n当添加节点成功以后，新增的节点不会有任何数据，因为它还没有分配任何的 slot，我们需要为新节点手工分配 slot：\n# 查看集群状态 /usr/local/redis-5.0.3/src/redis-cli -a zhuge -c -h 192.168.0.61 -p 8001 192.168.0.61:8001\u003e cluster nodes # 为 8007 分配 hash 槽，找到集群中的任意一个主节点，对其进行重新分片工作 /usr/local/redis-5.0.3/src/redis-cli -a zhuge --cluster reshard 192.168.0.61:8001 输出如下：\n# 需要多少个槽移动到新的节点上，自己设置，比如 600 个槽 How many slots do you want to move (from 1 to 16384)? 600 # 把这 600 个 hash 槽移动到哪个节点上去，需要指定节点 id What is the receiving node ID? 2728a594a0498e98e4b83a537e19f9a0a3790f38 Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node 1:all # 输入 all 为从所有主节点 (8001,8002,8003) 中分别抽取相应的槽数指定到新节点中，抽取的总槽数为 600 个 ... ... Do you want to proceed with the proposed reshard plan (yes/no)? yes # 输 入yes 确认开始执行分片任务 添加从节点 8008 到集群中去并查看集群状态：\n/usr/local/redis-5.0.3/src/redis-cli -a zhuge --cluster add-node 192.168.0.61:8008 192.168.0.61:8001 还是一个 master 节点，没有被分配任何的 hash 槽。新加入的节点默认就是 master 节点。\n执行 replicate 命令来指定当前节点(从节点)的主节点 id 为哪个,首先需要连接新加的 8008 节点的客户端，然后使用集群命令进行操作，把当前的 8008 (slave) 节点指定到一个主节点下：\n/usr/local/redis-5.0.3/src/redis-cli -a zhuge -c -h 192.168.0.61 -p 8008 192.168.0.61:8008\u003e cluster replicate 2728a594a0498e98e4b83a537e19f9a0a3790f38 #后面这串 id 为 8007 的节点 id 集群相关命令 create：创建一个集群环境。 call：可以执行 Redis 命令。 add-node：将一个节点添加到集群里，第一个参数为新节点的 ip:port ，第二个参数为集群中任意一个已经存在的节点的 ip:port。 del-node：移除一个节点。删除主节点，必须先把该主节点的 hash 槽移动到其他主节点上，然后才能删除。 reshard：重新分片。 check：检查集群状态 。 Redis 集群原理 Redis Cluster 将所有数据划分为 16384 个 slots(槽位)，每个节点负责其中一部分槽位。槽位的信息存储于每个节点中。当 Redis Cluster 的客户端来连接集群时，它也会得到一份集群的槽位配置信息并将其缓存在客户端本地。这样当客户端要查找某个 key 时，可以直接定位到目标节点。同时因为槽位的信息可能会存在客户端与服务器不一致的情况，还需要纠正机制来实现槽位信息的校验调整。\n哨兵架构访问瞬断的问题在集群中也没有完全解决，但是因为集群中的数据是分散存储在多个节点上的，所以当客户端访问某个节点时，如果这个节点挂了，并不会影响其他节点的数据。只有这个集群内的小的主从集群会出现访问瞬断的情况。\n槽位定位算法 Cluster 默认会对 key 值使用 crc16 算法进行 hash 得到一个整数值，然后用这个整数值对 16384 进行取模来得到具体槽位。\nCluster 还允许用户强制某个 key 挂在特定槽位上，通过在 key 字符串里面嵌入 tag 标记，这就可以强制 key 所挂在的槽位等于 tag 所在的槽位。\n跳转重定向 当客户端向一个错误的节点发出了指令，该节点会发现指令的 key 所在的槽位并不归自己管理，这时它会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，告诉客户端去连这个节点去获取数据。\nGET x -MOVED 3999 127.0.0.1:6381 MOVED 指令的第一个参数 3999 是 key 对应的槽位编号，后面是目标节点地址。MOVED 指令前面有一个减号，表示该指令是一个错误消息。\n客户端收到 MOVED 指令后，要立即纠正本地的槽位映射表。后续所有 key 将使用新的槽位映射表。\nRedis 集群节点间的通信机制 Redis cluster 节点间采取 gossip 协议进行通信维护集群的元数据 (集群节点信息，主从角色，节点数量，各节点共享的数据等) 有两种方式：集中式和 gossip\n集中式： 优点在于元数据的更新和读取，时效性非常好，一旦元数据出现变更立即就会更新到集中式的存储中，其他节点读取的时候立即就可以立即感知到；不足在于所有的元数据的更新压力全部集中在一个地方，可能导致元数据的存储压力。很多中间件都会借助 zookeeper 集中式存储元数据。\ngossip：\ngossip 协议包含多种消息，包括 ping，pong，meet，fail 等等。 meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信； ping：每个节点都会频繁给其他节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据(类似自己感知到的集群节点增加和移除，hash slot 信息等)； pong: 对 ping 和 meet 消息的返回，包含自己的状态和其他信息，也可以用于信息广播和更新； fail: 某个节点判断另一个节点 fail 之后，就发送 fail 给其他节点，通知其他节点，指定的节点宕机了。 gossip 协议的优点在于元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续，打到所有节点上去更新，有一定的延时，降低了压力；缺点在于元数据更新有延时可能导致集群的一些操作会有一些滞后。\ngossip 通信的端口： 每个节点都有一个专门用于节点间 gossip 通信的端口，就是自己提供服务的 端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其他几点接收到 ping 消息之后返回 pong 消息。\n这也是为什么不推荐集群的节点超过 1000 个的原因，因为集群内部节点的心跳通知非常频繁，这对网络带宽是一个非常大的消耗。\n网络抖动 真实世界的机房网络往往并不是风平浪静的，它们经常会发生各种各样的小问题。比如网络抖动就是非常常见的一种现象，突然之间部分连接变得不可访问，然后很快又恢复正常。\n为解决这种问题，Redis Cluster 提供了一种选项 cluster-node-timeout，表示当某个节点持续 timeout 的时间失联时，才可以认定该节点出现故障，需要进行主从切换。如果没有这个选项，网络抖动会导致主从频繁切换 (数据的重新复制)。\nRedis 集群选举原理 当 slave 发现自己的 master 变为 FAIL 状态时，便尝试进行 Failover，以期成为新的 master。由于挂掉的 master 可能会有多个 slave，从而存在多个 slave 竞争成为 master 节点的过程，其过程如下：\nslave 发现自己的 master 变为 FAIL 将自己记录的集群 currentEpoch 加 1，并广播 FAILOVER_AUTH_REQUEST 信息 其他节点收到该信息，只有 master 响应，判断请求者的合法性，并发送 FAILOVER_AUTH_ACK，对每一个 epoch 只发送一次 ack。 尝试 failover 的 slave 收集 master 返回的 FAILOVER_AUTH_ACK。 slave 收到超过半数 master 的 ack 后变成新 master (这里解释了集群为什么至少需要三个主节点，如果只有两个，当其中一个挂了，只剩一个主节点是不能选举成功的)。 slave 广播 Pong 消息通知其他集群节点。 为了避免多个从节点在选举获得的票数一样：\n从节点并不是在主节点一进入 FAIL 状态就马上尝试发起选举，而是有一定延迟，一定的延迟确保我们等待 FAIL 状态在集群中传播，slave 如果立即尝试选举，其它 masters 或许尚未意识到 FAIL 状态，可能会拒绝投票。\n延迟计算公式：DELAY = 500ms + random(0 ~ 500ms) + SLAVE_RANK * 1000ms。\nSLAVE_RANK 表示此 slave 已经从 master 复制数据的总量的 rank。Rank 越小代表已复制的数据越新。这种方式下，持有最新数据的 slave 将会首先发起选举（理论上）。\n集群脑裂数据丢失问题 脑裂数据丢失问题，网络分区导致脑裂后多个主节点对外提供写服务，一旦网络分区恢复，会将其中一个主节点变为从节点，这时就会有数据丢失。\nRedis 可以通过配置 min-slaves-to-write 参数来规避脑裂数据丢失问题 （这种方法不可能百分百避免数据丢失，参考集群 master 选举机制）：\n// 写数据成功最少同步的 slave 数量，这个数量可以模仿大于半数机制配置，比如集群总共三个节点可以配置 1，加上 master 就是 2，超过了半数（也就是说至少要有一个从节点同步成功之后，才会返回客户端写入成功）。 // 该参数在 Redis 最新版本里名字已经换成了 min-replicas-to-write。 min-slaves-to-write 1 ℹ️ 这个配置在一定程度上会影响集群的可用性，比如 slave 要是少于 1 个，这个集群就算 master 正常也不能提供服务了，需要具体场景权衡选择。一般情况下可以不用考虑这个配置，可用性是更重要的，丢一点缓存数据是可以接受的。如果因为 Redis 不可用，导致大量请求打到数据库，数据库可能会直接挂掉，这是无法接受的。 主从复制数据丢失问题 一样是通过 min-slaves-to-write 参数来规避：\nmin-slaves-to-write 1 主节点必须在指定数量的从节点确认同步后，才返回写入成功。降低数据丢失风险。\n缺点：写入延迟增加，如果从节点宕机或延迟高，主节点会拒绝写入。影响集群的可用性。\n集群是否完整才能对外提供服务 当 redis.conf 的配置 cluster-require-full-coverage 为 no 时，表示当负责一部分插槽的 master 节点下线且没有相应的 slave 节点进行故障恢复时，集群仍然可用，如果为 yes 则集群不可用。\nRedis 集群为什么至少需要三个 master 节点，并且推荐节点数为奇数？ 因为新 master 的选举需要大于半数的集群 master 节点同意才能选举成功，如果只有两个 master 节点，当其中一个挂了，是达不到选举新 master 的条件的。\n奇数个 master 节点可以在满足选举该条件的基础上节省一个节点，比如三个 master 节点和四个 master 节点的集群相比，如果都挂了一个 master 节点，三个 master 节点的集群只需要两个节点就可以选举，而四个 master 节点的集群需要三个节点（过半）才能选举 。所以三个 master 节点和四个 master 节点的集群都只能挂一个节点。如果都挂了两个 master 节点都没法选举新 master 节点了，所以奇数的 master 节点更多的是从节省机器资源角度出发说的。\nRedis 集群对批量操作命令的支持 对于 Redis 集群，批量操作命令一定要在集群上操作，因为在集群中多个 key 可能是不同的 master 节点的 slot 上，或者在同一个 master 节点的不同的 slot 上，客户端会直接返回错误。这是由于 Redis 要保证批量操作命令的原子性，要么全部成功，要么全部失败。在不同的 master 节点上操作，如果其中的一个 master 节点挂了，会导致有些 key 写入成功，有些 key 写入失败，这就破坏了原子性。\n为了解决这个问题，则可以在 key 的前面加上 {XX}，这样参数数据分片 hash 计算的只会是大括号里的值，这样能确保不同的 key 能落到同一 slot 里去，示例如下：\nmset {user1}:1:name zhuge {user1}:1:age 18 假设 name 和 age 计算的 hash slot 值不一样，但是这条命令在集群下执行，Redis 只会用大括号里的 user1 做 hash slot 计算，所以算出来的 slot 值肯定相同，最后都能落在同一 slot。\n为什么是 16384 个槽位？ 如果槽位为 65536，发送心跳信息的消息头达 8KB，发送的心跳包过于庞大。 当槽位为 65536 时，这块的大小是 65536 / 8 / 1024= 8kb。因为每秒钟，Redis 节点需要发送一定数量的 ping 消息作为心跳包，如果槽位为 65536，这个 ping 消息的消息头太大了，会导致网络拥堵。\nRedis 的集群主节点数量官方建议不超过 1000 个。 集群节点越多，心跳包的消息体内携带的数据越多。如果节点过 1000 个，也会导致网络拥堵。因此官方不建议 Redis cluster 节点数量超过 1000 个。那么，对于节点数在 1000 以内的 Redis cluster 集群，16384 个槽位够用了。没有必要拓展到 65536 个。\n槽位越小，节点少的情况下，压缩率高 Redis 主节点的配置信息中，它所负责的哈希槽是通过一张 bitmap 的形式来保存的，在传输过程中，会对 bitmap 进行压缩，但是如果 bitmap 的填充率 slots / N 很高的话(N 表示节点数)，bitmap 的压缩率就很低。如果节点数很少，而哈希槽数量很多的话，bitmap 的压缩率就很低。","sentinel-哨兵架构#Sentinel 哨兵架构":"Redis 主从架构虽然可以实现数据的高可用，但是当主节点挂掉后，需要手动将从节点提升为主节点，这是一个比较麻烦的过程。\n为了解决这个问题，Redis 引入了 Sentinel 哨兵架构。\nSentinel 是一个分布式架构，它由多个 Sentinel 实例组成，每个 Sentinel 实例都可以监控多个主从节点，它会持续监控主从节点的健康，当主节点挂掉后，它会自动选择一个最优的从节点切换为主节点。\nSentinel 哨兵是特殊的 Redis 服务，不提供读写服务，主要用来监控 Redis 实例节点。\n哨兵架构下 Client 第一次要从哨兵获取到 Redis 的主节点，后续就直接访问 Redis 的主节点，不会每次都通过 Sentinel 代理访问 Redis 的主节点。当 Redis 的主节点发生变化，哨兵会第一时间感知到，并且将新的 Redis 主节点通知给 Client 端 (这里面 Redis 的 Client 一般都实现了订阅功能，订阅 Sentinel 发布的节点变动消息)。\nRedis 哨兵架构搭建 1、复制一份 sentinel.conf 文件 cp sentinel.conf sentinel-26379.conf 2、将相关配置修改为如下值： port 26379 daemonize yes pidfile \"/var/run/redis-sentinel-26379.pid\" logfile \"26379.log\" dir \"/usr/local/redis-5.0.3/data\" # sentinel monitor \u003cmaster-redis-name\u003e \u003cmaster-redis-ip\u003e \u003cmaster-redis-port\u003e \u003cquorum\u003e # quorum 是一个数字，指明当有多少个 sentinel 认为一个 master 失效时(值一般为：sentinel总数/2 + 1)，master 才算真正失效 sentinel monitor mymaster 192.168.0.60 6379 2 # mymaster 这个名字随便取，客户端访问时会用到 3、启动 Sentinel 哨兵实例 src/redis-sentinel sentinel-26379.conf 4、查看 Sentinel 的 info 信息 src/redis-cli -p 26379 127.0.0.1:26379\u003einfo 可以看到 Sentinel 的 info 里已经识别出了 Redis 的主从 5、可以再配置两个 Sentinel，端口 26380 和 26381，注意上述配置文件里的对应数字都要修改 Sentinel 集群都启动完毕后，会将哨兵集群的元数据信息写入所有 Sentinel 的配置文件里去(追加在文件的最下面)，查看下如下配置文件 sentinel-26379.conf，如下所示：\nsentinel known-replica mymaster 192.168.0.60 6380 # 代表 Redis 主节点的从节点信息 sentinel known-replica mymaster 192.168.0.60 6381 # 代表 Redis 主节点的从节点信息 sentinel known-sentinel mymaster 192.168.0.60 26380 52d0a5d70c1f90475b4fc03b6ce7c3c56935760f # 代表感知到的其它哨兵节点 sentinel known-sentinel mymaster 192.168.0.60 26381 e9f530d3882f8043f76ebb8e1686438ba8bd5ca6 # 代表感知到的其它哨兵节点 当 Redis 主节点如果挂了，哨兵集群会重新选举出新的 Redis 主节点，同时会修改所有 Sentinel 节点配置文件的集群元数据信息，比如 6379 的 Redis 如果挂了，假设选举出的新主节点是 6380，则 Sentinel 文件里的集群元数据信息会变成如下所示：\nsentinel known-replica mymaster 192.168.0.60 6379 # 代表主节点的从节点信息 sentinel known-replica mymaster 192.168.0.60 6381 # 代表主节点的从节点信息 sentinel known-sentinel mymaster 192.168.0.60 26380 52d0a5d70c1f90475b4fc03b6ce7c3c56935760f # 代表感知到的其它哨兵节点 sentinel known-sentinel mymaster 192.168.0.60 26381 e9f530d3882f8043f76ebb8e1686438ba8bd5ca6 # 代表感知到的其它哨兵节点 同时还会修改 Sentinel 文件里之前配置的 mymaster 对应的 6379 端口，改为 6380\nsentinel monitor mymaster 192.168.0.60 6380 2 当 6379 的 Redis 实例再次启动时，哨兵集群根据集群元数据信息就可以将 6379 端口的 Redis 节点作为从节点加入集群。","主从同步#主从同步":"CAP 原理 C - Consistent ，一致性 A - Availability ，可用性 P - Partition tolerance ，分区容忍性 分布式系统的节点往往都是分布在不同的机器上进行网络隔离开的，这意味着必然会有网络断开的风险，这个网络断开的场景的专业词汇叫着网络分区。\n在网络分区发生时，两个分布式节点之间无法进行通信，我们对一个节点进行的修改操作将无法同步到另外一个节点，所以数据的一致性将无法满足，因为两个分布式节点的数据不再保持一致。除非我们牺牲可用性，也就是暂停分布式节点服务，在网络分区发生时，不再提供修改数据的功能，直到网络状况完全恢复正常再继续对外提供服务。\n一句话概括 CAP 原理就是——网络分区发生时，一致性和可用性两难全。\nAP 架构 AP 架构是选择了可用性和分区容忍性。\n在 AP 架构中，分布式系统会设计成在网络分区发生时，仍然可以对外提供服务，即使此时数据的一致性无法满足。\nCP 架构 CP 架构是选择了一致性和分区容忍性。\n在 CP 架构中，分布式系统会设计成在网络分区发生时，为了保证数据的一致性，失去联系的节点暂停对外提供服务，直到网络状况完全恢复正常再继续对外提供服务。\nZookeeper 是一个典型的 CP 架构的分布式系统。\n最终一致性 Redis 保证最终一致性，从节点会努力追赶主节点，最终从节点的状态会和主节点的状态将保持一致。如果网络断开了，主从节点的数据将会出现大量不一致，一旦网络恢复，从节点会采用多种策略努力追赶上落后的数据，继续尽力保持和主节点一致。\n主从同步 Redis 支持主从同步和从从同步，从从同步功能是 Redis 后续版本增加的功能，为了减轻主库的同步负担。\nRedis 主从架构搭建 1、复制一份redis.conf文件 2、将相关配置修改为如下值： port 6380 pidfile /var/run/redis_6380.pid # 把 pid 进程号写入 pidfile 配置的文件 logfile \"6380.log\" dir /usr/local/redis-5.0.3/data/6380 # 指定数据存放目录 # 需要注释掉 bind # bind 127.0.0.1（bind 绑定的是自己机器网卡的 ip，如果有多块网卡可以配多个 ip，代表允许客户端通过机器的哪些网卡 ip 去访问，内网一般可以不配置 bind，注释掉即可） 3、配置主从复制 replicaof 192.168.0.60 6379 # 从本机 6379 的 Redis 实例复制数据，Redis 5.0之前使用 slaveof replica-read-only yes # 配置从节点只读 4、启动从节点 redis-server redis.conf # redis.conf 文件务必用你复制并修改了之后的 redis.conf 文件 5、连接从节点 redis-cli -p 6380 6、测试在 6379 实例上写数据，6380 实例是否能及时同步新修改数据 7、可以自己再配置一个 6381 的从节点 主从同步原理 如果你为 master 配置了一个 slave，不管这个 slave 是否是第一次连接上 master，它都会发送一个 PSYNC 命令给 master 请求复制数据。 master 收到 PSYNC 命令后，会在后台进行数据持久化通过 bgsave 生成最新的 rdb 快照文件（这里的 rdb 与开不开启 rdb 持久化没有关系），持久化期间，master 会继续接收客户端的请求，它会把这些可能修改数据集的请求缓存在内存中。 当持久化进行完毕以后，master 会把这份 rdb 文件数据集发送给 slave。 slave 会把接收到的数据进行持久化生成 rdb，然后再加载到内存中。 然后，master 再将之前缓存在内存中的命令发送给 slave。 当 master 与 slave 之间的连接由于某些原因而断开时，slave 能够自动重连 master，如果 master 收到了多个 slave 并发连接请求，它只会进行一次持久化，而不是一个连接一次，然后再把这一份持久化的数据发送给多个并发连接的 slave。 为什么不使用 AOF 来做数据同步？\n因为 RDB 更快。RDB 是内存快照，而 AOF 是增量日志，重放是需要时间的，所以 RDB 更适合做数据同步。\n增量同步 Redis 同步的是指令流，主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存中，然后异步将内存中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一边向主节点反馈自己同步到哪里了 (offset)。\n因为内存的 buffer 是有限的，所以 Redis 主库不能将所有的指令都记录在内存 buffer 中。Redis 的复制内存 buffer 是一个定长的环形数组，如果数组内容满了，就会从头开始覆盖前面的内容。\n部分复制 就是说一个 slave 之前连接了 master，已经有部分数据了，后面又和 master 断开了连接，然后又重新连接上 master，master 会把断开连接期间修改的数据发送给 slave。\nmaster 会在其内存中创建一个复制数据用的缓存队列，缓存最近一段时间的数据，master 和它所有的 slave 都维护了复制的数据下标 offset 和 master 的进程 ID，因此，当网络连接断开重连后，slave 会请求 master 继续进行未完成的复制，从所记录的数据下标开始。如果 master 进程 ID 变化了，或者从节点数据下标 offset 太旧，已经不在 master 的缓存队列里了，那么将会进行一次全量数据的复制。\n从从同步 从从同步是从 Redis 3.0 开始支持的功能，它的出现主要是为了分担主节点的同步压力，在主从同步中，从节点也可以作为其他从节点的主节点，从而形成一个树状结构。为了缓解主从复制风暴 (多个从节点同时复制主节点导致主节点压力过大)。"},"title":"集群"},"/db-learn/docs/redis/practice/08_distributed-lock/":{"data":{"":"分布式锁是用来解决并发问题的。比如一个操作要修改用户的状态，修改状态需要先读出用户的状态，在内存里进行修改，改完了再存回去。如果这样的操作同时进行了，就会出现并发问题，因为读取和保存状态这两个操作不是原子的。\n分布式锁本质上要实现的目标就是在 Redis 里面占一个坑，当别的进程也要来占时，发现已经有人蹲在那里了，就只好放弃或者稍后再试。\n分布式锁一般是使用 SETNX 命令实现，SETNX 命令的作用是设置一个键值对，如果键不存在，则设置成功，返回 1；如果键已经存在，则设置失败，返回 0。释放锁可以使用 DEL 命令删除键值对。\n# 加锁 SETNX lock:order:{id} true # 解锁 DEL lock:order:{id} 仅仅这么设置是不够的，因为如果逻辑执行到中间出现异常了，DEL 没有被调用那么锁就会一直存在，导致其他线程无法获取锁，导致死锁。\n为了避免这种情况，可以使用 EXPIRE 命令设置锁的过期时间，比如 5s，这样即使中间出现异常也可以保证 5 秒之后锁会自动释放：\n# 加锁并设置过期时间 SETNX lock:order:{id} true EXPIRE lock:order:{id} 5 但是这样也会有问题，因为 SETNX 和 EXPIRE 是两个命令，它们不是原子性的，如果 SETNX 成功了，但是 EXPIRE 失败了，那么锁就会一直存在，导致死锁。\n为了避免这种情况，Redis 2.8 版本中加入了 SET 指令的扩展参数，使得 SETNX 和 EXPIRE 可以一起执行。\n# 加锁并设置过期时间 SET lock:order:{id} true EX 5 NX 这样就可以保证 SETNX 和 EXPIRE 是原子性的，要么都执行成功，要么都执行失败。","redis-lua-脚本#Redis Lua 脚本":"Redis在 2.6 推出了脚本功能，允许开发者使用Lua 语言编写脚本传到 Redis 中执行。使用脚本的好处如下:\n减少网络开销：本来 5 次网络请求的操作，可以用一个请求完成，原先 5 次请求的逻辑放在 Redis 服务器上完成。使用脚本，减少了网络往返时延。这点跟管道类似。 原子操作：Redis 会将整个脚本作为一个整体执行，中间不会被其他命令插入。管道不是原子的，不过 Redis 的批量操作命令(类似 mset )是原子的。 替代 Redis 的事务功能：Redis 自带的事务功能很鸡肋，而 Redis 的 lua 脚本几乎实现了常规的事务功能，官方推荐用 Redis lua 替代 Redis 的事务功能。 Redis 2.6 版本开始，通过内置的 Lua 解释器，可以使用 EVAL 命令对 Lua 脚本进行求值。\nEVAL script numkeys key [key ...] arg [arg ...] script 参数是一段 Lua 脚本程序。Redis 使用 EVAL 命令的第一个参数来传递脚本程序。这段脚本不必(也不应该)定义为一个 Lua 函数。 numkeys 参数用于指定键名参数的个数。 键名参数 key [key ...] 从 EVAL 的第三个参数开始算起，表示在脚本中所用到的那些 Redis 键 (key)，这些键名参数可以在 Lua 中通过全局变量 KEYS 数组，用 1 为基址的形式访问( KEYS[1] ， KEYS[2] ，以此类推)。 127.0.0.1:6379\u003e eval \"return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}\" 2 key1 key2 first second 1) \"key1\" 2) \"key2\" 3) \"first\" 4) \"second\" 在 Lua 脚本中，可以使用 redis.call() 函数来执行 Redis 命令：\njedis.set(\"product_stock_10016\", \"15\"); // 初始化商品10016的库存 String script = \" local count = redis.call('get', KEYS[1]) \" + \" local a = tonumber(count) \" + \" local b = tonumber(ARGV[1]) \" + \" if a \u003e= b then \" + \" redis.call('set', KEYS[1], a-b) \" + \" return 1 \" + \" end \" + \" return 0 \"; Object obj = jedis.eval(script, Arrays.asList(\"product_stock_10016\"), Arrays.asList(\"10\")); System.out.println(obj); ℹ️ 不要在 Lua 脚本中出现死循环和耗时的运算，否则 Redis 会阻塞，将不接受其他的命令，所以使用时要注意不能出现死循环、耗时的运算。Redis 是单进程、单线程执行脚本。管道不会阻塞 Redis。 ","redlock#RedLock":"Redis 一般都是集群架构，很少有使用单机部署的。但是分布式锁在集群架构中是存在问题的。\n比如在 Sentinel 集群中，主节点挂掉时，从节点会取而代之，客户端上却并没有明显感知。原先第一个客户端在主节点中申请成功了一把锁，但是这把锁还没有来得及同步到从节点，主节点突然挂掉了。然后从节点变成了主节点，这个新的节点内部没有这个锁，所以当另一个客户端过来请求加锁时，立即就批准了。这样就会导致系统中同样一把锁被两个客户端同时持有，不安全性由此产生。\n为了解决这个问题，Redis 作者 antirez 提出了 RedLock 算法。\n为了使用 Redlock，需要提供多个 Redis 实例，这些实例之前相互独立没有主从关系。同很多分布式算法一样，redlock 也使用大多数机制。\n加锁时，它会向过半节点发送 set(key, value, nx=True, ex=xxx) 指令，只要过半节点 set 成功，那就认为加锁成功。释放锁时，需要向所有节点发送 del 指令。不过 Redlock 算法还需要考虑出错重试、时钟漂移等很多细节问题，同时因为 Redlock 需要向多个节点进行读写，意味着相比单实例 Redis 性能会下降一些。\n但是 RedLock 并不是一个推荐的方案，因为 RedLock 还存在一些问题：\n主从同步：如果主节点还没来得及把锁同步到从节点，主节点就挂掉了，那么这个锁就会丢失。那就又回到了 Redlock 最初要解决的问题上。 当然也可以不部署主从节点，但是如果主节点挂了超过一半的节点，就会导致无法加锁。而且如果持久化机制是设置的每秒执行一次，如果正好在执行持久化时，主节点挂掉了，那么这个锁就会丢失。 如果主节点太多，那么加锁和释放锁的时间就会比较长。 如果非要这种高一致性的锁，那么可以使用 Zookeeper 来实现。","优化#优化":" 分布式锁的粒度要尽量小，不需要被锁住的代码尽量并发执行。 分段锁：比如一个商品（product:10011:stock）有 1000 个库存，那么可以把库存分成 10 段（product:10011:stock1、product:10011:stock2 等等），每一段都有一个锁，这样就可以有 10 个线程并发执行了。 ","可重入锁#可重入锁":"如果一个锁支持同一个线程的多次加锁，那么这个锁就是可重入的。","超时问题#超时问题":"上面的加锁方式，还是有超时问题的。\n假设第一个进程逻辑执行时间执行了 15s，锁的过期时间为 10s，那么第一个 10s 后锁就会自动释放，第二个进程就可以拿到锁。\n如果第二个进程在执行 5s 后还没有结束，这个时候第一个进程的逻辑执行完了，释放了锁。那这个时候第一个进程就会把第二个进程的锁释放了。在高并发场景下，会出现大量的锁被错误释放的情况，也就意味会有大量的进程可以拿到这个锁。\n进程 1 获取锁成功。 进程 1 在某个操作上阻塞了很长时间。 过期时间到了，锁自动释放了。 进程 2 获取到了对应同一个资源的锁。 进程 1 从阻塞中恢复过来，释放掉了进程 2 持有的锁。 解决方案 这个问题的根源就是错误的释放锁。可以 set 的 value 设置为一个随机数或者唯一的 uuid，释放锁时先匹配随机数是否一致，然后再删除 key，这是为了确保当前线程占有的锁不会被其它线程释放，除非这个锁是过期了被服务器自动释放的。\ntag = random.nextint() # 随机数 if redis.set(key, tag, nx=True, ex=5): do_something() redis.delifequals(key, tag) # 假想的 delifequals 指令 上面的方案还有一点小问题，就是 delifequals 指令，这是一个自定义的指令，匹配 value 和删除 key 并不是一个原子操作，还是会有原子性问题。例如在匹配 value 后，还没来得及删除 key，锁就过期了，此时其它线程就可以获取到锁了。然后又执行了删除 key 的操作，这样就会把其它线程的锁给释放了。\nRedis 也没有提供类似于 delifequals 这样的指令，这就需要使用 Lua 脚本来处理了，因为 Lua 脚本可以保证连续多个指令的原子性执行。\n# delifequals if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 这段 Lua 脚本在执行的时候要把前面的 tag 作为 ARGV[1] 的值传进去，把 key 作为 KEYS[1] 的值传进去。\n锁续命（Watchdog）方案 上面的方案，只是相对安全一点，因为如果真的超时了，当前线程的逻辑没有执行完，其它线程也会乘虚而入。\n为了解决这个问题，我们可以在获取锁之后，开启一个守护线程，用来给快要过期的锁“续命”，也就是不断的延长锁的过期时间。现在已经有很成熟的方案，例如 redisson。\nredisson 是一个在 Redis 的基础上提供了许多分布式服务。其中就包含了各种分布式锁的实现。\nredisson 自旋尝试加锁的逻辑，如果加锁失败，会拿到当前锁的剩余时间 ttl，然后让出 CPU 让其它线程执行，等待 ttl 时间后再继续尝试加锁。加锁失败的同时还会去订阅一个 Redis channel，监听锁释放的消息，当锁释放后会收到消息，然后重新尝试加锁。\nGo 实现锁续命 核心设计思路\n后台定时续期：获取锁成功后启动一个 goroutine 定期续期 线程(协程)标识验证：续期时验证锁是否仍由当前协程持有 自动停止机制：锁释放或协程退出时自动停止续期 package redistlock import ( \"context\" \"crypto/rand\" \"encoding/hex\" \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/go-redis/redis/v8\" ) const ( defaultWatchdogInterval = 10 * time.Second // 默认续期间隔 defaultLockTimeout = 30 * time.Second // 默认锁超时时间 ) type DistLock struct { client *redis.Client key string value string // 唯一标识，格式: UUID:goroutineID watchdogActive bool stopWatchdog chan struct{} mutex sync.Mutex } // NewDistLock 创建一个新的分布式锁实例 func NewDistLock(client *redis.Client, key string) *DistLock { return \u0026DistLock{ client: client, key: key, value: generateLockValue(), stopWatchdog: make(chan struct{}), } } // generateLockValue 生成锁的唯一标识值 func generateLockValue() string { // 生成随机 UUID 部分 buf := make([]byte, 16) _, _ = rand.Read(buf) uuid := hex.EncodeToString(buf) // 获取当前 goroutine ID goid := getGoroutineID() return fmt.Sprintf(\"%s:%d\", uuid, goid) } // 获取 goroutine ID (简化实现) func getGoroutineID() uint64 { var buf [64]byte n := runtime.Stack(buf[:], false) idField := strings.Fields(strings.TrimPrefix(string(buf[:n]), \"goroutine \"))[0] id, _ := strconv.ParseUint(idField, 10, 64) return id } // Lock 获取分布式锁 func (dl *DistLock) Lock(ctx context.Context, timeout time.Duration) error { dl.mutex.Lock() defer dl.mutex.Unlock() // 尝试获取锁 acquired, err := dl.client.SetNX(ctx, dl.key, dl.value, defaultLockTimeout).Result() if err != nil { return err } if acquired { // 启动看门狗 dl.startWatchdog(ctx) return nil } // 等待锁释放或超时 if timeout \u003e 0 { expire := time.Now().Add(timeout) ticker := time.NewTicker(100 * time.Millisecond) defer ticker.Stop() for { select { case \u003c-ticker.C: acquired, err := dl.client.SetNX(ctx, dl.key, dl.value, defaultLockTimeout).Result() if err != nil { return err } if acquired { dl.startWatchdog(ctx) return nil } if time.Now().After(expire) { return errors.New(\"lock timeout\") } case \u003c-ctx.Done(): return ctx.Err() } } } return errors.New(\"lock acquisition failed\") } // startWatchdog 启动看门狗续期机制 func (dl *DistLock) startWatchdog(ctx context.Context) { if dl.watchdogActive { return } dl.watchdogActive = true go func() { ticker := time.NewTicker(defaultWatchdogInterval) defer ticker.Stop() for { select { case \u003c-ticker.C: // 续期操作 renewed, err := dl.renewLock(ctx) if err != nil || !renewed { // 续期失败，可能是锁已释放或已失去所有权 dl.mutex.Lock() dl.watchdogActive = false dl.mutex.Unlock() return } case \u003c-dl.stopWatchdog: // 收到停止信号 dl.mutex.Lock() dl.watchdogActive = false dl.mutex.Unlock() return case \u003c-ctx.Done(): // 上下文取消 dl.mutex.Lock() dl.watchdogActive = false dl.mutex.Unlock() return } } }() } // renewLock 续期锁 func (dl *DistLock) renewLock(ctx context.Context) (bool, error) { // 使用 Lua 脚本保证原子性 script := ` if redis.call(\"get\", KEYS[1]) == ARGV[1] then return redis.call(\"pexpire\", KEYS[1], ARGV[2]) else return 0 end ` result, err := dl.client.Eval(ctx, script, []string{dl.key}, dl.value, defaultLockTimeout.Milliseconds()).Result() if err != nil { return false, err } if val, ok := result.(int64); ok { return val == 1, nil } return false, nil } // Unlock 释放锁 func (dl *DistLock) Unlock(ctx context.Context) error { dl.mutex.Lock() defer dl.mutex.Unlock() // 先停止看门狗 if dl.watchdogActive { close(dl.stopWatchdog) dl.watchdogActive = false dl.stopWatchdog = make(chan struct{}) } // 使用 Lua 脚本保证原子性 script := ` if redis.call(\"get\", KEYS[1]) == ARGV[1] then return redis.call(\"del\", KEYS[1]) else return 0 end ` _, err := dl.client.Eval(ctx, script, []string{dl.key}, dl.value).Result() return err } // IsLocked 检查锁是否仍被当前实例持有 func (dl *DistLock) IsLocked(ctx context.Context) (bool, error) { val, err := dl.client.Get(ctx, dl.key).Result() if err == redis.Nil { return false, nil } if err != nil { return false, err } return val == dl.value, nil } "},"title":"分布式锁"},"/db-learn/docs/redis/practice/09_design/":{"data":{"":"","总结#总结":"针对读多写少的情况加入缓存提高性能，如果写多读多的情况又不能容忍缓存数据不一致，那就没必要加缓存了，可以直接操作数据库。当然，如果数据库抗不住压力，还可以把缓存作为数据读写的主存储，异步将数据同步到数据库，数据库只是作为数据的备份。 放入缓存的数据应该是对实时性、一致性要求不是很高的数据。切记不要为了用缓存，同时又要保证绝对的一致性做大量的过度设计和控制，增加系统复杂性。","热点缓存-key-的重建优化#热点缓存 key 的重建优化":"开发人员使用 “缓存+过期时间” 的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。但是有两个问题如果同时出现，可能就会对应用造成致命的危害：\n当前 key 是一个热点 key（例如一个热门的娱乐新闻），并发量非常大。 重建缓存不能在短时间完成，可能是一个复杂计算，例如复杂的 SQL、多次 IO、多个依赖等。 在缓存失效的瞬间，有大量线程来重建缓存，造成后端负载加大，甚至可能会让应用崩溃。\n解决方案 要解决这个问题主要就是要避免大量线程同时重建缓存。可以利用互斥锁，（多个服务实例，使用分布式锁）来解决，此方法只允许一个线程重建缓存，其他线程等待重建缓存的线程执行完，重新从缓存获取数据即可。\nString get(String key) { // 从 Redis 中获取数据 String value = redis.get(key); // 如果 value 为空， 则开始重构缓存 if (value == null) { // 只允许一个线程重建缓存， 使用 nx， 并设置过期时间 ex String mutexKey = \"mutex:key:\" + key; if (redis.set(mutexKey, \"1\", \"ex 180\", \"nx\")) { // 从数据源获取数据 value = db.get(key); // 回写 Redis， 并设置过期时间 redis.setex(key, timeout, value); // 删除 key_mutex redis.delete(mutexKey); } else { // 其他线程休息 50 毫秒后重试，重试时缓存中已经有值了 Thread.sleep(50); get(key); } } return value; } 对于不同的商品，可以使用不同的 key， 避免不同的商品竞争同一把锁，提高并发度。\nℹ️ 这里要注意，如果使用了缓存空对象来解决缓存穿透的问题，那么这里在判断缓存为空的时候，要区分一下是真的不存在，还是缓存的空对象，如果是缓存的空对象，就不需要去重建缓存了，因为数据库里也没有。 ","简单的冷热分离实现#简单的冷热分离实现":"比如一个电商网站，商品可能会有很多，但是真正热门的，每天都有人访问的商品可能不足 1%，对于这种热门的商品，可以延长其缓存的有效期，这样可以减少数据库的访问次数，提高系统的性能。\n示例代码：\nvar ( ValidityDuration = 24 * time.Hour // 缓存有效期 ) func GetProduct(id int) (*Product, error) { // 从缓存中获取商品信息 product, err := GetProductFromCache(id) if err == nil { // 如果缓存中存在商品信息，延长有效期 UpdateProductExpireTime(id, ValidityDuration) return product, nil } // 从数据库中获取商品信息 product, err = GetProductFromDB(id) if err != nil { return nil, err } // 将商品信息存入缓存，有效期为 24 小时 SetProductToCache(product, ValidityDuration) return product, nil } 上面的代码中，只要商品被访问过，并且在缓存中，那么就会延长其有效期，这样可以保证热门的商品一直存在于缓存中。","缓存与数据库双写不一致#缓存与数据库双写不一致":"在大并发下，同时操作数据库与缓存会存在数据不一致性问题：\n上图线程 1 先执行了更新数据库的操作，但是卡了一会还没来得及更新缓存，然后线程 2 也执行了更新数据库的操作并且更新的缓存，最后线程 1 更新了缓存。\n最后数据库中的 stock=6 而缓存中的 stock=10。这就是缓存与数据库的数据不一致问题。\n有些业务实现，在写完数据库之后可能不会去更新缓存，而是删除缓存，在查询数据库的时候再去更新缓存。这种方式也有一样的问题：\n图中，线程 1 写入数据库 stock=10 并删除缓存，然后线程 3 查询数据缓存为空，接着查询数据库得到 stock=10，这个时候如果在线程 3 更新缓存之前，线程 2 写入数据库 stock=6 并删除缓存，最后线程 3 写入缓存 stock=10。一样的缓存与数据库的数据不一致问题。\n解决方案 问题主要是出在了查询数据库和更新缓存之间，在高并发的场景下，可能会出现别的线程更新数据库的操作。\n直接使用分布式锁就能解决这种问题。\n对于并发几率很小的数据(如个人维度的订单数据、用户数据等)，这种几乎不用考虑这个问题，很少会发生缓存不一致，可以给缓存数据加上过期时间，每隔一段时间触发读的主动更新即可。 就算并发很高，如果业务上能容忍短时间的缓存数据不一致(如商品名称，商品分类菜单等)，缓存加上过期时间依然可以解决大部分业务对于缓存的要求。 如果不能容忍缓存数据不一致，可以通过加分布式读写锁来保证并发读写或写写的时候按顺序排好队，读读的时候相当于无锁。 也可以用阿里开源的 canal 通过监听数据库的 binlog 日志及时的去修改缓存，但是引入了新的中间件，增加了系统的复杂度。 ","缓存失效击穿#缓存失效（击穿）":"缓存失效（击穿）是指由于大批量缓存在同一时间失效可能导致大量请求同时穿透缓存直达数据库，可能会造成数据库瞬间压力过大甚至挂掉。例如电商系统中，如果有一大批商品同时上架，这批商品的缓存数据可能会在同一时间失效。\n解决方案 可以在批量增加缓存时，对于这一批数据中的每一个 key 的缓存过期时间都增加为一个随机的值，这样的话，每个 key 的过期时间都不同，从而避免了大量缓存同时失效的问题。\nString get(String key) { // 从缓存中获取数据 String cacheValue = cache.get(key); // 缓存为空 if (StringUtils.isBlank(cacheValue)) { // 从存储中获取 String storageValue = storage.get(key); cache.set(key, storageValue); // 设置一个过期时间(300到600之间的一个随机数) int expireTime = new Random().nextInt(300) + 300; if (storageValue == null) { cache.expire(key, expireTime); } return storageValue; } else { // 缓存非空 return cacheValue; } } ","缓存穿透#缓存穿透":"缓存穿透是指查询一个根本不存在的数据（缓存击穿的区别就在于数据至少在数据库中还是存在的，击穿只是击穿了缓存层，穿透是整个后端都被穿透了），缓存层和存储层都不会命中，通常出于容错的考虑，如果从存储层查不到数据则不写入缓存层。\n缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义。\n造成缓存穿透的基本原因有两个：\n自身业务代码或者数据出现问题。 一些恶意攻击、 爬虫等造成大量空命中。 解决方案 缓存空对象 如果一个查询返回的数据为空（不管是数据是否不存在），仍然把这个空结果（null）进行缓存，并且设置一个过期时间。\nString get(String key) { // 从缓存中获取数据 String cacheValue = cache.get(key); // 缓存为空 if (StringUtils.isBlank(cacheValue)) { // 从存储中获取 String storageValue = storage.get(key); cache.set(key, storageValue); // 如果存储数据为空， 需要设置一个过期时间 300s if (storageValue == null) { cache.expire(key, 60 * 5); } return storageValue; } else { // 缓存非空 return cacheValue; } } 如果是被恶意攻击，每次攻击可能都会换不一样的 key，如果缓存中存储上百万个空值，占用了大量的内存空间。可以为空值缓存设置一个短的过期时间。对于空值缓存，也需要设置延期，避免同一个空值的 key 被不停的访问。\n布隆过滤器 对于恶意攻击，向服务器请求大量不存在的数据造成的缓存穿透，还可以用布隆过滤器先做一次过滤，对于不存在的数据布隆过滤器一般都能够过滤掉，不让请求再往后端发送。当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在。\n布隆过滤器就是一个大型的位数组和一组的无偏 hash 函数。所谓无偏就是能够把元素的 hash 值算得比较均匀。\n向布隆过滤器中添加 key 时，会使用多个 hash 函数对 key 进行 hash 算得一个整数索引值然后对位数组长度进行取模运算得到一个位置，每个 hash 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 1 就完成了 add 操作。\n向布隆过滤器询问 key 是否存在时，跟 add 一样，也会把 hash 的几个位置都算出来，看看位数组中这几个位置是否都为 1，只要有一个位为 0，那么说明布隆过滤器中这个 key 不存在。如果都是 1，这并不能说明这个 key 就一定存在，只是极有可能存在，因为这些位被置为 1 可能是因为其它的 key 存在所致。如果这个位数组长度比较大，存在概率就会很大，如果这个位数组长度比较小，存在概率就会降低。\n可以用 redisson 实现布隆过滤器：\npackage com.redisson; import org.redisson.Redisson; import org.redisson.api.RBloomFilter; import org.redisson.api.RedissonClient; import org.redisson.config.Config; public class RedissonBloomFilter { public static void main(String[] args) { Config config = new Config(); config.useSingleServer().setAddress(\"redis://localhost:6379\"); // 构造 Redisson RedissonClient redisson = Redisson.create(config); RBloomFilter\u003cString\u003e bloomFilter = redisson.getBloomFilter(\"nameList\"); // 初始化布隆过滤器：预计元素为 100000000L,误差率为 3%,根据这两个参数会计算出底层的 bit 数组大小 bloomFilter.tryInit(100000000L,0.03); //将 zhuge 插入到布隆过滤器中 bloomFilter.add(\"zhuge\"); // 判断下面号码是否在布隆过滤器中 System.out.println(bloomFilter.contains(\"guojia\"));//false System.out.println(bloomFilter.contains(\"baiqi\"));//false System.out.println(bloomFilter.contains(\"zhuge\"));//true } } 使用布隆过滤器需要把所有数据提前放入布隆过滤器，并且在增加数据时也要往布隆过滤器里放，布隆过滤器缓存过滤伪代码：\n//初始化布隆过滤器 RBloomFilter\u003cString\u003e bloomFilter = redisson.getBloomFilter(\"nameList\"); //初始化布隆过滤器：预计元素为100000000L,误差率为3% bloomFilter.tryInit(100000000L,0.03); //把所有数据存入布隆过滤器 void init(){ for (String key: keys) { bloomFilter.put(key); } } String get(String key) { // 从布隆过滤器这一级缓存判断下 key 是否存在 Boolean exist = bloomFilter.contains(key); if(!exist){ return \"\"; } // 从缓存中获取数据 String cacheValue = cache.get(key); // 缓存为空 if (StringUtils.isBlank(cacheValue)) { // 从存储中获取 String storageValue = storage.get(key); cache.set(key, storageValue); // 如果存储数据为空， 需要设置一个过期时间(300秒) if (storageValue == null) { cache.expire(key, 60 * 5); } return storageValue; } else { // 缓存非空 return cacheValue; } } ℹ️ 这种方法适用于数据命中不高、数据相对固定（因为添加删除元素需要重建）、实时性低（通常是数据集较大）的应用场景，代码维护较为复杂，但是缓存空间占用很少。 ","缓存雪崩#缓存雪崩":"缓存雪崩指的是缓存层支撑不住或宕掉后，流量会像奔逃的野牛一样，打向后端存储层。\n由于缓存层承载着大量请求，有效地保护了存储层，但是如果缓存层由于某些原因不能提供服务(比如超大并发过来，缓存层支撑不住，或者由于缓存设计不好，类似大量请求访问 bigkey，导致缓存能支撑的并发急剧下降)，于是大量请求都会打到存储层，存储层的调用量会暴增，造成存储层也会级联宕机的情况。\n解决方案 预防和解决缓存雪崩问题， 可以从以下三个方面进行着手：\n保证缓存层服务高可用性，比如使用 Redis Sentinel 或 Redis Cluster。 依赖隔离组件为后端限流熔断并降级。比如使用 Sentinel 或 Hystrix 限流降级组件。比如服务降级，我们可以针对不同的数据采取不同的处理方式。当业务应用访问的是非核心数据（例如电商商品属性，用户信息等）时，暂时停止从缓存中查询这些数据，而是直接返回预定义的默认降级信息、空值或是错误提示信息；当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。 多级缓存，进程内存 -\u003e Redis -\u003e 数据库。对于进程内存缓存，可以使用一些轻量级的缓存组件，比如 Google 的 Guava Cache 或者 Caffeine，这些组件都实现了进程内缓存，并且支持多种缓存过期策略。可以避免内存泄露的问题。多级缓存架构也会有数据不一致的问题，可以通过异步的方式来更新缓存。不过一点点的不一致是可以接受的，没有必要继续增加系统的复杂性。真正实践中会有一个独立的 HotKey 监测系统来监控热点 key 的，然后将热点 key 加入到多级缓存中。 提前演练。 在项目上线前，演练缓存层宕掉后，应用以及后端的负载情况以及可能出现的问题，在此基础上做一些预案设定。 "},"title":"缓存设计"},"/db-learn/docs/redis/practice/10_hotkey/":{"data":{"":"","多级缓存#多级缓存":"一般多级缓存分为：\n本地缓存 远程缓存 本地缓存的优势：\n可以减少网络请求，提高性能。 减少远程缓存的压力。 本地缓存的缺点：\n进程空间的大小有限，不能存储大量的数据。 进程重启后，本地缓存会丢失。 分布式场景下，本地缓存会出现数据不一致的问题。 和远程缓存的一致性问题。 对于数据不一致的问题，其实只要保证最终一致性即可。缩短本地缓存的过期时间，根据业务能够接受不一致的时间来设置，比如 10s 或者更短的过期时间。\n多级缓存的使用场景 热点的商品详情页 热搜 热门帖子 热门用户主页 一般都是在高并发的场景下使用。\n热点产生的条件：\n有限时间 流量高聚 在互联网领域，热点被分为 2 类：\n有预期的热点：比如在电商活动中退出的爆款联名限量款商品，又或者是秒杀会场活动等。 无预期的热点：比如受到了黑客的恶意攻击，网络爬虫的频繁访问，又或者突发新闻带来的流量冲击等。 对于有预期的热点，我们可以通过提前预热，提前把数据加载到缓存中，或者提前扩容，降级等方式来解决。\n对于无预期的热点，就需要热点探测系统来探测热点，在热点还没有爆火之前探测出来，提前把数据加载到缓存中，进行扩容等。\n热点探测使用场景 MySQL 中被频繁访问的数据 ，如热门商品的主键 id。 Redis 缓存中被密集访问的 Key，如热门商品的详情需要 get goods$id。 恶意攻击或机器人爬虫的请求信息，如特定标识的 userId、机器 IP。 频繁被访问的接口地址，如获取用户信息接口 /userInfo/ + userId。 使用热点探测的好处 提升性能，规避风险。\n对于无预期的热数据（即突发场景下形成的热 Key），可能会对业务系统带来极大的风险，可将风险分为两个层次：\n对数据层的风险 正常情况下，Redis 缓存单机就可支持十万左右 QPS，并能通过集群部署提高整体负载能力。对于并发量一般的系统，用 Redis 做缓存就足够了。但是对于瞬时过高并发的请求，因为 Redis 单线程原因会导致正常请求排队，或者因为热点集中导致分片集群压力过载而瘫痪，从而击穿到 DB 引起服务器雪崩。\n对应用服务的风险 每个应用在单位时间所能接受和处理的请求量是有限的，如果受到恶意请求的攻击，让恶意用户独自占用了大量请求处理资源，就会导致其他正常用户的请求无法及时响应。\n因此，需要一套动态热 Key 检测机制，通过对需要检测的热 Key 规则进行配置，实时监听统计热 Key 数据，当无预期的热点数据出现时，第一时间发现他，并针对这些数据进行特殊处理。如本地缓存、拒绝恶意用户、接口限流/降级等\n如何实现热点探测 热点产生的条件是 2 个：一个时间，一个流量。那么根据这个条件可以简单定义一个规则：比如 1 秒内访问 1000 次的数据算是热数据，当然这个数据需要根据具体的业务场景和过往数据进行具体评估。\n对于单机应用，检测热数据很简单，直接在本地为每个 Key 创建一个滑动窗口计数器，统计单位时间内的访问总数（频率），并通过一个集合存放检测到的热 Key。\n对于分布式应用，对热 Key 的访问是分散在不同的机器上的，无法在本地独立地进行计算，因此，需要一个独立的、集中的热 Key 计算单元。\n可以分为五个步骤：\n热点规则：配置热 Key 的上报规则，圈出需要重点监测的 Key。 热点上报：应用服务将自己的热 Key 访问情况上报给集中计算单元。 热点统计：收集各应用实例上报的信息，使用滑动窗口算法计算 Key 的热度。 热点推送：当 Key 的热度达到设定值时，推送热 Key 信息至所有应用实例。 热点缓存：各应用实例收到热 Key 信息后，对 Key 值进行本地缓存。 单机应用示例 public class HotKeyDetector { private final int WINDOW_SIZE = 10; // 滑动窗口大小 private final int THRESHOLD = 5; // 阈值，达到该条件时即判定为热 Key private final Cache\u003cString, Obejct\u003e hotCache = CacheBuilder.newBuilder() // 本地缓存 .expireAfterWrite(5, TimeUnit.SECONDS) .maximumSize(1000) // 缓存最大容量 .build(); private Map\u003cString, Queue\u003cLong\u003e\u003e window = new HashMap\u003c\u003e(); // 滑动窗口 private Map\u003cString, Integer\u003e counts = new HashMap\u003c\u003e(); // 用来计数，用来和阈值比较 // 判断是否为热 Key public boolean isHotKey(String data) { // 如果缓存中有数据，说明已经是 hot key，直接返回 true if (hotCache.getIfPresent(data) != null) { return true; } // 获取当前数据在计数器中的统计次数 int count = counts.getOrDefault(data, 0); // 如果次数大于阈值，说明是热 Key，将数据加入本地缓存，清空队列并返回 true if (count \u003e THRESHOLD) { hotCache.put(data, data); // 加入本地缓存 clear(data) // 清空滑动窗口中相应的队列 return true; } else { // 如果次数小于阈值，说明不是热 Key，将数据加入滑动窗口，并返回 false counts.put(data, count + 1); // 次数加 1 // 获取对应数据的时间队列 Queue\u003cLong\u003e queue = window.get(data); // 如果队列不存在，就创建一个新的队列 if (queue == null) { queue = new LinkedList\u003cLong\u003e(); window.put(data, queue); } // 获取当前时间（秒） long currentTime = System.currentTimeMillis() / 1000; queue.add(currentTime); // 将当前时间加入队列，用于后面数据滑动窗口的统计 // 如果队列中数据的时间超过了滑动窗口的时间区间，则将该时间从队列中移除 while (!queue.isEmpty() \u0026\u0026 currentTime - queue.peek() \u003e WINDOW_SIZE) { queue.poll(); // 移除队列头部的时间 counts.put(data, counts.get(data) - 1); // 统计次数减 1 } return false; // 不是热 Key，返回 false } } // 清除指定数据的队列和计数 public void clear(String data) { window.remove(data); // 移除指定数据的队列 counts.remove(data); // 移除指定数据的计数 } // 添加数据到本地缓存 public void set(String key, Object value) { hotCache.put(key, value); // 将数据加入本地缓存 } // 从本地缓存中获取数据 public Object get(String key) { return hotCache.getIfPresent(key); // 从本地缓存中获取数据 } } 上面并没有考虑并发安全的问题，只是简单的示例。\n滑动窗口示例：\ntype TimeWindow struct { requests []int64 // 存储请求时间戳 size int // 窗口大小（请求次数限制） duration int64 // 窗口时间范围（纳秒） } func NewTimeWindow(size int, duration time.Duration) *TimeWindow { return \u0026TimeWindow{ requests: make([]int64, 0, size), size: size, duration: duration.Nanoseconds(), } } // 检查是否允许通过 func (tw *TimeWindow) Allow() bool { now := time.Now().UnixNano() // 移除过期请求 for len(tw.requests) \u003e 0 \u0026\u0026 now-tw.requests[0] \u003e tw.duration { tw.requests = tw.requests[1:] } // 检查是否超过限制 if len(tw.requests) \u003e= tw.size { return false } // 记录当前请求 tw.requests = append(tw.requests, now) return true } 分布式应用 JD-hotkey。 https://my.oschina.net/1Gk2fdm43/blog/4331985。 该框架主要由 4 个部分组成：\netcd 集群 etcd 作为一个高性能的配置中心，可以以极小的资源占用，提供高效的监听订阅服务。主要用于存放规则配置，各 worker 的 ip 地址，以及探测出的热 key、手工添加的热 key 等。\nclient 端 jar 包 就是在服务中添加的引用 jar，引入后，就可以以便捷的方式去判断某 key 是否热 key。同时，该 jar 完成了 key 上报、监听 etcd 里的 rule 变化、worker 信息变化、热 key 变化，对热 key 进行本地 caffeine 缓存等。\nworker 端集群 worker 端是一个独立部署的 Java 程序，启动后会连接 etcd，并定期上报自己的 ip 信息，供 client 端获取地址并进行长连接。之后，主要就是对各个 client 发来的待测 key 进行累加计算，当达到 etcd 里设定的 rule 阈值后，将热 key 推送到各个 client。\ndashboard 控制台 控制台是一个带可视化界面的 Java 程序，也是连接到 etcd，之后在控制台设置各个 APP 的 key 规则，譬如 2 秒出现 20 次算热 key。然后当 worker 探测出来热 key 后，会将 key 发往 etcd，dashboard 也会监听热 key 信息，进行入库保存记录。同时，dashboard 也可以手工添加、删除热 key，供各个 client 端监听。\n上图中的第一步，其实是不需要的，因为这里是热点探测系统主动将热 Key 推送给应用实例，不需要应用实例去拉取。\n写操作通过 MQ 或者长连接的方式将数据推送给热点探测系统。"},"title":"热点缓存探测系统"},"/db-learn/docs/redis/practice/11_specifications/":{"data":{"":"","命令使用#命令使用":" O(N) 命令关注 N 的数量 例如 hgetall、lrange、smembers、zrange、sinter 等并非不能使用，但是需要明确 N 的值。有遍历的需求可以使用 hscan、sscan、zscan 代替。\n禁用命令 禁止线上使用 keys、flushall、flushdb 等，通过 Redis 的 rename 机制禁掉命令，或者使用 scan 的方式渐进式处理。\n合理使用 select Redis 的多数据库较弱，使用数字进行区分，很多客户端支持较差，同时多业务用多数据库实际还是单线程处理，会有干扰。\n使用批量操作提高效率 原生命令：例如 mget、mset。 非原生命令：可以使用 pipeline 提高效率。\n但要注意控制一次批量操作的元素个数(例如 500 以内，实际也和元素字节数有关)。 注意两者不同：\n原生命令是原子操作，pipeline 是非原子操作。 pipeline 可以打包不同的命令，原生命令做不到 pipeline 需要客户端和服务端同时支持。 Redis 事务功能较弱，不建议过多使用，可以用 lua 替代。 ","客户端使用#客户端使用":" 避免多个应用使用一个 Redis 实例。不相干的业务拆分，公共数据做服务化。 使用带有连接池的数据库，可以有效控制连接，同时提高效率。 高并发下建议客户端添加熔断功能(例如 sentinel、hystrix)。 设置合理的密码，如有必要可以使用 SSL 加密访问。 连接池 使用带有连接池，可以有效控制连接，同时提高效率，标准使用方式：\nJedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMaxTotal(5); jedisPoolConfig.setMaxIdle(2); jedisPoolConfig.setTestOnBorrow(true); JedisPool jedisPool = new JedisPool(jedisPoolConfig, \"192.168.0.60\", 6379, 3000, null); Jedis jedis = null; try { jedis = jedisPool.getResource(); //具体的命令 jedis.executeCommand() } catch (Exception e) { logger.error(\"op key {} error: \" + e.getMessage(), key, e); } finally { //注意这里不是关闭连接，在JedisPool模式下，Jedis会被归还给资源池。 if (jedis != null) jedis.close(); } 连接池参数含义：\nmaxTotal：最大连接数，早期的版本叫 maxActive。设置该值，需要考虑的因素 业务期望的 QPS 客户端执行命令时间 Redis 资源：例如 nodes(例如应用个数) * maxTotal 是不能超过 Redis 的最大连接数 maxclients。 资源开销：例如虽然希望控制空闲连接(连接池此刻可马上使用的连接)，但是不希望因为连接池的频繁释放创建连接造成不必靠开销。 假设: 一次命令时间（borrow|return resource + Jedis 执行命令(含网络) ）的平均耗时约为 1ms，一个连接的 QPS 大约是 1000。业务期望的 QPS 是 50000。那么理论上需要的资源池大小是 50000 / 1000 = 50 个。但事实上这是个理论值，还要考虑到要比理论值预留一些资源，通常来讲 maxTotal 可以比理论值大一些。但这个值不是越大越好，一方面连接太多占用客户端和服务端资源，另一方面对于 Redis 这种高 QPS 的服务器，一个大命令的阻塞即使设置再大资源池仍然会无济于事。 maxIdle 和 minIdle：maxIdle 实际上才是业务需要的最大连接数，maxTotal 是为了给出余量，所以 maxIdle 不要设置过小，否则会有 new Jedis (新连接)开销。连接池的最佳性能是 maxTotal = maxIdle。这样就避免连接池伸缩带来的性能干扰。但是如果并发量不大或者 maxTotal 设置过高，会导致不必要的连接资源浪费。一般推荐 maxIdle 可以设置为按业务期望 QPS 计算出来的理论连接数，maxTotal 可以再放大一倍。 minIdle：minIdle（最小空闲连接数），与其说是最小空闲连接数，不如说是\"至少需要保持的空闲连接数\"，在使用连接的过程中，如果连接数超过了 minIdle，那么继续建立连接，如果超过了 maxIdle，当超过的连接执行完业务后会慢慢被移出连接池释放掉。 testOnBorrow：在borrow一个 jedis 实例时，是否提前进行 validate 操作；如果为 true，则得到的 jedis 实例均是可用的； testOnReturn：在 return 一个 jedis 实例时，是否提前进行 validate 操作；如果为 true，则返回的 jedis 实例均是可用的。 连接池预热 Redis 初始化后是没有连接的，当需要使用连接时，才会创建连接。\n连接池预热在应用启动时，就创建好一定数量的连接，避免在使用时创建连接。\nList\u003cJedis\u003e minIdleJedisList = new ArrayList\u003cJedis\u003e(jedisPoolConfig.getMinIdle()); for (int i = 0; i \u003c jedisPoolConfig.getMinIdle(); i++) { Jedis jedis = null; try { jedis = pool.getResource(); minIdleJedisList.add(jedis); jedis.ping(); } catch (Exception e) { logger.error(e.getMessage(), e); } finally { // 注意，这里不能马上close将连接还回连接池，否则最后连接池里只会建立 1 个连接。。 // jedis.close(); } } // 统一将预热的连接还回连接池 for (int i = 0; i \u003c jedisPoolConfig.getMinIdle(); i++) { Jedis jedis = null; try { jedis = minIdleJedisList.get(i); //将连接归还回连接池 jedis.close(); } catch (Exception e) { logger.error(e.getMessage(), e); } finally { } } ","键值设计#键值设计":"key 名设计 可读性和可管理性 以业务名(或数据库名)为前缀(防止 key 冲突)，用冒号分隔，比如 业务名:表名:id：trade:order:1。\n简洁性 保证语义的前提下，控制 key 的长度，当 key 较多时，内存占用也不容忽视，例如：\nuser:{uid}:friends:messages:{mid} 简化为 u:{uid}:fr:m:{mid}。 不要包含特殊字符 反例：包含空格、换行、单双引号以及其他转义字符。\nvalue 设计 拒绝 bigkey 在 Redis 中，一个字符串最大 512MB，一个二级数据结构（例如 hash、list、set、zset）可以存储大约 40 亿个 (2^32-1) 个元素，但实际中如果下面两种情况，我就会认为它是 bigkey。\n字符串类型：它的 big 体现在单个 value 值很大，一般认为超过 10KB 就是 bigkey。 非字符串类型：哈希、列表、集合、有序集合，它们的 big 体现在元素个数太多。 一般来说，string 类型控制在 10KB 以内，hash、list、set、zset 元素个数不要超过5000。\nℹ️ 非字符串的 bigkey，不要使用 del 删除，使用 hscan、sscan、zscan 方式渐进式删除，同时要注意防止 bigkey 过期时间自动删除问题（例如一个 200 万的 zset 设置 1 小时过期，会触发 del 操作，造成阻塞）。 bigkey 的危害 导致 Redis 阻塞。 网络拥塞。bigkey 也就意味着每次获取要产生的网络流量较大，假设一个 bigkey 为 1MB，客户端每秒访问量为 1000，那么每秒产生 1000MB 的流量，对于普通的千兆网卡(按照字节算是 128MB/s)的服务器来说简直是灭顶之灾，而且一般服务器会采用单机多实例的方式来部署，也就是说一个 bigkey 可能会对其他实例也造成影响，其后果不堪设想。 过期删除。有个 bigkey，它安分守己（只执行简单的命令，例如 hget、lpop、zscore 等），但它设置了过期时间，当它过期后，会被删除，如果没有使用 Redis 4.0 的过期异步删除(lazyfree-lazy-expire yes)，就会存在阻塞 Redis 的可能性。 bigkey 的产生 一般来说，bigkey 的产生都是由于程序设计不当，或者对于数据规模预料不清楚造成的，来看几个例子：\n社交类：粉丝列表，如果某些明星或者大 v 不精心设计下，必是 bigkey。 统计类：例如按天存储某项功能或者网站的用户集合，除非没几个人用，否则必是 bigkey。 缓存类：将数据从数据库 load 出来序列化放到 Redis 里，这个方式非常常用，但有两个地方需要注意，第一，是不是有必要把所有字段都缓存；第二，有没有相关关联的数据，有的同学为了图方便把相关数据都存一个 key 下，产生 bigkey。 如何优化 bigkey 拆 big list： list1、list2、…listN big hash：可以将数据分段存储，比如一个大的 key，假设存了 1 百万的用户数据，可以拆分成 200 个 key，每个 key 下面存放 5000 个用户数据\n如果 bigkey 不可避免，也要思考一下要不要每次把所有元素都取出来 (例如有时候仅仅需要 hmget，而不是 hgetall)，删除也是一样，尽量使用优雅的方式来处理。 选择适合的数据类型 例如：实体类型(要合理控制和使用数据结构内存编码优化配置,例如 ziplist，但也要注意节省内存和性能之间的平衡)，设置合理的过期时间。\n反例：\nset user:1:name tom set user:1:age 19 set user:1:favor football 正例:\nhmset user:1 name tom age 19 favor football 控制 key 的生命周期 使用 expire 设置过期时间(条件允许可以打散过期时间，防止集中过期)。"},"title":"开发规范"},"/db-learn/docs/redis/practice/12_protect_redis/":{"data":{"":"","lua-脚本安全#Lua 脚本安全":"禁止 Lua 脚本由用户输入的内容 (UGC) 生成，避免黑客利用以植入恶意的攻击代码来得到 Redis 的主机权限。Redis 应该以普通用户的身份启动。","ssl-代理#SSL 代理":"Redis 并不支持 SSL 链接，意味着客户端和服务器之间交互的数据不应该直接暴露在公网上传输，否则会有被窃听的风险。如果必须要用在公网上，可 以考虑使用 SSL 代理。\nRedis 官方推荐使用 spiped 工具，可能是因为 spiped 的功能相对比较单一，使用也比较 简单，易于理解。\nspiped 原理 spiped 会在客户端和服务器各启动一个 spiped 进程。\nspiped 进程 A 负责接受来自 Redis Client 发送过来的请求数据，加密后传送到右边的 spiped 进程 B。spiped B 将接收到的数据解密后传 递到 Redis Server。然后 Redis Server 再走一个反向的流程将响应回复给 Redis Client。","指令安全#指令安全":"Redis 有一些非常危险的指令。比如 keys 指令会导致 Redis 卡顿，flushdb 和 flushall 会让 Redis 的所有数据全部清空。 如何避免人为操作失误导致这些灾难性的后果也是运维人员特别需要注意的风险点之一。\nRedis 在配置文件中提供了 rename-command 指令用于将某些危险的指令修改成特别的名称，用来避免人为误操作。比如在配置文 件的 security 块增加下面的内容:\nrename-command keys abckeysabc 如果还想执行 keys 方法，需要键入 abckeysabc。如果想完全封杀某条指令，将指令 rename 成空串，就无法通过任何字符串指令来执行这 条指令了：\nrename-command flushall \"\" ","端口安全#端口安全":"Redis 默认会监听 *:6379，Redis 的服务地址一旦可以被外网直接访问，黑客可以通过 Redis 执行 Lua 脚本拿到服务器权限。\n所以，务必在 Redis 的配置文件中指定监听的 IP 地址。更进一步，还可以增加 Redis 的密码访问限制，客户端必须使用 auth 指令传入正 确的密码才可以访问 Redis，这样即使地址暴露出去了，普通黑客也无法对 Redis 进行任何指令操作。"},"title":"保护 Redis"},"/db-learn/docs/redis/practice/13_slowlog/":{"data":{"":"Redis 的慢查询日志功能用于记录执行时间超过给定时长的命令请求，可以通过这个功能产生的日志来监视和优化查询速度。\n服务器配置有两个和慢查询日志相关的选项：\nslowlog-log-slower-than 选项指定执行时间超过多少微秒（1 秒等于 1,000,000 微秒）的命令请求会被记录到日志上。 slowlog-max-len 选项指定服务器最多保存多少条慢查询日志。当服务器储存的慢查询日志数量等于 slowlog-max-len 选项的值时，服务器在添加一条新的慢查询日志之前， 会先将最旧的一条慢查询日志删除。 ","查看慢查询日志#查看慢查询日志":"使用 SLOWLOG GET 命令查看服务器所保存的慢查询日志：\nredis\u003e SLOWLOG GET 1) 1) (integer) 4 # 日志的唯一标识符（uid） 2) (integer) 1378781447 # 命令执行时的 UNIX 时间戳 3) (integer) 13 # 命令执行的时长，以微秒计算 4) 1) \"SET\" # 命令以及命令参数 2) \"database\" 3) \"Redis\" "},"title":"慢查询日志"},"/db-learn/docs/redis/practice/14_funnel/":{"data":{"":"Redis 漏斗限流（Rate Limiter）是一种常用的限流技术，用于控制对某个资源或服务的访问频率，以防止服务被过度使用或遭受滥用。漏斗限流算法通过模拟水流从一个漏斗中流出，来限制数据的传输速率。在 Redis 中，可以通过使用 Redis 的原子操作和一些数据结构来实现漏斗限流，例如使用 Redis 的 INCR、INCRBY、EXPIRE、SETEX 等命令，或者使用 Redis 的 Lua 脚本来实现更复杂的逻辑。\n漏斗限流的基本原理\n固定容量和速率： 漏斗有一个固定的容量（capacity），表示在单位时间内可以处理的最大请求数。 漏斗还有一个固定的泄漏速率（rate），表示每秒可以从漏斗中泄漏（即允许处理的）请求数。 时间窗口： 通常，漏斗限流是基于一个固定时间窗口（例如1秒）来计算和执行限制的。 ","实现方式#实现方式":"使用 Redis 命令实现 存储请求数：\n使用一个 Redis 键来存储当前时间窗口内的请求数。例如，使用 INCR 命令增加请求数。\nINCR key:requests:user_id 设置时间窗口：\n使用 EXPIRE 命令设置时间窗口的过期时间，例如每秒重置一次。\nEXPIRE key:requests:user_id 1 检查请求数：\n在执行请求前，先检查当前时间窗口内的请求数是否超过了容量限制。\nGET key:requests:user_id 如果请求数大于等于容量，则拒绝请求；否则，继续处理。\n使用 Lua 脚本实现更复杂的逻辑 对于更复杂的漏斗限流逻辑（如动态调整速率），可以使用 Redis 的 Lua 脚本来实现。Lua 脚本可以在服务器端原子地执行多个命令，避免了多命令执行中的竞态条件。\n--[[ 漏斗限流算法核心逻辑： 1. KEYS[1] 限流器唯一标识 2. ARGV[1] rate 每秒允许的请求数 3. ARGV[2] capacity 漏斗总容量 --]] local key = KEYS[1] -- 限流器存储 key，例如: user_123_api_limit local rate = tonumber(ARGV[1]) -- 漏嘴流速（每秒允许请求数） local capacity = tonumber(ARGV[2]) -- 漏斗总容量（最大突发请求数） -- 获取当前漏斗状态 local current = tonumber(redis.call('get', key) or \"0\") -- 当前剩余容量 local timestamp = tonumber(redis.call('get', key .. ':timestamp') or \"0\") -- 上次更新时间戳 local now = tonumber(redis.call('time')[1]) -- 当前 Redis 服务器时间（秒） -- 时间窗口检测（每秒重置一次漏斗） if now \u003e timestamp + 1 then -- 距离上次请求超过 1 秒，重置漏斗 current = capacity -- 恢复漏斗最大容量 timestamp = now -- 记录新时间窗口起点 -- 打印调试信息（生产环境需移除） -- redis.log(redis.LOG_NOTICE, \"漏斗重置 current:\"..current..\" timestamp:\"..timestamp) end -- 计算剩余容量（核心算法） if current \u003e 0 then -- 漏斗有剩余容量时： local allowed = 1 -- 允许本次请求 current = current - 1 -- 消耗 1 个容量单位 -- 更新存储（带1秒过期时间） redis.call('setex', key, 1, current) -- 存储剩余容量 redis.call('setex', key .. ':timestamp', 1, timestamp) -- 存储时间窗口起点 -- 返回允许请求，并返回剩余容量 return {1, current} -- 第一个值1表示允许，第二个值返回剩余容量 else -- 漏斗容量已耗尽 -- 返回拒绝请求，并返回剩余容量 return {0, 0} end ","总结#总结":"Redis 的漏斗限流通过结合原子操作和过期策略，可以有效地限制对资源的访问速率。无论是使用简单的 Redis 命令还是通过 Lua 脚本实现更复杂的逻辑，都可以根据实际需求选择合适的方法来达到限流的目的。"},"title":"漏斗限流"},"/db-learn/docs/redis/practice/19_skills/":{"data":{"":"","info-指令#Info 指令":"Info 指令，可以清晰地知道 Redis 内部一系列运行参数。\nInfo 指令显示的信息非常繁多，分为 9 大块，每个块都有非常多的参数，这 9 个块分别是:\nServer 服务器运行的环境参数 Clients 客户端相关信息 Memory 服务器运行内存统计数据 Persistence 持久化信息 Stats 通用统计数据 Replication 主从复制相关信息 CPU CPU 使用情况 Cluster 集群信息 KeySpace 键值对统计数量信息 Info 可以一次性获取所有的信息，也可以按块取信息。\n# 获取所有信息 \u003e info # 获取内存相关信息 \u003e info memory # 获取复制相关信息 \u003e info replication ","set-多行字符串#set 多行字符串":"如果一个字符串有多行，如何传入 set 指令？使用 -x 选项，该选项会使用标准输入的内容作为最后一个参数。\n$ cat str.txt Ernest Hemingway once wrote, \"The world is a fine place and worth fighting for.\" I agree with the second part. $ redis-cli -x set foo \u003c str.txt OK $ redis-cli get foo \"Ernest Hemingway once wrote,\\n\\\"The world is a fine place and worth fighting for.\\\"\\nI agree with the second part.\\n\" ","扫描大-key#扫描大 KEY":"遇到 Redis 偶然卡顿问题，第一个想到的就是实例中是否存在大 KEY，大 KEY 的内存扩容以及释放都会导致主线程卡顿。--bigkeys 参数可以很快扫出内存里的大 KEY，使用 -i 参数控制扫描间隔，避免扫描指令导致服务器的 ops 陡增报警。\n$ ./redis-cli --bigkeys -i 0.01 # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest zset found so far 'hist:aht:main:async_finish:20180425:17' with 1440 members [00.00%] Biggest zset found so far 'hist:qps:async:authorize:20170311:27' with 2465 members [00.00%] Biggest hash found so far 'job:counters:6ya9ypu6ckcl' with 3 fields [00.01%] Biggest string found so far 'rt:aht:main:device_online:68:{-4}' with 4 bytes [00.01%] Biggest zset found so far 'machine:load:20180709' with 2879 members [00.02%] Biggest string found so far '6y6fze8kj7cy:{-7}' with 90 bytes redis-cli 对于每一种对象类型都会记录长度最大的 KEY，对于每一种对象类型，刷新一次最高记录就会立即输出一次。它能保证输出长度为 Top1 的 KEY，但是 Top2、Top3 等 KEY 是无法保证可以扫描出来的。一般的处理方法是多扫描几次，或者是消灭了 Top1 的 KEY 之后再扫描确认还有没有次大的 KEY。","批量执行命令#批量执行命令":" $ cat cmds.txt set foo1 bar1 set foo2 bar2 set foo3 bar3 ...... $ cat cmds.txt | redis-cli OK OK OK ... # 或者 $ redis-cli \u003c cmds.txt OK OK OK ... ","模拟从库#模拟从库":"如果你想观察主从服务器之间都同步了那些数据，可以使用 redis-cli 模拟从库。\n$ ./redis-cli --host 192.168.x.x --port 6379 --slave SYNC with master, discarding 51778306 bytes of bulk transfer... SYNC done. Logging commands from master. ... 从库连上主库的第一件事是全量同步，所以看到上面的指令卡顿这很正常，待首次全量同步完成后，就会输出增量的 aof 日志。","监控服务器状态#监控服务器状态":"可以使用 --stat 参数来实时监控服务器的状态，间隔 1s 实时输出一次。\n$ redis-cli --stat ------- data ------ --------------------- load -------------------- - child - keys mem clients blocked requests connections 2 6.66M 100 0 11591628 (+0) 335 2 6.66M 100 0 11653169 (+61541) 335 2 6.66M 100 0 11706550 (+53381) 335 2 6.54M 100 0 11758831 (+52281) 335 2 6.66M 100 0 11803132 (+44301) 335 2 6.66M 100 0 11854183 (+51051) 335 可以使用 -i 参数调整输出间隔。","直接模式#直接模式":"一般使用 redis-cli 都会进入交互模式，然后一问一答来读写服务器，这是交互模式。还有一种直接模式，通过将命令参数直接 传递给 redis-cli 来执行指令并获取输出结果。\n$ redis-cli incrby foo 5 (integer) 5 # 输出的内容较大，可以将输出重定向到外部文件 $ redis-cli info \u003e info.txt $ wc -l info.txt 120 info.txt # 如果想指向特定的服务器 # -n 2 表示使用第 2 个库，相当于 select 2 $ redis-cli -h localhost -p 6379 -n 2 ping PONG ","诊断服务器时延#诊断服务器时延":"平时诊断两台机器的时延一般是使用 Unix 的 ping 指令。Redis 也提供了时延诊断指令，不过它的原理不太一样，它是诊断当前机器和 Redis 服务器之间的指令(PING 指令)时延，它不仅仅是物理网络的时延，还和当前的 Redis 主线程是否忙碌有关。如果你发现 Unix 的 ping 指令时延很小，而 Redis 的时延很大，那说明 Redis 服务器在执行指令时有微弱卡顿。\n$ redis-cli --host 192.168.x.x --port 6379 --latency min: 0, max: 5, avg: 0.08 (305 samples) 时延单位是 ms。redis-cli 还能显示时延的分布情况，而且是图形化输出。\nredis-cli --latency-dist ","远程-rdb-备份#远程 rdb 备份":"执行下面的命令就可以将远程的 Redis 实例备份到本地机器，远程服务器会执行一次 bgsave 操作，然后将 rdb 文件传输到客户端。\n$ ./redis-cli --host 192.168.x.x --port 6379 --rdb ./user.rdb SYNC sent to master, writing 2501265095 bytes to './user.rdb' Transfer finished with success. ","采样服务器指令#采样服务器指令":"现在线上有一台 Redis 服务器的 OPS 太高，有很多业务模块都在使用这个 Redis，如何才能判断出来是哪个业务导致了 OPS 异常的高。这时可以对线上服务器的指令进行采样，观察采样的指令大致就可以分析出 OPS 占比高的业务点。这时就要使用 monitor 指令，它会将服务器瞬间执行的指令全部显示出来。不过使用的时候要注意即使使用 ctrl+c 中断，否则你的显示器会噼里啪啦太多的指令瞬间让你眼花缭乱。\n$ redis-cli --host 192.168.x.x --port 6379 monitor 1539853410.458483 [0 10.100.90.62:34365] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.459212 [0 10.100.90.61:56659] \"PFADD\" \"growth:dau:20181018\" \"2klxkimass8w\" 1539853410.462938 [0 10.100.90.62:20681] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.467231 [0 10.100.90.61:40277] \"PFADD\" \"growth:dau:20181018\" \"2kei0to86ps1\" 1539853410.470319 [0 10.100.90.62:34365] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.473927 [0 10.100.90.61:58128] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.475712 [0 10.100.90.61:40277] \"PFADD\" \"growth:dau:20181018\" \"2km8sqhlefpc\" 1539853410.477053 [0 10.100.90.62:61292] \"GET\" \"6yax3eb6etq8:{-7}\" ","重复执行指令#重复执行指令":"redis-cli 还支持重复执行指令多次，每条指令执行之间设置一个间隔时间，如此便可以观察某条指令的输出内容随时间变化。\n// 间隔 1s，执行 5 次，观察 qps 的变化 $ redis-cli -r 5 -i 1 info | grep ops instantaneous_ops_per_sec:43469 instantaneous_ops_per_sec:47460 instantaneous_ops_per_sec:47699 instantaneous_ops_per_sec:46434 instantaneous_ops_per_sec:47216 如果将次数设置为 -1 那就是重复无数次永远执行下去。如果不提供 -i 参数，那就没有间隔，连续重复执行。\n在交互模式下也可以重复执行指令，形式上比较怪异，在指令前面增加次数\n127.0.0.1:6379\u003e 5 ping PONG PONG PONG PONG PONG "},"title":"一些命令行技巧"}}