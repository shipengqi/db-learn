---
title: 分库分表
weight: 6
---

**分库分表，能不分就不分**。

## 为什么要分库分表

随着现在互联网应用越来越大，数据库会频繁的成为整个应用的性能瓶颈。我们经常使用的 MySQL 数据库，也就不断面临数据量太大、数据访问太频繁、数据读写速度太快等一系列的问题。而传统的这些调优方式，在真正面对海量数据冲击时，往往就会显得很无力。因此，现在互联网对于数据库的使用也越来越小心谨慎。例如添加Redis缓存、增加MQ进行流量削峰等。但是，数据库本身如果性能得不到提升，这就相当于是水桶理论中的最短板。

要提升数据库的性能，最直接的思路，当然是对数据库本身进行优化。例如对 MySQL 进行调优，优化 SQL 逻辑，优化索引结构，甚至像阿里等互联网大厂一样，直接优化 MySQL 的源码。但是这种思路在面对互联网环境时，会有很多非常明显的弊端。

- 数据量和业务量快速增长，会带来性能瓶颈、服务宕机等很多问题。
- 单点部署的数据库无法保证服务高可用。
- 单点部署的数据库无法进行水平扩展，难以应对业务爆发式的增长。
​
这些问题背后的核心还是数据。数据库不同于上层的一些服务，它所管理的数据甚至比服务本身更重要。即要保证数据能够持续稳定的写入，又不能因为服务故障造成数据丢失。现在互联网上的大型应用，动辄几千万上亿的数据量，就算做好数据的压缩，随随便便也可以超过任何服务器的存储能力。并且，服务器单点部署，也无法真正解决把鸡蛋放在一个篮子里的问题。将数据放在同一个服务器上，如果服务器出现崩溃，就很难保证数据的安全性。这些都不是光靠优化 MySQL 产品，优化服务器配置能够解决的问题。

## 分库分表的优势
​
那么自然就需要换另外一种思路了。我们可以像微服务架构一样，来维护数据库的服务。把数据库从单体服务升级到数据库集群，这样才能真正全方位解放数据库的性能瓶颈，并且能够通过水平扩展的方式，灵活提升数据库的存储能力。这也就是我们常说的分库分表。通过分库分表可以给数据库带来很大的好处：

- 提高系统性能：分库分表可以将大型数据库分成多个小型数据库，每个小型数据库只需要处理部分数据，因此可以提高数据库的并发处理能力和查询性能。
- 提高系统可用性：分库分表可以将数据复制到多个数据库中，以提高数据的可用性和可靠性。如果一个数据库崩溃了，其他数据库可以接管其工作，以保持系统的正常运行。
- 提高系统可扩展性：分库分表可以使系统更容易扩展。当数据量增加时，只需要增加更多的数据库和表，而不是替换整个数据库，因此系统的可扩展性更高。
- 提高系统灵活性：分库分表可以根据数据的使用情况，对不同的数据库和表进行不同的优化。例如，可以将经常使用的数据存储在性能更好的数据库中，或者将特定类型的数据存储在特定的表中，以提高系统的灵活性。
- 降低系统成本：分库分表可以使系统更加高效，因此可以降低系统的运营成本。此外，分库分表可以使用更便宜的硬件和存储设备，因为每个小型数据库和表需要的资源更少。

## 分库分表的挑战
​
可能你会说，分库分表嘛， 也不是很难。一个库存不下，那就把数据拆分到多个库。一张表数据太多了，就把同一张表的数据拆分到多张。至于怎么做，也不难啊。要操作多个数据库，那么建立多个 JDBC 连接就行了。要写到多张表，修改下 SQL 语句就行了。

​如果你也这么觉得，那么就大错特错了。分库分表也并不是字面意义上的将数据分到多个库或者多个表这么简单，他需要的是一系列的分布式解决方案。

​因为数据的特殊性，造成数据库服务其实是几乎没有试错的成本的。在微服务阶段，从单机架构升级到微服务架构是很灵活的，中间很多细节步骤都可以随时做调整。比如对于微服务的接口限流功能，你并不需要一上来就用 Sentinel 这样复杂的流控工具。一开始不考虑性能，自己进行限流是很容易的事情。然后你可以慢慢尝试用 Guava 等框架提供的一些简单的流控工具进行零散的接口限流。直到整个应用的负载真正上来了之后，流控的要求更高更复杂了，再开始引入 Sentinel，进行统一流控，这都没有问题。这种试错的过程其实是你能够真正用好一项技术的基础。

​但是对于数据库就不一样了。当应用中用来存储数据的数据库，从一个单机的数据库服务升级到多个数据库组成的集群服务时，需要考虑的，除了分布式的各种让人摸不着边际的复杂问题外，还要考虑到一个更重要的因素，数据。**数据的安全性甚至比数据库服务本身更重要**！因此，如果你在一开始做分库分表时的方案不太成熟，对数据的规划不是很合理，那么这些问题大概率会随着数据永远沉淀下去，成为日后对分库分表方案进行调整时最大的拦路虎。

​所以在决定进行分库分表之前，一定需要提前对于所需要面对的各种问题进行考量。如果你没有考虑清楚数据要如何存储、计算、使用，或者你对于分库分表的各种问题都还没有进行过思考，那么千万不要在真实项目中贸然的进行分库分表。


分库分表，也称为 Sharding。其实我觉得，Sharding应该比中文的分库分表更为贴切，他表示将数据拆分到不同的数据片中。由于数据往往是一个应用的基础，随着数据从单体服务拆分到多个数据分片，应用层面也需要面临很多新的问题。比如：

- **主键避重问题**

在分库分表环境中，由于表中数据同时存在不同数据库中，某个分区数据库生成的 ID 就无法保证全局唯一。因此**需要单独设计全局主键，以避免跨库主键重复问题**。

- **数据备份问题**

​随着数据库由单机变为集群，整体服务的稳定性也会随之降低。如何保证集群在各个服务不稳定的情况下，依然保持整体服务稳定就是数据库集群需要面对的重要问题。而对于数据库，还需要对数据安全性做更多的考量。

- **数据迁移问题**

​当数据库集群需要进行扩缩容时，集群中的数据也需要随着服务进行迁移。如何在不影响业务稳定性的情况下进行数据迁移也是数据库集群化后需要考虑的问题。

- **分布式事务问题**

​原本单机数据库有很好的事务机制能够帮我们保证数据一致性。但是**库分表后，由于数据分布在不同库甚至不同服务器，不可避免会带来分布式事务问题**。

- **SQL 路由问题**

​数据被拆分到多个分散的数据库服务当中，每个数据库服务只能保存一部分的数据。这时，在**执行 SQL 语句检索数据时，如何快速定位到目标数据所在的数据库服务，并将 SQL 语句转到对应的数据库服务中执行**，也是提升检索效率必须要考虑的问题。

- **跨节点查询，归并问题**

**​跨节点进行查询时，每个分散的数据库中只能查出一部分的数据，这时要对整体结果进行归并时，就会变得非常复杂**。比如常见的 `limit`、`order by` 等操作。

​在实际项目中，遇到的问题还会更多。从这里可以看出，Sharding 其实是一个很复杂的问题，往往很难通过项目定制的方式整体解决。因此，大部分情况下，都是通过第三方的服务来解决 Sharding 的问题。比如像 TiDB、ClickHouse、Hadoop 这一类的 NewSQL 产品，大部分情况下是将数据问题整体封装到一起，从而提供 Sharding 方案。但是这些产品毕竟太重了。更灵活的方式还是使用传统数据库，通过软件层面来解决多个数据库之间的数据问题。这也诞生了很多的产品，比如早前的 MyCat，还有后面我们要学习的ShardingSphere 等。

​另外，关于何时要开始考虑分库分表呢？当然是数据太大了，数据库服务器压力太大了就要进行分库分表。但是这其实是没有一个具体的标准的，需要根据项目情况进行灵活设计。业界目前唯一比较值得参考的详细标准，是阿里公开的开发手册中提到的，建议**预估三年内，单表数据超过 500W，或者单表数据大小超过 2G，就需要考虑分库分表**。


## ShardingSphere

​ShardingSphere 是一款起源于当当网内部的应用框架。2015年在当当网内部诞生，最初就叫 ShardingJDBC。2016 年的时候，由其中一个主要的开发人员张亮，带入到京东数科，组件团队继续开发。在国内历经了当当网、电信翼支付、京东数科等多家大型互联网企业的考验，在 2017 年开始开源。并逐渐由原本只关注于关系型数据库增强工具的 ShardingJDBC 升级成为一整套以数据分片为基础的数据生态圈，更名为 ShardingSphere。到 2020 年 4 月，已经成为了 Apache 软件基金会的顶级项目。发展至今，已经成为了业界分库分表最成熟的产品。

ShardingSphere 这个词可以分为两个部分，其中 **Sharding 就是我们之前介绍过的数据分片**。从官网介绍上就能看到，他的核心功能就是可以将任意数据库组合，转换成为一个分布式的数据库，提供整体的数据库集群服务。只是给自己的定位并不是 Database，而是Database plus。其实这个意思是他自己并不做数据存储，而是对其他数据库产品进行整合。整个 ShardingSphere 其实就是围绕数据分片这个核心功能发展起来的。后面的 **Sphere 是生态的意思**。这意味着 ShardingSphere 不是一个单独的框架或者产品，而是一个由多个框架以及产品构成的一个完整的技术生态。目前 ShardingSphere 中比较成型的产品主要**包含核心的 ShardingJDBC 以及 ShardingProxy 两个产品**，以及一个用于数据迁移的子项目 ElasticJob，另外还包含围绕云原生设计的一系列未太成型的产品。

​ShardingSphere 经过这么多年的发展，已经不仅仅只是用来做分库分表，而是形成了一个围绕分库分表核心的技术生态。他的核心功能已经包括了数据分片、分布式事务、读写分离、高可用、数据迁移、联邦查询、数据加密、影子库、DistSQL 庞大的技术体系。

### 客户端分库分表与服务端分库分表

ShardingSphere  最为核心的产品有两个：**一个是 ShardingJDBC，这是一个进行客户端分库分表的框架。另一个是 ShardingProxy，这是一个进行服务端分库分表的产品**。他们代表了两种不同的分库分表的实现思路。

#### ShardingJDBC 客户端分库分表

ShardingSphere-JDBC 定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。

- 适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC；
- 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, HikariCP 等；
- 支持任意实现 JDBC 规范的数据库，目前支持 MySQL，PostgreSQL，Oracle，SQLServer 以及任何可使用 JDBC 访问的数据库。

#### ShardingProxy 服务端分库分表

ShardingSphere-Proxy 定位为透明化的数据库代理端，通过实现数据库二进制协议，对异构语言提供支持。 目前提供 MySQL 和 PostgreSQL 协议，透明化数据库操作，对 DBA 更加友好。

- 向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用；
- 兼容 MariaDB 等基于 MySQL 协议的数据库，以及 openGauss 等基于 PostgreSQL 协议的数据库；
- 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端，如：MySQL Command Client, MySQL Workbench, Navicat 等。

![sharding-proxy](https://raw.gitcode.com/shipengqi/illustrations/files/main/db/sharding-proxy.png)

#### ShardingSphere 混合部署架构

ShardingJDBC 跟客户端在一起，使用更加灵活。而 ShardingProxy 是一个独立部署的服务，所以他的功能相对就比较固定。他们的整体区别如下：

|     | ShardingSphere-JDBC | ShardingSphere-Proxy |
| --- | ------------------- | -------------------- |
| 数据库 | 任意 | MySQL/PostgreSQL |
| 连接消耗数 | 高 | 低 |
| 异构语言 | 仅 Java | 任意 |
| 性能 | 损耗低 | 损耗略高 |
| 无中心化 | 是 | 否 |
| 静态入口 | 无 | 有 |

在产品图中，Governance Center 也是其中重要的部分。他的作用有点类似于微服务架构中的配置中心，可以使用第三方服务统一管理分库分表的配置信息，当前建议使用的第三方服务是 Zookeeper，同时也支持 Nacos，Etcd 等其他第三方产品。

​由于 ShardingJDBC 和 ShardingProxy 都支持通过 Governance Center，将配置信息交个第三方服务管理，因此，也就自然支持了通过 Governance Center 进行整合的混合部署架构。

![sharding-mixed](https://raw.gitcode.com/shipengqi/illustrations/files/main/db/sharding-mixed.png)

### ShardingSphere 的核心概念

#### 垂直分片与水平分片

这是设计分库分表方案时经常会提到的概念。**其中垂直分片表示按照业务的纬度，将不同的表拆分到不同的库当中**。这样可以减少每个数据库的数据量以及客户端的连接数，提高查询效率。而**水平分表表示按照数据的纬度，将原本存在同一张表中的数据，拆分到多张子表当中。每个子表只存储一份的数据**。这样可以将数据量分散到多张表当中，减少每一张表的数据量，提升查询效率。

![sharding-split](https://raw.gitcode.com/shipengqi/illustrations/files/main/db/sharding-split.png)

#### ShardingSphere 实现分库分表的核心概念

- 虚拟库：ShardingSphere 的核心就是提供一个具备分库分表功能的虚拟库，他是一个 `ShardingSphereDatasource` 实例。应用程序只需要像操作单数据源一样访问这个 `ShardingSphereDatasource` 即可。
- 真实库：实际保存数据的数据库。这些数据库都被包含在 `ShardingSphereDatasource` 实例当中，由 ShardingSphere 决定未来需要使用哪个真实库。
- 逻辑表：应用程序直接操作的逻辑表。
- 真实表：实际保存数据的表。这些真实表与逻辑表表名不需要一致，但是需要有相同的表结构，可以分布在不同的真实库中。应用可以维护一个逻辑表与真实表的对应关系，所有的真实表默认也会映射成为 ShardingSphere 的虚拟表。
- 分布式主键生成算法：**给逻辑表生成唯一主键**。由于逻辑表的数据是分布在多个真实表当中的，所有，单表的索引就无法保证逻辑表的 ID 唯一性。ShardingSphere 集成了几种常见的基于单机生成的分布式主键生成器。比如 SNOWFLAKE，COSID_SNOWFLAKE 雪花算法可以生成单调递增的 long 类型的数字主键，还有 UUID，NANOID 可以生成字符串类型的主键。当然，ShardingSphere 也支持应用自行扩展主键生成算法。比如基于 Redis，Zookeeper 等第三方服务，自行生成主键。
- 分片策略：表示逻辑表要如何分配到真实库和真实表当中，分为分库策略和分表策略两个部分。分片策略由分片键和分片算法组成。分片键是进行数据水平拆分的关键字段。如果没有分片键，ShardingSphere 将只能进行全路由，SQL 执行的性能会非常差。分片算法则表示根据分片键如何寻找对应的真实库和真实表。简单的分片策略可以使用 Groovy 表达式直接配置，当然，ShardingSphere 也支持自行扩展更为复杂的分片算法。


### ShardingJDBC 客户端分库分表机制

### ShardingProxy 服务端分库分表机制

​ShardingSphere-Proxy，早前版本就叫做 ShardingProxy。定位为一个透明化的数据库代理，目前提供 MySQL 和 PostgreSQL 协议，透明化数据库操作。简单理解就是，他会部署成一个 MySQL 或者 PostgreSQL 的数据库服务，**应用程序只需要像操作单个数据库一样去访问 ShardingSphere-proxy，由 ShardingProxy 去完成分库分表功能**。

## 如何规划分库分表

除了访问MySQL的并发问题，还要解决海量数据的问题，很多的时候，我们会使⽤分布式的存储集群，因为 MySQL 本质上是⼀个单机数据库，所以很多场景下，其并不适合存储 TB 级别以上的数据。

但是绝⼤部分电商企业的在线交易类业务，⽐如订单、⽀付相关的系统，还是⽆法离开 MySQL 的。原因是只有 **MySOL 之类的关系型数据库，才能提供⾦融级的事务保证**。

选择分厍或是分表的⽬的是解决两个问题。

第⼀，是为了解决因数据量太⼤⽽导致查询慢的问题。这⾥所说的“查询”，其实主要是事务中的查询和更新操
作，因为只读的查询可以通过缓存和主从分离来解决。分表主要⽤于解决因数据量⼤⽽导致的查询慢的问题。

第⼆，是为了应对⾼并发的问题。如果⼀个数据库实例撑不住，就把并发请求分散到多个实例中，所以分库可⽤
于解决⾼并发的问题。

简单地说，如果**数据量太⼤，就分表；如果并发请求量⾼，就分库**。


### 商城订单服务的实现

在设计系统，假设预估订单的数量每个⽉订单 2000W，⼀年的订单数可达 2.4 亿。⽽每条订单的⼤⼩⼤致为 1KB，按照我们在MySQL 中学习到的知识，为了让 B+ 树的⾼度控制在⼀定范围，保证查询的性能，每个表中的数据不宜超过 2000W。在这种情况下，为了存下 2.4 亿的订单，似乎应该将订单表分为 16（12 往上取最近的 2 的幂）张表。

{{< callout type="info" >}}
**之所选择表的数量为 2 的幂，是因为这样在扩容表的数量时，迁移的数量是比较少的**。例如，原来有 16 张表，现在需要扩容为 32 张表，只需要迁移 16 张表中的数据即可。比如 ID 为 1 的订单，在原来的 2 张表中，应该在第 `1 % 2 = 1` 张表中。在扩容为 4 张表后，ID 为 1 的订单，应该在第 `1 % 4 = 1` 张表中。ID 为 3 的订单在原来的 2 张表中，应该在第 `3 % 2 = 1` 张表中。在扩容为 4 张表后，ID 为 3 的订单，应该在第 `3 % 4 = 3` 张表中。ID 为 3 的订单从 1 号表迁移到了 3 号表，但是 ID 为 1 的订单不需要迁移。
{{< /callout >}}

但是这样设计，有个问题，我们只考虑了订单表，没有考虑订单详情表。假设预估⼀张订单下的商品平均为 10 个，那既是⼀年的订单详情数可以达到 24 亿，同样以每表 2000W 记录计算，应该订单详情表为 128（120 往上取最近的 2 的幂）张，⽽订单表和订单详情表虽然记录数上是⼀对⼀的关系，但是表之间还是⼀对⼀，也就是说订单表也要为 128 张。经过再三分析，最终将订单表和订单详情表的张数定为 32 张。

这会导致订单详情表单表的数据量达到 8000W，为何要这么设计呢？因为还要做冷热分离，老的数据要迁移到其他地方去。

#### 选择分片键

如何选择⼀个合适的列作为分表的依据，，该列⼀般称为**分⽚键**（Sharding Key）。选择合适的分⽚键和分⽚算法⾮常重要，因为其将直接影响分库分表的效果。

选择分⽚键有⼀个最重要的参考因素是我们的业务是如何访问数据的？

⽐如我们把订单 ID 作为分⽚键来诉分订单表。那么拆分之后,如果按照订单 ID 来查询订单,就需要先根据订单 ID 和分⽚算法,计算所要查的这个订单具体在哪个分⽚上，也就是哪个库的哪张表中，然后再去那个分⽚执⾏查询操作即可。

但是当⽤户打开“我的订单”这个⻚⾯的时候，它的查询条件是⽤户 ID，由于这⾥没有订单 ID，因此我们⽆法知道所要查询的订单具体在哪个分⽚上，也就没法查了。

那么如果是把⽤户 ID 作为分⽚键呢？答案是也会⾯临同样的问题，使⽤订单 ID 作为查询条件时⽆法定位到具体的分⽚上。

这个问题的解决办法是，**在⽣成订单 ID 的时候，把⽤户 ID 的后⼏位作为订单 ID 的⼀部分**。这样按订单 ID 查询的时候，就可以根据订单 ID 中的⽤户 ID 找到分⽚。所以在我们的系统中订单 ID **从唯⼀ ID 服务获取 ID 后，还会将⽤户 ID 的后两位拼接，形成最终的订单 ID**。**分片的时候按照后两位来分片，这样既查询某个用户的订单就可以知道在哪个分片中**。这种方法叫做**基因法**。

然⽽，系统对订单的查询万式，肯定不只是按订单 ID 或按⽤户 ID 查询两种⽅式。⽐如如果有商家希望查询⾃家家店的订单，有与订单相关的各种报表。对订单做了分库分表，就没法解决了。这个问题⼜该怎么解决呢？

⼀般的做法是，**把订单⾥数据同步到其他存储系统中，然后在其他存储系统⾥解决该问题**。⽐如可以再构建⼀个以店铺 ID 作为分⽚键的只读订单库，专供商家使⽤。或者数据同步到 Hadoop 分布式⽂件系统（HDFS）中，然后通过⼀些⼤数据技术⽣成与订单相关的报表。

在分⽚算法上，我们知道常⽤的有按范围，⽐如时间范围分⽚，哈希分⽚，查表法分⽚。我们这⾥直接使⽤哈希分⽚，对表的个数 32 直接取模。

⼀旦做了分库分表，就会极⼤地限制数据库的查询能⼒，原本很简单的查询，分库分表之后，可能就没法实现了。**分库分表⼀定是在数据量和并发请求量⼤到所有招数都⽆效的情况下，我们才会采⽤的最后⼀招**。

#### 具体实现

如何在代码中实现读写分离和分库分表呢？⼀般来说有三种⽅法。

1. 纯⼿⼯⽅式：修改应⽤程序的 DAO 层代码，定义多个数据源，在代码中需要访问数据库的每个地⽅指定每个数据库请求的数据源。
2. 组件⽅式：使⽤像 Sharding-JDBC 这些组件集成在应⽤程序内，⽤于代理应⽤程序的所有数据库请求，并**把请求⾃动路由到对应的数据库实例上**。
3. 代理⽅式:在应⽤程序和数据库实例之间部署⼀组数据库代理实例,⽐如 Atlas 或 Sharding-Proxy。对于应⽤程序来说,数据库代理把⾃⼰伪装成⼀个单节点的 MySQL 实例,应⽤程序的所有数据库请求都将发送给代理，代理分离请求，然后将分离后的请求转发给对应的数据库实例。

**⼀般推荐第⼆种，使⽤组件的⽅式**。采⽤这种⽅式，代码侵⼊⾮常少，同时还能兼顾性能和稳定性。如果应⽤程序是⼀个逻辑⾮常简单的微服务,简单到只有⼏个 SQL,或者应⽤程序使⽤的编程语⾔没有合适的读写分离组件,那么也可以考虑通过纯⼿⼯的⽅式。

**不推荐使⽤代理⽅式**，原因是代理⽅式**加⻓了系统运⾏时数据库请求的调⽤链路，会浩成⼀定的性能损失，⽽且代理服务本身也可能会出现故障和性能瓶颈等问题**。代理⽅式有⼀个好处，**对应⽤程序完全透明**。

### 历史数据归档

订单数据会随着时间⼀直累积的数据，前⾯我们说过预估订单的数量每个⽉订单 2000W，⼀年的订单数可达 2.4 亿，三年可达 7.2 亿。

数据量越⼤，数据库就会越慢。


#### 存档历史订单数据

订单数据⼀般保存在 MySQL 的订单表⾥，说到拆分 MySQL 的表，前⾯我们不是已经将到了“分库分表”吗？**其实分库分表很多的时候并不是⾸选的⽅案，应该先考虑归档历史数据**。

以京东为例，在“我的订单”中查询时，分为了近三个⽉订单、今年内订单、2021 年订单、2020 年订单等等，这就是典型的将订单数据归档处理。

所谓归档，也是⼀种拆分数据的策略。简单地说，就是把⼤量的历史订单移到另外⼀张历史订单表或数据存储中。为什这么做呢？订单数据有个特点：具备时间属性的，并且随着系统的运⾏，数据累计增⻓越来越多。但其实订单数据在使⽤上有个特点，最近的数据使⽤最频繁，超过⼀定时间的数据很少使⽤，这被称之为热尾效应。

因为新数据只占数据息量中很少的⼀部分，所以把新⽼数据分开之后，新数据的数据量就少很多，查询速度也会因此快很多。虽然与之前的总量相⽐，⽼数据没有减少太多，但是因为⽼数据很少会被访问到，所以即使慢⼀点⼉也不会有太⼤的问题，⽽且还可以使⽤其他的存储系统提升查询速度。

这样拆分数据的另外⼀个好处是，拆分订单时，系统需要改动的代码⾮常少。对订单表的⼤部分操作都是在订单完成之前执⾏的，这些业务逻辑都是完全不⽤修改的。即使是像退货退款这类订单完成之后的操作，也是有时限的，这些业务逻辑也不需要修改,还是按照之前那样操作订单即可。

基本上只有查询统计类的功能会查到历史订单，这些都需要稍微做些调整。按照查询条件中的时间范围，选择去订单表还是历史订单中查询就可以了。很多⼤型互联⽹电商在逐步发展壮⼤的过程中，⻓达数年的时间采⽤的都是这种订单拆分的⽅案。

#### 商城历史订单服务的实现

既然是历史订单的归档，归档到哪⾥去呢？可以归档到另外 MySQL 数据库，也可以归档到另外的存储系统，这个看⾃⼰的业务需求即可，比如归档到 MongoDB 数据库。

对于数据的迁移归档，我们总是在 MySQL 中保留 3 个⽉的订单数据，超过三个⽉的数据则迁出。前⾯我们说过，预估每⽉订单 2000W，⼀张订单下的商品平均为 10 个，如果只保留 3 个⽉的数据，则订单详情数为 6 亿，分布到 32 个表中，每个表容纳的记录数刚好在 2000W 左右，这也是为什么前⾯的分库分表将订单表设定为 32 个的原因。

实现两个 Service，OperateDbService 负责读取 MySQL 的订单数据和删除已迁出的订单，OperateMgDbService 负责将订单数据批量插⼊ MongoDB，MigrateCentreService 负责进⾏调度服务。

##### 分布式事务？

迁移的过程，我们是逐表批次删除，对于每张订单表，先从 MySQL 从获得指定批量的数据，写⼊ MongoDB，再从 MySQL 中删除已写⼊ MongoDB 的部分，这⾥存在着⼀个多源的数据操作，为了保证数据的⼀致性，看起来似乎需要分布式事务。但是其实这⾥并不需要分布式事务，解决的关键在于写⼊订单数据到 MongoDB 时，我们要记住**同时写⼊当前迁⼊数据的最⼤订单 ID，让这两个操作执⾏在同⼀个事务之中**。

这样，在 **MySQL 执⾏数据迁移时，总是去 MongoDB 中获得上次处理的最⼤ OrderId，作为本次迁移的查询起始 ID**。

当然数据写⼊ MongoDB 后，还要记得删除 MySQL 中对应的数据。

尽量不要影响线上的业务。迁移如此⼤量的数据，或多或少都会影响数据库的性能，因此应该尽量选择在闲时迁移⽽且每次数据库操作的记录数不宜太多。按照⼀般的经验，对 MySQL 的操作的记录条数每次控制在 10000 ⼀下是⽐较合适，在我们的系统中缺省是2000 条。更重要的是，迁移之前⼀定要做好备份，这样的话，即使不⼩⼼误操作了，也能⽤备份来恢复。

##### 如何批量删除⼤量数据

如何从订单表中删除已经迁⾛的历史订单数据？

虽然我们是按时间迁出订单表中的数据，但是删除最好还是按 ID 来删除，并且同样要控制住每次删除的记录条数，太⼤的数量容易遇到错误。

这样每次删除的时候，由于条件变成了主键⽐较，⽽在 MySQL 的 InnoDB 存储引擎中，表数据结构就是按照主键组织的⼀棵 B+ 树，同时 B+ 树本身就是有序的，因此优化后不仅查找变得⾮常快,⽽且也不需要再进⾏额外的排序操作了。

并且删除以后，要间隔一段时间，再继续删除下⼀批数据，给 MySQL 一点时间去整理数据。因为**删除后肯定会牵涉到⼤量的 B+ 树⻚⾯分裂和合并**，这个时候 MySQL 的本身的负载就不⼩了，停顿⼀⼩会，可以让MySQL的负载更加均衡。

### 分布式 ID 服务

日常开发中，需要对系统中的各种数据使用 ID 唯一表示，比如用户 ID 对应且仅对应一个人，商品 ID 对应且仅对应一件商品，订单 ID 对应且仅对应一个订单。

一般情况下，会使用数据库的自增主键作为数据 ID，但是在大数量的情况下，我们往往会引入分布式、分库分表等手段来应对，很明显对数据分库分表后我们依然需要有一个唯一 ID 来标识一条数据或消息，数据库的自增 ID 已经无法满足需求。此时一个能够生成全局唯一 ID 的系统是非常必要的。那业务系统对 ID 号的要求有哪些呢？

- **全局唯一性**：不能出现重复的 ID 号，既然是唯一标识，这是最基本的要求。
- **趋势递增、单调递增**：保证下一个 ID 一定大于上一个 ID。
- **信息安全**：如果 ID 是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定 URL 即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要 **ID 无规则、不规则**。

#### 常见方法

##### UUID

UUID (Universally Unique Identifier) 的标准型式包含 32 个 16 进制数字，以连字号分为五段，形式为 `8-4-4-4-12` 的 36 个字符，示例：`550e8400-e29b-41d4-a716-446655440000`。


优点：

性能非常高：本地生成，没有网络消耗。
缺点：

- 不易于存储：UUID 太长，16 字节 128 位，通常以 36 长度的字符串表示，很多场景不适用。
- 信息不安全：基于 MAC 地址生成 UUID 的算法可能会造成 MAC 地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。
- UUID 的无序性可能会引起数据位置频繁变动，严重影响性能。

##### 雪花算法

是一种以划分命名空间（UUID 也算，由于比较常见，所以单独分析）来生成 ID 的一种算法，Snowflake 是 Twitter 开源的分布式 ID 生成算法。Snowflake 把 64-bit 分别划分成多段，分开来标示机器、时间等，比如在 snowflake 中的 64-bit 分别表示：

- 第 0 位： 符号位（标识正负），始终为 0，没有用，不用管。
- 第 `1~41` 位 ：一共 41 位，用来表示时间戳，单位是毫秒，可以支撑 `2^41` 毫秒（约 69 年）
- 第 `42~52` 位 ：一共 10 位，一般来说，前 5 位表示机房 ID，后 5 位表示机器 ID（实际项目中可以根据实际情况调整）这样就可以区分不同集群/机房的节点，这样就可以表示 32 个 IDC，每个 IDC 下可以有 32 台机器。
- 第 `53~64` 位 ：一共 12 位，用来表示序列号。序列号为自增值，**代表单台机器每毫秒能够产生的最大 ID 数** (`2^12 = 4096`),也就是说**单台机器每毫秒最多可以生成 4096 个 唯一 ID**。

优点：
​
- 毫秒数在高位，自增序列在低位，整个 ID 都是趋势递增的。
- 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成 ID 的性能也是非常高的。**可以根据自身业务特性分配 bit 位**，非常灵活。

缺点：
​
- 强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。
​
当然，在我们自己的项目如果不想自行实现唯一性 ID，还可以利用外部中间件，比如 Mongdb objectID，它也可以算作是和 snowflake 类似方法，通过 “时间+机器码+pid+inc” 共 12 个字节，通过 `4+3+2+3` 的方式最终标识成一个 24 长度的十六进制字符。

##### 数据库生成

**MySQL**：

```sql
CREATE TABLE `sequence_id` (
 `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
 `stub` char(10) NOT NULL DEFAULT '',
 PRIMARY KEY (`id`),
 UNIQUE KEY `stub` (`stub`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```
stub 字段无意义，只是为了占位，便于我们插入或者修改数据。并且，给 stub 字段创建了唯一索引，保证其唯一性。

通过 replace into 来插入数据：

```sql
BEGIN;
REPLACE INTO sequence_id (stub) VALUES ('stub');
SELECT LAST_INSERT_ID();
COMMIT;
```

插入数据这里，我们没有使用 insert into 而是使用 replace into 来插入数据。replace 是 insert 的增强版，replace into 首先尝试插入数据到表中：

1. 如果发现表中已经有此行数据（根据主键或者唯一索引判断）则先删除此行数据，然后插入新的数据。 
2. 否则，直接插入新数据。

优点：

​非常简单，利用现有数据库系统的功能实现，成本小，有 DBA 专业维护。ID号单调自增，存储消耗空间小。

缺点：

​支持的并发量不大、存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）、ID 没有具体业务含义、安全问题（比如根据订单 ID 的递增规律就能推算出每天的订单量，商业机密啊！）、每次获取 ID 都要访问一次数据库（增加了对数据库的压力，获取速度也慢）。

**Redis**：

通过 Redis 的 incr 命令即可实现对 id 原子顺序递增，例如：

```bash
127.0.0.1:6379> incr sequence_id_biz_type
(integer) 2
```

为了提高可用性和并发，我们可以使用 Redis Cluster。除了高可用和并发之外，我们知道 Redis 基于内存，我们需要持久化数据，避免重启机器或者机器故障后数据丢失。很明显，Redis 方案性能很好并且生成的 ID 是有序递增的。

不过，我们也知道，即使 Redis 开启了持久化，不管是快照（snapshotting，RDB）、只追加文件（append-only file, AOF）还是 RDB 和 AOF 的混合持久化依然存在着丢失数据的可能，那就意味着产生的 ID 存在着重复的概率。

#### 分布式 ID 微服务

基于美团的 Leaf 实现了自己的分布式 ID 微服务。

##### Leaf-segment 数据库方案

Leaf-segment 方案，是在哪数据库的方案上，做了改变：

- 原 MySQL 方案每次获取 ID 都得读写一次数据库，造成数据库压力大。改为批量获取，每次获取一个 segment(step 决定大小) 号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。
- 各个业务不同的发号需求用 `biz_tag` 字段来区分，每个 `biz_tag` 的 ID 获取相互隔离，互不影响。

表设计如下：

- `biz_tag`，`varchar(128)`
- `max_id`，`bigint`
- `step`，`int`

- `biz_tag` 用来区分业务
- `max_id` 表示该 `biz_tag` 目前所被分配的 ID 号段的最大值。
- `step` 表示每次分配的号段长度。

原来获取 ID 每次都需要写数据库，现在只需要把 step 设置得足够大，比如 1000。那么只有当 1000 个号被消耗完了之后才会去重新读写一次数据库。读写数据库的频率从 1 减小到了 `1/step`。

例如现在有 3 台机器，每台机器各取 1000 个，很明显在第一台 Leaf 机器上是 `1~1000` 的号段，当这个号段用完时，会去加载另一个长度为  `step=1000` 的号段，假设另外两台号段都没有更新，这个时候第一台机器新加载的号段就应该是 `3001~4000`。同时数据库对应的 `biz_tag` 这条数据的 `max_id` 会从 3000 被更新成 4000，更新号段的 SQL 语句如下：

```sql
Begin
UPDATE table SET max_id=max_id+step WHERE biz_tag=xxx
SELECT tag, max_id, step FROM table WHERE biz_tag=xxx
Commit
```

优点：

- Leaf 服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景。
- ID 号码是趋势递增的 8byte 的 64 位数字，满足上述数据库存储的主键要求。
- 容灾性高：Leaf 服务内部有号段缓存，即使 DB 宕机，短时间内 Leaf 仍能正常对外提供服务。可以自定义 max_id 的大小，非常方便业务从原有的 ID 方式上迁移过来。

缺点：

​- ID 号码不够随机，能够泄露发号数量的信息，不太安全。
​- TP999 波动，当号段使用完之后还是会在获取新号段时在更新数据库的 I/O 依然会存在着等待，下订单的速度可能在这个时候会出现偶尔的尖刺。
​- DB 宕机会造成整个系统不可用。

##### 双 buffer 优化

对于第二个缺点，Leaf-segment 做了一些优化，简单的说就是：

应用申请 ID 的时候，在本地准备两个 buffer 来保存。例如先申请 1000 个 ID，存储在第一个 buffer，当第一个 buffer 中的 ID 消耗了 90%，就要重新申请 ID 了，新申请的 1000 个 ID 存储在第二个 buffer。第二个 buffer 的 ID 申请的期间，第一个 buffer 中还有 10% 的 ID 可以使用，第一个 buffer 中的 ID 完全消耗完了，第二个 buffer 的 ID 也可用了。

对于第三个缺点，**其实可以通过增加更多的 buffer 来解决，用链表将 buffer 连接。这样，就算 DB 宕机了，只要应用内存中的 ID 够用到 DB 重启就不会有问题**。


##### Leaf-snowflake 数据库方案

eaf-segment 方案可以生成趋势递增的 ID，能够泄露发号数量的信息，不太安全。因此不太适合订单 ID 的生成场景。

Leaf-snowflake 方案完全沿用 snowflake 方案的 bit 位设计，即是 “1+41+10+12” 的方式组装 ID 号。

##### 解决时钟问题

因为这种方案依赖时间，如果机器的时钟发生了回拨，那么就会有可能生成重复的 ID 号，需要解决时钟回退的问题。

通过一个第三方的 zookeeper 来协调时间戳，Leaf 节点的时间戳上报到 zookeeper，Leaf 节点需要比较 ZooKeeper 上本节点曾经的记录时间来保证整体的时间戳是递增的。