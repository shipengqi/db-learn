---
title: Buffer Pool
---


不管是用于存储用户数据的索引（包括聚簇索引和二级索引），还是各种系统数据，最终都是存储在磁盘上的。磁盘的读取速度是非常慢的。

所以 InnoDB 存储引擎在处理客户端的请求时，当需要**访问某个页的数据时，就会把完整的页的数据全部加载到内存中**，也就是说即使只需要访问一个页的一条记录，那也需要先把整个页的数据加载到内存中。将整个页加载到内存中后就可以进行读写访问了，在进行完读写访问之后并不着急把该页对应的内存空间释放掉，而是将其缓存起来，这样将来有请求再次访问该页面时，就可以省去磁盘 IO 的开销了。

## Buffer Pool 是什么

MySQL 服务器启动的时候就向操作系统申请了一片连续的内存，叫做 `Buffer Pool`。默认情况下 Buffer Pool 只有 `128M` 大小。服务器配置 `innodb_buffer_pool_size` 这个参数可以设置 Buffer Pool 的值：

```cnf
[server]
innodb_buffer_pool_size = 268435456
```

`268435456` 的单位是字节，也就是 `256M`。Buffer Pool 最小值为 5M(当小于该值时会自动设置成 5M)。

## Buffer Pool 组成

Buffer Pool 中默认的缓存页大小和在磁盘上默认的页大小是一样的，都是 `16KB`。为了更好的管理这些在 Buffer Pool 中的缓存页，InnoDB 为每一个缓存页都创建了一些**控制信息**，这些控制信息包括该页所属的表空间编号、页号、缓存页在Buffer Pool中的地址、链表节点信息、一些锁信息以及 LSN 信息

每个页对应的控制信息占用的一块内存称为一个**控制块**。

**控制块和缓存页是一一对应的，它们都被存放到 Buffer Pool 中，其中控制块被存放到 Buffer Pool 的前边，缓存页被存放到 Buffer Pool 后边**。

![buffer-pool-control-info](../../../images/buffer-pool-control-info.jpg)

每一个控制块都对应一个缓存页，那在分配足够多的控制块和缓存页后，可能剩余的那点儿空间不够一对控制块和缓存页的大小，自然就用不到喽，这个用不到的那点儿内存空间就被称为**碎片**了。

## free 链表的管理

怎么区分 Buffer Pool 中哪些缓存页是空闲的，哪些已经被使用了呢？

这个时候缓存页对应的控制块就派上大用场了，我们可以把所有**空闲的缓存页对应的控制块作为一个节点放到一个链表中**，这个链表也可以被称作**free 链表**，也叫空闲链表。

![buffer-pool-control-info](../../../images/buffer-pool-free-link.jpg)

## 缓存页哈希

当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。如何知道该页在不在Buffer Pool中呢？

可以用**表空间号 + 页号**作为 `key`，**缓存页**作为 `value` 创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。

## flush 链表

如果我们修改了Buffer Pool中某个缓存页的数据，那它就和磁盘上的页不一致了，这样的缓存页也被称为**脏页**。

最简单的做法就是每发生一次修改就立即同步到磁盘上对应的页上，但是频繁的往磁盘中写数据会严重的影响程序的性能（毕竟磁盘慢的像乌龟一样）。所以每次修改缓存页后，我们并不着急立即把修改同步到磁盘上，而是在未来的某个时间点进行同步

但是如果不立即同步到磁盘的话，那之后再同步的时候我们怎么知道Buffer Pool中哪些页是**脏页**？

创建一个存储脏页的链表，凡是修改过的缓存页对应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的，所以也叫**flush 链表**。链表的构造和free链表差不多

## LRU 链表

Buffer Pool对应的内存大小毕竟是有限的，如果需要缓存的页占用的内存大小超过了Buffer Pool大小，free链表中已经没有多余的空闲缓存页怎么办？

把某些旧的缓存页从Buffer Pool中移除，然后再把新的页放进来，那么移除哪些缓存页？

留下频繁使用的缓存页

### 简单的LRU

创建一个链表，由于这个链表是为了按照**最近最少使用的原则去淘汰缓存页**的，所以这个链表可以被称为**LRU链表**

访问某个页时，可以这样处理LRU链表：

如果该页不在Buffer Pool中，在把该页从磁盘加载到Buffer Pool中的缓存页时，就把该缓存页对应的控制块作为节点塞到链表的头部。

如果该页已经缓存在Buffer Pool中，则直接把该页对应的控制块移动到LRU链表的头部。

**只要使用到某个缓存页，就把该缓存页调整到LRU链表的头部，这样LRU链表尾部就是最近最少使用的缓存页了**

### 划分区域的 LRU 链表

简单的 LRU 会出现的问题：

情况一：InnoDB 提供了**预读**（英文名：read ahead）。所谓预读，就是InnoDB认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到Buffer Pool中。

- 线性预读

InnoDB 提供了一个系统变量 `innodb_read_ahead_threshold`，如果顺序访问了某个区（extent）的页面超过这个系统变量的值，就会触发一次异步读取下一个区中全部的页面到Buffer Pool的请求，注意异步读取意味着从磁盘中加载这些被预读的页面并不会影响到当前工作线程的正常执行。这个 `innodb_read_ahead_threshold` 系统变量的值默认是 `56`

- 随机预读

如果Buffer Pool中已经缓存了某个区的13个连续的页面，不论这些页面是不是顺序读取的，都会触发一次异步读取本区中所有其的页面到Buffer Pool的请求

InnoDB 提供了 `innodb_random_read_ahead` 系统变量，它的默认值为 `OFF`，也就意味着 InnoDB 并不会默认开启随机预读的功能

预读本来是个好事儿，如果预读到Buffer Pool中的页成功的被使用到，那就可以极大的提高语句执行的效率。如果用不到呢？这些预读的页都会放到LRU链表的头部，如果此时Buffer Pool的容量不太大而且很多预读的页面都没有用到的话，这就会导致处在LRU链表尾部的一些缓存页会很快的被淘汰掉，也就是所谓的劣币驱逐良币，会大大降低**缓存命中率**。

情况二：有的小伙伴可能会写一些需要扫描全表的查询语句

意味着将访问到该表所在的所有页！假设那该表会占用特别多的页，当需要访问这些页时，会把它们统统都加载到Buffer Pool中，这也就意味着，Buffer Pool中的所有页都被换了一次血，而这种全表扫描的语句执行的频率也不高，每次执行都要把Buffer Pool中的缓存页换一次血，这严重的影响到其他查询对 Buffer Pool的使用，从而大大降低了**缓存命中率**。

所以 InnoDB 把这个 LRU 链表按照一定比例分成两截，分别是：

- 一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做**热数据**，或者称 **young 区域**。
- 另一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做**冷数据**，或者称 **old 区域**。

![](../../../images/buffer-pool-lru-link.jpg)

**当磁盘上的某个页面在初次加载到Buffer Pool中的某个缓存页时，该缓存页对应的控制块会被放到old区域的头部**。

全表扫描有一个特点，那就是它的执行频率非常低。

**在对某个处在old区域的缓存页进行第一次访问时就在它对应的控制块中记录下来这个访问时间，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从old区域移动到young区域的头部，否则将它移动到young区域的头部**。

### 优化 LRU 链表

我们每次访问一个缓存页就要把它移动到LRU链表的头部，开销太大。young区域的缓存页都是热点数据，也就是可能被经常访问的，这样频繁的对LRU链表进行节点移动操作是不是不太好？

一些优化策略，比如只有被访问的缓存页位于young区域的1/4的后边，才会被移动到LRU链表头部，这样就可以降低调整LRU链表的频率

## 刷新脏页到磁盘

后台有专门的线程每隔一段时间负责把脏页刷新到磁盘。

主要有两种刷新路径：

- 从LRU链表的冷数据中刷新一部分页面到磁盘。

后台线程会定时从LRU链表尾部开始扫描一些页面，扫描的页面数量可以通过系统变量innodb_lru_scan_depth来指定，如果从里边儿发现脏页，会把它们刷新到磁盘。这种刷新页面的方式被称之为BUF_FLUSH_LRU。

- 从flush链表中刷新一部分页面到磁盘。

后台线程也会定时从flush链表中刷新一部分页面到磁盘，刷新的速率取决于当时系统是不是很繁忙。这种刷新页面的方式被称之为BUF_FLUSH_LIST。

有时候后台线程刷新脏页的进度比较慢，导致用户线程在准备加载一个磁盘页到Buffer Pool时没有可用的缓存页，这时就会尝试看看LRU链表尾部有没有可以直接释放掉的未修改页面，如果没有的话会不得不将LRU链表尾部的一个脏页同步刷新到磁盘（和磁盘交互是很慢的，这会降低处理用户请求的速度）。这种刷新单个页面到磁盘中的刷新方式被称之为BUF_FLUSH_SINGLE_PAGE。

有时候系统特别繁忙时，也可能出现用户线程批量的从flush链表中刷新脏页的情况，很显然在处理用户请求过程中去刷新脏页是一种严重降低处理速度的行为，这属于一种迫不得已的情况。

## 多个Buffer Pool实例

uffer Pool本质是InnoDB向操作系统申请的一块连续的内存空间，在多线程环境下，访问Buffer Pool中的各种链表都需要加锁处理啥的，在Buffer Pool特别大而且多线程并发访问特别高的情况下，单一的Buffer Pool可能会影响请求的处理速度。

可以把它们拆分成若干个小的Buffer Pool，每个Buffer Pool都称为一个**实例**。独立的去申请内存空间，独立的管理各种链表等。

`innodb_buffer_pool_instances` 的值来修改Buffer Pool实例的个数

**当 `innodb_buffer_pool_size` 的值小于1G的时候设置多个实例是无效的，InnoDB会默认把 `innodb_buffer_pool_instances` 的值修改为1**。

可以在服务器运行过程中调整Buffer Pool大小。每个Buffer Pool实例由若干个chunk组成，每个chunk的大小可以在服务器启动时通过启动参数调整。

可以用下边的命令查看Buffer Pool的状态信息：

SHOW ENGINE INNODB STATUS\G
